# RESEARCH PAPERS COMPENDIUM: LAYER2

Generated: Sat 19 Jul 18:02:13 NZST 2025
Purpose: Research foundation for layer2 domain
Source: ./layer2/

---

# TABLE OF CONTENTS

Chapter 1: PAPERS IN LAYER2

---

# CHAPTER 1: PAPERS IN LAYER2

Directory: ./layer2/
Papers: 12 research papers

## Section 1.1: Surface reconstruction from unorganized points

Source File: Surface reconstruction from unorganized points.txt

### Paper Content:

```
# Surface reconstruction from unorganized points.pdf
# Converted: 2025-07-18 12:58:53
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Surface reconstruction from unorganized points.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Surface reconstruction from unorganized points.txt


--- Page 1 ---

z
Computer Graphics, 26, 2,July 1992
Surface Reconstruction from Unorganized Points
Hugues Hoppe* Tony DeRose* Tom Duchampt
John McDonald$ Werner Stuetzle~
University of Washington
Seattle, WA98195
Abstract
including those assimple asacoffee cup with ahandle (asur-
face ofgenus 1),ortheobject depictedirt Figure 1a(a surface
We describe and demonstrate an algorithm that takes as input an of genus 3), cannot be accomplished by either of these meth-
unorganized set of points {xl, ....x.} c IR3 on or near an un- ods. To adequately scan these objects, multiple view points
known manifold M, and produces as output a simplicial surface must be used. Merging the data generated from multiple view
that approximates M. Neither the topology, thepresence ofbound- points to reconstruct apolyhedral surface representation is a
aries, nor the geometry of M are assumed tobe known in advance non-trivial task [11].
— all are inferred automatically from the data. This problem natu-
rally arises inavariety ofpractical situations such asrange scanning
an object from multiple view points, recovery of biological shapes Surfaces from corriours: In many medicai studies it is com-
from two-dimensional slices, and interactive surface sketching. mon to slice biological specimens into thin layers with ami-
crotome. The outlines of the structures of interest are then
digitized to create a stack of contours. The problem is to
CR Categories and Subject Descriptors: 1.3.5 [Computer reconsmct the three-dimensional smtctures tlom the stacks
Graphics]: Computational Geometry and Object Modeling. of two-dimensional contours. Although this problem has re-
Additional Keywords: Geomernc Modeling, Surface Fitting, ceived agood deal ofattention, there remain severe limitations
Three-Dimensional Shape Recovery, Range Data Analysis. with current methods. Perhaps foremost among these is the
difficulty of automatically dealing with branching structures
1 Introduction [3, 12].
Broadly speaking, the class of problems we are interested in can Itieractive surface sketching: A number of researchers, in-
be stated as follows: Given partial information of an unknown sur- cluding Schneider [21] and‘Eisenman [6], have investigated
face, construct, to the extent possible, acompact representation of the creation of curves in IFt2by tracing the path of a stylus
the surface. Reconstruction problems of this sort occur in diverse or mouse as the user sketches the desired shape. Sachs et al.
scientific and engineering application domains, including: [19] describe a system, called 3-Draw, that permits the cre-
ation of free-form curves in IR3 by recording the motion of a
s Surfaces from range data: The data produced by laser range
stylus fitted with aPolhemus sensor. This can be extended to
scanning systems is typically a rectangular grid of distances
thedesign of free-form surfaces byignoring theorder inwhich
from the sensor to the object being scanned. If the sensor and
positions arerecorded, allowing the user tomove thestylus ar-
object are fixed, only objects that are ‘“pointviewable” can be
bitrarily back and forth over the surface. The problem is then
fuily digitized. More sophisticated systems, such asthose pro-
to construct asurface representation faithful to the unordered
duced byCyberware Laboratory, Inc., arecapable ofdigitizing
collection ofpoints.
cylindrical objec~ by rotating either the sensor or the object.
However, the scanning of topologically more complex objects,
Reconstruction algorithms addressing these problems have typi-
lDepatient ofComputerScience andEngineering,FR-35
tDepartmentofMathematics,GN-50 cally been crafted on acase bycase basis toexpioit partiaf smtcture
$Depan.mentofStatistics,GN-22 in the data. For instance, algorithms solving the surface from con-
ThisworkwassupportedinpartbyBeflcore,theXeroxCorporation, tours problem make heavy useofthefact thatdata areorganize-dinto
IBM, Hewlett-Packard, the Digital Equipment Corporation, the Depart- contours (i.e., closed polygons), and that the contours lie in paral-
mentofEnergyundergrant DE-FG06-85-ER25006,theNationalLlbraty lelplanes. Similarly, specialized algorithms toreconstruct surfaces
ofMedicineundergrantNIHL&4-04174,andtheNationalScienceFounda- from multiple view point range data might exploit the adjacency
tionundergrantsCCR-8957323andDMS-9103002. relationship of the data points within each view.
In contras~ our approach is to pose a unifying general problem
that does notassume any structure onthedata points. This approach
PermissiontocopywithoutfeeallorpartO(thismaterial isgrmrted has both theoretical and practical merit. On the theoretical side,
providedthatthecopiesarenolmate urdistributedfordirect
abstracting toageneral probiem often sheds iight onthetruiy criticai
commercial advantage,(heACM copyrightnoticeandthetitleofthe
aspects oftheproblem. On thepractical side, asingle algorithm that
pufrlicatmnandItsdateappear.andrmticeMgiventhatcopyingisby
solves thegeneral problem canbeused tosolve anyspecific problem
fxrmlssion oftheAswcta[ion forComputing Machinery. Tocopy
otherwise.or10republish.requiresafeeand/orspeciticpermission. instance.
[[~Iwz AcM-1)-x9791-479- l/92/M)7/(x)71 $01.50 71

--- Page 2 ---

1.1 Terminology fn contrast to implicit reconstruction techniques, parametic re-
construction techniques represent the reconstructed surface as a
By a surface we mean a “compac~ connected, orientable two- topological embedding ~(A) of a2dimensional parameter domain
dimensional manifold, possibly with boundary, embedded in lR3° Ainto IR3. Previous workhas concentrated on domain spaces with
(cf. O’Neilf [17]). A surface without boundary will be called a simple topology, i.e. the plane and the sphere. Hastie and Stuet-
closed surface. ff we want toemphasize that asurface possesses a zle [9] and Vemun [26, 27] dwcuss reconsmtction of surfaces by a
non-emptyboundary,wewillcallitaborderedsurface. Apiecewise
topological embedding f(A) of aplanar region Ainto IR3. Schudy
liiear surface with triangular faces will be referred to as asimpli-
and Ballard [2Z 23] md Brinkley [4] consider the reconstruction
ciai surfme. We use IIxIItodenote the Euclidean length ofavector
of surfaces that are slightfy deformed spheres, and thus choose A
x, and we use d(X, Y) to denote the Hausdorff distance between to be a sphere. Sclaroff and Pentland [24] describe a hybrid im-
the sets of points X and Y (the Hausdorff distance is simply the plicit/parametric method for fitting a deformed sphere to a set of
distance between the two closest points of X and Y). points using deformations of asuperqttadric.
Letx= {x,,..., x.} be sampled data points on or near an Compared to the techniques mentioned above, our method has
unknown surface M (see Figure lb). To capture the error in most several advantages
sampling processes, we assume that each of thepoints x, E X isof
the form x: = yt + ei, where yi c M is apoint on the unknown l Itrequires only anunorganized collection ofpoints onor near
surface and ei c lR3 is an error vector. Wecall such asample X the surface. No additional information isneeded (such asnor-
6-noisy if [IelIIs 6for all i. Avalue for 6can be estimated in most maf information used by Muraki’s method).
applications (e.g., the accuracy ofthelaser scanner). Features of M l Unlike theparametric methods mentioned abov~ itcan recon-
that are smalf compared to 6will obviously not berecoverable. struct surfaces of arbhry topology.
It is also impossible to recover features of M in regions where
l Unlike previously suggested implicit methods, it deals with
insufficient sampling hasoccurred. Inparticular, ifM isabordered boundaries in anatural way, and itdoes not generate spurious
surface, such as a sphere with a disc removed, it is impossible to
surface components not supported by the data.
distinguish holes in thesample from holes inthe surface. Tocapture
the intuitive notion of sampling density we need to make another
2.2 Surface Reconstruction vs Function Recon-
definition: Let Y = {yl ,. ... Y~} c M bea(noiseless) sampleof
asurface M. The sample Y issaid tobe p-dense ifany sphere with struction
radius pandcenter inM contains atleast one sample point in Y. A
Terms like “surface fitting” appear in reference to two distinct
6-noisy sample {xl,... ,x.} C IR3of asurface M issaid tobep-
classes of problems: surface reconstruction and function recon-
dense ifthere exists anoiseless pdense sample{ yl, ..., Y*} c M
struction. The goal ofsurface reconstmction was stated earlier. The
such thatx, =y, +e,, lle,ll~6, i=l, ....n.
goal of function reconstruction may be stated as foflows: Given a
surface h4, a set {x, E M}, and a set {y, E El}, determine a
1.2 Problem Statement function ~ : M ~ Et, such that f(x, ) x w.
The goalofsurface reconstruction istodetermine asurface M’ (see The domain surface M is most commonly aplane embedded in
Figure 2f3that approximates an unknown surface M (Figure 1a), IR3, in which case theproblem is astandard one considered in ap-
using asample X (Ftgure lb) and information about the sampling proximation theo~. The case where M is a sphere has also been
process, forexample, bounds on thenoise magnitude 6andthesant- extensively treated (cf. [7]). Some recent work under the titfe sur-
pling density p. faces on surjaces addresses the case when M is a general curved
surface such as the skin of an airplane [16].
We are cturently working to develop conditions on the original
surface M and the sample X that are sufficient to allow M to be Function reconstruction methods can be used for surface recon-
reliably reconstructed. As that work is still preliminary, we are un- struction in simple, speciaf cases, where the surface to be recon-
able togive guarantees for the afgorithm presented here. However, structed is, roughly speakiig, the graph of afunction over aknown
the algorithm has worked welI in practiu where the results can be surface M. Itis important torecognize just how limited these spe-
compared to the original surface (see Section 4). ciaf cases are— forexample, not evety surface homomorphic toa
sphere isthegraph of afunction over the sphere. The point wewant
to make isthat function recons~ction must not bemisconstrued to
2 Related Work
solve the general surface reconstruction problem.
2.1 Surface Reconstruction 3 A Description of the Algorithm
Surface reconstruction methods can be classified according to the
way inwhich they represent the reconstmcted surface. 3.1 Overview
Impficit reconstruction methods attempt to find a smooth func-
Our surface reconstruction algorithm consists of two stages. In the
tion f : IR3 ~ lR such that {xl ,.. .,xn}is close totfte zero set
first stage we define a function ~ : D ~ IR, where D c IR3 is
Z(t). Theydiffer withrespect totheformof~ andthemeasureof
aregion near the data, such that ~ estimates the signed geometric
closeness, Ratt[18] and Taubin [25] minimize the sum of squared
distance to the unknown surface M. The zero set Z(f) isour esti-
Hausdorff distances from the data points to the zero set of apoly-
mate for M. In the second stage we use acontouring afgorithm to
nomial inthree variables. Muraki [15] takes ~ tobe alinear combi-
approximate Z(j) by asimpliciaf surface.
nation of three-dimensional Gaussian kemefs with different means
and spreads. His goodness-of-fit function measures how close the Although the unsigned distance function [~1would be easier to
values of j atthedata poin~ are tozero, and how well the unit nor- estimate, zero is not a regular value of Ifl. Zero is, however, a
mals tothe zero setof j match the normals estimated from the data. regular value of j, andtheimplicit function theorem thus guarantees
Moore and Warren [13] fit apiecewise polynomial recursively and that our approximation Z(f) is amanifold.
then enforce continuity using atechnique they callfiee form blend- The key ingredient to defining the signed distance function is to
ing. associate anoriented plane with each ofthe data points. These tan-
72

--- Page 3 ---

Computer Graphics, 26,2, July 1992
genl planes serve aslocal linear approximations tothe surface. Al- the surface is assumed toconsist of asingle connected componen~
though the construction of the tangent planes is relatively simple, the graph should be connected. A simple connected graph for aset
the selection of their orientations so as to define a globally consis- ofpoints that tends toconnect neighbors isthe Euclidean Minimum
tent orientation for the surface isone of the major obstacles facing Spanning Tree (EMST). However, the EMST over the tangent plane
the algorithm. As indicated in Figure 2b, the tangent planes do not centers {01, ....on} (Figure 1c)isnotsufficiently dense inedges
directly define the surface, since their union may have a compli- to serve our purposes. We therefore enrich it by adding a number
cated non-manifold smscture. Rather, we use the tangent planes to of edges to it. Specifically, we add the edge (i, j) if either o, is
define thesigned distance function tothesurface. Anexample ofthe in the k-neighborhood of OJ, oro, is inthe k-neighborhood ofo,
simplicial surface obtained by contouring the zero set ofthe signed (where k-neighborhood is defined over {01,..., on }as it was for
distance function is shown in Figure 2e. The next several sections X). The resulting graph (Figure Id), calledtheRiemmrnianGraph,
develop inmore detail the successive steps ofthe algorithm. isthusconstructed tobe aconnected graph that encodes geometric
proximity of the tangent plane centers.
3.2 l%rtgent Plane Estimation A relatively simple-minded algorithm toorient theplanes would
be to arbitrarily choose an orientation for some plane, then “’prop-
The first step toward defining asigned distance function is tocom-
agate” the orientation to neighboring planes in tie Riemannian
pute an oriented tangent plane for each data point. The tangent
Graph. In practice, we found that the order in which the orienta-
plane Tp(x, ) associated with the data point xi is represented as
tion is propagated is important. Figure 3b shows what may result
apoint o,, called the center, together with aunit normal vector n,.
when propagating orientation solely on thebasis of geometric prox-
The signed distance of an arbitrary point p E IR3 to Tp(x, ) is
imity; acorrect reconsmtction is shown in Figure 3C. Intuitively,
defined to be dish(p) = (p – o,) .n,. The center and normal
we would like tochoose anorder ofpropagation that favors propa-
for Tp(x, )are determined by gathering together the k points of X
gation horn T’p(x, ) to Tp(x, ) if the unoriented planes are nearly
nearest to x,; this set is denoted by Nbhd(x, ) and is called the k-
parallel. This can be accornp~shed by assigning to each edge (i, j)
neighborhood of x,. (We currently assume k tobe auser-specified
in the Riemannian Graph thecost 1– @ ftj 1.In addition mbeing
parameter, although inSection 5wepropose amethod fordetermin-
non-negative, this assignment has theproperty that acost issmall if
ing k automatically.) The center and unit normal are computed so
the unoriented tangent planes are nearly parallel. Afavorable prop-
that the plane {dis~ (p) = O}is the least squares best fitting plane
agation order can therefore be achieved by traversing the minimal
to Nbhd(x, ). That is, the center o, is taken to be the centzoid of
spanning lree (MST) of the resulting graph. This order is advarrta-
Nbhd(x, ), and the normal n, isdetermined using principal compo-
geous because ittends to propagate orientation stfongdirections of
nent analysis. To compute n,, the covariance matrix of Nbhd(xi )
low curvature inthedata, thereby largely avoiding ambiguous situa-
isformed. This isthe symmetric 3x 3positive semidefinite matrix
tions encountered when trying topropagate orientation across sharp
edges (asatthe tipofthecat’s ears inFigure 3b). IntheMST shown
Cv= ~ (y-ot)C3(Y-sh) in Figure 2a, the edges are colored according to their cos~ with the
y~Nbhd(X, ) brightly colored edges corresponding to regions of high variation
(where n, n] is somewhat less than 1).
where @denotes the outer product vector operatorl. If ~! ~ ~~ ~ To assign orientation to an initial plane, the unit normal of the
J: denote the eigenvaiues of CV associated with unit eigenvectors plane whose center has the hugest z coordinate is forced to point
V:, V?,t:, respectively, we choose f~,tobeeither t: or –O:. The toward the +Z axis. Then, rooting the tree at this initial node, we
selection determines theorientation ofthe tangent plane, anditmust traverse the tree in depth-first order, assigning each plane an or-
bedone sothat nearby planes are “consistently oriented”, ientation that is consistent with that of its parent. That is, ifduring
traversal, thecurrent plane 7’p(x, )hasbeen assigned theorientation
3.3 Consistent Tangent Plane Orientation fit and TP(x7 ) is the next plane to be visited, then n, isreplaced
with —n, ifn, .tij < 0.
Suppose two data points x,, XJ E X are geometrically close. Ide-
This orientation algorithm haabeen used inall our examples and
ally, when the data is dense and the surface is smooth, the corre-
has produced correct orientations in all the cases we have run. The
sponding tangent planes Tp(x, )= (o,, n,) and Tp(xJ) = (o,, fij)
resulting oriented tangent planes are represented ass haded rectan-
arenearly parallel, i.e. n, tij ~ +1. Ifthe planes are consistently
gles in Figure 2b.
oriented, then n, n] x +1; otherwise, either n, or n] should
be flipped. The difficulty in finding aconsistent global orientation
3.4 Signed Distance Function
is that this condition should hold between all pairs of “sufficiently
close” data points.
The signed distance f(p) from an arbitmy point p E IR3 to a
Wecan model theproblem asgraph optimization. The graph con- known surface M is the distance between p and the closest point
tains one node N, per tangent plane Tp(x, ), with an edge (i, j) z E M, multiplied by *1, depending on which side of the surface
between N, and Nj ifthe tangent plane centers o, and o, are suffi- p lies. In reali~ AI is not known, but we can mimic this proce-
ciently close (we will be more precise about what wemean by suffi- dure using theoriented tangent planes as follows. First we findthe
ciently close shortly). The cost onedge (i, j)encodes tie degree to tangent plane Tp(x, )whose center o, isclosest to p.This tangent
which N, and NJ areconsistently oriented andistaken toben,. f~j. plane is a local linear approximation to M, so we take the signed
The problem isthen toselect orientations forthetangent planes soas distance f(p) to M to be the signed distance between p and its
tomaximize the total cost of the graph. Unfortunately, this problem projection z onto Tp(x, ); that is,
can be shown tobe NP-complete via areduction to MAXCXJT [8].
Toefficiently solve tie orientation problem we must therefore resort ~(p) = dis~(p) = (p–o,). n,
to an approximation algorithm.
Before describing the approximation algorithm we use, we must If M is known not to have boundaries, this simple rule works
decide when apair ofnodes are tobe connected inthe graph. Since well. However, therule must beextended toaccommodate surfaces
that might have boundaries. Recall that the set X = {XI, . ,x. )
1Ifa andb havecomponentsa, andb] respectively,thenthematrix is assumed to be ap-dense, 6-noisy sample of M. If there was no
a ISIb hasa,bj asitstj-th entry. noise, we could deduce that a point z with d(z, X ) > p cannot
73

--- Page 4 ---

SIGGRAPH ’92Chicago, July26-31, 1992
4 Results
be a point of M since that would violate X being p-dense. Intu-
itively, the sample points do not leave holes ofradius larger than p.
ff the sample is $-noisy, the radius of the holes may increase, but We have experimented with the reconstruction method on data sets
by no more than 6. We therefore conclude that apoint z cannot be obtained from several different sources. In all cases, any s~cture
apoint of M if d(z, X) > p + 6. If the projection z of p onto (including ordering) that might have been present in the point sets
theclosest tangent plane haad(z, X) > p+6, we take f(p) tobe was discarded.
undefined. Undefined vahtes are used by the contouring afgorithm
Meshes : Points wererandomly sampled from anumber ofexist-
of Section 3.5 toidentify boundaries.
ing simplicial surfaces3. For instance, the mesh of Figure 3a
Stated procedurally, our signed distance function isdefined as: was randomly sampled toyield 1000 unorgsmized points, and
these in turn were used toreconstruct the surface in Figure 3c.
i - index of tangent plane whose center isclosest top This particular case illustrates the behavior of the method on a
bordered surface (the cat has no base and is thus homomorp-
{Compute zas theprojection ofponto Tp(xi) } hic to a disc). The reconstructed knot (original mesh from
ZtCSi-((p -O, )fli)ti,
Rob Scharein) of Figure 3d is an example of a surface with
simple topology yet complex geometrical embeddmg.
if d(z, X) < p+ 6 then
Ray ‘lkaced Points : To simulate laser range imaging from mul-
eke /(P) - (P – Oi) .fli {= *lip - 211}
tiple view points, CSG models were ray traced from multiple
eye points. The ray tracer recorded the point of first intersec-
f(P) + undefined
tion along each ray. Eight eye points (the vertices of a large
endff
cube centered atthe object) were used to generate thepoint set
The simple approach outlkted above creates azero set Z(f) that of Figure lb from the CSG object shown in Figure la. This
ispiecewise linear but contains discontinuities. The discontinuities is the point set used in Section 3 to illus~ate the steps of the
result from the implicit partitioning of space into regions within algorithm (Figures 1a-2f).
which a single tangent plane is used to define the signed distance Range Images :The bust of Speck (Figure 3e) was reconstructed
function. (’These regions are in fact the Voronoi regions associ- from pointa taken from anactual cylindrical rangeimage (gen-
ated with thecenters o,.) Fortunately, thediscontinuities do not ad- erated by Cyberware Laboratory, Inc.). Only 25~0of theorig-
versely affect our algorithm. The contouring algorithm discussed in inal points were used.
the next section will discretely sample the function f over aportion
Contours :Pointsfrom39planar (horizontal) slices ofthe~ scan
of a3dimensionaf grid near the data and reconstruct acontinuous
of a femur were combined together to obtain the surface of
piecewise linear approximation to Z(f).
Figure 3f.
The algorithm’s parameters are shown in the next table for each
3.5 Contour ‘h-acing of the examples. The execution times were obtained on a20MIPS
workstation. The parameter p + iiand the marching cube cell size
Contour tracing, the extraction of anisosurface from ascalar func- are both expressed as afraction of the object’s size. The parameter
tion, is a well-studied problem [1, 5, 28]. We chose to implement p+ 6isset toinfinity for those surfaces that areknown tobeclosed.
avariation of the marching cubes algorithm (cf. [28]) that samples . 0
Object n k p+6 cefl size
the function atthe vertices of acubical lattice and finds the contour
(seCo:&;
intersections within tetrahedral decompositions ofthe cubical cells.
cat 1000 15 .06 1/30 19
To accurately estimate boundaries, thecube size should be set so
knot 1000O 20 co 1/50 137
that edges are of length less than p + 6. In practice we have often
mechpart 4102 12 1/40 54
found it convenient to set the cube size somewhat larger than this
speck 21760 8 .; 1/80 514
value, simply to increase the speed of execution and toreduce the
femur 18224 40 .06 1/50 2135
number of biangular facets generated.
The algorithm only visits cubes that intersect thezero setbypush- 5 Discussion
ingonto aqueueonly theappropriate neighboring cubes (Figure 2c).
Inthisway, thesigned distance function f isevaluated only atpoints
5.1 Tangent Plane Approximation
close to the data. Figure 2d ilhts~ates the signed distance function
by showing line segments between the query points p (at the cube The neighborhood /Vbhd(X, ) of adata point x, is defined to con-
vertices) and their associated projected points z. As suggested in
sist of its k nearest neighbors, where k iacurrently assumul to be
Section 3.4, no intersection is reported within acube if the signed
an input parameter. In the case where the data contains little or no
distance function is undefined at any vertex of the cube, thereby
noise, kisnot acritical parameter since the output hasbeen empiri-
giving rise toboundaries inthe simplicial surface.
cally observed tobe stable over awide range ofsetrings. However,
The resulting simplicial surface can contain triangles with arbi- it would be best if k could be selected automatically. Furthermore,
trarily poor aspect ratio (Figure 2e). We alleviate this problem us- allowing k to adapt locally would make less stringent the require-
ing apost-processing procedure that collapses edges in the surface ment that the data be uniformly distributed over the surface. To
using an aspect ratio criterion.2 The final result is shown in Fig- select and adapt k, the algorithm could incrementally gather points
ure 2f. Alternatively, other contouring methods exist that cartguar- while monitoring thechanging eigenvafues ofthecovariance matrix
antee bounds on the triangle aspect ratio [14]. (see Section 3.2). For small values of k, data noise tends to domi-
nate, the eigenvahses are similar, and theeigenvectora donotreveal
thesurface’s true tangent plane. Atthe other extreme, askbecomes
2~e ~ge~ am keptinapriorityqueue;thecriteriontominimizeisthe
productoftheedgelengrhtimestheminimuminscribedradiusofitatwoad- af)is~re~inversetmnsfom sampling[1O,page469]ontri~gle am was
jacentfaces.TestaarealsoperformedLOensurethatedgecolfapsespreserve usedtoselectfaceindicesfromthemesh,anduniformsamplingwasused
thetopologicaltypeofshesurface. withinrftefaces.
74

--- Page 5 ---

Computer Graphics, 26, 2,July 1992
large, the k-neighborhoods become less localized and the surface the sample and theoriginal surface. To further improve the geomet-
curvature tends toincrease the“thickness” ,4: ofthe neighborhood. ric accuracy of the fi~ and to reduce the space required to store the
Another possible criterion istocompare A: tosome local or global reconstruction, weenvision using theoutput of our algorithm asthe
estimate of data noise. Although we have done some initial-exper- starting point for asubsequent spline surface fitting procedure. We
imentation in this direction, we have not yet fully examined these arecurrently investigating such amethod basedon anonlinear least
options. squares approach using triangular B4zier surfaces.
If the data is obtained born range images, there exists some
knowledge of surface orientation at each data point. Indeed, each References
datapoint isknown tobevisible from aparticular viewing direction,
sotha~ unless the surface incident angle islarge, thepoint’s tangent [1] 3.L.Allgower and P.H. Schmidt. An algorithm forpiecewise
plane orientation can be inferred from that viewing direction. Our inear approximation of animplicitly defined manifold. SIAM
method could exploit this additional information inthetangent plane ~ournalofNumerical Analysis, 22:322–346, April 1985.
orientation step (Section 3.3) by augmenting theRiemannisn Graph [2] ‘.L. Bentley. Multidimensional divide and conquer. Comm.
with anadditional pseudo-node and n additional edges. !CM, 23(4):214-229, 1980.
[3] KBreseler, J. A. Fessler, and A. Macovski. A Bayesiarr ap-
5.2 Algorithm Complexity xoach toreconstruction from incomplete projections ofamul-
iple object 3D domain. IEEETrans. Pat. Anal. Mach. Intell.,
A spatial partitioning Abstract Data Type greatly improves perfor-
~1(8):840-858, August 1989.
mance of many of the subproblems discussed previously. The crit-
[4] 1lames F. Brinkley. Knowledge-driven ultnsonic three-
ical subproblems are (with their standard time complexity):
iimensional organ modeling. IEEE Traiu. Pat. And. Mach.
l EMST graph (O(n2 )) hlell., 7(4)431+41, July 1985.
l k-nearest neighbors to agiven point (O(rs + klog n.)) [5] David P.Dobkin, Silvio V.F.hwy, William P.Thurston, and
Mlan R. Wilks. Contour tracing by piecewise linear approxi-
c nearest tangent plane origin to agiven point (O(n))
mations. ACIU TOG, 9(4):389423, October 1990.
Hierarchical spatial partitioning schemes such as octrees [20] [6] John A. Eisenman. Graphical editing of composite bezier
and k-D trees [2] can be used to solve these problems more effi- curves. Master’s thesis, Department of Elecaical Engineer-
ciently. However, the uniform sampling density assumed in our ingand Computer Science, M.I.T., 1988.
data allows simple spatial cubic partitioning towork efficiently. The [7] T.A. Foley. Interpolation to scattered data on a spherical do-
axis-aligned bounding box of the points is partitioned by a cubi- main. In M. Cox and J. Mason, editors, AlgorMrnsfor Ap-
cal grid. Points are entered into sers comesponding to the cube to woxinurtion II, pages 303–3 10.Chapman and Hafl, London,
which they belong, and these sets are accessed through ahash table 1990.
indexed by the cube indices. It is difficult to analyze the resulting [8] Michael R. Garey and David S.Johnson. Computers and In-
improvements analytically, but, empirically, the time complexity of fracfabifity. W. H. Freeman and Company, 1979.
the above problems is effectively reduced by a factor of n, except
[9] T. Hastie and W. Stuetzle. Principaf curves. JASA, 84:502-
for the k-nearest neighbors problem which becomes O(k).
516, 1989.
As aresult of the spatial partitioning, the Riemannian Graph can [10] Averill M. Law and W. David Kelton. .$itnulatwn Modeling
beconstructed in O(nk) time, Because the Riemannian Graph has and Analysis. McGraw-Hill, Inc., second edition, 1991.
O(n) edges (at most n + nk), the MST computation used in find- [11] Marshal L. Merriam. Experience with the cyberware 3D dig-
ing the best path on which to propagate orientation requires only itizer. InNCGA Proceedings, pages 125-133, March 1992.
O(n log n) time. Traversal of the MST isof course O(n).
[12] David Meyers, Shelly Skinner, and Kenneth Sloan. Surfaces
The time complexity of the contouring algorithm depends only from contours: The correspondence and branching problems.
on the number of cubes visited, since the evaluation of the signed In Proceedings of Graphics Irsterfwe ’91, pages 246-254,
distance function ~ at apoint p can be done in constant time (the June 1991.
closest tartgent plane origin o, top and the closest data point Xj to [13]I Doug Moore and Joe Warren. Approximation of dense scat-
theprojected point z can both befound inconstant time with spatial tered data using algebraic surfaces. TR 90-135, Rice Univer-
partitioning). sity, October 1990.
[14:I Doug Moore and Joe Warren. Adaptive mesh generation ii:
6 Conclusions and Future Work Packing solids. TR 90-139, Rice University, March 1991.
[15] Shigeru Muraki. Volumetric shape description of range data
We have developed an algorithm to reconstruct a surface in three- using “blobby model”. Computer Graphics (SIGGRAPH ’91
dimensional space with or without boundary from aset of unorga- Proceedings), 25(4):227-235, July 1991.
nized points scattered on or near the surface. The algorithm, based [16] Gregory M. Nielson, Thomas A. Foley, Bemd Hsrnann, and
on the idea of determining the zero set of an estimated signed dis- David Lane. Visualizing and modeling scattered multivariate
tance function, was demonstrated on data gathered horn a variety data. IEEE CGKd, 11(3):47-55, May 1991.
of sources. It is capable of automatically infernng the topological [17] Barrett O’Neill. Elementary DljierentialGeornetry. Academic
type of the surface, including the presence of boundary curves. Press, Orlando, Florida, 1966.
The algorithm can, inprinciple, beextended toreconstruct man- [18] Vaughan Pratt. Direct least-squares fitting of algebraic sur-
ifolds of co-dimension one in spaces of srbitr~ dimension; that faces. Computer Graphics (SIGGRAPH ’87 Proceedings),
is, to reconswuct d – 1 dimensional manifolds in d dimensional 21(4):145-152, July 1987.
space. Thus, essentially the same algorithm can be used to recon- [19] Emanuel Sachs, Andrew Roberts, and David Stoops. 3-Draw:
struct curves in the plane orvolumes in four-dimensional space. Atool fordesigning 3Dshapes. IEEECo~uferGraphics and
The output of our reconstruction method produced the correct Applications, 11(6):18-26, November 1991.
topology in all theexamples, Weare myingtodevelop formal guar- [20] Hanan Samet. Applica!ionr of Spalial Data Structures.
antees on thecorrectness of thereconstruction, given constraints on Addison-Wesley, 1990.
75

--- Page 6 ---

SIGGRAPH ‘92 Chicago, July 26-31, 1992
[21] Philip J. Schneider. Phoenix: An interactive curve de- [25] G. Taubin. Estimation of planar curves, surfaces and nonpla-
sign syste.m based on the automatic fitting of hand-sketched nar space curves defined by implicit equations, with applica-
curves. Master’s thesis, Department of Computer Science, U. tions to edge and range image segmentation. Technical Report
of Washington, 1988. LEMS-66, Division of Engineering, Brown University, 1990.
[22] R. B. Schudy and D. H. Ballard. Model detection of cardiac [26] B. C. Vemuri. Representation and Recognition of Objects
chambers in ultrasound images. Technical Report 12, Com- From Dense Range Maps. PhD thesis, Department of Electri-
puter Science Departmet& University of Rochester, 1978. cal and Computer Engineering, University of Texas at Austin,
[23] R. B. Schudy and D. H. Ballard. Towards an anatomical model 1987.
of heart motion as seen in 4-d cardiac ultrasound data. In Pro- [27] B. C. Vemuri, A. Mitiche, and J. K. Aggarwal. Curvature-
ceedings of the 6th Conference on Computer Applications in based representation of objects from range data. Image and
Radiology and Computer-Aided Analysis of Radiological Im- Vision Computing, 4(2):107-114, 1986.
ages, 1979. 1281 G. Wyvill, C. McPheeters, and B. Wyvill. Data structures
[24] Stan Sclaroff and Alex Pentland. Generalized implicit func- for soft objects. The Visual Computer. 2(4):227-234, August
tions for computer graphics. Computer Graphics (SIGGRAPH 1986.
‘91 Proceedings), 25(4):247-250, July 1991.
(b)sunplUlpO~U(Xi)(~~ = 4102)
(c) EMST of tangetuplane centers 0, (d) Riemannian Graph over 0;
Figure 1: Reconstruction of ray-traced CSG object (simulated multi-view range data).
76

--- Page 7 ---

Computer Graphics, 26, 2, July 1992
ing cut
Figure 2: Reconstruction of ray-traced CSG object (continued).

--- Page 8 ---

SIGGRAPH ‘92 Chlcago, July 26-31, 1992
omplex
Figure 3: Reconstruction examples.```

---

## Section 1.2: Nesti-Net_Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks

Source File: Nesti-Net_Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks.txt

### Paper Content:

```
# Nesti-Net_Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks.pdf
# Converted: 2025-07-18 12:58:55
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Nesti-Net_Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Nesti-Net_Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks.txt


--- Page 1 ---

2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds
using Convolutional Neural Networks
Yizhak Ben-Shabat Michael Lindenbaum Anath Fischer
Mechanical Engineering Computer Science Mechanical Engineering
Techion IIT Techion IIT Techion IIT
Haifa, Israel Haifa, Israel Haifa, Israel
sitzikbs@gmail.com mic@cs.technion.ac.il meranath@technion.ac.il
Abstract tion [21] and surface reconstruction [11].
Estimating the normals from a raw, point-only,
In this paper, we propose a normal estimation cloud, is a challenging task due to difficulties asso-
methodforunstructured3Dpointclouds. Thismethod, ciated with sampling density, noise, outliers, and de-
called Nesti-Net, builds on a new local point cloud rep- tail level. The common approach is to specify a local
resentation which consists of multi-scale point statis- neighborhood around a point and to fit a local basic
tics(MuPS),estimatedonalocalcoarseGaussiangrid. geometric surface (e.g. a plane) to the points in this
This representation is a suitable input to a CNN archi- neighborhood. Then the normal is estimated from the
tecture. The normals are estimated using a mixture- fitted geometric entity. The chosen size (or scale) of
of-experts (MoE) architecture, which relies on a data- the neighborhood introduces an unavoidable tradeoff
driven approach for selecting the optimal scale around between robustness to noise and accuracy of fine de-
each point and encourages sub-network specialization. tails. A large-scale neighborhood over-smoothes sharp
Interesting insights into the network’s resource dis- corners and small details but is otherwise robust to
tribution are provided. The scale prediction signif- noise. A small neighborhood, on the other hand, may
icantly improves robustness to different noise levels, reproduce the normals more accurately around small
point density variations and different levels of detail. details but is more sensitive to noise. Thus it seems
Weachievestate-of-the-artresultsonabenchmarksyn- that an adaptive, data-driven scale may improve esti-
thetic dataset and present qualitative results on real mation performance.
scanned scenes.
We propose a normal estimation method for un-
structured 3D point clouds. It features a mixture-
of-experts network for scale prediction, which signif-
1. Introduction
icantly increases its robustness to different noise lev-
els, outliers, and varying levels of detail. In addition,
Commodity 3D sensors are rapidly becoming an in-
this method overcomes the challenge of feeding point
tegralpartofautonomoussystems. Thesesensors,e.g.
clouds into CNNs by extending the recently proposed
RGB-D cameras or LiDAR, provide a 3D point cloud
3D modified Fischer Vector (3DmFV) representation
representing the geometry of the scanned objects and
[4] to encode local geometry on a coarse multi-scale
surroundings. This raw representation is challenging
grid. It outperforms state-of-the-art methods for nor-
to process since it lacks connectivity information or
mal vector estimation.
structure, and is often incomplete, noisy and contains
The main contributions of this paper are:
pointdensityvariations. Inparticular,processingitby
means of the highly effective convolutional neural net-
• Anewnormalestimationmethodforunstructured
works (CNNs) is problematic because CNNs require
3D point clouds based on mixture of experts and
structured, grid-like data as input.
scale prediction.
Whenavailable, additionallocalgeometricinforma-
tion,suchasthesurfacenormalsateachpoint,induces
a partial local structure and improves performance of • A local point representation which can be used as
differenttaskssuchasover-segmentation[3],classifica- input to a CNN.
978-1-7281-3293-8/19/$31.00 ©2019 IEEE 10104
DOI 10.1109/CVPR.2019.01035
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 2 ---

Figure 1. Nesti-Net pipeline for normal estimation. For each point in a given point cloud, we compute a multi-scale point
statistics representation (MuPS). Then, a scale manager network is used to determine the optimal scale and uses the
corresponding expert sub-network to estimate the normal.
2. Related-work to fail in estimating normals near edges. Computing
the optimal neighborhood size can decrease the esti-
2.1.Deeplearningforunstructured3Dpointclouds
mation error [18] but requires the (usually unknown)
noise standard deviation value and a costly iterative
The point cloud representation is challenging for
process to estimate the local curvature and additional
deep learning methods because it is both unstructured
density parameters.
and point-wise unordered. In addition, the number of
points in the point cloud is usually not constant. Sev- OtherapproachesrelyonusingVoronoicellsofpoint
eral methods were proposed to overcome these chal- clouds [2, 17, 9]. These methods are characterized by
lenges. Voxel-based methods embed the point cloud robustness to sharp features but are sensitive to noise.
into a voxel grid but suffer from several accuracy- To overcome this challenge, Alliez et al. [1] proposed
complexity tradeoffs [16]. The PointNet approach PCA-Voronoi approach to create cell sets which group
[20, 21] applies a symmetric, order-insensitive, func- adjacentcellstoprovidesomecontroloversmoothness.
tiononahigh-dimensionalrepresentationofindividual While many of these methods hold theoretical guaran-
points. The Kd-Network [14] imposes a kd-tree struc- teesonapproximationandrobustness,inpracticethey
ture on the points and uses it to learn shared weights rely on a preprocessing stage in the presence of strong
fornodesinthetree. Therecentlyproposed3DmFV[4] or unstructured noise in addition to a fine-tuned set of
represents the points by their deviation from a Gaus- parameters.
sian Mixture Model (GMM) whose Gaussians are uni-
Afewdeeplearningapproacheshavebeenproposed
formly positioned on a coarse grid.
to estimate normal vectors from unstructured point
In this paper, we propose a point-wise and multi-
clouds. Boulchetal. proposedtotransformlocalpoint
scale variation of 3DmFV. Instead of generating a
cloud patches into a 2D Hough space accumulator by
structuredrepresentationfortheentirepointcloud,we
randomly selecting point triplets and voting for that
represent each point and its neighbors within several
plane’s normal. Then, the normal is estimated from
scales.
theaccumulatorbydesigningexplicitcriteria[5]forbin
selectionor,morerecently,bytraininga2DCNN[6]to
2.2.Normalestimation
estimate it continuously as a regression problem. This
A classic method for normal estimation uses Princi- method does not fully utilize the 3D information since
pal Component Analysis (PCA) [12]. It first specifies it loses information during the transformation stage.
the neighbors within some scale, and then uses PCA We reffer to this method as HoughCNN in the evalu-
regression to estimate a tangent plane. Variants fit- ation section. A more recent method, PCPNnet [11],
ting local spherical surfaces [10] or jets [7] (truncated uses a PointNet [20] architecture on local point neigh-
Taylorexpansion)werealsoproposed. Toberobustto borhoods of multiple scales. It achieves good normal
noise,thesemethodsusuallychoosealarge-scaleneigh- estimationperformanceandhasbeenextendedtoesti-
borhood, leading them to smooth sharp features and mating other surface properties. However, it processes
10105
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 3 ---

themulti-scalepointcloudsjointlyanddoesnotselect with the GMM density is:
an optimal scale. This type of architecture tends to
(cid:4)K
encourage averaging during training rather than spe-
u (p)= w u (p). (2)
cialization [13]. λ k k
k=1
In this paper we propose a method that approxi-
matesthelocalnormalvectorusingapoint-wise,multi- The 3DmFV uses a uniform GMM on a coarse
scale 3DmFV representation which serves as an input m×m×m3Dgrid,wheremisanadjustableparame-
to a deep 3D CNN architecture. In addition, we learn ter usually chosen to be from m=3 to 8. The weights
the neighborhood size that minimizes the normal es- are set to be w = 1 , the standard deviation is set to
timation error using a mixture of experts (MoE) [13], beσ = 1 ,and k the K covariancematrixtobeΣ =σ I.
k m k k
which encourages specialization. Although the parameters in GMMs are usually set us-
ing maximum likelihood estimation, here uniformity is
2.3.Representingpointcloudsusing3DmFV crucial for shared weight filtering (convolutions).
The FV is expressed as the sum of normalized gra-
The 3DmFV representation for point clouds [4] dients for each point p . The 3DmFV is specified sim-
t
achieved good results for point cloud classification us- ilarly using additional symmetric functions, i.e. min
ing a 3D CNN. See Section 3.1 for details. In this pa- and max. They are symmetric in the sense proposed
perweproposetheMulti-scalePointStatistics(MuPS) in [20] and are therefore adequate for representing the
representation, which extends the 3DmFV and com- structureless and orderless set of points. Adding these
putes a point-wise multi-scale 3DmFV. functions makes the representation more informative
and the associated classification more accurate [4]:
3. Approach
(cid:4)T
GX = L ∇ logu (p ), (3)
The proposed method is outlined in Figure 1. It FVλ λ λ λ t
t=1
receives a 3D point cloud as input and consists of two ⎡ (cid:8) (cid:9) (cid:9) ⎤
mainstages. Inthefirststage,wecomputeamultiscale T L ∇ logu (p )(cid:9)
p st o a i g n e t w re e p f r e e e s d en it ta i t n i t o o n, a d m e i n x o tu te r d e-o M f-e u x P p S e . rt I s n (M th o e E) se C co N n N d G 3 X DmFVλ = ⎢ ⎣ ma t x = t 1 (L λ λ ∇ λ λ logu λ λ (p t t )| λ λ = = α α , , μ μ , , σ σ ⎥ ⎦ (4)
min (L ∇ logu (p ))|
architecture and estimate the normal at each point as t λ λ λ t λ=μ,σ
output. The stages are detailed below. whereL isthesquarerootoftheinverseFisherInfor-
λ
mation Matrix, and the normalized gradients are:
3.1.MuPS-Multi-scalepointstatistics
(cid:4)T
1
GX = √ (γ (k)−w ), (5)
MuPS is a local multi-scale representation which αk w t k
computes point statistics on a coarse Gaussian grid.
k t=1
(cid:13) (cid:14)
It builds on the well-known Fisher Vector (FV) [22], 1 (cid:4)T p −μ
and the recently proposed 3DmFV representation [4]. GX = √ γ (k) t k , (6)
μk w t σ
Therefore, we first outline the FV and the 3DmFV, k t=1 k
and then continue to the MuPS representation and its 1 (cid:4)T (cid:15) (p −μ ) 2 (cid:16)
attractive properties. GX = √ γ (k) t k −1 . (7)
FV and 3DmFV for 3D point clouds: Given
σk 2w
k t=1
t σ
k
2
a set of T 3D points X = {p t ∈ R3,t = 1,...T} Here,wefollow[15]andensurethatu (x)isavaliddis-
and a set of parameters for a K component GMM, tributionbychangingthevariablew λ toα tosimplify
λ = {(w ,μ ,Σ ),k = 1,...K}, where w ,μ ,Σ are k k
k k k k k k the gradient calculation using :
the mixture weight, center, and covariance matrix of
the k-th Gaussian. The likelihood of a single 3D point w = (cid:8) exp(α k ) . (8)
p associated with the k-th Gaussian density is k K exp(α )
j=1 j
(cid:2) (cid:3)
u (p)= 1 exp − 1 (p−μ ) (cid:2) Σ −1 (p−μ ) . Inaddition,γ t (k)expressesthesoftassignmentofpoint
k (2π)D/2|Σ k |1/2 2 k k k p t to Gaussian k, as obtained from the derivatives:
(1)
w u (p )
γ (k)= (cid:8) k k t . (9)
Therefore, the likelihood of a single point associated t K w u (p )
j=1 j j t
10106
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 4 ---

The FV and 3DmFV are normalized by the number of Scale manager Expert
points in order to be sample-size independent [22]: Input: MuPS
3D Inception: [3, 5, 128] GX˜ i(ri)
G F X Vλ ← T
1
G F X Vλ ,G 3 X DmFVλ ← T
1
G 3 X DmFVλ . (10)
3
3
D
D
I
I m
n
n
c
c a
e
e x
p
p p
t
t o
i
i
o
o o
n
n l
:
: [ 2
[
[
3
3 ,
,
, 2
5
5 , 2
,
, ]
2
2
5
5
6
6
]
] 3 3 D D I I
I
n n
n
c c
p
e e
u
p p
t
t t
:
i i o o n n : :
3
[ [
D
3 3
m
, , 5 5
F
, ,
V
2 1 5 2 6 8] ]
3D Inception: [2, 4, 512] maxpool [2, 2, 2]
3D Inception: [2, 4, 512] 3D Inception: [2, 4, 256]
maxpool [2, 2, 2]
maxpool [2, 2, 2]
Note that these are global representations which are 3D Inception: [1, 2, 512]
3D Inception: [2, 4, 512]
maxpool [2, 2, 2]
applied to the entire set, i.e., the entire point cloud. FC [1024] maxpool [2, 2, 2]
MuPS definition : For each point p in point set X F F C C [ [ 2 1 5 2 6 8 ] ] F F C C [ [ 5 1 1 2 2 8 ] ]
we first extract n point subsets X˜ i (r i )| i=1,...n ⊂ X s F o C ftm [n a ] x F F C C [ [ 6 3 4 ] ]
whichcontainT i (p,r i )pointsandliewithinadistance Output: q Output: N
of radius r i from p. We refer to each of these subsets 3D In i ception: [c, c, M] i
1 2
as a scale. Note that each scale may contain a Input: [m x m x m x L]
different number of points. For scales with many
Avg. pool: CNN
points, we set a maximal point threshold, and sample Filter: [cx cx c] Filter: [1x1x1]
1 1 1
a random subset of T points for that scale. Here, Channels: 1 Channels : M
max
r and T are design parameters. Next, the scales CNN CNN CNN
i max
Filter: [1x1x1] Filter: [cx cx c] Filter: [cx cx c]
(subsets) are independently translated and uniformly Channels : M Channe 1 l s : M 1 /2 1 Channe 2 l s : M 2 /2 2
scaled so that they fit into a zero-centered unit sphere
Concatenate
with p mapped to the origin. Then, the 3DmFV
Output: [m x m x m x 3N]
representation is computed for each scale relative to a
Figure 2. The mixture of experts and 3D Inception mod-
Gaussian grid positioned around the origin; see above.
ule architecture details. The scale manager and experts
Concatenating the 3DmFVs of all scales yields the use several convolutional and maxpooling layers followed
MuPS representation: by fully connected layers.
(cid:17) (cid:18)
Gp = GX ˜ 1(r1) ,...,GX ˜ n(rn) . (11)
MuPS 3DmFV 3DmFV
MuPSproperties: TheMuPSrepresentationovercomes
the main challenges associated with feeding point
estimate the normal correctly.
clouds into CNNs. The symmetric functions make it Experts: Thenormalisestimatedusingnseparate”ex-
independent of the number and order of points within
pert” networks. Each is a multi-layered 3D Inception
eachscale. Inaddition,theGMMgivesititsgridstruc-
inspired CNN followed by four fully connected layers.
ture, necessary for the use of CNNs. Furthermore, the
TheMuPSrepresentationisdistributedtotheexperts.
multi-scale representation incorporates description of
This distribution is a design choice. We obtained the
fine detail as well as robustness to noise.
bestresultswhenfeedingeachscaletotwodifferentex-
perts in addition to one expert which receives the en-
3.2.TheNesti-Netarchitecture
tire MuPS representation as input. Specifically, Nesti-
The deep network architecture is outlined in Fig- Net uses 7 experts: experts 1-2 receive the smallest
ure 1 (the green part). It is a mixture-of-experts ar- scale (1%), 3-4 the medium scale (3%), 5-6 the large
chitecture [13] which consists of two modules: a scale scale (5%), and expert 7 receives all the scales. The
managernetworkmodule,andanexpertsmodule. The last layer of each expert outputs a three-element vec-
MoE architecture was chosen in order to overcome the tor N = (N ,N ,N ) . The final predicted normal
i x y z i
averaging effect of typical networks when solving a re- (forpointp)isN argmax(qi),i.e.,thenormalassociated
gression problem. with the expert expected to give the best results. The
Scale manager network: This module receives the architecture is specified in the top right of Figure 2.
MuPS representation as input and processes it using Loss function: We train the network to minimize the
several 3D Inception inspired convolutions, and max- differencebetweenapredictednormalN andaground
i
pool layers, followed by four fully connected layers, af- truthnormalN . Thisdifferenceisquantifiedbythe
GT
ter which a softmax operator is applied. The architec- metric D = sinθ, where the angle θ is the difference
N
ture is specified in the top left part of Figure 2. The betweenthevectors, andD iscalculatedasthemag-
N
output is a vector of n scalars q i , which can be in- nitude of the cross product between these two vectors;
tuitively interpreted as the probability of expert i to see Eq. 12. In addition, to encourage specialization of
each expert network, we follow [13] and minimize the
10107
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 5 ---

loss: • Gaussian noise - perturbing the points with three
(cid:4)n (cid:4)n (cid:7)N ×N (cid:7)
levelsofnoisespecifiedbyσ,givenasapercentage
L= q ·D = q i GT . (12) of the bounding box.
i N i(cid:7)N (cid:7)·(cid:7)N (cid:7)
i=1 i=1 i GT
• Densityvariation-selectingasubsetofthepoints
Usingthisloss,eachexpertisrewardedforspecializing basedontwosamplingregimes: gradient,simulat-
in a specific input type. Note that during training, all ingeffectsofdistancefromthesensor,andstripes,
n normal vectors are predicted and used to compute simulating occlusions.
the loss and derivatives. However, at test time, we
compute only one normal, which is associated with For the geometric methods, we show results for three
the maximal q . differentscales: small, mediumandlarge, whichcorre-
i
spond to 18, 112, 450 nearest neighbors. For the deep
learning based methods we show the results for the
4. Evaluation single-scale (ss) and multi-scale (ms) versions. Addi-
tional evaluation results using other metrics are avail-
4.1.Datasets able in the supplemental material.
Table 1 shows the unoriented normal estimation re-
FortrainingandtestingweusedthePCPNetshape
sults for the methods detailed above. It can be seen
dataset [11]. The trainset consists of 8 shapes: four
that our method outperforms all other methods across
CAD objects (fandisk, boxunion, flower, cup) and
all noise levels and most density variations. It also
four high quality scans of figurines (bunny, armadillo,
shows that both PCA and Jet perform well for spe-
dragon and turtle). All shapes are given as triangle
cific noise-scale pairs. In addition, for PCPNet and
meshes and densely sampled with 100k points. The
HoughCNN, using a multi-scale approach only mildly
data is augmented by introducing Gaussian noise for
improves performance.
eachpoint’sspaciallocationwithastandarddeviation
Figure3illustratesNesti-Net’sresultsonthreepoint
of 0.012, 0.006, 0.00125 w.r.t the bounding box. This
clouds. Forvisualization,thenormalvectorismapped
yields a set with 3.2M points and 3.2M corresponding
to the RGB cube. It shows that for complex shapes
training examples. The test set consists of 22 shapes,
(pillar, liberty) with high noise levels, the general di-
including figurines, CAD objects, and analytic shapes.
rectionofthenormalvectorispredictedcorrectly,but,
For evaluation we use the same 5000 point subset per
the fine details and exact normal vector are not ob-
shape as in [11].
tained. For a basic shape (Boxysmooth) the added
Forqualitativetestingonscanneddata,weusedthe
noise does not affect the results substantially. Most
NYU Depth V2 dataset [19] and the recent ScanNet
notably, Nesti-Net shows robustness to point density
dataset [8], which include RGB-D images of indoor
corruptions. The angular error in each point is visual-
scenes.
ized in Figure 4 for the different methods using a heat
4.2.Trainingdetails map. For PCA and Jet we display the best result out
ofthethreescales(small,medium,andlarge,specified
All variations of our method were trained using
above), andforPCPNetthebestoutofitssingle-scale
32,768 (1024 samples×32 shapes) random subsets of
andmulti-scaleoptions. Forallmethods,itcanbeseen
the 3.2M training samples at each epoch. For each
that more errors occur near edges, corners and small
point, we extract 512 neighboring points enclosed
regions with a lot of detail and high curvature. Nesti-
within a sphere of radius r. For neighborhoods with
Net suffers the least from this effect due to its scale
more than 512 points, we perform random sampling,
manager,whichallowsittoadapttothedifferentlocal
and for those with fewer points we use the maximum
geometry types. Figure 5 shows the performance of
number of points available. For the MuPS represen-
thescalemanagernetwork. Acolorisassignedtoeach
taiton we chose to use an m = 8 Gaussian grid. We
expertandthechosenexpertcolorisvisualizedoverthe
used Tensorflow on a single NVIDIA Titan Xp GPU.
pointcloud. Thisprovidessomeinsightregardingeach
expert’s specialization. For example, the figure shows
4.3.Normalestimationperformance
that experts 1, 2 (small scale) specialize in points in
WeusetheRMSnormalestimationerrormetricfor regions with high curvatures (near corners). Experts 3
comparing the proposed NestiNet to other deep learn- and 4 (medium scale) specialize in the complex cases
ing based [11, 6] and geometric methods [12, 7]. We where multiple surfaces are close to each other, or in
also analyze robustness for two types of data corrup- the presence of noise. As for the large-scale experts,
tions (augmentations): expert5specializesinplanarsurfaceswithnormalvec-
10108
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 6 ---

Our
Aug. PCA [12] Jet [7] PCPNet [11] HoughCNN [6]
Nesti-Net
scale ms small med large small med large ss ms ss ms
None 6.99 8.31 12.29 16.77 7.60 12.35 17.35 9.68 9.62 10.23 10.02
Noise
σ =0.00125 10.11 12.00 12.87 16.87 12.36 12.84 17.42 11.46 11.37 11.62 11.51
σ =0.006 17.63 40.36 18.38 18.94 41.39 18.33 18.85 18.26 18.87 22.66 23.36
σ =0.012 22.28 52.63 27.5 23.5 53.21 27.68 23.41 22.8 23.28 33.39 36.7
Density
Gradient 9.00 9.14 12.81 17.26 8.49 13.13 17.8 13.42 11.7 11.02 10.67
Stripes 8.47 9.42 13.66 19.87 8.61 13.39 19.29 11.74 11.16 12.47 11.95
average 12.41 21.97 16.25 18.87 21.95 16.29 19.02 14.56 14.34 16.9 17.37
Table 1. Comparison of the RMS angle error for unoriented normal vector estimation of our Nesti-Net method to classic
geometric methods (PCA [12] , Jet [7]) with three scales, and deep learning methods (PCPNet [11], HoughCNN [6])
Figure 3. Nesti-Net normal prediction results for different
noiselevels(columns1-4),anddensitydistortions(columns
5-6). Thecolorsofthepointsarenormalvectorsmappedto
RGB.Thisfigureisbestvieweddigitallyonalargescreen.
Figure 4. Normal estimation error results for Nesti-Net
compared to other methods for three types of point clouds
withlownoiselevel(σ=1%). Thecolorsofthepointscor-
tors, which have a large component in the x direction,
respond to angular difference, mapped to a heatmap rang-
whereas expert 6 specializes in planar surfaces, which
ing from 0-60 degrees; see bottom color bar. The number
have a large component in the y direction. Expert 5 under each point cloud is its RMS error.
also specializes in very noisy planar surfaces with a
large component in the z direction. Expert 7 (com-
4.4.Scaleselectionperformance
bined scales) plays multiple roles; it handles points on
planar surfaces which have a large component in the z We analyze the influence of scale selection on the
direction,complexgeometry,andlowtomediumnoise. normal estimation performance. We create several ab-
Figure 6 shows the number of points assigned to each lations of our method.
expertforallpointsinthetestset,andtheaverageer-
• ss - A single scale version which directly feeds a
rorperexpert. Itshowsaninverserelationbetweenthe
3DmFV representation into a CNN architecture
numberofpointsassignedtoanexpertanditsaverage
(a single-scale MuPS).
error: themorepointsassignedtotheexpert,thelower
the error. This is consistent with the definition of the • ms - A multi-scale version which feeds the MuPS
cost function. Timing performance and visualization representation into a CNN architecture.
of additional results are provided in the supplemental
material. • ms-sw - A multi scale version which first tries to
10109
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 7 ---

Aug. ss ms ms-sw NestiNet
0.01
0.01 0.01 0.01
scale 0.01 0.05 0.03
0.05 0.05 0.05
0.05
None 9.32 12.73 10.83 7.88 7.76 6.99
Noise
σ =0.00125 11.31 13.36 12.98 10.46 10.29 10.11
σ =0.006 36.5 18.37 21.06 18.43 18.45 17.63
σ =0.012 55.24 23.14 26.03 22.59 22.25 22.28
Density
Gradient 16.61 14.65 12.81 11.89 9.44 9.00
Stripes 14.5 14.57 12.97 10.06 9.65 8.47
average 23.91 16.14 16.11 13.55 12.97 12.41
Table2.ComparisonoftheRMSangleerrorforunoriented
normal vector estimation of our method using single-scale
(SS), multi-scale (MS), multi-scale with switching (MS-Sw
Figure 5. Nesti-Net predicted experts (scales). Each color and multi-scale with mixture of experts (Nesti-Net)
representsthepredictedexpertforoptimalnormalestima-
tion. Color coding is given at the bottom.
unsupervised, i.e., does not need the additional noise
parameters as input during training.
4.5.Resultsonscanneddata
Weshowqualitativeresultsonscannedpointclouds
fromtheScanNet[8]andNYUDepthV2[19]datasets
in Figure 7. For visualization we project the normal
vectors’ color back to the depth image plane. column
(c)showstheresultsforNesti-Net,trainedonsynthetic
Figure 6. Nesti-Net expert (scale) prediction statistics.
data with Gaussian noise. The estimated normals re-
Number of points assigned to each expert (left), and av-
veal the nonsmoothness of the scanned data associ-
erage expert error (right).
ated with the correlated, non-Gaussian, noise signa-
ture associated with the scanning process. Essentially
estimatethenoiselevelandthenfeedsthe3DmFV
itshowsnormalestimationoftherawdata,ratherthan
representation of the corresponding input scale
the desired normal of the underlying surface. The raw
into different sub-networks for a discrete number
point clouds suffer from ”bumps” which get bigger as
of noise levels (switching). Note that for this ver-
the distance from the sensor increases. Further im-
sion, the noise level is provided during training.
provement may be obtained by training Nesti-Net on
data corrupted with scanner noise and with ground
• NestiNet - The method described in Section 3
truth normals, but such data is is currently not avail-
which uses an MoE network to learn the scale.
ableandisdifficulttomanuallylabel. Instead,wetrain
Details of the architectures for the above methods are Nesti-Net with normal vectors obtained from applying
provided in the supplemental material. a Total Variation (TV) algorithm on the depth map,
Table 2 summarizes the results of the scale selec- provided by Ladicky et al. [23] for the NYU depth V2
tion performance analysis. It shows that Nesti-Net’s dataset. Note that TV substantially smooths fine de-
scale selection performs better than all other varia- tailandusesthedepthimageratherthanunstructured
tions. Thisisduetothetrainedscale-managernetwork point clouds. Column (d) in Figure 7 shows that after
withintheMoE.Thesingle-scaleversionperformswell trainingontheTVdata, thenormalvectorestimation
for specific noise-scale pairs but inferior performance of the underlying surface improves significantly. Col-
foraninadequatescaleselection. Themulti-scalevaria- umn(b)showstheresultsofPCAwithamediumscale
tionsshowimprovement;however,selectingthecorrect forreference,forsmallradius,theresultissignificantly
scale yields improved performance over concatenating noisier and for large radius it over-smooths detail, see
multiple scales. The main advantage of Nesti-Net over supplemental material. Note that Nesti-Net performs
the switching variation is that the scale prediction is theestimationontherawpointcloudanddoesnotuse
the depth image grid structure.
10110
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 8 ---

[5] A. Boulch and R. Marlet. Fast and robust normal
estimation for point clouds with sharp features. In
Computer Graphics Forum, volume 31, pages 1765–
1774. Wiley Online Library, 2012.
[6] A. Boulch and R. Marlet. Deep learning for robust
normalestimationinunstructuredpointclouds. Com-
puter Graphics Forum, 35(5):281–290, 2016.
[7] F.CazalsandM.Pouget.Estimatingdifferentialquan-
titiesusingpolynomialfittingofosculatingjets. Com-
puter Aided Geometric Design, 22(2):121–146, 2005.
Figure7.Normalestimationresultsonscannedpointclouds
fromtheScanNet[8](top),andNYUDepthV2dataset[19] [8] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A.
(bottom). (a)RGBimage,(b)PCAresultsusingamedium Funkhouser, and M. Nießner. Scannet: Richly-
scale, (c) Nesti-Net results trained on synthetic data (d) annotated3dreconstructionsofindoorscenes. InThe
Nesti-net results trained on TV algorithm data. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), volume 2, page 10, 2017.
[9] T. K. Dey and S. Goswami. Provable surface recon-
5. Summary struction from noisy samples. Computational Geome-
try, 35(1-2):124–141, 2006.
Inthiswork,weproposemulti-scalepointstatistics,
[10] G. Guennebaud and M. Gross. Algebraic point set
a new representation for 3D point clouds that encodes
surfaces. ACM Transactions on Graphics (TOG),
fineandcoarsedetailswhileusingagridstructure. The 26(3):23, 2007.
representation is effective processed by a CNN archi-
[11] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J.
tecture(Nesti-Net)forprovideaccuratenormalestima- Mitra.Pcpnetlearninglocalshapepropertiesfromraw
tion, which can be used for various applications, e.g. pointclouds. ComputerGraphicsForum,37(2):75–85,
surface reconstruction. The mixture-of-experts design 2018.
ofthearchitectureenablesthepredictionofanoptimal [12] H. Hoppe, T. DeRose, T. Duchampt, J. McDonald,
local scale and provides insights into the network’s re- andW.Stuetzle. Surfacereconstructionfromunorga-
source distribution. The proposed representation and nized points. Computer Graphics, 26:2, 1992.
architecture achieve state-of-the-art results relative to [13] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E.
allothermethodsanddemonstraterobustnesstonoise Hinton. Adaptive mixtures of local experts. Neural
and occlusion data corruptions. Computation, 3(1):79–87, 1991.
[14] R.KlokovandV.Lempitsky. Escapefromcells: Deep
6. Acknowledegment kd-networksfortherecognitionof3dpointcloudmod-
els. In The IEEE International Conference on Com-
We gratefully acknowledge the support of NVIDIA puter Vision (ICCV), pages 863–872, Oct 2017.
Corporation with the donation of the Titan Xp GPU [15] J. Krapac, J. Verbeek, and F. Jurie. Modeling spatial
used for this research. layout with fishervectors for imagecategorization. In
The IEEE International Conference on Computer Vi-
sion (ICCV), pages 1487–1494. IEEE, 2011.
References
[16] D. Maturana and S. Scherer. Voxnet: A 3d convo-
[1] P.Alliez,D.Cohen-Steiner,Y.Tong,andM.Desbrun. lutional neural network for real-time object recogni-
Voronoi-basedvariationalreconstructionofunoriented tion. In IEEE/RSJ International Conference on In-
point sets. In Symposium on Geometry Processing, telligent Robots and Systems (IROS), pages 922–928.
volume 7, pages 39–48, 2007. IEEE, 2015.
[2] N. Amenta and M. Bern. Surface reconstruction by [17] Q.M´erigot,M.Ovsjanikov,andL.J.Guibas.Voronoi-
voronoifiltering.Discrete&ComputationalGeometry, based curvature and feature estimation from point
22(4):481–504, 1999. clouds.IEEETransactionsonVisualizationandCom-
puter Graphics, 17(6):743–756, 2011.
[3] Y. Ben-Shabat, T. Avraham, M. Lindenbaum, and
A. Fischer. Graph based over-segmentation methods [18] N. J. Mitra and A. Nguyen. Estimating surface nor-
for 3d point clouds. Computer Vision and Image Un- mals in noisy point cloud data. In Proceedings of the
derstanding, 2018. Nineteenth Annual Symposium on Computational ge-
ometry, pages 322–328. ACM, 2003.
[4] Y. Ben-Shabat, M. Lindenbaum, and A. Fischer.
3dmfv: Three-dimensionalpointcloudclassificationin [19] P. K. Nathan Silberman, Derek Hoiem and R. Fer-
real-time using convolutional neural networks. IEEE gus. Indoor segmentation and support inference from
Robotics and Automation Letters, 3(4):3145–3152, RGBD images. In European Conference on Computer
2018. Vision (ECCV), pages 746–760, 2012.
10111
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.

--- Page 9 ---

[20] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet:
Deep learning on point sets for 3d classification and
segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
[21] C.R.Qi,L.Yi,H.Su,andL.J.Guibas. Pointnet++:
Deep hierarchical feature learning on point sets in a
metric space. arXiv preprint arXiv:1706.02413, 2017.
[22] J.Sa´nchez,F.Perronnin,T.Mensink,andJ.Verbeek.
Imageclassificationwiththefishervector: Theoryand
practice. International Journal of Computer Vision,
105(3):222–245, 2013.
[23] B. Zeisl, M. Pollefeys, et al. Discriminatively trained
dense surface normal estimation. In European con-
ference on computer vision, pages 468–484. Springer,
2014.
10112
Authorized licensed use limited to: University of Canberra. Downloaded on June 06,2020 at 15:08:13 UTC from IEEE Xplore. Restrictions apply.```

---

## Section 1.3: Physics-informed neural networks.pt1

Source File: Physics-informed neural networks.pt1.txt

### Paper Content:

```
# Physics-informed neural networks.pt1.pdf
# Converted: 2025-07-18 12:58:55
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Physics-informed neural networks.pt1.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Physics-informed neural networks.pt1.txt


--- Page 1 ---

Physics Informed Deep Learning (Part I): Data-driven
Solutions of Nonlinear Partial Differential Equations
Maziar Raissi1, Paris Perdikaris2, and George Em Karniadakis1
1Division of Applied Mathematics, Brown University,
Providence, RI, 02912, USA
2Department of Mechanical Engineering and Applied Mechanics,
University of Pennsylvania,
Philadelphia, PA, 19104, USA
Abstract
We introduce physics informed neural networks – neural networks that
are trained to solve supervised learning tasks while respecting any given law
of physics described by general nonlinear partial differential equations. In
this two part treatise, we present our developments in the context of solving
two main classes of problems: data-driven solution and data-driven discovery
of partial differential equations. Depending on the nature and arrangement
of the available data, we devise two distinct classes of algorithms, namely
continuous time and discrete time models. The resulting neural networks
form a new class of data-efficient universal function approximators that nat-
urally encode any underlying physical laws as prior information. In this first
part, we demonstrate how these networks can be used to infer solutions to
partial differential equations, and obtain physics-informed surrogate models
that are fully differentiable with respect to all input coordinates and free
parameters.
Keywords: Data-driven scientific computing, Machine learning, Predictive
modeling, Runge-Kutta methods, Nonlinear dynamics
1. Introduction
With the explosive growth of available data and computing resources, re-
cent advances in machine learning and data analytics have yielded transfor-
mativeresultsacrossdiversescientificdisciplines,includingimagerecognition
[1], natural language processing [2], cognitive science [3], and genomics [4].
Preprint submitted to Journal Name November 30, 2017
7102
voN
82
]IA.sc[
1v16501.1171:viXra

--- Page 2 ---

However, more often than not, in the course of analyzing complex physical,
biological or engineering systems, the cost of data acquisition is prohibitive,
and we are inevitably faced with the challenge of drawing conclusions and
making decisions under partial information. In this small data regime, the
vast majority of state-of-the art machine learning techniques (e.g., deep/-
convolutional/recurrent neural networks) are lacking robustness and fail to
provide any guarantees of convergence.
At first sight, the task of training a deep learning algorithm to accurately
identify a nonlinear map from a few – potentially very high-dimensional –
input and output data pairs seems at best naive. Coming to our rescue, for
many cases pertaining to the modeling of physical and biological systems,
there a exist a vast amount of prior knowledge that is currently not being
utilizedinmodernmachinelearningpractice. Letitbetheprincipledphysical
lawsthatgovernthetime-dependentdynamicsofasystem, orsomeempirical
validated rules or other domain expertise, this prior information can act as
a regularization agent that constrains the space of admissible solutions to a
manageable size (for e.g., in incompressible fluid dynamics problems by dis-
carding any non realistic flow solutions that violate the conservation of mass
principle). In return, encoding such structured information into a learning
algorithm results in amplifying the information content of the data that the
algorithm sees, enabling it to quickly steer itself towards the right solution
and generalize well even when only a few training examples are available.
The first glimpses of promise for exploiting structured prior information
to construct data-efficient and physics-informed learning machines have al-
ready been showcased in the recent studies of [5, 6, 7]. There, the authors
employed Gaussian process regression [8] to devise functional representations
that are tailored to a given linear operator, and were able to accurately infer
solutions and provide uncertainty estimates for several prototype problems
in mathematical physics. Extensions to nonlinear problems were proposed
in subsequent studies by Raissi et. al. [9, 10] in the context of both in-
ference and systems identification. Despite the flexibility and mathematical
elegance of Gaussian processes in encoding prior information, the treatment
of nonlinear problems introduces two important limitations. First, in [9, 10]
the authors had to locally linearize any nonlinear terms in time, thus limit-
ing the applicability of the proposed methods to discrete-time domains and
compromising the accuracy of their predictions in strongly nonlinear regimes.
2

--- Page 3 ---

Secondly, the Bayesian nature of Gaussian process regression requires certain
prior assumptions that may limit the representation capacity of the model
and give rise to robustness/brittleness issues, especially for nonlinear prob-
lems [11].
1.1. Problem setup and summary of contributions
In this work we take a different approach by employing deep neural net-
works and leverage their well known capability as universal function ap-
proximators [12]. In this setting, we can directly tackle nonlinear problems
without the need for committing to any prior assumptions, linearization, or
local time-stepping. We exploit recent developments in automatic differen-
tiation [13] – one of the most useful but perhaps underused techniques in
scientific computing – to differentiate neural networks with respect to their
input coordinates and model parameters to obtain physics informed neural
networks. Such neural networks are constrained to respect any symmetry,
invariance, or conservation principles originating from the physical laws that
govern the observed data, as modeled by general time-dependent and non-
linear partial differential equations. This simple yet powerful construction
allows us to tackle a wide range of problems in computational science and
introduces a potentially disruptive technology leading to the development
of new data-efficient and physics-informed learning machines, new classes of
numerical solvers for partial differential equations, as well as new data-driven
approaches for model inversion and systems identification.
The general aim of this work is to set the foundations for a new paradigm
in modeling and computation that enriches deep learning with the long-
standing developments in mathematical physics. These developments are
presented in the context of two main problem classes: data-driven solution
and data-driven discovery of partial differential equations. To this end, let
us consider parametrized and nonlinear partial differential equations of the
general form
u + [u;λ] = 0,
t
N
where u(t,x) denotes the latent (hidden) solution and [ ;λ] is a nonlinear
N ·
operator parametrized by λ. This setup encapsulates a wide range of prob-
lemsinmathematicalphysicsincludingconservationlaws,diffusionprocesses,
advection-diffusion-reaction systems, and kinetic equations. As a motivating
example, the one dimensional Burgers’ equation [14] corresponds to the case
3

--- Page 4 ---

where [u;λ] = λ uu λ u and λ = (λ ,λ ). Here, the subscripts denote
1 x 2 xx 1 2
N −
partial differentiation in either time or space. Given noisy measurements of
the system, we are interested in the solution of two distinct problems. The
first problem is that of predictive inference, filtering and smoothing, or data
driven solutions of partial differential equations [9, 5] which states: given
fixed model parameters λ what can be said about the unknown hidden state
u(t,x) of the system? The second problem is that of learning, system identi-
fication, or data-driven discovery of partial differential equations [10, 6, 15]
stating: what are the parameters λ that best describe the observed data?
In this first part of our two-part treatise, we focus on computing data-
driven solutions to partial differential equations of the general form
u + [u] = 0, x Ω, t [0,T], (1)
t
N ∈ ∈
where u(t,x) denotes the latent (hidden) solution, [ ] is a nonlinear differ-
ential operator, and Ω is a subset of RD. In what f N oll · ows, we put forth two
distinct classes of algorithms, namely continuous and discrete time models,
and highlight their properties and performance through the lens of different
benchmark problems. All code and data-sets accompanying this manuscript
are available at https://github.com/maziarraissi/PINNs.
2. Continuous Time Models
We define f(t,x) to be given by the left-hand-side of equation (1); i.e.,
f := u + [u], (2)
t
N
and proceed by approximating u(t,x) by a deep neural network. This as-
sumption along with equation (2) result in a physics informed neural net-
work f(t,x). This network can be derived by applying the chain rule for
differentiating compositions of functions using automatic differentiation [13].
2.1. Example (Burgers’ Equation)
Asanexample, letusconsidertheBurgers’equation. Thisequationarises
in various areas of applied mathematics, including fluid mechanics, nonlinear
acoustics, gas dynamics, and traffic flow [14]. It is a fundamental partial
differential equation and can be derived from the Navier-Stokes equations for
the velocity field by dropping the pressure gradient term. For small values of
4

--- Page 5 ---

the viscosity parameters, Burgers’ equation can lead to shock formation that
is notoriously hard to resolve by classical numerical methods. In one space
dimension, the Burger’s equation along with Dirichlet boundary conditions
reads as
u +uu (0.01/π)u = 0, x [ 1,1], t [0,1], (3)
t x xx
− ∈ − ∈
u(0,x) = sin(πx),
−
u(t, 1) = u(t,1) = 0.
−
Let us define f(t,x) to be given by
f := u +uu (0.01/π)u ,
t x xx
−
and proceed by approximating u(t,x) by a deep neural network. To highlight
the simplicity in implementing this idea we have included a Python code
snippet using Tensorflow [16]; currently one of the most popular and well
documented open source libraries for machine learning computations. To
this end, u(t,x) can be simply defined as
def u(t, x):
u = neural_net(tf.concat([t,x],1), weights, biases)
return u
Correspondingly, the physics informed neural network f(t,x) takes the form
def f(t, x):
u = u(t, x)
u_t = tf.gradients(u, t)[0]
u_x = tf.gradients(u, x)[0]
u_xx = tf.gradients(u_x, x)[0]
f = u_t + u*u_x - (0.01/tf.pi)*u_xx
return f
The shared parameters between the neural networks u(t,x) and f(t,x) can
be learned by minimizing the mean squared error loss
MSE = MSE +MSE , (4)
u f
5

--- Page 6 ---

where
1 (cid:88)
Nu
MSE = u(ti,xi) ui 2,
u N | u u − |
u
i=1
and
1 (cid:88)
Nf
MSE = f(ti,xi) 2.
f N | f f |
f
i=1
Here, ti,xi,ui Nu denote the initial and boundary training data on u(t,x)
{ u u }i=1
and ti,xi Nf specify the collocations points for f(t,x). The loss MSE
{ f f}i=1 u
corresponds to the initial and boundary data while MSE enforces the struc-
f
ture imposed by equation (3) at a finite set of collocation points.
In all benchmarks considered in this work, the total number of training
data N is relatively small (a few hundred up to a few thousand points), and
u
we chose to optimize all loss functions using L-BFGS; a quasi-Newton, full-
batchgradient-basedoptimizationalgorithm[17]. Forlargerdata-setsamore
computationally efficient mini-batch setting can be readily employed using
stochastic gradient descent and its modern variants [18, 19]. Despite the
fact that there is no theoretical guarantee that this procedure converges to
a global minimum, our empirical evidence indicates that, if the given partial
differential equation is well-posed and its solution is unique, our method is
capable of achieving good prediction accuracy given a sufficiently expressive
neural network architecture and a sufficient number of collocation points N .
f
This general observation deeply relates to the resulting optimization land-
scape induced by the mean square error loss of equation 4, and defines an
open question for research that is in sync with recent theoretical develop-
ments in deep learning [20, 21]. Here, we will test the robustness of the
proposed methodology using a series of systematic sensitivity studies that
accompany the numerical results presented in the following.
Figure 1 summarizes our results for the data-driven solution of the Burg-
ers equation. Specifically, given a set of N = 100 randomly distributed
u
initial and boundary data, we learn the latent solution u(t,x) by training all
3021 parameters of a 9-layer deep neural network using the mean squared
error loss of (4). Each hidden layer contained 20 neurons and a hyperbolic
tangent activation function. In general, the neural network should be given
sufficient approximation capacity in order to accommodate the anticipated
6

--- Page 7 ---

complexity of u(t,x). However, in this example, our choice aims to highlight
the robustness of the proposed method with respect to the well known issue
of over-fitting. Specifically, the term in MSE in equation (4) acts as a reg-
f
ularization mechanism that penalizes solutions that do not satisfy equation
(3). Therefore, a key property of physics informed neural networks is that
they can be effectively trained using small data sets; a setting often encoun-
tered in the study of physical systems for which the cost of data acquisition
may be prohibitive.
The top panel of Figure 1 shows the predicted spatio-temporal solution
u(t,x), along with the locations of the initial and boundary training data.
We must underline that, unlike any classical numerical method for solv-
ing partial differential equations, this prediction is obtained without any
sort of discretization of the spatio-temporal domain. The exact solution for
this problem is analytically available [14], and the resulting prediction error
is measured at 6.7 10 4 in the relative -norm. Note that this error is
− 2
· L
about two orders of magnitude lower than the one reported in our previous
work on data-driven solution of partial differential equation using Gaussian
processes [9]. A more detailed assessment of the predicted solution is pre-
sented in the bottom panel of figure 1. In particular, we present a compari-
son between the exact and the predicted solutions at different time instants
t = 0.25,0.50,0.75. Using only a handful of initial and boundary data, the
physics informed neural network can accurately capture the intricate non-
linear behavior of the Burgers’ equation that leads to the development of a
sharp internal layer around t = 0.4. The latter is notoriously hard to ac-
curately resolve with classical numerical methods and requires a laborious
spatio-temporal discretization of equation (3).
To further analyze the performance of our method, we have performed
the following systematic studies to quantify its predictive accuracy for differ-
ent number of training and collocation points, as well as for different neural
network architectures. In table 1 we report the resulting relative error
2
L
for different number of initial and boundary training data N and different
u
number of collocation points N , while keeping the 9-layer network archi-
f
tecture fixed. The general trend shows increased prediction accuracy as the
total number of training data N is increased, given a sufficient number of
u
collocation points N . This observation highlights a key strength of physics
f
informed neural networks: by encoding the structure of the underlying phys-
7

--- Page 8 ---

1.0
0.5
0.0
0.5
−
1.0
−
0.0 0.2 0.4 0.6 0.8
t
x
u(t,x)
Data (100 points) 0.75
0.50
0.25
0.00
0.25
−
0.50
−
0.75
−
1
0
1
−
1 0 1
−
x
)x,t(u
t = 0.25
1
0
1
−
1 0 1
−
x
)x,t(u
t = 0.50
1
0
1
−
1 0 1
−
x
Exact Prediction
)x,t(u
t = 0.75
Figure 1: Burgers’ equation: Top: Predicted solution u(t,x) along with the initial and
boundarytrainingdata. Inadditionweareusing10,000collocationpointsgeneratedusing
a Latin Hypercube Sampling strategy. Bottom: Comparison of the predicted and exact
solutions corresponding to the three temporal snapshots depicted by the white vertical
linesinthetoppanel. Therelative errorforthiscaseis6.7 10−4. Modeltrainingtook
2
L ·
approximately 60 seconds on a single NVIDIA Titan X GPU card.
ical law through the collocation points N , one can obtain a more accurate
f
and data-efficient learning algorithm.1 Finally, table 2 shows the resulting
relative for different number of hidden layers, and different number of
2
L
neurons per layer, while the total number of training and collocation points
is kept fixed to N = 100 and N = 10,000, respectively. As expected, we
u f
observe that as the number of layers and neurons is increased (hence the
capacity of the neural network to approximate more complex functions), the
1Note that the case N = 0 corresponds to a standard neural network model, i.e., a
f
neural network that does not take into account the underlying governing equation.
8

--- Page 9 ---

N
f 2000 4000 6000 7000 8000 10000
N
u
20 2.9e-01 4.4e-01 8.9e-01 1.2e+00 9.9e-02 4.2e-02
40 6.5e-02 1.1e-02 5.0e-01 9.6e-03 4.6e-01 7.5e-02
60 3.6e-01 1.2e-02 1.7e-01 5.9e-03 1.9e-03 8.2e-03
80 5.5e-03 1.0e-03 3.2e-03 7.8e-03 4.9e-02 4.5e-03
100 6.6e-02 2.7e-01 7.2e-03 6.8e-04 2.2e-03 6.7e-04
200 1.5e-01 2.3e-03 8.2e-04 8.9e-04 6.1e-04 4.9e-04
Table1: Burgers’equation: Relative errorbetweenthepredictedandtheexactsolution
2
L
u(t,x)fordifferentnumberofinitialandboundarytrainingdataN ,anddifferentnumber
u
ofcollocationpointsN . Here,thenetworkarchitectureisfixedto9layerswith20neurons
f
per hidden layer.
Neurons
10 20 40
Layers
2 7.4e-02 5.3e-02 1.0e-01
4 3.0e-03 9.4e-04 6.4e-04
6 9.6e-03 1.3e-03 6.1e-04
8 2.5e-03 9.6e-04 5.6e-04
Table2: Burgers’equation: Relative errorbetweenthepredictedandtheexactsolution
2
L
u(t,x) for different number of hidden layers and different number of neurons per layer.
Here, the total number of training and collocation points is fixed to N =100 and N =
u f
10,000, respectively.
predictive accuracy is increased.
2.2. Example (Shr¨odinger Equation)
This example aims to highlight the ability of our method to handle pe-
riodic boundary conditions, complex-valued solutions, as well as different
types of nonlinearities in the governing partial differential equations. The
one-dimensional nonlinear Schro¨dinger equation is a classical field equation
that is used to study quantum mechanical systems, including nonlinear wave
propagation in optical fibers and/or waveguides, Bose-Einstein condensates,
and plasma waves. In optics, the nonlinear term arises from the intensity
dependent index of refraction of a given material. Similarly, the nonlinear
term for Bose-Einstein condensates is a result of the mean-field interactions
of an interacting, N-body system. The nonlinear Schro¨dinger equation along
9

--- Page 10 ---

with periodic boundary conditions is given by
ih +0.5h + h 2h = 0, x [ 5,5], t [0,π/2], (5)
t xx
| | ∈ − ∈
h(0,x) = 2 sech(x),
h(t, 5) = h(t,5),
−
h (t, 5) = h (t,5),
x x
−
where h(t,x) is the complex-valued solution. Let us define f(t,x) to be given
by
f := ih +0.5h + h 2h,
t xx
| |
and proceed by placing a complex-valued neural network prior on h(t,x).
In fact, if u denotes the real part of h and v is the imaginary part, we
(cid:2) (cid:3)
are placing a multi-out neural network prior on h(t,x) = u(t,x) v(t,x) .
This will result in the complex-valued (multi-output) physic informed neural
network f(t,x). The shared parameters of the neural networks h(t,x) and
f(t,x) can be learned by minimizing the mean squared error loss
MSE = MSE +MSE +MSE , (6)
0 b f
where
1 (cid:88)
N0
MSE = h(0,xi) hi 2,
0 N | 0 − 0|
0
i=1
1 (cid:88)
Nb
(cid:0) (cid:1)
MSE = hi(ti, 5) hi(ti,5) 2 + hi(ti, 5) hi(ti,5) 2 ,
b N | b − − b | | x b − − x b |
b
i=1
and
1 (cid:88)
Nf
MSE = f(ti,xi) 2.
f N | f f |
f
i=1
Here, xi,hi N0 denotes the initial data, ti Nb corresponds to the colloca-
{ 0 0}i=1 { b}i=1
tion points on the boundary, and ti,xi Nf represents the collocation points
{ f f}i=1
on f(t,x). Consequently, MSE corresponds to the loss on the initial data,
0
MSE enforces the periodic boundary conditions, and MSE penalizes the
b f
Schro¨dinger equation not being satisfied on the collocation points.
Inordertoassesstheaccuracyofourmethod, wehavesimulatedequation
10

--- Page 11 ---

(5) using conventional spectral methods to create a high-resolution data set.
Specifically, starting from an initial state h(0,x) = 2 sech(x) and assuming
periodic boundary conditions h(t, 5) = h(t,5) and h (t, 5) = h (t,5), we
x x
− −
have integrated equation (5) up to a final time t = π/2 using the Chebfun
package [22] with a spectral Fourier discretization with 256 modes and a
fourth-order explicit Runge-Kutta temporal integrator with time-step ∆t =
π/2 10 6. Under our data-driven setting, all we observe are measurements
−
·
xi,hi N0 ofthelatentfunctionh(t,x)attimet = 0. Inparticular,thetrain-
{ 0 0}i=1
ing set consists of a total of N = 50 data points on h(0,x) randomly parsed
0
from the full high-resolution data-set, as well as N = 50 randomly sampled
b
collocation points ti Nb for enforcing the periodic boundaries. Moreover,
{ b}i=1
we have assumed N = 20,000 randomly sampled collocation points used
f
to enforce equation (5) inside the solution domain. All randomly sampled
point locations were generated using a space filling Latin Hypercube Sam-
pling strategy [23].
Here our goal is to infer the entire spatio-temporal solution h(t,x) of the
Schro¨dinger equation (5). We chose to jointly represent the latent func-
tion h(t,x) = [u(t,x) v(t,x)] using a 5-layer deep neural network with
100 neurons per layer and a hyperbolic tangent activation function. Fig-
ure 2 summarizes the results of our experiment. Specifically, the top panel
of figure 2 shows the magnitude of the predicted spatio-temporal solution
(cid:112)
h(t,x) = u2(t,x)+v2(t,x), along with the locations of the initial and
| |
boundary training data. The resulting prediction error is validated against
the test data for this problem, and is measured at 1.97 10 3 in the rela-
−
·
tive -norm. A more detailed assessment of the predicted solution is pre-
2
L
sented in the bottom panel of Figure 2. In particular, we present a compar-
ison between the exact and the predicted solutions at different time instants
t = 0.59,0.79,0.98. Using only a handful of initial data, the physics informed
neural network can accurately capture the intricate nonlinear behavior of the
Schro¨dinger equation.
One potential limitation of the continuous time neural network models
considered so far, stems from the need to use a large number of collocation
pointsN inordertoenforcephysicsinformedconstraintsintheentirespatio-
f
temporal domain. Although this poses no significant issues for problems
in one or two spatial dimensions, it may introduce a severe bottleneck in
higherdimensionalproblems,asthetotalnumberofcollocationpointsneeded
11

--- Page 12 ---

5
0
5
−
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
t
x
h(t,x)
| |
3.5
Data (150 points)
3.0
2.5
2.0
1.5
1.0
0.5
5
0
5 0 5
−
x
)x,t(h
|
|
t = 0.59
5
0
5 0 5
−
x
)x,t(h
|
|
t = 0.79
5
0
5 0 5
−
x
Exact Prediction
)x,t(h
|
|
t = 0.98
Figure2: Shr¨odinger equation: Top: Predictedsolution h(t,x) alongwiththeinitialand
| |
boundarytrainingdata. Inadditionweareusing20,000collocationpointsgeneratedusing
a Latin Hypercube Sampling strategy. Bottom: Comparison of the predicted and exact
solutions corresponding to the three temporal snapshots depicted by the dashed vertical
lines in the top panel. The relative error for this case is 1.97 10−3.
2
L ·
to globally enforce a physics informed constrain (i.e., in our case a partial
differential equation) will increase exponentially. In the next section, we put
forth a different approach that circumvents the need for collocation points
by introducing a more structured neural network representation leveraging
the classical Runge-Kutta time-stepping schemes [24].
3. Discrete Time Models
Let us apply the general form of Runge-Kutta methods with q stages [24]
to equation (1) and obtain
un+ci = un ∆t
(cid:80)q
a [un+cj], i = 1,...,q,
− j=1 ij N (7)
un+1 = un ∆t
(cid:80)q
b [un+cj].
− j=1 j N
12

--- Page 13 ---

Here, un+cj(x) = u(tn + c ∆t,x) for j = 1,...,q. This general form en-
j
capsulates both implicit and explicit time-stepping schemes, depending on
the choice of the parameters a ,b ,c . Equations (7) can be equivalently
ij j j
{ }
expressed as
un = un, i = 1,...,q,
i (8)
un = un ,
q+1
where
un := un+ci +∆t
(cid:80)q
a [un+cj], i = 1,...,q,
i j=1 ij N (9)
un := un+1 +∆t
(cid:80)q
b [un+cj].
q+1 j=1 j N
We proceed by placing a multi-output neural network prior on
(cid:2) (cid:3)
un+c1(x),...,un+cq(x),un+1(x) . (10)
This prior assumption along with equations (9) result in a physics informed
neural network that takes x as an input and outputs
(cid:2) (cid:3)
un(x),...,un(x),un (x) . (11)
1 q q+1
3.1. Example (Burgers’ Equation)
To highlight the key features of the discrete time representation we revisit
the problem of data-driven solution of the Burgers’ equation. For this case,
the nonlinear operator in equation (9) is given by
[un+cj] = un+cjun+cj (0.01/π)un+cj,
N x − xx
and the shared parameters of the neural networks (10) and (11) can be
learned by minimizing the sum of squared errors
SSE = SSE +SSE , (12)
n b
where
(cid:88)
q+1
(cid:88)
Nn
SSE = un(xn,i) un,i 2,
n | j − |
j=1 i=1
13

--- Page 14 ---

and
q
(cid:88)(cid:0) (cid:1)
SSE = un+ci( 1) 2 + un+ci(1) 2 + un+1( 1) 2 + un+1(1) 2.
b
| − | | | | − | | |
i=1
Here, xn,i,un,i Nn corresponds to the data at time tn. The Runge-Kutta
{ }i=1
scheme now allows us to infer the latent solution u(t,x) in a sequential fash-
ion. Starting from initial data xn,i,un,i Nn at time tn and data at the
{ }i=1
domain boundaries x = 1 and x = 1, we can use the aforementioned loss
−
function (12) to train the networks of (10), (11), and predict the solution at
time tn+1. A Runge-Kutta time-stepping scheme would then use this predic-
tion as initial data for the next step and proceed to train again and predict
u(tn+2,x), u(tn+3,x), etc., one step at a time.
Inclassicalnumericalanalysis, thesestepsareusuallyconfinedtobesmall
due to stability constraints for explicit schemes or computational complexity
constrains for implicit formulations [24]. These constraints become more se-
vere as the total number of Runge-Kutta stages q is increased, and, for most
problems of practical interest, one needs to take thousands to millions of such
steps until the solution is resolved up to a desired final time. In sharp con-
trast to classical methods, here we can employ implicit Runge-Kutta schemes
with an arbitrarily large number of stages at effectively no extra cost.2 This
enablesustotakeverylargetimestepswhileretainingstabilityandhighpre-
dictive accuracy, therefore allowing us to resolve the entire spatio-temporal
solution in a single step.
The result of applying this process to the Burgers’ equation is presented
in figure 3. For illustration purposes, we start with a set of N = 250 initial
n
data at t = 0.1, and employ a physics informed neural network induced by an
implicit Runge-Kutta scheme with 500 stages to predict the solution at time
t = 0.9inasinglestep. Thetheoreticalerrorestimatesforthisschemepredict
a temporal error accumulation of (∆t2q) [24], which in our case translates
O
into an error way below machine precision, i.e., ∆t2q = 0.81000 10 97. To
−
≈
our knowledge, this is the first time that an implicit Runge-Kutta scheme
2To be precise, it is only the number of parameters in the last layer of the neural
network that increases linearly with the total number of stages.
14

--- Page 15 ---

1.0
0.5
0.0
0.5
−
1.0
−
0.0 0.2 0.4 0.6 0.8
t
x
u(t,x)
0.75
0.50
0.25
0.00
0.25
−
0.50
−
0.75
−
1.0
0.5
0.0
0.5
−
1.0
−
1 0 1
−
x
)x,t(u
t = 0.10
0.5
0.0
0.5
−
1 0 1
−
x
Data
)x,t(u
t = 0.90
Exact Prediction
Figure 3: Burgers equation: Top: Solution u(t,x) along with the location of the initial
training snapshot at t=0.1 and the final prediction snapshot at t=0.9. Bottom: Initial
training data and final prediction at the snapshots depicted by the white vertical lines in
the top panel. The relative error for this case is 8.2 10−4.
2
L ·
of that high-order has ever been used. Remarkably, starting from smooth
initial data at t = 0.1 we can predict the nearly discontinuous solution at
t = 0.9 in a single time-step with a relative error of 8.2 10 4. This error is
2 −
L ·
two orders of magnitude lower that the one reported in [9], and it is entirely
attributed to the neural network’s capacity to approximate u(t,x), as well as
to the degree that the sum of squared errors loss allows interpolation of the
training data. The network architecture used here consists of 4 layers with
50 neurons in each hidden layer.
15

--- Page 16 ---

Neurons
10 25 50
Layers
1 4.1e-02 4.1e-02 1.5e-01
2 2.7e-03 5.0e-03 2.4e-03
3 3.6e-03 1.9e-03 9.5e-04
Table 3: Burgers’ equation: Relative final prediction error measure in the norm for
2
L
different number of hidden layers and neurons in each layer. Here, the number of Runge-
Kutta stages is fixed to 500 and the time-step size to ∆t=0.8.
A detailed systematic study to quantify the effect of different network
architectures is presented in table 3. By keeping the number of Runge-Kutta
stages fixed to q = 500 and the time-step size to ∆t = 0.8, we have var-
ied the number of hidden layers and the number of neurons per layer, and
monitored the resulting relative error for the predicted solution at time
2
L
t = 0.9. Evidently, as the neural network capacity is increased the predictive
accuracy is enhanced.
The key parameters controlling the performance of our discrete time al-
gorithm are the total number of Runge-Kutta stages q and the time-step size
∆t. In table 4 we summarize the results of an extensive systematic study
where we fix the network architecture to 4 hidden layers with 50 neurons
per layer, and vary the number of Runge-Kutta stages q and the time-step
size ∆t. Specifically, we see how cases with low numbers of stages fail to
yield accurate results when the time-step size is large. For instance, the case
q = 1 corresponding to the classical trapezoidal rule, and the case q = 2
corresponding to the 4th-order Gauss-Legendre method, cannot retain their
predictive accuracy for time-steps larger than 0.2, thus mandating a solu-
tion strategy with multiple time-steps of small size. On the other hand, the
ability to push the number of Runge-Kutta stages to 32 and even higher
allows us to take very large time steps, and effectively resolve the solution
in a single step without sacrificing the accuracy of our predictions. More-
over, numerical stability is not sacrificed either as implicit Runge-Kutta is
the only family of time-stepping schemes that remain A-stable regardless of
their order, thus making them ideal for stiff problems [24]. These properties
are unprecedented for an algorithm of such implementation simplicity, and
illustrate one of the key highlights of our discrete time approach.
16

--- Page 17 ---

∆t
0.2 0.4 0.6 0.8
q
1 3.5e-02 1.1e-01 2.3e-01 3.8e-01
2 5.4e-03 5.1e-02 9.3e-02 2.2e-01
4 1.2e-03 1.5e-02 3.6e-02 5.4e-02
8 6.7e-04 1.8e-03 8.7e-03 5.8e-02
16 5.1e-04 7.6e-02 8.4e-04 1.1e-03
32 7.4e-04 5.2e-04 4.2e-04 7.0e-04
64 4.5e-04 4.8e-04 1.2e-03 7.8e-04
100 5.1e-04 5.7e-04 1.8e-02 1.2e-03
500 4.1e-04 3.8e-04 4.2e-04 8.2e-04
Table 4: Burgers’ equation: Relative final prediction error measured in the norm for
2
L
different number of Runge-Kutta stages q and time-step sizes ∆t. Here, the network
architecture is fixed to 4 hidden layers with 50 neurons in each layer.
3.1.1. Example (Allen-Cahn Equation)
This example aims to highlight the ability of the proposed discrete time
models to handle different types of nonlinearity in the governing partial dif-
ferentialequation. Tothisend, letusconsidertheAllen-Cahnequationalong
with periodic boundary conditions
u 0.0001u +5u3 5u = 0, x [ 1,1], t [0,1], (13)
t xx
− − ∈ − ∈
u(0,x) = x2cos(πx),
u(t, 1) = u(t,1),
−
u (t, 1) = u (t,1).
x x
−
The Allen-Cahn equation is a well-known equation from the area of reaction-
diffusion systems. It describes the process of phase separation in multi-
componentalloysystems, includingorder-disordertransitions. FortheAllen-
Cahn equation, the nonlinear operator in equation (9) is given by
[un+cj] = 0.0001un+cj +5
(cid:0)
un+cj
(cid:1)3
5un+cj,
N − xx −
and the shared parameters of the neural networks (10) and (11) can be
learned by minimizing the sum of squared errors
SSE = SSE +SSE , (14)
n b
17

--- Page 18 ---

where
(cid:88)
q+1
(cid:88)
Nn
SSE = un(xn,i) un,i 2,
n | j − |
j=1 i=1
and
q
(cid:88)
SSE = un+ci( 1) un+ci(1) 2 + un+1( 1) un+1(1) 2
b
| − − | | − − |
i=1
q
(cid:88)
+ un+ci( 1) un+ci(1) 2 + un+1( 1) un+1(1) 2.
| x − − x | | x − − x |
i=1
Here, xn,i,un,i Nn corresponds to the data at time tn. We have generated
{ }i=1
a training and test data-set set by simulating the Allen-Cahn equation (13)
using conventional spectral methods. Specifically, starting from an initial
condition u(0,x) = x2cos(πx) and assuming periodic boundary conditions
u(t, 1) = u(t,1) and u (t, 1) = u (t,1), we have integrated equation (13)
x x
− −
up to a final time t = 1.0 using the Chebfun package [22] with a spectral
Fourier discretization with 512 modes and a fourth-order explicit Runge-
Kutta temporal integrator with time-step ∆t = 10 5.
−
In this example, we assume N = 200 initial data points that are ran-
n
domly sub-sampled from the exact solution at time t = 0.1, and our goal
is to predict the solution at time t = 0.9 using a single time-step with size
∆t = 0.8. To this end, we employ a discrete time physics informed neural
network with 4 hidden layers and 200 neurons per layer, while the output
layer predicts 101 quantities of interest corresponding to the q = 100 Runge-
Kutta stages un+ci(x), i = 1,...,q, and the solution at final time un+1(x).
Figure 4summarizes our predictions after the network has beentrained using
the loss function of equation (14). Evidently, despite the complex dynamics
leading to a solution with two sharp internal layers, we are able to obtain an
accurate prediction of the solution at t = 0.9 using only a small number of
scattered measurements at t = 0.1.
4. Summary and Discussion
We have introduced physics informed neural networks, a new class of uni-
versal function approximators that is capable of encoding any underlying
18

--- Page 19 ---

1.0
0.5
0.0
0.5
−
1.0
−
0.0 0.2 0.4 0.6 0.8 1.0
t
x
u(t,x)
0.75
0.50
0.25
0.00
0.25
−
0.50
−
0.75
−
1.00
−
0.00
0.25
−
0.50
−
0.75
−
1.00
−
1 0 1
−
x
)x,t(u
t = 0.10
1.0
0.5
0.0
0.5
−
1.0
−
1 0 1
−
x
Data
)x,t(u
t = 0.90
Exact Prediction
Figure4: Allen-Cahn equation: Top: Solutionu(t,x)alongwiththelocationoftheinitial
training snapshot at t=0.1 and the final prediction snapshot at t=0.9. Bottom: Initial
training data and final prediction at the snapshots depicted by the white vertical lines in
the top panel. The relative error for this case is 6.99 10−3.
2
L ·
physical laws that govern a given data-set, and can be described by par-
tial differential equations. In this work, we design data-driven algorithms for
inferringsolutionstogeneralnonlinearpartialdifferentialequations, andcon-
structing computationally efficient physics-informed surrogate models. The
resulting methods showcase a series of promising results for a diverse collec-
tion of problems in computational science, and open the path for endowing
deep learning with the powerful capacity of mathematical physics to model
the world around us. As deep learning technology is continuing to grow
rapidly both in terms of methodological and algorithmic developments, we
believe that this is a timely contribution that can benefit practitioners across
19

--- Page 20 ---

a wide range of scientific domains. Specific applications that can readily en-
joy these benefits include, but are not limited to, data-driven forecasting of
physical processes, model predictive control, multi-physics/multi-scale mod-
eling and simulation.
We must note however that the proposed methods should not be viewed
as replacements of classical numerical methods for solving partial differen-
tial equations (e.g., finite elements, spectral methods, etc.). Such methods
have matured over the last 50 years and, in many cases, meet the robustness
and computational efficiency standards required in practice. Our message
here, as advocated in Section 3, is that classical methods such as the Runge-
Kutta time-stepping schemes can coexist in harmony with deep neural net-
works, and offer invaluable intuition in constructing structured predictive
algorithms. Moreover, the implementation simplicity of the latter greatly
favors rapid development and testing of new ideas, potentially opening the
path for a new era in data-driven scientific computing. This will be further
highlighted in the second part of this paper in which physics informed neural
networks are put to the test of data-driven discovery of partial differential
equations.
Finally, in terms of future work, one pressing question involves addressing
the problem of quantifying the uncertainty associated with the neural net-
work predictions. Although this important element was naturally addressed
in previous work employing Gaussian processes [9], it not captured by the
proposed methodology in its present form and requires further investigation.
Acknowledgements
This work received support by the DARPA EQUiPS grant N66001-15-
2-4055, the MURI/ARO grant W911NF-15-1-0562, and the AFOSR grant
FA9550-17-1-0013. All data and codes used in this manuscript are publicly
available on GitHub at https://github.com/maziarraissi/PINNs.
References
[1] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with
deep convolutional neural networks, in: Advances in neural information
processing systems, pp. 1097–1105.
20

--- Page 21 ---

[2] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015)
436–444.
[3] B. M. Lake, R. Salakhutdinov, J. B. Tenenbaum, Human-level concept
learning through probabilistic program induction, Science 350 (2015)
1332–1338.
[4] B. Alipanahi, A. Delong, M. T. Weirauch, B. J. Frey, Predicting the se-
quencespecificitiesofDNA-andRNA-bindingproteinsbydeeplearning,
Nature biotechnology 33 (2015) 831–838.
[5] M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of dif-
ferential equations using noisy multi-fidelity data, Journal of Computa-
tional Physics 335 (2017) 736–746.
[6] M. Raissi, P. Perdikaris, G. E. Karniadakis, Machine learning of linear
differential equations using Gaussian processes, Journal of Computa-
tional Physics 348 (2017) 683 – 693.
[7] H. Owhadi, Bayesian numerical homogenization, Multiscale Modeling
& Simulation 13 (2015) 812–828.
[8] C. E. Rasmussen, C. K. Williams, Gaussian processes for machine learn-
ing, volume 1, MIT press Cambridge, 2006.
[9] M. Raissi, P. Perdikaris, G. E. Karniadakis, Numerical Gaussian pro-
cesses for time-dependent and non-linear partial differential equations,
arXiv preprint arXiv:1703.10230 (2017).
[10] M. Raissi, G. E. Karniadakis, Hidden physics models: Machine
learning of nonlinear partial differential equations, arXiv preprint
arXiv:1708.00588 (2017).
[11] H. Owhadi, C. Scovel, T. Sullivan, et al., Brittleness of Bayesian infer-
ence under finite information in a continuous world, Electronic Journal
of Statistics 9 (2015) 1–79.
[12] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks
are universal approximators, Neural networks 2 (1989) 359–366.
21

--- Page 22 ---

[13] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, J. M. Siskind, Au-
tomatic differentiation in machine learning: a survey, arXiv preprint
arXiv:1502.05767 (2015).
[14] C. Basdevant, M. Deville, P. Haldenwang, J. Lacroix, J. Ouazzani,
R. Peyret, P. Orlandi, A. Patera, Spectral and finite difference solu-
tions of the Burgers equation, Computers & fluids 14 (1986) 23–41.
[15] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven
discovery of partial differential equations, Science Advances 3 (2017).
[16] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, et al., Tensorflow: Large-scale
machine learning on heterogeneous distributed systems, arXiv preprint
arXiv:1603.04467 (2016).
[17] D. C. Liu, J. Nocedal, On the limited memory BFGS method for large
scale optimization, Mathematical programming 45 (1989) 503–528.
[18] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.
[19] D. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv
preprint arXiv:1412.6980 (2014).
[20] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, Y. LeCun, The
losssurfacesofmultilayernetworks, in: ArtificialIntelligenceandStatis-
tics, pp. 192–204.
[21] R. Shwartz-Ziv, N. Tishby, Opening the black box of deep neural net-
works via information, arXiv preprint arXiv:1703.00810 (2017).
[22] T. A. Driscoll, N. Hale, L. N. Trefethen, Chebfun guide, 2014.
[23] M. Stein, Large sample properties of simulations using latin hypercube
sampling, Technometrics 29 (1987) 143–151.
[24] A. Iserles, A first course in the numerical analysis of differential equa-
tions, 44, Cambridge University Press, 2009.
22```

---

## Section 1.4: Estimating differential quantities using polynomial fitting of osculating jets

Source File: Estimating differential quantities using polynomial fitting of osculating jets.txt

### Paper Content:

```
# Estimating differential quantities using polynomial fitting of osculating jets.pdf
# Converted: 2025-07-18 12:58:57
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Estimating differential quantities using polynomial fitting of osculating jets.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Estimating differential quantities using polynomial fitting of osculating jets.txt


--- Page 1 ---

ComputerAidedGeometricDesign22(2005)121–146
www.elsevier.com/locate/cagd
Estimating differential quantities using polynomial fitting of
osculating jets✩
F. Cazals∗, M. Pouget
INRIASophia-Antipolis,2004routedesLucioles,F-06902Sophia-Antipolis,France
Received14October2003;receivedinrevisedform30September2004;accepted30September2004
Availableonline14October2004
Abstract
This paper addresses the point-wise estimation of differential properties of a smooth manifold S—a curve in
theplaneorasurfacein3D—assumingapointcloudsampledoverS isprovided.Themethodconsistsoffitting
thelocalrepresentationofthemanifoldusingajet,andeitherinterpolationorapproximation.Ajetisatruncated
Taylorexpansion,andtheincentiveforusingjetsisthattheyencodealllocalgeometricquantities—suchasnormal,
curvatures,extremaofcurvature.
On the way to using jets, the question of estimating differential properties is recasted into the more general
framework of multivariate interpolation/approximation,a well-studied problem in numericalanalysis. On a the-
oreticalperspective,we proveseveralconvergenceresultswhenthesamplesgetdenser.Forcurvesandsurfaces,
theseresultsinvolveasymptoticestimateswithconvergenceratesdependinguponthedegreeofthejetused.For
theparticularcaseofcurves,anerrorboundisalsoderived.Tothebestofourknowledge,theseresultsareamong
the firstonesprovidingaccurateestimatesfordifferentialquantitiesoforderthreeandmore.On thealgorithmic
side,wesolvetheinterpolation/approximationproblemusingVandermondesystems.Experimentalresultsforsur-
facesofR3 arereported.Theseexperimentsillustratetheasymptoticconvergenceresults,butalsotherobustness
ofthemethodsongeneralComputerGraphicsmodels.
2004ElsevierB.V.Allrightsreserved.
Keywords:Meshes;Pointclouds;Differentialgeometry;Interpolation;Approximation
✩ AnextendedabstractofthispaperispartoftheSymposiumonGeometryProcessing(SGP),2003.Thispaperprovidesthe
proofsoftheSGPpaperandalsofeaturesanenhancedexperimentalsection.WorkpartiallysupportedbytheEuropeanProject
EffectiveComputationalGeometryforCurvesandSurfaces,Shared-costRTD(FETOpen)ProjectNoIST-2000-26473.
* Correspondingauthor.
E-mailaddresses:frederic.cazals@inria.fr(F.Cazals),marc.pouget@inria.fr(M.Pouget).
0167-8396/$–seefrontmatter 2004ElsevierB.V.Allrightsreserved.
doi:10.1016/j.cagd.2004.09.004

--- Page 2 ---

122 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
1. Introduction
1.1. Estimatingdifferential quantities
Several applications from Computer Vision, Computer Graphics, Computer Aided Design or Com-
putational Geometry require estimating local differential quantities. Examples of such applications are
surfacesegmentation,surfacesmoothing/denoising,surfacereconstruction,shapedesign.Inanycase,the
input consists ofapointcloudoramesh.Mostofthetime,estimating firstandsecondorder differential
quantities,thatisthetangentplaneandcurvature-relatedquantities,issufficient.However,someapplica-
tionsinvolving shapeanalysisrequireestimatingthirdorderdifferentialquantities (Hallinanetal.,1999;
Porteous, 2001).
Given these ubiquitous needs, a wealth of different estimators have been proposed in the vast lit-
erature of applied geometry (Petitjean, 2001; Goldfeather and Interrante, 2004). Most of these are
adaptations to the discrete setting of smooth differential geometry results. For example, several def-
initions of normals, principal directions and curvatures over a mesh can be found in (Taubin, 1995;
Csàkàny and Wallace, 2000). Ridges of polyhedral surfaces as well as cuspidal edges of the focal sets
are computed in (Watanabe and Belyaev, 2001). Geodesics and discrete versions of the Gauss-Bonnet
theorem areconsidered in(PolthierandSchmies,1998).
Out ofall these contributions, few of them address the question of the accuracy of the estimates pro-
posed or that of their convergence when the mesh or the sample points get denser. This lack of sound
theoretical analysis is however a major issue since discrete versions of smooth operators may not con-
verge, or may converge to unexpected values. Examples of such phenomena are the surface area of a
mesh which may not converge to that of the discretized surface (Morvan and Thibert, 2002), or the an-
gular defect at a vertex of a triangulation which usually does not provide any information on the Gauss
curvature oftheunderlying smoothsurface (Borrellietal.,2002).
The estimation methods providing convergence guarantees are all concerned with first and second
orderdifferentialquantities.In(AmentaandBern,1999),anerrorboundisprovedonthenormalestimate
to asmooth surface sampled according toa criterion involving the skeleton. The surface area of amesh
and its normal vector field versus those of a smooth surface are considered in (Morvan and Thibert,
2002). Asymptotic estimates for the normal and the Gauss curvature of a sampled surface for several
methods are given in (Meek and Walton, 2000). Based upon the normal cycle and restricted Delaunay
triangulations, anestimate forthesecond fundamental form ofasurface isdeveloped in(Cohen-Steiner
andMorvan,2003).
Another striking fact about the estimation of first and second order differential quantities is that for
plane curves, these quantities are often estimated using the osculating circle, while for surfaces, using
osculating paraboloids areubiquitous. Whynotosculatingparabolasforcurvesandosculating ellipsoids
or hyperboloids for surfaces? Answering this question and developing estimation methods providing
guarantees forthirdandhigherorderestimatesmotivatesthefollowingcontributions.
1.2. Contributions
Themaincontributionofthispaperistorecasttheproblemofestimatingdifferentialpropertiestothat
of fitting thelocal representation ofthe manifold byajet. A jetisatruncated Taylor expansion, and the
incentive for using jets is that they encode all local geometric quantities—such as normal, curvatures,

--- Page 3 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 123
extrema ofcurvature. Accurate estimates of thecoefficients ofthe jet therefore translate immediately to
accurateestimatesofdifferentialquantities.Sincethemethodproposedconsistsofperformingapolyno-
mialfitting,connectionswiththeclassicalquestionsofinterpolationandapproximationdeserveacareful
discussion.
Interpolation isawellstudied topics innumerical analysis. Mostofthetimehowever, theparameter-
ization domain of interest is a subset of Rn. In particular, convergence results on the coefficients of the
Lagrange interpolation polynomial versus the Taylor expansion of afunction are proved in(Coatmelec,
1966;CiarletandRaviart,1972;QuarteroniandValli,1994).Ourresultsdifferfromtheseinseveralcon-
cerns. On one hand, we are interested in interpolation and approximation over a manifold rather than a
Euclideandomain.Ontheotherhand,theafore-mentioned papersproveerrorboundswhileweestablish
asymptoticerrorestimates.Itshouldhoweverbenoticedthatthedominanttermsofthese—asafunction
ofthesamplingdensity—are thesame.
Regarding polynomial fitting of differential properties for a surface, our results are closely related to
(MeekandWalton,2000,Lemma4.1).Inthatpaper,adegreetwointerpolationisusedandanalyzed.We
generalize thisresultforjetsorarbitrarydegree, underinterpolation andapproximation.
Tocomplete the description, twocomments arein order. First, itshould emphasized that our focus is
a local interpolation/approximation problem, that is we are not concerned with the convergence of the
Lagrange interpolation polynomial to the height function on a whole given set. This problem requires
specific conditions onthefunction and theposition ofthepoints, asillustrated bytheRunge divergence
phenomena (Lancaster and Salkauskas, 1986, Chapter 2). Therefore, our study is not to be confused
with global fitting such as piecewise polynomial fitting encountered in CAD. Second, our focus is on
the estimation of point-wise differential quantities and not the identification of loci of points exhibiting
prescribed differential properties—examples ofsuchlociareridges orparabolic lines. Whiletheformer
problemislocal,thelatterisglobalandthereforefacestheissueofreportinglociwithaglobaltopological
coherence (Porteous,2001;CazalsandPouget,2004).
1.3. Paperoverview
Fundamentals about jets and numerical issues are recalled in Sections 2 and 3. The case of surfaces
and curves are examined in Sections 4 and 5. Finally, the overall algorithm together with experimental
resultsarepresented inSections6and7.
2. Geometricpre-requisites
2.1. Curvesandsurfaces, heightfunctions andjets
Itiswellknown(doCarmo,1976;Spivak,1999)thatanyregularembeddedsmooth1 curveorsurface
canbelocallywrittenasthegraphofaunivariateorbivariatefunctionwithrespecttoanyzdirectionthat
1 Regular means that the tangent space has dimension one/two for a curve/surface everywhere. Embedded forbids self-
intersections.Smoothmeansasmanytimesdifferentiableasweneed,typicallyC3orC4.

--- Page 4 ---

124 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
does not belong to the tangent space. Weshall call such a function a height function. Taking an order n
Taylorexpansion oftheheightfunction overacurveyields:
f(x)=J (x)+O(xn+1), (1)
B,n
with
J (x)=B +B x+B x2+B x3+···+B xn. (2)
B,n 0 1 2 3 n
Similarlyforasurface:
(cid:1)(cid:2) (cid:2) (cid:3)
f(x,y)=J (x,y)+O (cid:2) (x,y) (cid:2)n+1 , (3)
B,n
with
(cid:4)n (cid:4)k
J B,n (x,y)= H B,k (x,y), H B,k (x,y)= B k−j,j xk−jyj. (4)
k=0 j=0
NoticethatinEq.(3),thetermO((cid:2)(x,y)(cid:2)n+1)standsfortheremainderinTaylor’smultivariateformula.
Borrowingtothejargonofsingularitytheory(BruceandGiblin,1992),thetruncatedTaylorexpansion
J (x) or J (x,y) is called a degree n jet, or n-jet. Since the differential properties of a n-jet match
B,n B,n
thoseofitsdefiningcurve/surface uptoordern,thejetissaidtohaveanordercontactwithitsdefining
curveorsurface.Thisalsoaccountsforthetermosculatingjet—althoughosculating2 wasinitiallymeant
for2-jets.Thedegreen-jetofacurveinvolvesn+1terms.Forasurface,sincetherearei+1monomials
of degree i, the n-jet involves N =1+2+···+(n+1)=(n+1)(n+2)/2 terms. Notice that when
n
the z direction used is aligned with the normal vector to the curve/surface, one has B =0 or B =
1 1,0
B =0. The osculating n-jet encloses differential properties of the curve/surface up to order n, that is
0,1
anydifferentialquantityoforderncanbecomputedfromthen-jet.Inparticular,thetangentspacecanbe
computedfromthe1-jet,thecurvaturerelatedinformation canbeobtainedfromthe2-jet,....Toclarify
thepresentation, wesummarizeasfollows:
Definition1.Foragivenpointonacurveorsurfaceandn(cid:1)1:
• given a coordinate system, the osculating n-jet is the Taylor expansion of the height function trun-
catedatorder n;
• theosculatingn-jetisprincipalinagivencoordinatesystemifthelineartermsvanish(i.e.,thez-axis
is the normal direction of the manifold). Note that this is rather a property of the coordinate system
whichreadsonthejet;
• an osculating conic/quadric is a conic/quadric whose 2-jet matches that of the curve/surface (inde-
pendently ofagivencoordinate system);
• an osculating conic/quadric is degenerate if the quadratic form this conic/quadric is defined with is
degenerate (thatistheformdoesnothavefullrank);
• anosculating conic/quadric isprincipal if,inthegivencoordinate system,itsn-jetareprincipal.
Degenerateosculating conics/quadrics arespecificcurvesandsurfacessince:
2 FromtheLatinosculare,tokiss.

--- Page 5 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 125
Theorem1(Berger, 1987,Chapter15).Thereare9Euclidean conicsand17Euclideanquadrics.
Observation 1. The degenerate osculating conics to a smooth curve are parabola or lines. The degen-
erate osculating quadrics to a smooth surface are either paraboloids (elliptic, hyperbolic), parabolic
cylinders, orplanes.
Degenerateosculating conicsandquadricsarethereforerespectively 2outof9conicsand4outof17
quadrics.
The Monge coordinate system of a curve is defined by its tangent and normal vectors. For a surface,
the Monge coordinate system is such that the z axis is aligned with the normal and the x,y axis are
alignedwiththeprincipaldirections.Notethatthen-jetisprincipalintheMongecoordinatesystem.The
Monge form ofthe curveorsurface atoneofitspoint isthelocal Taylorexpansion ofthecurve/surface
intheMongecoordinate system.Inthisparticular system,theheightfunction iscalledtheMongeform,
andletting k ,k standfortheprincipalcurvatures, onehas
1 2
(cid:1)(cid:2) (cid:2) (cid:3)
f(x,y)= 1 (k x2+k y2)+O (cid:2) (x,y) (cid:2)3 . (5)
1 2
2
Fromtheseobservations,thequestionweendedSection1.1withcannowbeanswered.ByTheorem1
andObservation1,usingageneralconic/quadric oradegenerateonetoapproximateacurveorasurface
does not make any difference. In both case and up to order two, the local differential properties of the
curve/surface, degenerate conic/quadric, or full rank conic/quadric are identical. All these local differ-
ential properties are enclosed is the 2-jet of the manifold. Yet, from a practical point of view, finding a
conic/quadric requires moreconstrains thanfindingadegenarate onesincethelatter reduces toadegree
twopolynomialfitting.Noticealsothatthenormaldirectiontothemanifoldbeingaprioriunknown,this
polynomial fittinghastobeperformedinacoordinate systemwheretheosculating jetis(ingeneral)not
principal.
As an example, consider Fig. 1(a,b,c,d). Fig. 1(a) features a curve C and an osculating conic.3 In
(b), the osculating circle is drawn, in (c), the osculating circle is replaced by the principal osculating
parabola—whose symmetry axis is the normal to C and whose curvature matches that of C. At last, in
(d) a general parabola locally approximates C in a coordinate system where the jet is not principal (its
symmetry axis isnotaligned withthe normalto C). Thislast case illustrates the setting weareworking
withinthesequel.
2.2. Interpolation, approximation andrelatedvariations
Ourmethodology toretrievedifferential quantities consists offittingtheosculating jet.Thefollowing
variations need to be discussed in order to state our contributions precisely. The case of curves and
surfaces being tantamount, our description focuses on surfaces. Assume we are given a set of N points
p (x ,y ,z ), i =1,...,N, in the neighborhood of a given point p on the surface processed. Point p
i i i i
itself may or may not be one of the N samples, and one can assume without loss of generality that p is
located attheoriginofthecoordinate system used.
3 A point worth noticing is the relative position of C and an osculating curve: the former usually crosses the latter at the
intersectionpoint.Toseewhy,comparetheorderthreeTaylorexpansionsoftheoftwocurves.

--- Page 6 ---

126 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
(a) (b) (c) (d)
Fig.1.Acurveand(a)anosculatingellipse,(b)itsosculatingcircle(specialcaseofosculatingconic),(c)itsprincipaldegenerate
conic,(d)andegenerateosculatingconic:aparabola.
Interpolation versus approximation. Interpolating consists of finding a polynomial that fits exactly a
setofdatapoints.InourcaseandfollowingEq.(3),letB indexacoefficientofthejetofthesurface,and
Aindexacoefficientofthejetsought.4 Weaimatfindingan-jetJ suchthat
A,n
(cid:1)(cid:2) (cid:2) (cid:3)
f(x ,y )=J (x ,y )+O (cid:2) (x ,y ) (cid:2)n+1 =J (x ,y ), ∀i=1,...,N. (6)
i i B,n i i i i A,n i i
Approximation, on the other hand, gives up on exactness, that is the graph of the jet sought may not
contain the sample points. Weshall focus onleast-square approximation, which consists of minimizing
the sum of the square errors between the value of the jet and that of the function. The quantity to be
minimizedistherefore
(cid:4)N (cid:1) (cid:3)
J (x ,y )−f(x ,y ) 2 . (7)
A,n i i i i
i=1
The two problems can actually be written in the same matrix form. To see why, let us write the jets
in the polynomial basis consisting of monomials xiyj. Examples of other basis that could be used are
theBezier–Bernstein basisortheNewtonbasis. Weusethemonomialssincethisbasisisconvenient for
the asymptotic analysis but also the design of effective algorithms. Denote by A the N -vector of the
n
coefficients ofthejetsought, thatis
A=(A ,A ,A ,...,A )t.
0,0 1,0 0,1 0,n
DenotebyZ theN-vectoroftheordinates, i.e.,withz =f(x ,y ),
i i i
(cid:1) (cid:1)(cid:2) (cid:2) (cid:3)(cid:3)
Z=(z ,z ,...,z )t = J (x ,y )+O (cid:2) (x ,y ) (cid:2)n+1 .
1 2 N B,n i i i i i=1,...,N
Eqs.(6)and(7)yieldthefollowing N ×N Vandermonde matrix
n
M =(1,x i ,y i ,x i 2,...,x i y i n−1,y i n) i=1,...,N . (8)
For the interpolation case, the number of points matches the number of parameters, so that matrix M is
square and Eq. (6) can be written as MA=Z. For the approximation case, M is a rectangular N ×N
n
matrix,andEq.(7)issummarizedasmin(cid:2)MA−Z(cid:2) .
2
Choosing between interpolation and approximation depends upon the problem tackled. For noisy
datasets,approximationisthemethodofchoice.Otherwise,thealternativedependsoftherelativevalues
4 Asamnemonic,thereadermaywanttoremindthatindexAstandsfortheAnswertothefittingproblem.

--- Page 7 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 127
ofthenumberofmodelparameters versus thenumberofavailable points. Ifthetwomatchone-another,
anaturalchoiceisinterpolation. Inanycase,fittingyieldsalinearsystem,sothatnumericalissuesarise.
FacingthesedifficultiesisthetopicofSection3.
Meshormesh-lessmethods. Animportant difference betweenlocalgeometryestimation algorithms is
whether or not they require some topological information—typically the connectivity of amesh. Mesh-
based methods are usually faster. Mesh-less techniques are more general and better suited for noisy
datasets. A difficulty of the latter methods, however, is to select the relevant points used to perform
the estimates. While one can always resort to heuristics of the k-nearest-neighbors type, user defined
parameters shouldbeavoided. Thisissueisaddressed inSection6.
Oneortwostagesmethods. Fittinga2-jetprovidesestimatesofthetangentplaneandthecurvaturere-
latedinformation. Thesestepscanbecarriedoutsequentially orsimultaneously. Followingtheguideline
of (Sander and Zucker, 1990), most of the methods already mentioned proceed sequentially. The prov-
ablygoodalgorithmweproposeproceedssimultaneously. Alongitsanalysis,wealsoprovidetheoretical
resultsontheaccuracyofsequential methods.
2.3. Contributions revisited
Equipped with the language of jets, we can now state our contributions precisely. Consider Eqs. (6)
and (7). We expect J and J to be equivalent in some sense. To specify this, we shall study the
A,n B,n
convergence properties of the coefficients of J when the points p converge to p. More precisely,
A,n i
assume that the coordinates of the p are given by p (x =a h,y =b h,z =f(x ,y )). Parameters a
i i i i i i i i i i
and b arearbitrary realnumbers, while h specifies that the p uniformly tend totheorigin. Weactually
i i
expect
(cid:1) (cid:3)
A =B +O r(h) .
ij ij
Function r(h) describes the convergence rate or the precision of the fitting, and the main contribution
of this paper is to quantify r(h) for interpolation and approximation methods. This is done applying
classical results of numerical analysis. These results are then translated for geometric quantities such
as normal and curvatures. As we shall see, interpolation or approximation of the same degree yield the
same convergence rate. The difficulties posed are also similar and are essentially to deal with singular
matrices. Thisenablethedesignofanalgorithm fortheestimation ofgeometricquantities together with
someknowledgeaboutthequalityoftheseestimates.
3. Numericalpre-requisites
In this section, we recall the fundamentals of the fitting methods used, namely interpolation and ap-
proximation, together withthenumericalissuesarisingfromtheresolutions.
3.1. Interpolation
The interpolation fitting is based upon the Lagrange interpolation, that is the construction of a poly-
nomial constrained to fit a set of data points. Although this problem is classical for the univariate case,

--- Page 8 ---

128 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
themultivariatecaseisstillanactiveresearchfieldfromboththetheoreticalandcomputationalpointsof
view.Webrieflyreviewtheunivariate andmultivariate basicsofLagrangeinterpolation.
Univariate Lagrange interpolation. Let X ={x ,...,x } be n+1 distinct real values, the so-called
0 n
nodes. Then, for any real function f, there is a unique polynomial P of degree n so that P(x ) =
i
f(x ),∀i =0,...,n. Polynomial P is called the Lagrange interpolation polynomial of f at the nodes
i
X. For any choice of distinct nodes, this polynomial exists and is unique, and in that case the Lagrange
interpolation issaidtobepoised.
Multivariate Lagrange interpolation. Consider now the following bivariate problem. Let Π
n
b(cid:1)e th(cid:3)e
subspace of bivariate polynomials of total degree equal or less than n, whose dimension is N = n+2 ,
n n
and let X ={x ,...,x } consist of N =N values in R2 called nodes. (Notice that N is exactly the
1 N n
number of monomials found in the jet of Eq. (3).) The Lagrange interpolation problem is said to be
poised for X if for any function f :R2→R, there exists a unique polynomial P in Π so that P(x )=
n i
f(x ), ∀i =1,...,N.Itisintuitive and wellknownthat this problem ispoised iffthe setofnodes X is
i
not a subset of any algebraic curve of degree at most n, or equivalently the Vandermonde determinant
formed by the interpolation equations does not vanish. Asnoticed in (Sauer and Yuan, 1995), the set of
nodesforwhichtheproblemisnotpoisedhasmeasurezero,henceitisalmostalwayspoised.
Howeverletusillustratenon-poisedcasesordegenerateconfigurationsofnodes,togetherwithalmost
degenerate configurations—a moreprecise definition willbegivenwiththeconditioning inSection 3.3.
Considerthetwoquadricsq (x,y,z)=2x+x2−y2−zandq (x,y,z)=x2+y2−z,whoseintersection
1 2
curve I projects in the (x,y) plane to the conic C(x,y)=0 with C(x,y)=x −y2 (cf. Fig. 2). If one
tries to interpolate a height function using points on I, uniqueness of the interpolant is not achieved
since any quadric in the pencil of q and q goes through I. A similar example featuring the four one-
1 2
ring and one two-ring neighbors of a point p is depicted on Fig. 3. Notice that being able to figure out
Fig. 2. Two quadrics whose intersec- Fig.3.TheKite(almost)degenerateconfig-
tion curve I projects onto the parabola uration—tangentplaneseenfromabove:the
C: x =y2. Interpolation points located on 6pointsusedforadegreetwointerpolation
I do not uniquely define an interpolating are (almost) located on a conic, that is two
heightfunction. intersectinglines.

--- Page 9 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 129
such configurations is rather a strength than a weakness of the method since a surface is sought and,
the amount of information available does not determine uniquely such a surface. A first fundamental
difference between theunivariate and multivariate cases istherefore thecritical issue ofchoosing nodes
sothattheinterpolation ispoised.
In the particular case where the points lies on a regular square grid of the plane, the geometry of the
configuration leadstothefollowingremarks.Ononehand,anon-poised degreeninterpolation occursif
thepointsliesonnlines,sincetheydefineanalgebraiccurveofdegreen.Onetheotherhand,triangular
lattices yield poised problems for every degree. These results and further extensions can be found in
(GascaandSauer,2000)andreferences therein.
3.2. Leastsquareapproximation
It is well known that the minimization problem of Eq. (7) has a unique solution iff the matrix M is
of maximum rank N . In that case, the minimum value ρ is called the residual of the system, that is
n
ρ =min(cid:2)MA−Z(cid:2) . The important issue is again the rank of the matrix M. In terms of the relative
2
valuesofN versus N ,usingtoomanypointscertainly smoothesoutgeometricfeatures, butalsomakes
n
rankdeficientmatriceslesslikely.
3.3. Numericalissues
Thedifficulties ofsolving linear andleast-squares systems consist ofdealing withrank-deficient ma-
trices.Wenowdiscusstheseissuesinmoredetail.Distancesbetweenmatricesandmatrixnormsreferto
theEuclideannorm.
Singular systems and condition numbers. Toquantify degeneracies, weresort to a Singular Value De-
composition (SVD)(Golub and van Loan, 1983). Denote by σ ,...,σ the singular values of M sorted
n 1
in decreasing order. It is well known that the least singular value of M is the distance from M to rank
deficient matrices. The singular values also characterizes the sensitivity of the problem, that is the way
errorsontheinputdatainduceerrorsonthecomputedsolution.Noticethaterrorsrefertotheuncertainty
attachedtotheinputdataandnottotheroundingerrorsinherenttothefloatingpointcalculations. Inour
case,inputdataarethesamplepoints, sothaterrorsaretypically relatedtotheacquisition system—e.g.,
alaserrangescanner.
Toquantify theprevious arguments, weresort tothe conditioning orcondition number ofthesystem
(GolubandvanLoan,1983;Higham,1996).Theconditioning isdefinedasamagnificationfactorwhich
relatestheafore-mentioned errorsbythefollowingrule:
Erroronsolution=Erroroninput×conditioning.
Denoteby
κ (M)=(cid:2)M(cid:2) (cid:2)M −1(cid:2) =σ /σ
2 2 2 n 1
the condition number of the matrix M. The conditioning of the linear problem MX =Z and the least
squareproblem min(cid:2)MX−Z(cid:2) arerespectively givenby
2
(cid:5)
linearsquaresystem: 2κ (M),
2
leastsquaresystem: 2κ (M)+ κ2(M)(κ2(M)+1)ρ withρ=(cid:2)MX−Z(cid:2) theresidual. (9)
2 (cid:2)M(cid:2)
2
(cid:2)X(cid:2)
2
2

--- Page 10 ---

130 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
Thefollowingtheorem provides preciseerrorbounds:
(cid:6)
Theorem2(Higham,1996,pp.133and392). Suppose X andX arethesolutions oftheproblems
(cid:5)
linearsquaresystem: MX=Z and(M +(cid:5)M)X (cid:6)=Z+(cid:5)Z,
(10)
leastsquaresystem: min(cid:2)MX−Z(cid:2) and min(cid:2)(M+(cid:5)M)X (cid:6)−(Z+(cid:5)Z)(cid:2) ,
2 2
with ε a positive real value such that (cid:2)(cid:5)M(cid:2) (cid:2)ε(cid:2)M(cid:2) , (cid:2)(cid:5)Z(cid:2) (cid:2)ε(cid:2)Z(cid:2) , and εκ (M)<1. Then one
2 2 2 2 2
has:
(cid:2)X−X (cid:6)(cid:2) ε
2 (cid:2) conditioning. (11)
(cid:2)X(cid:2) 1−κ (M)ε
2 2
In practice, if the conditioning is of order 10a and the relative error on the input is ε≈10−b—with
εκ (M)<1,thentherelativeerroronthesolution isoforder10a−b.
2
Pre-conditioning the Vandermonde system. As already discussed, a convenient way to solve Eqs. (6)
and (7) consists ofusing thebasis of monomials. Oneends upwiththe Vandermonde matrix ofEq.(8),
thatcanbesolvedwithusualmethodsoflinearalgebra.Unfortunately, Vandermondesystemsareknown
to be ill-conditioned due to the change of magnitude of the terms. We therefore pre-condition so as to
improve the condition number. Assuming the {x }, {y } are of order h, the pre-conditioning consists of
i i
performing acolumnscalingbydividingeachmonomial xkyl byhk+l.
i i
ThenewsystemisM(cid:6)Y =MDY =ZwithDthediagonalmatrixD=(1,h,h,h2,...,hn,hn),sothat
the solution A of the original system is A=DY. The condition number used in the sequel is precisely
κ(M(cid:6)). (Notice it has the geometric advantage to be invariant under homothetic transformations of the
input points.) Thentheaccuracy ofthe result canbeestimated aposteriori, andalmost degenerate cases
hight-lighted bylargeconditioning.
Alternativesfortheinterpolation case. AnalternativetotheVandermondesystemconsistsofusingthe
basis of Newton polynomials. Resolution of the system can be done using divided differences (Sauer,
1995), anumerically accurate yetinstable method(Higham,1996).
4. Surfaces
4.1. Problemaddressed
Let S be a surface and p be a point of S. Without loss of generality, we assume p is located at the
origin and we aim at investigating differential quantities at p. Consider the height function f given by
Eq. (3) in any coordinate system whose z axis is not in the tangent plane. We shall interpolate S by a
bivariate n-jetJ (x,y) whosegraphisdenoted Q.
A,n
ThenormaltoasurfacegivenbyEq.(3)is
(cid:7)
n =(−B ,−B ,1)t/ 1+B2 +B2 . (12)
S 10 01 10 01
In order to characterize curvature properties, we resort to the Weingarten map A of the surface also
called the shape operator, that is the tangent map of the Gauss map. (Recall that the first and second

--- Page 11 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 131
fundamental forms I,II and A satisfy II(v,v)=I(A(v),v) for any vector v of the tangent space.) The
principal curvatures and principal directions are the eigenvalues (eigenvectors) of A, and the reader is
referred to (do Carmo, 1976, Section 3.3). If the z axis is aligned with the normal, the linear terms of
Eq. (3) vanish, and the second fundamental form reduces to the Hessian of the height function. Further
simplifications are obtained in the Monge coordinate system, where I =Id , the Hessian is a diagonal
2
matrix,andtheprincipal curvatures aregivenby2B and2B .
20 02
4.2. Polynomialfittingoftheheightfunction
We begin by an approximation result on the coefficients of the height function. We focus on the
convergence rategivenbythevalueoftheexponent ofparameter h.
Proposition1.Let{(x i ,y i )} i=1,...,N beasetofpointsofR2 definingapoisedpolynomialinterpolationor
approximation problem of degree n, with x =O(h),y =O(h) (N =N for interpolation and N >N
i i n n
forapproximation). LetJ bethepolynomial solution oftheproblem associated tothefunction f that
A,n
is,forinterpolation
J (x ,y )=f(x ,y ), ∀i=1,...,N;
A,n i i i i
andforapproximation
(cid:8) (cid:9)
(cid:4)N (cid:1) (cid:3)
J =argmin J (x ,y )−f(x ,y ) 2 .
A,n A,n i i i i
i=1
Thenthecoefficients B k−j,j ofdegreek oftheTaylorexpansion off areestimatedbythoseofJ A,n upto
accuracy O(hn−k+1):
A k−j,j =B k−j,j +O(hn−k+1) ∀k=0,...,n, ∀j =0,...,k.
Moreover, iftheoriginisoneofthechosenpointsandinterpolation isused, thenA =B =0.
0,0 0,0
Proof of Proposition 1, interpolation case. Let K be the convex hull of the set {(x i ,y i )} i=1,...,N , d max
bethediameterof K and d bethesupremum ofthediameter ofdisks inscribed inK.Alsodenote by
min
Dk thedifferential oforderk.
The result is a direct consequence of Theorem 2 of (Ciarlet and Raviart, 1972) or Remark 3.4.2 of
(Quarteroni andValli,1994), whichstatesthat
(cid:10)(cid:2) (cid:2) (cid:11)
sup (cid:2) Dkf(x,y)−DkJ (x,y) (cid:2);(x,y)∈K =O(hn−k+1). (13)
A,n
Rephrasing itwithournotations yields:
(cid:12) (cid:12)
|B k−j,j −A k−j,j |= j!(k 1 −j)! (cid:12) Dk(f −J A,n ) (0,0) (1,0)k−j(0,1)j (cid:12)
(cid:10)(cid:12) (cid:12) (cid:11)
(cid:2)sup (cid:12) Dk(f −J ) (ζ ,...,ζ ) (cid:12);ζ ∈R2,(cid:2)ζ (cid:2)(cid:2)1
(cid:2) A,n(cid:2)(0,0) 1 k i i
(cid:2)(cid:2) Dk(f −J ) (cid:2)
(cid:10)(cid:2) A,n (0,0) (cid:2) (cid:11)
(cid:2)sup (cid:2) Dkf(x,y)−DkJ (x,y) (cid:2);(x,y)∈K .
A,n
For the particular case where the origin is one of the samples, notice that the equation involving the
point (0,0)isA =f(0,0)=0. (cid:1)
0,0

--- Page 12 ---

132 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
Asoutlinedbytheproof,theconstanthiddeninthetermO(hn−k+1)dependsuponsup{(cid:2)Dn+1f(x,y)(cid:2);
(x,y) ∈K}, and the geometry of K. In particular, the estimates are better when the ratio d /d is
max min
small,whichintuitively meansthatthesetK isnottoo“flat”.
Forthe approximation case, the result might be aconsequence of Theorem 5 of(Ciarlet and Raviart,
1972) formulated in the Sobolev setting. But in order to meet its hypothesis, one must prove that the
operatorofdiscreteleastsquareapproximationiscontinuous,whichisnotstraightforward.Alternatively,
wegivethefollowingpedestrian proof.
Proof of Proposition 1, approximation case. Using the notations introduced in Section 2.2, consider
the least-square system min(cid:2)MA−Z(cid:2)
2
.With the assumption that M isof rank N
n
,(cid:13)the approximation
isequivalent totheinvertible linearsystem MTMA=MTZ.Withthenotation Σk= N xkyl andthe
l i=1 i i
assumption (cid:2)(x ,y )(cid:2)=O(h):
i i
 
B Σ0+B Σ1+B Σ0+··· +B Σ0+O(hn+1)
0,0 0 1,0 0 0,1 1 0,n n
  B Σ1+··· +B Σ1+O(hn+2)  
 0,0 0 0,n n 
MTZ=  B 0,0 Σ 1 0+··· +O(hn+2)  ,
 . . 
 . . 
. .
B Σ0+··· +O(hn+n+1)
0,0 n
 
Σ0 Σ1 Σ0 ... Σ0
0 0 1 n
 
Σ1 Σ2 Σ1 ... Σ1 
 0 0 1 n 
MTM =  Σ 1 0 Σ 1 1 Σ 2 0 ... Σ n 0 +1  .
 
. . . . .
 . . . . . 
. . . . .
Σ0 Σ1 Σ0 ... Σ0
n n n+1 2n
LetD bedet(MTM),applying theCramer’srulegives:
 
Σ0 ... B Σ0+B Σ1+B Σ0+···+B Σ0+O(hn+1) ... Σ0
0 0,0 0 1,0 0 0,1 1 0,n n n
 
Σ1 ... B Σ1+···+B Σ1+O(hn+2) ... Σ1 
 0 0,0 0 0,n n n 
A j,k−j =det    Σ 1 0 ... B 0,0 Σ 1 0+···+O(hn+2) ... Σ n 0 +1    (cid:20) D.
 . . . . . 
 . . . . . 
. . . . .
Σ0 ... B Σ0+···+O(hn+n+1) ... Σ0
n 0,0 n 2n
Linearcombinations ofthecolumnsgives:
 
Σ 0 0 ... B j,k−j Σ k j −j +O(hn+1) ... Σ n 0
 
   Σ 0 1 ... B j,k−j Σ k j − + j 1+O(hn+2) ... Σ n 1   (cid:20)
A j,k−j =det  Σ 1 0 ... B j,k−j Σ k j −j+1 +O(hn+2) ... Σ n 0 +1   D.
 
. . . . .
 . . . . . 
. . . . .
Σ n 0 ... B j,k−j Σ k j −j+n +O(hn+n+1) ... Σ 2 0 n
The numerator of this formula splits due to the multi-linearity of the determinant, and noticing that
Σ
l
k =O(hk+l)givesA j,k−j =B j,k−j +O(hn−k+1). (cid:1)

--- Page 13 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 133
With respect to the order of convergence, it is equivalent to performing the fitting in any coordinate
system. Nevertheless, as noted above, the error estimates are better if the convex hull of the sample
points is not too flat. Consequently, it means that for the best estimates one should take a coordinate
system as close as possible to the Monge system. Using the previous proposition, the order of accuracy
of a differential quantity is easily related to the degree of the interpolant and the order of this quantity.
Moreprecisely:
Theorem 3. A polynomial fitting of degree n estimates any kth-order differential quantity to accuracy
O(hn−k+1).Inparticular:
• thecoefficients ofthefirstfundamental formandtheunitnormalvectorareestimatedwithaccuracy
O(hn),andsoistheanglebetweenthenormalandtheestimatednormal;
• thecoefficients ofthesecondfundamental formandtheshapeoperatorareapproximated withaccu-
racy O(hn−1), and so are the principal curvatures and directions (as long as they are well defined,
i.e.,awayfromumbilics).
Proof. It is easily checked that the formula corresponding to the geometric quantities (as long as
they are well defined) are C1 functions of the coefficients. The result follows from Proposition 1 and
LemmaA.1—seeAppendix A. (cid:1)
The previous theorem generalizes (Meek and Walton, 2000, Lemma 4.1) where 2-jet interpolations
onlyarestudied. TheO(hn)boundonthenormalshould alsobecomparedtotheestimateofthenormal
vector using specific Voronoi centers called poles considered in (Amenta and Bern, 1999). The error
bound proved there is equivalent to 2ε with ε the sampling density of the surface. Letting lfs stand for
local feature size, setting h=εlfs and assuming lfs is bounded from above, the estimation stemming
from apolynomial fitting therefore yields moreaccurate results for the tangent plane, and also provides
information onhigherorderquantities.
4.3. Influence ofnormalaccuracyonhigherorderestimates
Following the guideline initiated in (Sander and Zucker, 1990) several algorithms first estimate the
normaltothesurfaceandthenproceedwithHessianoftheheightfunction.Weanalyzetheerrorincurred
bythelatterasafunctionoftheaccuracyontheformer.Wedenotebyθ theanglebetweenthenormaln
S
to the surface and the normal n estimated by the two-stages method. In order to simplify calculations,
Q
we assume that n is aligned with the z-axis and n is in the (x,z)-plane, so that f(x,y) =B x +
Q S 10
B x2 +B xy +B y2+O((cid:2)(x,y)(cid:2)3), with B =−tanθ. Expressed in the same coordinate system,
20 11 02 10
theinterpolant—a 2-jettosimplifycalculations—reads asJ (x,y)=A x2+A xy+A y2.
A,2 20 11 02
Proposition 2. If a small error θ is done on the estimated normal, a 2-jet interpolation give the Gauss
curvature withalinearerrorwrtθ:
k −k =θO(h −1)+O(h)+O(θ2).
Q S
Proof. Thesystemofequations fortheinterpolation is:

--- Page 14 ---

134 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
(cid:1)(cid:2) (cid:2) (cid:3)
A x2h+A x y +A y2=B x +B x2+B x y +B y2+O (cid:2) (x ,y ) (cid:2)3 ,
2,0 i 1,1 i i 0,2 i 1,0 i 2,0 i 1,1 i i 0,2 i i i
i=1,...,3.
LetD bethedeterminant: D=det(x i 2,x i y i ,y i 2) i=1,...,3 ,Cramer’srulegives:
(cid:1) (cid:1)(cid:2) (cid:2) (cid:3) (cid:3)
A =det B x +B x2+B x y +B y2+O (cid:2) (x ,y ) (cid:2)3 ,x y ,y2 /D
2,0 1,0 i 2,0 i 1,1 i i 0,2 i i i i i i i=1,...,3
=B O(h −1)+B +O(h).
1,0 2,0
Similarcalculationsgives:A =B O(h−1)+B +O(h),andA =B O(h−1)+B +O(h).The
1,1 1,0 1,1 0,2 1,0 0,2
Gaussiancurvature ofQisthen:
4A A −A2
k = 2,0 0,2 1,1
Q 1+A2
1,0
=4A A −A2
(cid:1) 2,0 0,2 1,1 (cid:3)(cid:1) (cid:3) (cid:1) (cid:3)
=4 B O(h −1)+B +O(h) B O(h −1)+B +O(h) − B O(h −1)+B +O(h) 2
1,0 2,0 1,0 0,2 1,0 1,1
=4B B −B2 +B O(h −1)+B2 O(h −2)+O(h)
2,0 0,2 1,1 1,0 1,0
=4B B −B2 +tanθO(h −1)+tan2θO(h −2)+O(h).
2,0 0,2 1,1
TheGaussiancurvature ofS is
4B B −B2 4B B −B2
k = 2,0 0,2 1,1 = 2,0 0,2 1,1 =(4B B −B2 )cos4θ.
S (1+B2 )2 (1+tan2θ)2 2,0 0,2 1,1
1,0
Thustheerroronthecurvatureis:
k −k =(4B B −B2 )cos4θ +tanθO(h −1)+tan2θO(h −2)+O(h)
Q S 2,0 0,2 1,1
=θO(h −1)+O(h)+O(θ2). (cid:1)
For a fixed h, the curvature error is a linear function of the angle between the normals. The term
θO(h−1) shows that if θ is fixed, the smaller h the worse the accuracy. Hence estimating the normal
deserves specificcare.
5. Planecurves
Alltheresults proved forsurfaces inthe previous section can also beproved for curves, and weomit
them. Instead, for the interpolation case, we prove an error bound between the coefficients of the curve
andthoseofthejet.
5.1. Problemaddressed
LetC beacurve,andconsidertheheightfunctionf followingEq.(1)inanycoordinatesystemwhose
y axisisnottangenttothecurveand,whoseoriginisonthecurve(thisimpliesthatB =0).Weshallfit
0
C byan-jetJ (x).Asalreadymentioned,therearen+1unknowncoefficientsA ,weassumeN data
A,n i
points P (x =a h,y =f(x )) are given, where N =n+1 for interpolation fitting. Notice again that
i i i i i

--- Page 15 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 135
parameter h specifies the uniform convergence of these data points to the origin. The fitting equations
are:
y =f(x )=J (x )+O(xn+1)=J (x ).
i i B,n i i A,n i
SincecurveC isgivenbyEq.(1),thenormalandthecurvature ofC attheoriginaregivenby
(cid:7)
n =(−B ,1)t/ 1+B2, k =2B /(1+B2)3/2. (14)
C 1 1 C 2 1
Moreover, in the Monge coordinate system—B = 0, these expressions simplify to n = (0,1)t and
1 C
k =2B .
C 2
5.2. Errorboundsfortheinterpolation
The equivalent of Proposition 1 for curves gives the magnitude of the accuracy of the interpolation.
Wecanactuallybemorepreciseandprovideerrorboundsdepending uponthefunctioninterpolated and
theposition oftheinterpolation points.
Proposition 3. Consider a degree n (n (cid:1) 1) interpolation problem for a curve y = f(x). Let h
be a positive number so that the interpolation point abscissa lie in the interval [−h,h], and c =
sup |f(n+1)(x)|.Thenfork=0,...,n:
x∈[−h,h]
hn−k+1c
|A −B |(cid:2) .
k k k!(n−k+1)!
Proof. Thisresultisasimpleapplication oftheanalysis oftheLagrange interpolation remainder which
can be found in (Isaacson and Keller, 1966). Let R (x)=f(x)−J (x), Theorem 1, p. 289 states that
n A
∀k=0,...,nand∀x∈[−h,h]:
n(cid:21)−k f(n+1)(η)
R(k)(x)= (x−ξ )
n j (n+1−k)!
j=0
withx,ξ ,η∈[−h,h].
j
Forx=0,thisleadsto:
(cid:12)
(cid:12) R(k)(0)
(cid:12)
(cid:12)=|k!A −k!B |(cid:2)
hn−k+1c
,
n k k (n−k+1)!
hn−k+1c
|A −B |(cid:2) . (cid:1)
k k k!(n−k+1)!
Here is an application of the previous result. Let θ denote the angle between the normal and the
estimated normal.Wehave
(cid:7)
sin(θ)=(cid:2)n ∧n (cid:2)=|A −B |/ (1+A2)(1+B2)(cid:2)|A −B |.
Q C 1 1 1 1 1 1
Itisfoundthat
θ (cid:2)arcsin(hnc/n!).

--- Page 16 ---

136 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
6. Algorithm
The fitting algorithm to estimate the differential properties at a point p consists of (i) collecting the
N points used for the fitting. (Recall that a n-jet involves N =(n+1)(n+2)/2 coefficients, so that
n
when interpolating (approximating) we assume N =N (N >N ).) (ii) Solving the linear system (iii)
n n
recovering thedifferential properties.
Weexamineinturnthedetails ofthesethreesteps.
6.1. Collecting N neighbors
Themeshcase. Althoughnotopologicalinformationisrequiredbythefittingmethod,theconnectivity
information of a mesh can be used as follows. We sequentially visit the one-ring neighbors, two-ring
neighbors, and so on until N points have been collected. Let R ,...,R be the k rings of neighbors
1 k
necessary tocollect N neighbors. Allthe points ofthe k−1 firstrings areused. Thecomplement upto
N pointsischosenarbitrarily outofthekthring.
The point-cloud case. The normal at p is first estimated, and the neighbors of p are further retrieved
from apowerdiagram intheestimated tangent plane(Boissonnat andFlototto, 2002)—a provably good
procedure if the samples are dense enough. If the number of neighbors collected is less than N, we
recursively collecttheneighbors oftheneighbors.
Collectingthepointsthereforeboilsdowntoestimatingthetangentplane.Onesolutionistoconstruct
the Voronoi diagram of the point set and use these Voronoi vertices called poles (Amenta and Bern,
1999). Polesyieldanaccurateestimateofthenormalvectorbutrequireaglobalconstruction.
AnalternativeistoresorttothealgorithmofSection4,andsolveadegreeoneinterpolationproblem—
whichrequiresthreepointsandiswellpoisedassoonasthethreepointsarenotcollinear.Geometrically,
thecloserthethreepointstobeingaligned,themoreunstablethetangentplaneestimate.Toseehowone
can get around this difficulty, denote by q the nearest neighbor of p. Also, let r be the sample point so
thatthecircum-radiusr ofthetrianglepqr isminimum.Theestimatednormalatpisthenormaltothe
circ
plane through pqr. Intuitively, minimizing thecircum-radius r prevents twodifficulties: ononehand
circ
triangleswithalargeangle(neartoπ)exhibitalargecircum-circleandarediscarded;ontheotherhand,
triangles involving a third point r which is not a local neighbor of p cannot minimize r and are also
circ
discarded. Amoreformalargumentadvocating thechoiceofthetrianglewithminimumr isprovided
circ
in (Shewchuk, 2002), where it is shown that the worst error on the approximation of the gradient of a
bivariate functionbyalinearinterpolant precisely involves r .
circ
6.2. Solving thefittingproblem
Thenextstageconsists ofchoosing thezdirection toworkwith.Sincethetangent planehasnotbeen
estimated, we use a principal component analysis to compute a rough estimate of the normal with the
collectedpoints.Thepolynomialfittingcanbedoneinanycoordinatesystemwhosezaxisisnottangent
to the surface. Hence at least one of the three axis of the world coordinate system matches this require-
ment. A natural choice is to select the coordinate axis whose angle with the rough estimated normal
is minimum. Another choice is that of a coordinate system for which the z axis is the rough estimated
normal. This choice may increase the ratio d /d and improve the results as explained in Section 4,
max min

--- Page 17 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 137
butthisrequiresthecalculationofanontrivialcoordinatetransformation. Theimprovementofthislatter
choicehasnotbeenstudiedexperimentallysincethefirstalternativeperformswellenoughtoconfirmthe
theoretical ordersofconvergence—cf. Section7.1.Forthechosencoordinates, wefilltheVandermonde
matrix.Thematrixisfurtherpre-conditioned asexplainedinSection3.3,withhtheaveragevalueofthe
norms(cid:2)(x ,y )(cid:2).ThecorrespondingsystemissolvedusingaSingularValueDecomposition.Practically,
i i
weusetheSVDoftheGnuScientificLibrary,available fromhttp://sources.redhat.com/gsl.
As pointed out in Section 3.3, the instability of the system is provided by the condition number.
Whenever degenerate configurations are detected, one can proceed as follows. For the approximation
strategy, one caneither keep the samedegree and increase thenumber of points used, orreuse thesame
points with a lower degree. These changes are likely to provide a non singular matrix M. In the worst-
case,adegreeonefittingmustbepossiblesincethenonlythreelinearlyindependent pointsarerequired!
For the interpolation, things are a bit more involved since reducing the interpolation degree requires
discarding some points. Selecting the subset yielding the best conditioning is a challenging problem
(Lassak, 1999; Higham, 1996). Notice also that for the approximation case, one can always retrieve a
solution from an under-constrained least-square problem by choosing, e.g., the solution vector of least
norm.
6.3. Retrievingdifferential quantities
We have already mentioned how to compute the normal. For the second order information, we com-
pute the Weingarten map of the surface (do Carmo, 1976, Section 3.3). Its eigenvalues (eigenvectors)
providetheprincipalcurvatures(directions) ofthesurface.Foraparameterizedsurfacegivenasaheight
function, one ends up with the formula given on Table 1. Notice that a basis of the tangent space asso-
ciated to the parameterization X(u,v)=(u,v,h(u,v)) consists of the two vectors X =(1,0,h )t and
u u
X =(0,1,h )t. A Gram–Schmidt orthonormalization of the basis {X ,X } gives another basis {Y,Z}
v v u v
of the tangent space. The diagonalization of the symmetric matrix representing the Weingarten map in
the basis {Y,Z} provides the expression of the principal curvature directions with respect to the {Y,Z}
orthonormal basis. Note that the sign of principal curvatures and hence the definition of minimal and
maximaldirectionsrelyontheorientationofthenormal.Aslongasourexperimentalstudyisperformed
on meshes of oriented surfaces, it is straightforward to find a global and coherent orientation of the
normals.
Table1
ComputingthematrixAoftheWeingartenmapofh(u,v)=a1u+a2v+a3u2+a4uv+a5v2
inthebasis{Xu,Xv }
E=1+a1 2 e=√
a1 2
2
+
a
1
3
+a2 2 (cid:22) (cid:23)(cid:22) (cid:23)
F =a2a1 f = √
a1 2+
a4
1+a2 2
At =−
f
ef
g
E
FG
F
−1
G=1+a 2 g= √ 2a5
2 a1 2+1+a2 2

--- Page 18 ---

138 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
7. Experimentalstudy
Wepresent results along twolines. First, weillustrate the convergence theorems proved inSection 4.
Second, wepresentillustrations onstandard computergraphics models.
7.1. Convergence estimatesonagraph
Setup. We illustrate the convergence properties with the smooth height fields f(u,v)=0.1e2u+v−v2
and g(u,v)=4u2 +2v2 defined over the parametric domain (u,v)∈ [−0.5,0.5]2—see Figs. 4 and 5.
At selected points on the graphs of f and g, we study the angle between the normals—more precisely
its sine sin(n,n˜), and the relative errors on principal curvatures. The values output by our algorithm
are compared against the exact values computed analytically with arbitrary precision under Maple, and
we report both average and maximum errors over sample points. More precisely, the graph of f or g
aresampledwithpoints p (x ,y ,f(x ,y )) wherethe(x ,y )liesonarandomly perturbed triangulated
i i i i i i i
squaregridofsideh.Thetriangulation israndomlyperturbed toavoidsimpledegenerate configurations
such as points lying on lines. The perturbation for the point (u,v) of the regular grid is the point (x,y)
withx=u+δh,y=v+δ(cid:6)handδ,δ(cid:6)randomnumbersin[0,0.9].Theconnectivityofthegraphisthatof
thetriangulated grid.Noticealsothatsincecalculations arecarriedoutonasurface patchparameterized
over a square grid, the direction chosen for the polynomial fitting is the z direction near the origin, and
eitherthex ory directions attheperiphery ofthedomain.
Theconvergence properties areillustrated (i)withrespect tothediscretization step hofthegrid—for
a given fitting degree n (ii) with respect to the fitting degree n—for a given discretization step h. We
compare the convergence properties ofthe interpolation and approximation schemes, for fitting degrees
ranging from one to nine. To quantify the observations, notice that according to Theorem 3, the error δ
onakth-order differential quantityisO(hn−k+1),hence
δ≈chn−k+1⇔log(1/δ)≈log(1/c)+(n−k+1)log(1/h)
(15)
log(1/δ) log(1/c)
⇔ ≈ +(n−k+1). (16)
log(1/h) log(1/h)
Convergence wrt to h. To highlight the convergence properties with respect to the size of the neigh-
borhood, we consider sequences of meshes with h→0, more precisely h ranges from 2−2 to 2−6. The
resultsforf andg beingalike,wefocusontheexponential f.
Curves of Figs. 6–11 show the average convergence behavior as the size of the neighborhood de-
creases. For a given degree and following Eq. (15), curves of Figs. 6–11 should be lines of slope
(n−k +1). The behavior is more regular for the approximation case, and the estimate is also better:
againofaboutadigitcanbeobserved between k estimatesofFigs.10and11.
max
Convergence wrt to the interpolation degree. For the convergence wrt to the interpolation degree—
withhfixed,wepresentresultsforthepolynomialg (cf.Figs.12–17).Conclusionsaresimilarforf (cf.
Figs. 24–29), but it should however be noticed that since the graph of g is more curvy than that of f, a
finer grid is required. (The higher the degree the more points required by the fitting ...andwecompute
localquantities!) Tobeprecise, weranexperiments withh=2−5 forf andh=2−7 forg.
Curves of Figs. 12 and 13 show the convergence as a function of the degree of the fitted polynomial
forafixedneighborhoodsize.AccordingtoEq.(16),curvesofthesefiguresshouldbelinesofunitslope,

--- Page 19 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 139
Fig.4.f(u,v)=0.1e2u+v−v2 . Fig.5.g(u,v)=4u2+2v2.
Fig.6.Exponentialmodel:Convergenceofthenormalesti- Fig.7.Exponentialmodel:Convergenceofthenormalesti-
matewrth,interpolationfitting. matewrth,approximationfitting.
Fig. 8. Exponential model: Convergence of the kmin esti- Fig. 9. Exponential model: Convergence of the kmin esti-
matewrth,interpolationfitting. matewrth,approximationfitting.

--- Page 20 ---

140 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
Fig.10.Exponentialmodel:Convergenceofthekmax esti- Fig.11.Exponentialmodel:Convergenceofthekmax esti-
matewrth,interpolationfitting. matewrth,approximationfitting.
Fig.12.Polynomialmodel:Convergenceofnormalandcur- Fig.13.Polynomialmodel:Convergenceofnormalandcur-
vatureestimateswrtthedegreeoftheinterpolationfitting. vatureestimateswrtthedegreeoftheapproximationfitting.
Fig.14.Polynomialmodel:Conditioningwrtthedegreeof Fig.15.Polynomialmodel:Conditioningwrtthedegreeof
theinterpolationfitting. theapproximationfitting.

--- Page 21 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 141
Fig.16.Polynomialmodel:Leastsingularvaluewrtthede- Fig.17.Polynomialmodel:Leastsingularvaluewrtthede-
greeoftheinterpolationfitting. greeoftheapproximationfitting.
Fig.18.Ellipticparaboloid.(Forcoloursseeon- Fig.19.Surfaceofrevolution.(Forcolourssee
lineversion.) onlineversion.)
with a vertical shift of one unit between normal and curvatures errors since curvature is a 2nd order
quantity whereas normal is 1st order. The gap between the average values and the maximal values is
greater for interpolation than forapproximation. Theother charts provide theconditioning and the least
singular value. Interpolation fitting is always more ill-conditioned than approximation, and closer to a
degenerateproblem(theleastsingularvalueisthedistanceofthematrixsystemtosingularmatrices).The
particularcaseofadegree7approximationrevealstobebadlyconditionedduetotheregularconnectivity
of the mesh used to find the neighbors: there is only one more point than for the degree 7 interpolation
fitting.
7.2. Illustrations
We depict differential informations on several models. When principal directions are displayed, blue
and red respectively correspond to k and k —that is k (cid:2) k —assuming the surface normal
min max min max

--- Page 22 ---

142 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
Fig.20.Michelangelo’sDavid:Principaldirectionsassociatedwithkmaxscaledbykmin.(Forcoloursseeonlineversion.)
Fig.21.Mechanic:Closeup.(Forcoloursseeonlineversion.)
pointstotheoutside.Todisplaypatchesofosculatingn-jets,itissufficienttoselectarectangulardomain
inparameterspace,sampleitwithagrid,andplotthecorresponding mesh.

--- Page 23 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 143
Fig.22.f(u,v)=u+3v+e2u+v−v2
with Fig.23.Principaldirectionsonanoisyellipsoid.
noise.(Forcoloursseeonlineversion.) (Forcoloursseeonlineversion.)
Consider the mesh models of th√e elliptic paraboloid z = 2x2 +y2—16k points, Fig. 18—and the
surfaceofrevolution z=0.1sin(10 (x2+y2))—8k points,Fig.19.Thearrangement ofcurvaturelines
providesinformations onumbilicalpoints—whereprincipal directionsarenotdefinedsincek =k .
min max
On the paraboloid, it is easy to follow curvature lines and see how they turn around an umbilic. The
surface of revolution provides an example of two parabolic lines (where the principal curvature k
max
vanishes), that is a curve along which the Gauss curvature K vanishes. This specific line splits the
Gauss
surface into elliptic (K >0) and hyperbolic regions (K <0). This model also illustrates a line
Gauss Gauss
ofumbilicalpointswhereminimumandmaximumprincipal directions swapeachover.
For a standard example from Computer Graphics, consider the Michelangelo’s David of Fig. 20. On
this modelof95922 pts, theprincipal curvatures provide meaningful information forshape perception.5
Tofinishup,weillustratetherobustnessofthemethod.Fig.21displaysrandompatchesontheMechanic
model, a 12,500 points model reconstructed from the output of a range scanner. In spite of the coarse
sampling, patches andprincipal directions providefaithfulinformation. Inasimilarvein,approximation
fitting with large neighborhoods Fig. 22 features a noisy triangulation of a graph. In spite of the severe
levelofnoise,surfacepatches averagetheavailable information. OnFig.23,anoisytriangulation ofan
ellipsoid, 15k points, principal directions areenoughprecisetorecognize anumbilic.
8. Conclusion
Estimatingdifferential quantities isofprimeimportanceinmanyapplications fromComputerVision,
Computer Graphics, Computer Aided Design or Computational Geometry. This importance accounts
for the many different differential estimators one can find in the vast literature of applied geometry.
Unfortunately, few of these have undergone a precise theoretical analysis. Another striking fact is that
estimates of second order differential quantities are always computed using degenerate conics/quadrics
withoutevenmentioning theclassification ofEuclidean conics/quadrics.
Themaincontribution ofthepaperistobridgethegapbetweenthequestionofestimatingdifferential
propertiesofarbitraryorderandmultivariateinterpolationandapproximation.Inmakingthisconnection,
5 Seealso(HilbertandCohn-Vossen,1952,p.197)aswellas(Hallinanetal.,1999).

--- Page 24 ---

144 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
Fig. 24. Exponential model: Convergence of normal and Fig. 25. Exponential model: Convergence of normal and
curvature estimateswrt the degree of the interpolation fit- curvatureestimateswrtthedegreeoftheapproximationfit-
ting. ting.
Fig.26.Exponentialmodel:Conditioningwrtthedegreeof Fig.27.Exponentialmodel:Conditioningwrtthedegreeof
theinterpolationfitting. theapproximationfitting.
Fig. 28. Exponential model: Least singular value wrt the Fig. 29. Exponential model: Least singular value wrt the
degreeoftheinterpolationfitting. degreeoftheapproximationfitting.

--- Page 25 ---

F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146 145
the use of jets—truncated Taylor expansions—is advocated. Precise asymptotic convergence rates are
provedforcurvesandsurfaces, bothfortheinterpolation andapproximation schemes.Tothebestofour
knowledge, theseresults areamong thefirstonesproviding accurate estimates fordifferential quantities
oforderthreeandmore.ExperimentalresultsforsurfacesofR3arereported.Theseexperimentsillustrate
theasymptoticconvergenceresults,butalsotherobustnessofthemethodsongeneralComputerGraphics
models.
Acknowledgements
TheauthorswishtothankP.CheninforpointingoutRef.(IsaacsonandKeller,1966),andthereview-
ersforproviding anindirect linktoRef.(Quarteroni andValli,1994).
AppendixA. Ausefullemma
Inseveral occasions wederivedifferential quantities—unit normal vectors, curvatures—as afunction
F ofthecoefficientsofthejet.Thefollowinglemmamakesiteasytoderivetheprecisiononthequantity
investigated ifF isregularenoughandifthesomeprecision ofthejet’scoefficientsisknown.
Lemma A.1. Define a kth-order differential quantity—for a curve or a surface—as a C1 function F of
theofthecoefficients ofthe k-jet oftheheight function. Alsoassume thatadegree nfitting yields apre-
cision A j =B j +O(hn+1−j),k=1,...,n for acurve, and A k−j,j =B k−j,j +O(hn−k+1),j =0,...,n,
k=0,...,nforasurface.
Then, a polynomial fitting of degree n estimates a kth-order differential quantity to accuracy
O(hn−k+1).
Proof. Theproofisthesameforcurvesandsurfaces,andweusethenotationscorresponding toacurve.
Tobeginwith,weperformasubstitution ontheerrortermsintheexpression of F:
(cid:1) (cid:3) (cid:1)(cid:1) (cid:3) (cid:3) (cid:1)(cid:1) (cid:3) (cid:3)
F (A i ) i=0,...,k =F B i +O(hn−i+1) i=0,...,k =F B i +O(hn−k+1) i=0,...,k .
Since F((B i ) i=0,...,k ) is a C1 function, and denoting DF p the differential of F at point p, an order one
Taylorformulayields:
(cid:1)(cid:1) (cid:3) (cid:3)
F B
+O(hn−k+1)
i
(cid:1) (cid:3)
i=0,...,k
(cid:1) (cid:3)
=F (cid:1) (B i ) i=0,...,k(cid:3) +DF (Bi +uO(hn−k+1))i=0,...,k O(hn−k+1),...,O(hn−k+1) t
=F (B i ) i=0,...,k +O(hn−k+1), u∈]0,1[. (cid:1)
References
Amenta,N.,Bern,M.,1999.SurfacereconstructionbyVoronoifiltering.DiscreteComput.Geom.22(4),481–504.
Borrelli,V.,Cazals,F.,2002.Ontheangulardefectoftriangulationsandthepointwiseapproximationofcurvatures.In:Curves
andSurfaces,StMalo,France.INRIAResearchReportRR-4590.

--- Page 26 ---

146 F.Cazals,M.Pouget/ComputerAidedGeometricDesign22(2005)121–146
Berger,M.,1987.Geometry,vols.1–2.Springer,Berlin.
Boissonnat, J.-D.,Flototto,J.,2002. Alocalcoordinatesystemonasurface. In:ACMSymposiumonSolidModelingSaar-
brücken.
Bruce,J.W.,Giblin,P.J.,1992.CurvesandSingularities,seconded.Cambridge.
Coatmelec,C.,1966.Approximationetinterpolationdesfonctionsdifferentiellesdeplusieursvariables.Ann.Sci.EcoleNorm.
Sup.83.
Cazals,F.,Pouget,M.,2004.Smoothsurfaces,umbilics,linesofcurvatures,foliations,ridgesandthemedialaxis:aconcise
overview.TechnicalReport5138,INRIA.
Ciarlet, P.G., Raviart, P.-A., 1972. General lagrange and hermit interpolation in Rn with applications to the finite element
method.Arch.RationalMesh.Engrg.46,177–199.
Cohen-Steiner,D.,Morvan,J.-M.,2003.RestrictedDelaunaytriangulationsandnormalcycle.In:ACMSymposiumonCom-
putationalGeometry.
Csàkàny,P.,Wallace,A.M.,2000.Computationoflocaldifferentialparametersonirregularmeshes.In:Cipolla,R.,Martin,R.
(Eds.),MathematicsofSurfaces.Springer,Berlin.
doCarmo,M.,1976.DifferentialGeometryofCurvesandSurfaces.Prentice-Hall,EnglewoodCliffs,NJ.
Isaacson,E.,Keller,H.B.,1966.AnalysisofNumericalMethods.Wiley,NewYork.
Goldfeather,J.,Interrante,V.,2004.Anovelcubic-orderalgorithmforapproximatingprincipaldirectionvectors.ACMTrans.
Graph.23(1),45–63.
Gasca,M.,Sauer,T.,2000.Polynomialinterpolationinseveralvariables.Adv.Comp.Math.12(4).
Golub,G.,vanLoan,C.,1983.MatrixComputations.JohnsHopkinsUniv.Press,Baltimore,MD.
Hilbert,D.,Cohn-Vossen,S.,1952.GeometryandtheImagination.Chelsea.
Hallinan, P.W.,Gordon, G., Yuille,A.L.,Giblin,P.,Mumford, D.,1999. Two- and Three-Dimensional Patternsof the Face.
A.K.Peters.
Higham,N.J.,1996.AccuracyandStabilityofNumericalAlgorithms.SIAM.
Lassak,M.,1999.Parallelotopesofmaximumvolumesinasimplex.Disc.Comput.Geom.21.
Lancaster,P.,Salkauskas,K.,1986.CurveandSurfaceFitting:AnIntroduction.Academic,NewYork.
Morvan,J.-M.,Thibert,B.,2002.Smoothsurfaceandtriangularmesh:Comparisonofthearea,thenormalsandtheunfolding.
In:ACMSymposiumonSolidModelingandApplications.
Meek, D., Walton, D., 2000. On surface normal and Gaussian curvature approximations given data sampled froma smooth
surface.ComputerAidedGeometricDesign17,521–543.
Petitjean,S.,2001.Asurveyofmethodsforrecoveringquadricsintrianglemeshes.ACMComput.Surv.34(2).
Porteous,I.,2001.GeometricDifferentiation,seconded.CambridgeUniversityPress,Cambridge.
Polthier,K.,Schmies,M.,1998.Straightestgeodesicsonpolyhedralsurfaces.In:Hege,H.C.,Polthier,K.(Eds.),Mathematical
Visualization.
Quarteroni,A.,Valli,A.,1994.NumericalApproximationofPartialDifferentialEquations.Springer,Berlin.
Sauer,T.,1995.Computationalaspectsofmultivariatepolynomialinterpolation.Adv.Comp.Math.3(3).
Shewchuk,J.R.,2002.Whatisagoodlinearelement?Interpolation,conditioning,andqualitymeasures.In:11thInternational
MeshingRoundtable,Ithaca,NY,USA.
Spivak,M.,1999.AComprehensiveIntroductiontoDifferentialGeometry,thirded.PublishorPerish.
Sauer,T.,Xu,Yuan,1995.Onmultivariatelagrangeinterpolation.Math.Comput.64.
Sander, P., Zucker, S., 1990. Inferring surface trace and differential structure from 3-D images. IEEE Trans. Pattern Anal.
MachineIntell.12(9),833–854.
Taubin, G., 1995. Estimating the tensor of curvature of a surface from a polyhedral approximation. In: Fifth International
ConferenceonComputerVision.
Watanabe,K.,Belyaev,A.G.,2001.Detectionofsalientcurvaturefeaturesonpolygonalsurfaces.In:Eurographics.```

---

## Section 1.5: PCPNet-Learning local shape properties from raw point clouds

Source File: PCPNet-Learning local shape properties from raw point clouds.txt

### Paper Content:

```
# PCPNet-Learning local shape properties from raw point clouds.pdf
# Converted: 2025-07-18 12:58:59
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/PCPNet-Learning local shape properties from raw point clouds.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/PCPNet-Learning local shape properties from raw point clouds.txt


--- Page 1 ---

EUROGRAPHICS2018/D.GutierrezandA.Sheffer Volume37(2018),Number2
(GuestEditors)
PCPNET
Learning Local Shape Properties from Raw Point Clouds
PaulGuerrero1 YanirKleiman2 MaksOvsjanikov2 NiloyJ.Mitra1
1UniversityCollegeLondon 2LIX,ÉcolePolytechnique,CNRS
Abstract
Inthispaper,weproposePCPNET,adeep-learningbasedapproachforestimatinglocal3Dshapepropertiesinpointclouds.
Incontrasttothemajorityofpriortechniquesthatconcentrateonglobalormid-levelattributes,e.g.,forshapeclassificationor
semanticlabeling,wesuggestapatch-basedlearningmethod,inwhichaseriesoflocalpatchesatmultiplescalesaroundeach
pointisencodedinastructuredmanner.Ourapproachisespeciallywell-adaptedforestimatinglocalshapepropertiessuchas
normals(bothunorientedandoriented)andcurvaturefromrawpointcloudsinthepresenceofstrongnoiseandmulti-scale
features.Ourmaincontributionsincludebothanovelmulti-scalevariantoftherecentlyproposedPointNetarchitecturewith
emphasisonlocalshapeinformation,andaseriesofnovelapplicationsinwhichwedemonstratehowlearningfromtraining
dataarisingfromwell-structuredtrianglemeshes,andapplyingthetrainedmodeltonoisypointcloudscanproducesuperior
resultscomparedtospecializedstate-of-the-arttechniques.Finally,wedemonstratetheutilityofourapproachinthecontextof
shapereconstruction,byshowinghowitcanbeusedtoextractnormalorientationinformationfrompointclouds.
CCSConcepts
•Computingmethodologies → Point-basedmodels;Shapeanalysis;•Computersystemsorganization → Neuralnetworks;
1. Introduction
A fundamental problem in shape analysis is to robustly estimate
localshapepropertiesdirectlyfromrawpointclouds.Althoughthe
problemhasbeenextensivelyresearched,aunifiedmethodthatis
robustundervariousdataimperfections(e.g.,varyingnoiselevel,
samplingdensity,levelofdetails,missingdata)remainselusive.
In the context of continuous surfaces, local surface properties
suchasnormalsandcurvatureareclassicaldifferentialgeometric
notions [dC76] and are known to uniquely characterize local ge-
ometryuptorigidtransformations.Inthecontextofdiscretesur-
faces(e.g.,polygonalmeshes),theestimationmethodsfallbroadly
intwogroups:evaluatenormal/curvaturesusingdiscretedifferen-
tialgeometryoperators,oruselocalprimitivefittingand‘readoff’
normal/curvaturesusingattributesfromthefittedprimitives.
Inthecaseofpointclouds,afirstchallengeisthelackofconnec-
tivityinformation.Further,inrealacquisitionsetups(e.g.,usinga
depthcamera),thedataisoftennoisy,incomplete,andtypicallyex-
hibitvaryingsamplingdensity(basedonscanningdirection).Fig-
ure1showsatypicalrawpointcloudscan.Theusualapproachis
tofirstlocallydefineaneigborhoodaroundeachpointusingar-
radiusball,andthenfitalocalprimitive(e.g.,planeorquadricsur-
face)totheneighboringpoints.Althoughthemethodissimpleand
regularly used, in practice, the challenge is to appropriately pick
thevariousparameters.Forexample,whileitisdesirabletopick
egamI
DBGR
slamron
duolc
tniop
tupni
erutavruc
naem
DOI: 10.1111/cgf.13343
Figure1:Ourmethodcanjointlyestimatevarioussurfaceproper-
tieslikenormalsandcurvaturefromnoisypointsets.Thissceneis
fromtheNYUdataset[NSF12]withaddedGaussiannoiseandthe
bottom row shows properties estimated with PCPNET. Note that
ourmethodwasnotspecificallytrainedonanyRGBDdataset.
asmallvalueofrtoavoidsmoothingoutsharpfeatures,thesame
parametercanleadtounstableresultsinthepresenceofnoise.Sim-
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohn
Wiley&SonsLtd.PublishedbyJohnWiley&SonsLtd.

--- Page 2 ---

76 P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
ilarly,alargevalueofr canover-smoothlocalfeatures,although thedirectionofminimalvarianceasthenormal.Itisextremelyef-
theestimatedvaluestendtobemorestableundernoise.Thekey ficient,hasbeenanalyzedextensively,anditsbehaviorinthepres-
challengeforsuchfitting-basedmethodsistomanuallysetthevar- ence of noise and curvature is well-understood [MNG04]. At the
iousparametersdependingonthe(unknown)featuredistributionin sametime,itsuffersfromseveralkeylimitations:firstitdepends
shapesand(unknown)noisemarginsintherawscans. heavilyonthechoiceoftheneighborhoodsize,whichcanleadto
oversmoothing for large regions or sensitivity to noise for small
Inthiswork,weproposeadata-drivenapproachforestimating
ones.Second,itisnotwell-adaptedtodealwithstructurednoise,
localshapepropertiesdirectlyfromrawpointclouds.Inabsenceof
andeventhetheoreticalanalysisdoesnotapplyinthecasemulti-
localconnectivityinformation,onecouldtrytovoxelizetheoccu-
ple,interacting,noisyshaperegions,whichcanleadtonon-local
piedspace,orestablishlocalconnectivityusingknearestneighbor
effects.Finally,itdoesnotoutputthenormalorientationsincethe
graph.However,suchdiscretizationsintroduceadditionalquantiza-
eigenvectorsofthecovariancematrixaredefinedonlyuptosign.
tionerrorsleadingtopoor-qualitylocalestimates.Inaninteresting
recentworkonnormalestimation,Boulchetal.[BM16]generate Severalmethodshavebeenproposedtoaddresstheselimitations
asetoflocalapproximatingplanes,andthenproposeadata-driven by both designing more robust estimation procedures, capable of
denoisingapproachintheresultantHough-spacetoestimatenor- handlingmorechallengingdata,andbyproposingtechniquesfor
mals. In Section 5, we demonstrate that even such a specialized estimating normal orientation. The first category includes more
approachleadstoerrorsonrawpointcloudsduetoadditionalam- robust distance-weighted approaches [PKKG03], methods such
biguitiesintroducedbythechoiceoftherepresentation(RANSAC- as osculating jets [CP03] based on fitting higher-order primitives
likeHoughspacevoting). robustly, the algebraic point set surface approach based on fit-
ting algebraic spheres [GG07], approaches based on analyzing
Inspired by the recently introduced PointNet architec-
the distribution of Voronoi cells in the neighborhood of a point
ture [QSMG17] for shape classification and semantic seg-
[AB99,ACSTD07,MOG11], and those based on edge-aware re-
mentation,weproposeanovelmulti-scalearchitectureforrobust
sampling [HWG∗13] among many others. Many of these tech-
estimation of local shape properties under a range of typical im-
niquescomewithstrongtheoreticalapproximationandrobustness
perfectionsfoundinrawpointclouds.Ourkeyinsightisthatlocal
guarantees(seee.g.,Theorem5.1in[MOG11]).However,inprac-
shapepropertiescanberobustlyestimatedbysuitablyaccounting
ticetheyrequireacarefulsettingofparameters,andoftendepend
forshapefeatures,noisemargin,andsamplingdistributions.How-
onspecialtreatmentinthepresenceofstrongorstructurednoise.
ever,sucharelationiscomplex,anddifficulttomanuallyaccount
Unfortunately,thereisnouniversalparametersetthatwouldwork
for. Hence, we propose a data-driven approach based on local
forallsettingsandshapetypes.
pointneighborhoods,whereinwetrainaneuralnetworkPCPNET
to directly learn local properties (normals and curvatures) using
Normal orientation. These challenges are arguably even more
groundtruthreferenceresultsunderdifferentinputperturbations.
pronounced when trying to estimate oriented normals, since they
WeevaluatedPCPNETonvariousinputpointcloudsandcom- depends on both local (for direction) and global (for orientation)
paredagainstarangeofalternatespecailizedapproaches.Ourex- shapeproperties.Earlymethodshavereliedonsimplegreedyori-
tensivetestsindicatethatPCPNETconsistentlyproducessuperior entation propagation, as done, for example in the seminal work
normal/curvatureestimatesonrawpointcloudsinchallengingsce- on shape reconstruction [HDD∗92]. However, these approaches
narios.Specifically,themethodisgeneral(i.e.,thesamearchitec- can easily fail in the presence of noisy normal estimates or high
ture simultaneously produces normal and curvature estimates), is complexity shapes. As a result, they have has been extended
robusttoarangeofnoisemarginsandsamplingvariations,andpre- significantly in recent years both through more robust estimates
servesfeaturesaroundhigh-curvatureregions.Finally,inaslightly [HLZ∗09],whichhavealsobeenadaptedtohandlelargemissing
surprisingresult,wedemonstratethatacascadeofsuchlocalanal- regionsthroughpointskeletonestimation[WHG∗15],andglobal
ysis(viathenetworkdepth)canlearntorobustlyextractoriented techniques based on signed distance computations [MDGD∗10],
normals,whichisaglobalproperty(similarpatchesmayhaveop- among others. Nevertheless, reliably estimating oriented normals
positeinside/outsidedirectionsindifferentshapes).CodeforPCP- remains challenging especially across different noise levels and
NET isavailableatgeometry.cs.ucl.ac.uk/projects/ shapestructures.
2018/pcpnet/.
Curvatureestimation. Similarlytosurfacenormals,alargenum-
berofapproacheshasalsobeenproposedforestimatingprincipal
2. RelatedWorks curvatures.Severalsuchtechniquesrelyonestimatingthenormals
first and then robustly fitting oriented curvatures [HM02,LP05],
2.1. Estimatinglocalattributes
whichinturnleadtoestimatesofprincipalcurvaturevalues.Ina
Normal estimation. Estimating local differential information similarspirit,curvaturescanbecomputedbyconsideringnormal
such as normals and curvature has a very long history in geome- variation in a local neighborhood [BC94,KNSS09]. While accu-
tryprocessing,motivatedinlargepartbyitsdirectutilityinshape rateinthepresenceofcleandata,thesetechniquesrelyonsurface
reconstruction. Perhaps the simplest and best-known method for normals,andanyerrorisonlybeexacerbatedbythefurtherpro-
normal estimation is based on the classical Principal Component cessing.Onepossiblewaytoestimatethenormalandthesecond
Analysis(PCA).Thismethod,usedasearlyas[HDD∗92],isbased fundamentalformjointlyisbydirectlyfittinghigherorderpolyno-
onanalyzingthevarianceinapatcharoundapointandreporting mialasisdoneforexampleinthelocaljetfittingapproach[CP03].
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 3 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
points ... points ...
FNN FNN FNN
3 64 64 64 64 128
...
H
H
H
... ...
STN STN
1 2 output
single h (p ) properties input l j
patch pj∈ Pr i
1 2 3
per-point 512 256
feature vector per-point per-patch
feature vector feature vector
points
FNN ...
points ...
...
H
H
H
...
...
...
77
Single
scale
r ... ... ... ... ... ...
P
i
Multi-scale
STN
patches with 1
multiple radii ...
h (p ) output
around the l j
same p
r
oint
... ...
pj∈ Pr
i
properties
P 1
i
P r 2 ... ... (same as above) H 3
i 512 256
per-point per-patch
feature vector H feature vector
Figure2:Weproposetwoarchitecturesforournetwork,asingle-andamulti-scalearchitecture.Inbotharchitectures,thenetworklearnsasetofpoint
functionshinthelocalspaceofapointsetpatch.Similartoadensityestimator,pointfunctionvaluesateachpointinthepatcharesummedup.Theresulting
per-patchfeaturevectorHcanbeusedtoregresspatchproperties.STNsarespatialtransformernetworksthatapplyeitheraquaternionrotation(STN1)ora
fulllineartransformation(STN2).FNNsarefullyconnectednetworks.Inthemulti-scalearchitecturepatchesofdifferentscalearetreatedasasinglelarger
patchbythepointfunctions.Thefinalsumisperformedseparatelyforeachpatch.
Thisapproachisfastandreliableforwell-structuredpointsets,but network for learning normal directions from a large ground-truth
alsorequiressettingascaleparameterwhichischallengingtofind corpus. While not projection based, this method still relies on a
foralltypesofnoiseorshapetypes.InSection5,wecompareour 2D-based representation for learning and moreover loses orienta-
PCPNET tojetfittinginawidevarietyofsettingsandshowthat tioninformation,whichcanbecrucialinpractice.Morerecently,
ourdata-drivenmethodcanproducesuperiorresultswithoutman- the PointNet architecture [QSMG17] has been designed to oper-
ualintervention,attheexpenseofextensiveoff-linelearning. ateon3Dpointcloudsdirectly.Thisapproachisveryversatileas
it allows to estimate properties of 3D shapes without relying on
2D projections or volumetric approximations. The original archi-
2.2. DeepLearningforGeometricData
tectureispurelyglobal,takingtheentirepointcloudintoaccount,
Intherecentyears,severalmethodshavebeenproposedforana- primarilytargetingshapeclassificationandsemanticlabeling,and
lyzingandprocessing3Dshapesbybuildingonthesuccessofma- hassincethenbeenextendedtoahierarchicalapproachinavery
chinelearningmethods,andespeciallythosebasedondeeplearn- recentPointNet++[QYSG17],whichisdesignedtobettercapture
ing (see, for example, recent overviews in [MRB∗16,BBL∗17]). localstructuresinpointclouds.
Thesemethodsarebasedonthenotionthatthesolutionstomany
OurapproachisbasedontheoriginalPointNetarchitecture,but
problems in geometric data analysis can benefit from large data
ratherthanusingitforestimatingglobalshapepropertiesforshape
repositories. Many learning-based approaches are aimed at esti-
classification or semantic labeling, as has also been the focus of
matingglobalpropertiesfortaskssuchasshapeclassificationand
PointNet++[QYSG17],weadaptitexplicitlyforestimatinglocal
often are based either on 2D projections (views) of 3D shapes
differentialpropertiesincludingorientednormalsandcurvature.
or on volumetric representations (see, e.g., [QSN∗16] for a com-
parison). However, several methods have also been proposed for
shapesegmentation[GZC15,MGA∗17]andevenshapecorrespon- 3. Overview
dence[MBBV15,WHC∗16,BMRB16],amongmanyothers.
Estimatinglocalsurfaceproperties,suchasnormalsorcurvature,
Although most deep learning-based techniques for 3D shapes fromnoisypointcloudsisadifficultproblemthatistraditionally
heavilyexploitthemeshstructureespeciallyfordefiningconvolu- solvedbyextractingthesepropertiesfromsmoothsurfacesfittedto
tionoperations,fewapproacheshavealsobeenproposedtodirectly localpatchesofthepointcloud.Howeverthesemethodsaresen-
operateonpointclouds.Perhapsthemostcloselyrelatedtooursare sitive to parameter settings such as the neighborhood size, or the
recentapproachesofBoulchetal.[BM16]andQietal.[QSMG17]. degreeofthefittedsurface,thatneedtobecarefullysetinpractice.
Theformerproposeanarchitectureforestimatingunorientednor- Instead,weproposeanalternativeapproachthatisrobusttoawide
malsonpointcloudsbycreatingaHoughtransform-basedrepre- range of conditions with the same parameter settings, based on a
sentationof localshapepatchesand usingaconvolutional neural deepneuralnetworktrainedonarelativelysmallsetofshapes.
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 4 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
RMS angle error ours jet pca Boulch et al. PointNet
absolute (deg.) relative ss ms ss+cms+c smallmedlarge smallmedlarge ss ms-3ms-5 train test
35 2 9.66 9.62 11.98 10.10 12.18 12.35 17.35 12.78 12.29 16.77 10.23 10.02 10.22 24.08 57.98
11.46 11.37 12.94 11.66 16.79 12.84 17.42 16.33 12.87 16.87 11.62 11.51 11.80 24.30 58.08
18.26 18.87 19.11 18.94 44.53 18.33 18.85 43.67 18.38 18.94 22.66 23.36 24.44 25.93 58.18
22.80 23.28 23.67 23.48 53.77 27.68 23.41 53.23 27.52 23.50 33.39 36.70 38.17 29.79 58.28
13.42 11.70 14.83 12.82 14.36 13.13 17.80 14.85 12.81 17.26 11.02 10.67 10.95 30.64 60.62
5 1 11.74 11.18 14.38 12.07 14.66 13.39 19.29 15.33 13.66 19.87 12.47 11.95 12.08 24.42 60.45
0 14.56 14.34 16.15 14.85 26.05 16.29 19.02 26.03 16.26 18.87 16.90 17.37 17.94 26.53 58.93
0 noise strength 0.1 0 noise strength 0.1
mrofinu-non
0
esion
+
gnilpmas
.gva
78
pca & jet, small
pca & jet, medium
pca & jet, large
Boulch et al.
m1m3m5
ours, single-scale
ours, multi-scale
ours, s.s. joint with curv.
ours, m.s. joint with curv.
Figure3:ComparisonoftheRMSnormalangleerrorofourmethod(ss:singlescale,ms:mult-scale,+c:jointnormalsandcurvature)togeometricmethods
(jetfittingandPCA)withthreepatchsizesandtwodeeplearningmethods(Boulchetal.[BM16]andPointNet[QSMG17]).Notethatgeometricmethods
requirecorrectparametersettings,suchasthepatchsize,toachievegoodresults.Ourmethodgivesgoodresultswithouttheneedtoadjustparameters.
Given a point cloud P={p 1,...,p N }, our PCPNET network vectorsarebasedonthepositionofasinglepointonly,anddonot
(see Figure 2 for an overview) is applied to local patches of this includeanyneighborhoodinformation.Thisrelianceononlyeither
pointcloudP r
i
∈P,centeredatpoints p iwithafixedradiusrpro- fullylocalorfullyglobalfeaturevectorsmakesithardtoestimate
portional to the point cloud’s bounding box extent. We then esti- propertiesthatdependonlocalneighborhoodinformation.
matelocalshapepropertiesatthecentralpointsofthesepatches.
Instead, we propose computing local feature vectors that de-
The architecture of our network is inspired by the recent Point-
scribe the local neighborhood around a point. These features are
Net [QSMG17], adapted to local r-neighborhood patches instead
better suited to estimate local surface properties. In this section,
oftheentirepointcloud.Thenetworklearnsasetofknon-linear
weprovideanalternativeanalysisofthePointNetarchitectureand
functions in the local patch neighborhoods, which can intuitively
showavariationofthemethodthatcanbeappliedtolocalpatches
beunderstoodasasetofdensityestimatorsfordifferentregionsof
insteadofgloballyontheentirepointcloudtogetastrongperfor-
thepatch.Thesegiveak-dimensionalfeaturevectorperpatchthat
manceincreaseforlocalsurfacepropertyestimation,outperform-
canthenbeusedtoregressvariouslocalfeatures.
ingthecurrentstate-of-theart.
4. Algorithm
4.1. Pre-processing
O fro u m r g a oa p l oi i n n t t c h l i o s u w d o th rk at i a s p t p o ro r x e i c m on at s e tr ly uc s t a l m oc p a le l s su an rfa u c n e kn p o ro w p n er s t u ie r s - GivenapointcloudP={p 1,...,p N },alocalpatchP r i iscentered
face. In real-world settings, these point clouds typically originate
atpointp iandcontainsallpointswithindistancerfromthecenter.
Ourtargetforthispatcharelocalsurfacepropertiessuchasnormal
fromscansorstereoreconstructionsandsufferfromawiderange
of deteriorating conditions, such as noise and varying sampling n i andprincipalcurvaturevaluesκ1 i andκ2 i atthecenterpoint p i.
Toremoveunnecessarydegreesoffreedomfromtheinputspace,
densities.Traditionalgeometricapproachesforrecoveringsurface
wetranslatethepatchtotheoriginandnormalizeitsradiusmul-
propertiesusuallyperformreasonablywellwiththecorrectparam-
tiplyingwith1/r.Sincethecurvaturevaluesdependonscale,we
etersettings,butfindingthesesettingsisadata-dependentandoften
transformoutputcurvaturestotheoriginalscaleofthepointcloud
difficulttask.Thesuccessofdeep-learningmethods,ontheother
bymultiplyingwithr.Ournetworktakesafixednumberofpoints
hand, is in part due to the fact that they can be made robust to
as input. Patches that have too few points are padded with zeros
awiderangeofconditionswithasinglehyper-parametersetting,
(thepatchcenter),andwepickarandomsubsetforpatcheswith
seemingly a natural fit to this problem. The current lack of deep
toomanypoints.
learning solutions may be due to the difficulty of applying deep
networkstounstructuredinputlikepointclouds.Simplyinputting
pointsasalistwouldmakethesubsequentcomputationsdependent 4.2. Architecture
ontheinputordering.
Our single-scale network follows the PointNet architecture, with
Apossiblesolutiontothisproblemwasrecentlyproposedunder two changes: we constrain the first spatial transformer to the do-
thenameofPointNetbyQietal.[QSMG17],whoproposecom- main of rotations and we exchange the max symmetric operation
bining input points with a symmetric operation to achieve order- withasum.AnoverviewofthearchitectureisshowninFigure2.
independence.However,PointNetisappliedgloballytotheentire Wewillnowprovideintuitionforthischoiceofarchitecture.
pointcloud,andwhiletheauthorsdodemonstrateestimationoflo-
calsurfacepropertiesaswell,theseresultssufferfromtheglobal Quaternionspatialtransformer. Thefirststepofthearchitecture
natureofthemethod.PointNetcomputestwotypesoffeaturesina transforms the input points to a canonical pose. This transforma-
pointcloud:asingleglobalfeaturevectorfortheentirepointcloud tion is optimized for during training, using a spatial transformer
andasetoflocalfeaturesvectorsforeachpoint.Thelocalfeature network [JSZK15]. We constrain this transformation to rotations
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 5 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
ours jet pca
small med large small med large
33.41 26.61 44.62 55.58 40.80 41.79 41.94
34.12 42.22 36.87 61.51 42.47 41.56 41.60
40.39 70.59 47.26 50.00 65.52 47.13 55.51
44.63 90.68 50.23 52.82 87.37 46.86 49.52
35.47 34.20 66.40 45.70 46.32 52.09 68.99
37.97 31.29 34.31 57.62 30.84 34.16 55.96
37.67 49.27 46.61 53.87 52.22 43.93 52.25
mrofinu-non
0
esion
+
gnilpmas
.gva
79
RMS angle error mean fraction
of flipped normals ms
absolute (deg.) relative relative
90 2 2
ours, multi-scale
pca & jet, small
pca & jet, medium
pca & jet, large 1 1
10 Boulch et al.
0 0.8 0.8
0 noise strength 0.1 0 noise strength 0.1 0 noise strength 0.1
Figure4:Comparisonofourapproachfororientednormalestimationwiththebaselinejet-fittingandPCAcoupledwithMST-basednormalorientation
propagationinapost-processingstep.WeshowtheRMSangleerror,therelativeerrorcomparedtoourperformance,aswellasrelativefractionofflipped
normalsinothermethodsacrossdifferentlevelsofnoise.Notethattheerrorsinorientednormalestimationarehighlycorrelatedwiththenumberoforientation
flips,suggestingthepost-processingstepasthemainsourceoferror.
byoutputtingaquaternioninsteadofa3×3matrix.Thisisdone patch.Thisintroducesglobalinformationintothepointfunctions,
fortworeasons:First,unlikeinsemanticclassification,ouroutputs increasingtheperformanceofthenetwork.
aregeometricpropertiesthataresensitivetotransformationsofthe
patch.Thiscausedunstableconvergencebehaviorinearlyexperi- Output regression. In a trained model, the patch feature vector
mentswherescalingwouldtakeonextremevalues.Rotation-only Hj=(H 1(P r i ),...,H k(P r i ))providesarichdescriptionofthepatch.
constraints stabilized convergence. Second, we have to apply the Varioussurfacepropertiescanberegressedfromthisfeaturevec-
inverse of this transformation to the final surface properties and tor.Weuseathree-layerfullyconnectednetworktoperformthis
computingtheinverseofarotationistrivial. regression.
Pointfunctionsandsymmetricoperation. Oneimportantprop- 4.3. Multi-scale
ertyofthenetworkisthatitshouldbeinvarianttotheinputpoint
Wewillshowintheresults,thatthearchitecturepresentedaboveis
ordering. Qi et al. [QSMG17] show that this can be achieved by
veryrobusttochangesinnoisestrengthandsampledensity.Forad-
applyingasetoffunctions{h 1,...,h
k
}withsharedparametersto
ditionalrobustness,weexperimentedwithamulti-scaleversionof
each point separately and then combine the resulting values for
thearchitecture.Insteadofusingasinglepatchasinput,weinput
eachpointusingasymmetricoperation:
H l(P r i)= ∑ h l(p j). t n h o r r e m e a p l a iz tc a h ti e o s n P o r i f 1, o P ur r I 2 p a a n tc d he P s r I , 1 th w e i s th e d ar if e fe s r c e a n le t d ra t d o ii t . h D e u sa e m to e t s h i e ze s , c b a u le t
pj∈ Pr i containdifferentlysizedregionsofthepointcloud.Thisallowsall
H l(P r i )isthenafeatureofthepatchandh l arecalledpointfunc- pointfunctionstofocusonthesameregion.Wealsotriplethenum-
tions; they are scalar-valued functions, defined in the in the local berofpointfunctions,butapplyeachfunctiontoallthreepatches.
coordinateframeofthepatch.ThefunctionsH canintuitivelybe Thesumhowever,iscomputedovereachpatchseparately.Thisre-
l
understoodasdensityestimatorsforaregiongivenbyh.Theirre- sultsinnine-foldincreaseinpatchfeaturesH,whicharethenused
l
sponseisstrongerthemorepointsareinthenon-zeroregionofh. toregresstheoutputproperties.Figure2illustratesasimpleversion
l
Comparedtousingthemaximumassymmetricoperation,aspro- ofthisarchitecturewithtwopatches.
posedbyQietal.,oursumdidnothaveasignificantperformance
difference; however we decided to use the sum to give our point 5. EvaluationandDiscussion
functionsasimpleconceptualinterpretationasdensityestimators.
Inthissection,weevaluateourmethodinvarioussettingsandcom-
Thepointfunctionsh arecomputedas:
l
pare it to current state of the art methods. In particular, we com-
(cid:0) (cid:1)
h l(p j)=(FNN 2 ◦STN 2) g 1(p j),...,g 64(p j) , pareourcurvatureestimationtotheosculatingjetsmethod[CP03]
and a baseline PCA method, and the normal estimation addition-
whereFNN isathree-layerfully-connectednetworkandSTN is
2 2
ally to PointNet [QSMG17] and the normal estimation method
thesecondspatialtransformer.Thefunctionsgcanbeunderstood
ofBoulchetal.[BM16].Wetesttheperformanceofthesemeth-
asalesscomplexsetofpointfunctions,sincetheyareatashallower
odsonshapeswithvariousnoiselevelsandsamplingrates,asde-
depthinthenetwork.Theyarecomputedanalogoustoh.
scribedbelow.ForthemethodofBoulchetal.,weusethecodeand
trainednetworksprovidedbytheauthors.PointNethascode,butno
Secondspatialtransformer. Thesecondspatialtransformerop-
(cid:0) (cid:1) pre-trainednetworkfornormalestimationavailable,sowere-train
erates on the feature vector gj = g 1(p j),...,g 64(p j) , giving a
onourdataset.Fortheothermethodsweusetheimplementation
64×64transformationmatrix.Someinsightcanbegainedbyinter-
withinCGAL[CGA].
pretingthetransformationasafullyconnectedlayerwithweights
that are computed from the feature vectors g of all points in the Bytrainingfordifferentoutputvalues,PCPNetcanbetrainedto
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 6 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
tes
gniniart
tes
tset
80
Figure5:Ourshapedataset.Wetrainandtestonamixofsimpleshapes
likethesphereortheboxes,andmoredetailedshapeslikestatuesorfig-
ures.Thetestsetadditionallycontainsthreeanalyticshapesnotshownhere
(sphere,cylinderandathinsheetshowninFigure8).
Figure 6: Afewpointcloudsfromourdatasetwiththeirvariants.The
baseshapeisshownintheleftcolumn,followedbyvariantswiththreenoise
output3Dnormals,curvatures,orbothatthesametime.Weeval- levels(0.0025,0.012and0.024),andtwonon-uniformsamplingschemes.
uate the performance of the network when trained separately for Shapesarecoloredaccordingtotheirunorientednormals.
normalsandcurvaturesorjointlyusingasinglenetworkforboth
outputs. We also compare the performance of a network trained
point clouds with added gaussian with a standard deviation of
withbothcurvaturevaluesagainstnetworkstrainedforindividual
0.0025, 0.012, and 0.024 of the length of the bounding box di-
curvaturevalues.
agonal of the shape. Examples are shown in Figure 6. The noise
is added to the point position but no change is made to ground
5.1. Dataset truthnormalsandcurvatures,asthegoalistorecovertheoriginal
One of the advantages of training the network on point cloud normals and curvatures of the mesh. In total, our training dataset
patchesratherthanpointcloudsofcompleteshapesisthatwecan contains4variantsofeachmesh,or32pointcloudsintotal.
sample a large number of patches from each shape. Two patches Ourtestsetcontains18shapeswithamixoffigurinesandman-
withnear-bycenterpointsmaystillbesignificantlydifferentfrom made objects. We also include 3 shapes that are constructed and
each other in terms of the type of structure they contain, such as sampledanalyticallyfromdifferentiablesurfaceswhichhavewell-
edges,corners,etc.Thus,wecanusearelativelysmalldatasetof definednormalsandcurvatureseverywhere.Fortheseshapes,the
labeledshapestotrainournetworkeffectively.Figure5showsthe normalsandcurvaturesarecomputedinanexactmannerforeach
shapesinourdataset. sampled point, rather than being approximated by the faces and
Our training dataset contains 8 shapes, half of which are man verticesofamesh.
madeobjectsorgeometricconstructswithflatfacesandsharpcor-
In addition to the three noise variants we described above, we
ners, and the other half are scans of figurines (bunny, armadillo,
generate two point clouds for each mesh that are sampled with
dragonandturtle).Allshapesaregivenastriangularmeshes.We
varyingdensity,suchthatcertainregionsoftheshapearesparsely
sample the faces of each mesh uniformly with 100000 points to
sampledcomparedtootherregions(seeFigure6).Thisgivesusa
generateapointcloud.Eachofthese100000pointscanbeusedas
totalof90pointcloudsinthetestset.
apatchcenter.Welabeleachpointwiththenormalofthefaceit
wassampledfrom.Wealsoestimatetheκ1 andκ2 curvatureval-
ues at the vertices and interpolate them for each sampled points. 5.2. EvaluationMetrics
Thecurvatureestimationisperformedusingthemethodsuggested
As loss function during training, and to evaluate our results, we
by Rusinkiewicz in [Rus04] (the code was provided by authors
computethedeviationofthepredictednormalsand/orcurvatures
of[SF15]).
from the ground truth data of the shape. For normals, we experi-
For each mesh we generate a noise-free point cloud and three mentedwithboththeEuclideandistanceandtheangledifference
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 7 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
k1: ours k1: jet k2: ours k2: jet
ms ms+nsmallmedlarge ms+nsmallmedlarge
mrofinu-non
0
esion
+
gnilpmas
.gva
mrofinu-non
0
esion
+
gnilpmas
.gva
81
Principal Curvature k1 Principal Curvature k2
RMS normalized error RMS normalized error
102 1.35 1.36 2.19 6.55 2.97 102 0.54 1.61 2.91 1.59
ours, multi-scale 1.53 1.48 57.35 6.68 2.90 ours, m.s. joint with normals 0.53 25.83 2.98 1.53
ours, m.s. joint with normals jet, small
jet, small 1.49 1.46 60.91 9.86 3.30 101 jet, medium 0.51 22.27 4.88 1.73
jet, medium
jet, large
jet, large 1.41 1.59 49.40 10.78 3.58 0.53 18.17 5.22 1.84
101
1.37 1.32 2.07 1.40 1.53 0.61 2.04 0.79 0.83
100
1.09 1.09 2.04 1.54 1.89 0.55 1.92 0.89 1.09
0 1.37 1.38 28.99 6.13 2.69 0 0.54 11.97 2.94 1.43
0 noise strength 0.1 0 noise strength 0.1
Figure7:Estimationofbothprincipalcurvaturevaluescomparedtojetfitting.Thelowperformanceofjetfittingshowsthatcurvatureestimationonpoint
cloudsisachallengingproblem.Ourmethodachievessignificantlybetterperformance,oftenmorethanoneorderofmagnitude.PleaseseeSection5.2fora
descriptionoftheerrormetric.
betweentheestimatednormalandgroundtruthnormal.Themean TitanXPascal.Afullrandomizationofthedataset,mixingpatches
squareEuclideandistanceoverabatchgaveslightlybetterperfor- ofdifferentshapesineachbatch,wasvitaltoachievestablecon-
mance during training, but for better interpretability, we use the vergence. All our training was performed in PyTorch [pyt] using
RMSangledifferencetoevaluateourresultsonthetestset. stochastic gradient descent with batch size 64, a learning rate of
For curvatures, we compute the mean-square of the following
10− 4and0.9momentum.
rectifiederrorfortrainingandtheRMSforevaluation.Therectified Forevaluation,weselectarandomsubsetof5000pointsfrom
errorL(κ˜)isdefinedas: eachshapeinthetestset,andoutputtheerrorofourmethodandthe
(cid:12) (cid:12) baseline methods over this subset. For the CGAL baseline meth-
(cid:12) κ˜−κ (cid:12)
L(κ˜)=(cid:12) (cid:12), ods [CGA], we use different patch sizes, where the size is deter-
(cid:12)max(|κ|,1)(cid:12)
mined by the number of nearest neighbors in the patch. For the
whereκ˜ istheestimatedcurvatureandκisthegroundtruthcur- small patch size we use the recommended setting of 18 nearest
vature.Thiserrorisrelativetothemagnitudeofthegroundtruth neighbors,andweincreasethenumberofnearestneighborsbythe
curvature,sinceerrorsaroundhighcurvatureareasareexpectedto same ratio as the area covered by our patches. This amounts to
belarger. 112and450nearestneighborsforthemediumandlargepatches,
respectively.ForBoulchetal.[BM16],weusethesingle-scale,3-
5.3. TrainingandEvaluationSetup scaleand5-scalenetworksprovidedbytheauthors.
Ournetworkcanbetrainedseparatelyfornormalsandcurvatures,
orjointlytooutputbothatthesametime.Inthejointnetwork,the 5.4. Results
lossfunctioniscombinedduringtraining.Weexperimentwithboth
Figure 3 shows the comparison of unoriented normal estimation
variants to test whether the information about the curvatures can
usingthemethodsdiscussedabove.Inthisexperiment,weconsider
helpnormalestimationandviceversa,sincethetwoarerelated.We
eithertheoutputnormalortheflippednormal,whicheverhasthe
train single-scale and multi-scale networks for each variant, each
lowesterror.Inthetopsectionofthetable,weshowtheresultsfor
with1024pointfunctionsh.
varyinglevelsofnoise,fromzeronoisetohighnoise.Thetworows
Thevariantsofournetworkaretrainedbyselectingpatchescen- inthemiddleshowtheresultsforpointcloudswithnon-uniform
teredrandomlyatoneofthe100Kpointsineachpointcloud.The samplingrate.Ineachofthecategoriesweshowtheaverageforall
radius of a patch is relative to the length of the bounding box of 20shapesinthecategory.Thelastrowshowstheglobalaverage
thepointcloud.Thesingle-scalenetworksaretrainedwithapatch erroroverallshapes.Ontheleft,weshowtheleveloferrorofeach
size of 0.05, and the multi-scale networks are trained with patch methodinrelationtothenoiseleveloftheinput.Thegraphonthe
sizesof0.01,0.03,and0.07.Weuseafixednumberof500points right shows the error of each method relative to the error of our
perpatch.Iftherearefewerpointwithinthepatchradius,wepad singlescalemethod(markedssinthetable).
withzeros(thepatchcenter).Thenetworkcaneasilylearntoignore
We can observe the following general trends in these results:
thesepaddedpoints.Iftherearemorepointswithintheradius,we
first,notethatallofourmethodsconsistentlyoutperformcompet-
selectarandomsubset.Amoresophisticatedsubsamplingmethod
ingtechniquesacrossallnoiselevels.Inparticular,observethatthe
canbeimplementedinfuturework,whichmaybeparticularlyben-
methodsbasedonjetfittingperformwellunderaspecificintensity
eficialforhandlingvaryingsamplingdensities.
ofnoise,dependingonthesizeoftheneighborhood.Forexample,
Ineachepoch,weiteratethrough1000patchesfromeachpoint whilejetsmallworkswellundersmallnoise,itproduceslargeer-
cloud in the training set. We train for up to 2000 epochs on our rorsstrongnoiselevels.Conversely,jetlargeisfairlystableunder
dataset,oruntilconvergence.Typicallytrainingconvergedbefore strongnoisebutover-smoothsforsmallnoiselevels,whichleadsto
reachingthisthreshold,takingbetween8hoursforsingle-scalear- alargeerrorforcleanpointclouds.Onesourceoferrorofourcur-
chitectures and 30 hours for multi-scale architectures on a single rentnetworkcompared,e.g.totheresultsofBoulchetal.[BM16]
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 8 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
input point cloud jet pca ours
90
0
).ged(
rorre
elgna
82
Figure8:PoissonreconstructionfromorientednormalsestimatedbyPCPNETcomparedtobaselinemethods.Ourmethodcanreconstructorientednormals
withsufficientqualitytosuccessfullyperformPoissonreconstruction,evenincasesthatarehardtohandlebytraditionalmethods.Althoughourmethod
performsbetteronaverage,therearealsofailurecases,asdemonstratedinthebottomrow.
isthatourmethoddoesnotperformaswellinthecaseofchangesin wholepointcloud,thereislessresolutionavailableforlocaldetails.
samplingdensity.Thisisbecauseournetworkwastrainedonlyon Thisresultsalargeperformancegaptoothermethodsondetailed
uniformlysampledpointsetsandthereforeisnotasrobusttosuch shapes,agapthatgetssmallerwithstrongernoise,wheretheother
changes.Nevertheless,itachievescomparableresultsacrossmost methodsstarttomissdetailsaswell.Notealsothatourmulti-scale
levelsofnon-uniformsamplingandshowssignificantimprovement architecture produces slightly better results than the single scale
overall.ForPointNet,weprovidetheperformanceonthetraining one for normal estimation. At the same time, the multi-scale ar-
setinadditiontothetestset.PointNetoverfitsourtrainingset,and chitecturewhichcombinesbothnormalsandcurvatureestimation
the training set performance gives and approximate lower bound (ms+c)producesslightlyinferiorresultsbuthastheadvantageof
fortheachievableerror.SincePointNet’spointfunctionscoverthe outputtingbothnormalsandcurvatureatthesametime.
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 9 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
ours jet small jet medium jet large Boulch et al.
60
0
).ged(
rorre
elgna
83
14.31 59.02 18.57 25.03 15.88
11.56 39.21 16.81 17.07 20.13
18.71 53.53 19.16 16.33 31.27
24.58 26.75 26.52 45.57 27.87
Figure9:Qualitativecomparisonofthenormalsestimatedbyourmethodtothenormalsestimatedbybaselinemethods.Shapesarecoloredaccordingto
normalangleerror,whereblueisnoerrorandyellowishigherror,numbersshowtheRMSerroroftheshape.
InFigure4weshowtheresultsoftheevaluationofourapproach tohandlefortheMST-basedpropagation.Thebottom-mostshape
comparedtobaselinesfororientednormalestimation.Namely,we showsafailurecaseofourmethod.Wesuspectthataconfiguration
useourpipelinedirectlyforestimatingorientednormals,andcom- liketheinsideofthispipewasnotencounteredduringtraining.In-
parethemtothejetfittingandPCAtogetherwithMST-basedori- creasingordiversifyingthetrainingsetshouldsolvethisproblem.
entationpropagationasimplementedinCGAL[CGA].Wedonot Notethatonaverage,weexpecttheperformanceofourmethodfor
includethemethodofBoulchetal.[BM16]inthiscomparison,asit Poissonreconstructiontobeproportionaltothefractionofflipped
isnotdesignedtoproduceorientednormals.InFigure4weplotthe normals,asshowninFigure4.
RMSangleerrorwhilepenalizingchangesinorientationaswellas
Figure 7 shows the comparison of our curvature estimation to
theerrorrelativetoourperformance.Finally,wealsoreporttherel-
jetfitting.Duetothesensitivityofcurvaturetonoise,noisypoint
ativefractionofnormalsthatareflippedwithrespecttotheground
clouds are a challenging domain for curvature estimation. Even
truthorientation,forothermethodsacrossdifferentlevelsofnoise.
thoughourmethoddoesnotachieveaveryhighlevelofaccuracy
Notethattheerrorsinorientednormalestimationarehighlycorre-
(recallthattheerrorisnormalizedbythemagnitudeoftheground
latedwiththenumberoforientationflips,suggestingthattheori-
truthcurvature),ourperformanceisconsistentlysuperiortojetfit-
entation propagation done during the post-processing is the main
tingonbothprincipalcurvaturevalues,byasignificantmargin.
sourceoferror.Alsonotethatorientationpropagationonlyworks
wellforextremelysmallnoiselevelsandveryquicklydeteriorates Qualitative comparisons of the normal error on four shapes of
leadingtolargeerrors.Givenasetoforientednormals,wecanalso our dataset are shown in Figure 9. Note that for classical surface
performPoissonreconstruction.Figure8showsafewexamplesof fitting(jetintheseexamples),smallpatchsizesworkwellonde-
objectsreconstructedwithourorientednormals.Notethatthetop- tailed structures like in the bottom row, but fail on noisy point
mostpointcloudsamplestwosidesofathinsheet,makingithard clouds,whilelargepatchesaremoretoleranttonoise,butsmooth
to determine which side of the sheet a point originated from. In outsurfacedetail.Boulchetal.[BM16]performwellinalow-noise
themiddlerow,thepointcloudexhibitssharpedgesthatarehard setting,buttheperformancequicklydegradeswithstrongernoise.
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 10 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds
ours jet small jet medium jet large
3
0
rorre
.vruc
.mron
84
0.58 4.80 0.90 0.78
1.92 48.15 3.38 3.80
0.87 38.5 6.30 2.74
0.93 8.20 2.26 1.45
Figure10:Qualitativecomparisonofcurvaturevaluesestimatedbyourmethodtothecurvaturevaluesestimatedbyabaselinemethodwiththreedifferent
patchsizes.Shapesarecoloredaccordingtothecurvatureestimationerror,whereblueisnoerrorandyellowishigherror,numbersshowtheRMSerrorof
theshape.
Eventhoughourmethoddoesnotalwaysperformbestincasesthat work on point clouds arising from triangle meshes corrupted by
areoptimalfortheparametersettingofaanothermethod(e.g.ver- variouslevelsofnoiseandshowthroughextensiveevaluationthat
susjetfittingwithlargepatchsizeinthethirdrow,wheretheshape our approach achieves state-of-the-art results across a wide vari-
isverysmoothandnoisy),itperformsbetterinmostcasesandon ety of challenging scenarios. In particular, our method allows to
average. replacethedifficultanderror-pronemanualtuningofparameters,
present in the majority of existing techniques with a data-driven
InFigure10,weshowqualitativeresultsofcurvatureestimation
training.Moreover,weshowimprovementwithrespecttootherre-
forselectedshapes.Thecolorofthepointsmarkstheerrorincur-
centlyproposedlearning-basedmethods.
vature,whereblueisnoerrorandyellowishigherror.Errorsare
computedinthesamemannerasdescribedinSection5.2,andare While producing promising results in a variety of challenging
clampedbetween0and3.Theerrorofourcurvatureestimationis scenarios,ourmethodcanstillfailinsomesettings,suchasinthe
typicallybelow1,whileforpreviousmethodtheestimationisoften presence of large flat areas, in which patch-based information is
ordersofmagnitudehigherthanthegroundtruthcurvature. notenoughtodeterminethenormalorientation.Forexample,our
orientednormalestimationprocedurecanproduceinconsistentre-
sults,e.g.,inthecentersoffacesalargecube.Amorein-depthanal-
6. Conclusion,Limitations,andFutureWork ysisandabetter-adaptedmulti-resolutionschememightbeneces-
sarytoalleviatesuchissues.
Wepresentedaunifiedmethodforestimatingorientednormalsand
principal curvature values in noisy point clouds. Our approach is In the future, we also plan to extend our technique to estimate
basedonamodificationofarecentlyproposedPointNetarchitec- other differential quantities such as principal curvature directions
ture,inwhichweplacespecialemphasisonextractinglocalprop- or even the full first and second fundamental forms, as well as
erties of a patch around a given central point. We train the net- othermid-levelfeaturessuchastheShapeDiameterFunctionfrom
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 11 ---

P.Guerrero,Y.Kleiman,M.Ovsjanikov&N.J.Mitra/ PCPNET:LearningLocalShapePropertiesfromRawPointClouds 85
anoisyincompletepointcloud.Finally,itwouldalsobeinteresting [HM02] HUANGJ.,MENQC.-H.:Combinatorialmanifoldmeshrecon-
to study the relation of our approach to graph-based neural net- structionandoptimizationfromunorganizedpointswitharbitrarytopol-
ogy.Computer-AidedDesign34,2(2002),149–165.2
works[DBV16,HBL15]ongraphsbuiltfromlocalneighborhoods
ofthepointcloud. [HWG∗13] HUANG H., WU S., GONG M., COHEN-OR D., ASCHER
U.,ZHANGH.R.:Edge-awarepointsetresampling.ACMTransactions
onGraphics(TOG)32,1(2013),9.2
Acknowledgement [JSZK15] JADERBERG M., SIMONYAN K., ZISSERMAN A.,
KAVUKCUOGLU K.: Spatial transformer networks. In Proc. NIPS.
This work was supported by the ERC Starting Grants Smart- 2015,pp.2017–2025.4
Geometry (StG-2013-335373) and EXPROTEA (StG-2017- [KNSS09] KALOGERAKIS E., NOWROUZEZAHRAI D., SIMARI P.,
758800), the Open3D Project (EPSRC Grant EP/M013685/1), SINGH K.: Extracting lines of curvature from noisy point clouds.
the Chateaubriand Fellowship, chaire Jean Marjoulet from Ecole Computer-AidedDesign41,4(2009),282–292.2
Polytechnique, FUI project TANDEM 2, and a Google Focused [LP05] LANGEC.,POLTHIERK.: Anisotropicsmoothingofpointsets.
ResearchAward. ComputerAidedGeometricDesign22,7(2005),680–692.2
[MBBV15] MASCI J., BOSCAINI D., BRONSTEIN M. M., VAN-
DERGHEYNSTP.: Geodesicconvolutionalneuralnetworksonrieman-
References nianmanifolds.InProc.ICCVWorkshops(December2015).3
[AB99] AMENTAN.,BERNM.: Surfacereconstructionbyvoronoifil-
[MDGD∗10] MULLEN P., DE GOES F., DESBRUN M., COHEN-
tering.Discrete&ComputationalGeometry22,4(1999),481–504.2
STEINER D., ALLIEZ P.: Signing the unsigned: Robust surface re-
constructionfromrawpointsets. InComputerGraphicsForum(2010),
[ACSTD07] ALLIEZP.,COHEN-STEINERD.,TONGY.,DESBRUNM.: vol.29,WileyOnlineLibrary,pp.1733–1741.2
Voronoi-based variational reconstruction of unoriented point sets. In
Proc.SGP(2007),pp.39–48.2
[MGA∗17] MARONH.,GALUNM.,AIGERMANN.,TROPEM.,DYM
N.,YUMERE.,KIMV.G.,LIPMANY.:Convolutionalneuralnetworks
[BBL∗17] BRONSTEIN M. M., BRUNA J., LECUN Y., SZLAM A., onsurfacesviaseamlesstoriccovers. ACMTrans.Graph.36,4(2017),
VANDERGHEYNST P.: Geometric deep learning: going beyond eu- 71:1–71:10.3
clideandata. IEEESignalProcessingMagazine34,4(2017),18–42.
[MNG04] MITRAN.J.,NGUYENA.,GUIBASL.: Estimatingsurface
3
normals in noisy point cloud data. In special issue of International
[BC94] BERKMANN J., CAELLI T.: Computationofsurfacegeometry JournalofComputationalGeometryandApplications(2004),vol.14,
andsegmentationusingcovariancetechniques. IEEETransactionson pp.261–276.2
PatternAnalysisandMachineIntelligence16,11(1994),1114–1116.2 [MOG11] MERIGOT Q., OVSJANIKOV M., GUIBAS L. J.: Voronoi-
[BM16] BOULCHA.,MARLETR.: Deeplearningforrobustnormales- basedcurvatureandfeatureestimationfrompointclouds. IEEETVCG
timationinunstructuredpointclouds. ComputerGraphicsForum35,5 17,6(2011),743–756.2
(2016),281–290.doi:10.1111/cgf.12983.2,3,4,5,7,9 [MRB∗16] MASCIJ.,RODOLÀE.,BOSCAINID.,BRONSTEINM.M.,
[BMRB16] BOSCAINI D., MASCI J., RODOLÀ E., BRONSTEIN M.: LIH.: Geometricdeeplearning. InSIGGRAPHASIACourses(2016).
Learning shape correspondence with anisotropic convolutional neural 3
networks. In Advances in Neural Information Processing Systems [NSF12] NATHANSILBERMANDEREKHOIEMP.K.,FERGUSR.: In-
(2016),pp.3189–3197.3 doorsegmentationandsupportinferencefromrgbdimages. InECCV
[CGA] Cgal4.11-pointsetprocessing. https://doc.cgal.org/ (2012).1
latest/Point_set_processing_3/index.html. Accessed: [PKKG03] PAULYM.,KEISERR.,KOBBELTL.P.,GROSSM.: Shape
2017-10-10.5,7,9 modelingwithpoint-sampledgeometry.ACMTransactionsonGraphics
(TOG)22,3(2003),641–650.2
[CP03] CAZALSF.,POUGETM.:Estimatingdifferentialquantitiesusing
polynomialfittingofosculatingjets.InProc.SGP(2003),pp.177–187. [pyt] PyTorchtensorsanddynamicneuralnetworksinpythonwithstrong
2,5 gpuacceleration. http://pytorch.org/. Accessed:2017-10-10.
7
[DBV16] DEFFERRARDM.,BRESSONX.,VANDERGHEYNSTP.:Con-
volutionalneuralnetworksongraphswithfastlocalizedspectralfilter- [QSMG17] QI C. R., SU H., MO K., GUIBAS L. J.: Pointnet:Deep
ing. In Advances in Neural Information Processing Systems (2016), learningonpointsetsfor3dclassificationandsegmentation. InProc.
pp.3844–3852.11 CVPR(2017).2,3,4,5
[dC76] DECARMOM.: DifferentialGeometryofCurvesandSurfaces.
[QSN∗16] QIC.R.,SUH.,NIESSNERM.,DAIA.,YANM.,GUIBAS
L. J.: Volumetricandmulti-viewcnnsforobjectclassificationon3d
PrenticeHall,EnglewoodCliffs,NJ,1976.1
data.InProc.CVPR(2016),pp.5648–5656.3
[GG07] GUENNEBAUD G., GROSS M.: Algebraic point set surfaces.
[QYSG17] QIC.R.,YIL.,SUH.,GUIBASL.J.:Pointnet++:Deephi-
ACMTransactionsonGraphics(Siggraph)26,3(2007),23–9.2
erarchicalfeaturelearningonpointsetsinametricspace.arXivpreprint
[GZC15] GUOK.,ZOUD.,CHENX.: 3dmeshlabelingviadeepcon- arXiv:1706.02413(2017).3
volutionalneuralnetworks. ACMTransactionsonGraphics(TOG)35,
[Rus04] RUSINKIEWICZS.: Estimatingcurvaturesandtheirderivatives
1(2015),3.3
ontrianglemeshes.InProc.3DPVT(Sept.2004).6
[HBL15] HENAFF M., BRUNA J., LECUN Y.: Deep convolutional [SF15] SHABATY.B.,FISCHERA.: Designofporousmicro-structures
networks on graph-structured data. arXiv preprint arXiv:1506.05163 usingcurvatureanalysisforadditive-manufacturing. ProcediaCIRP36
(2015).11 (2015),279–284.6
[HDD∗92] HOPPE H., DEROSE T., DUCHAMP T., MCDONALD J., [WHC∗16] WEIL.,HUANGQ.,CEYLAND.,VOUGAE.,LIH.:Dense
STUETZLEW.: Surfacereconstructionfromunorganizedpoints. Proc. humanbodycorrespondencesusingconvolutionalnetworks. InProc.
SIGGRAPH26,2(1992),71–78.2 CVPR(2016),pp.1544–1553.3
[HLZ∗09] HUANGH.,LID.,ZHANGH.,ASCHERU.,COHEN-ORD.: [WHG∗15] WUS.,HUANGH.,GONGM.,ZWICKERM.,COHEN-OR
Consolidationofunorganizedpointcloudsforsurfacereconstruction.In D.: Deeppointsconsolidation. ACMTransactionsonGraphics(TOG)
ACMtransactionsongraphics(TOG)(2009),vol.28,ACM,p.176.2 34,6(2015),176.2
(cid:13)c 2018TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2018TheEurographicsAssociationandJohnWiley&SonsLtd.```

---

## Section 1.6: Reconstruction filters in computer graphics

Source File: Reconstruction filters in computer graphics.txt

### Paper Content:

```
# Reconstruction filters in computer graphics.pdf
# Converted: 2025-07-18 12:59:01
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Reconstruction filters in computer graphics.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Reconstruction filters in computer graphics.txt


--- Page 1 ---

~
Computer Graphics, Volume 22, Number 4, August 1988
Reconstruction Filters in Computer Graphics
Don P. Mitchell
Arun N. Netravali
AT&T Bell Laboratories
Murray Hill, New Jersey 07974
ABSTRACT Many conversions between continuous and discrete representations may
occur in the course of generating an image. For example when ray trac-
ing a texture-mapped surface, a photograph may be sampled by a digi-
Problems of signal processing arise in image tizer to define the texture, then the texture samples are inteqxilated and
synthesis because of tlansformations between resampled when a ray strikes the textured surface, the ray samples are
continuous and discrete representations of 2D interpolated and resampled to generate pixel values, and the pixels are
images. Aliasing introduced by sampling has interpolated by a display and finally resampled by retinal cells when the
received much attention in graphics, but recon- image is viewed. Resampling may be more explicit, as in enlarging or
struction of samples into a continuous reducing a digital image or warping an image (e.g., with Catmull and
representation can also cause aliasing as well Smith's algorithm CAT80). Each of these conversions can introduce
as other defects in image quality. The prob- conspicuous errors into an image.
lem of designing a filter for use on images is
discussed, and a new family of piecewise Errors introduced by sampling (e.g., aliasing) have received considerable
cubic filters are investigated as a practical attention in the graphics community since Crow identified this as the
demonstration. Two interesting cubic filters cause of certain unwanted artifacts in synthetic images CRO77. Alias-
are found, one having good antialiasing pro- ing in images was discussed in the classic 1934 paper by Mertz and Gray
perties and the other having good image- MER34. Their discussion contains a description of artifacts well-
quality properties. It is also shown that recon- known to graphics researchers today and shows that the condition for
struction using derivative as well as amplitude preventing aliasing was known, as a rule of thumb, long before
values can greatly reduce aliasing. Shannon's proof of the sampling theorem:
CR Categories and Subject Descriptions: 1.3.3 Computer Graphics :
PictureImage Generation; 1.4.1 Image Processing : Digitization The interference usually manifests itself in the form of serra-
tions on diagonal lines and occasional moir6 effects in the
General Terms: Algorithms received picture. Confusion in the signal may be practically
eliminated by using an aperture of such a nature that it cuts
Additional Keywords and Phrases: Antialiasing, Cubic Filters, Filters, off all Fourier components with n numbers greater than
Derivative Reconstruction, Reconstruction, Sampling N/2 half the scanning rate ....
1. Introduction By comparison, the problems introduced by reconstruction have been
The issues of signal processing arise in image synthesis because of somewhat neglected in the graphics literature. Reconstruction can be
transformation between continuous and discrete representations of responsible for aliasing and other types of distortion that mar the subjec-
images. A continuous signal is converted to a discrete one by sampling, tive quality of an image. This paper will focus on the effects of recon-
and according to the sampling theorem SHA49, all the information in struction and how to design filters for graphics applications.
the continuous signal is preserved in the samples if they are evenly
spaced and the frequency of sampling is twice that of the highest fre- 2. Aliasing Caused by Reconstruction
quency contained in the signal. A discrete signal can be converted to a Aliasing in synthetic images is a serious problem and still not completely
continuous one by interpolating between samples, a process referred to in solved. In other digital-signal-processing applications, aliasing is elim-
the signal-processing literature as reconstruction. inated by prefiltering signals before sampling, as illustrated in Figure l.
Note that it is the prefiltered signal that is reconstructed in this ease.
,screte
reconstructed
sig° s,gnal- I filter I signal
Permission to copy without fee all or part of this material is granted
provided that the copies are not made or distributed for direct
commercial advantage, the ACM copyright notice and the title of the sampling
publication and its date appear, and notice is given that copying is by pulses
permission of the Association for Computing Machinery. To copy
otherwise, or to republish, requires a fee and/or specific permission.
Figure I. Sampling and Reconstruction
While prefi/lering is the classic solution to a/iasing problems, there is a
special problem encountered in computer graphics. Many synthetic
©1988 ACM-O-89791-275-6/88/O08/0221 $00.75 images originate from what we will call procedural signals, in which the
221

--- Page 2 ---

¢
SIGGRAPH '88, Atlanta, August 1-5, 1988
signal is only implicitly defined by an algorithm for computing point centered at the location of an impulse in the comb. Equation (,4) states
samples. Operations that require an explicit representation of the signal that, in the frequency domain, reconstruction can be interpreted as the
cannot be performed, and in particular, prefiltering is impractical. This multiplication by K (v) which is intended to eliminate all the extraneous
difficulty is unique to computer graphics, and ray tracing is the clearest replicas of the signal's spectrum and keep the original base-band cen-
example of it wHIgo}. tered at the origin. K(v) is indicated by the dashed curve in Figure 2.
To explain the role that reconstruction plays in aliasing, it will be helpful However, Figure 2 also demonstrates a problem. The replicas of the sig-
to review briefly the theory of sampling and define the operations of nal spectrum overlap, aad the reconstruction filter can not isolate a pure
sampling and reconstruction more precisely, tn one dimension, a signal version of the base-band signal. When part of the energy in a replica of
can be represented by a continuous function f(x). Producing a discrete the spectrum leaks into the reconstructed signal, aliasing results. If the
signal by sampling is equivalent to multiplying by an infinite train of bandwidth of the signal were narrower or the sampling rate higher, the
impulses known as a comb function: copies would not overlap, and exact reconstruction would be possible.
Even if the replicated spectra do not overlap, alining can result from
£(x) = f (x)" comb(x) (1)
poor reconstruction, as illustrated in Figure 3. When aliasing is a conse-
where quence of undersampling (or lack of prefiltering), it is referred to as
prealiasing, and when it results from poor reconstruction, it is called
comb(x) = ~ ~(x-n) (lb) postaliasing.
Unit spacing between samples is assumed in equation (lb), and ~x) is
the Dirac delta function. In this case, the sampling theorem states that K(v)
f(x) can be reconstructed exacdy from its samples if it contains no fre- r- - ........... -~ F~(V)
quencies greater than 0.5 cycles per sample. This critical frequency is
called the Nyquist frequency.
Reconstruction is accomplished by convolving (indicated by *) the
V
discrete signal with a reconstruction filter kernel, k(x): VN
f,(x) = ./~(x)* k(x) (2)
Figure 3. Postaliasing Resulting from Poor Reconstruction Filter
= ~ f,(u )" k (x - u)du (2b)
Figure 4 shows an extreme example of aliasing in an image. In this fig-
ure, the two-dimensional signal, f(x,y) = sin(x 2 +y2), was sampled on a
= ~ f(n)'k(x-n) (2c) 128 x 128 pixel grid. Then, these samples were reconstructed with a
cubic filter (to be described later in the paper) and resampled to 512 x
512 pixels.
Except in the mathematically ideal case, some error is introduced in the
process of sampling and reconstruction, and f(x) will be somewhat dif- The rings on the left side of the image are part of the actual signal, but
ferent from fr(X). To analyze this error, it is useful to view the problem the rings on the right side are Moit~ patterns due to prealiasing. In the
in the frequency domain. The Fourier transform of the signal is its spec- center of the image is a fainter set of concentric rings resulting from pos-
trum F(v), and the Fourier transform of the filter is its frequency taliasing. Postaliasing occurred when the discrete image of 128 x 128
response K(v). Since multiplication in the spatial domain is equivalent pixels was enlarged to 512 x 5t2 pixels by resampling. Note that this
to convolution in the ffrequency domain (and vice versa), sampling can conspicuous postaliasing pattern results from "treating" between the sig-
be described by: nal and its alias. This can also be understood from Figure 3, where it
can be seen that at the Nyquist frequency (indicated by VN) the signal's
spectrum and its nearest replica come close together. Power in the spec-
F~(v) = F(v)* Comb(v) (3) trum very near the Nyquist frequency is thus the cause of the most diffi-
and reconstruction by: cdt type of aliasing to remove from an image. This problem has been
noted by other graphics researchers COO87 and by Mertz et al.
F,(v) = F,(V) .K(v) (4) MER34.
The Fourier transform of a comb function is also a comb function (with Using the same set of samples as in Figure 4, a much better reconstruc-
reciprocal spacing between impulses). tion filter can be applied (a 30-unit-wide windowed sine filter). Figure 5
demonstrates a dramatic reduction of the postaliasing pattern, but the
prealiasing is unaffected. The spectrum of this reconstruction filter is
very close to the ideal step shape shown in Figures 2 and 3.
K(v)
3. Other Image Defects Caused by Reconstruction
Notice in Figure 2, that a reconstruction filter K(v) has two tasks. First
it must remove the extraneous replicas of the signal spectrum (to prevent
aliasing). Second, it should pass the original signal base band, but the
signal can be distorted if this is not done perfectly. This second type of
reconstruction error will be referred to as base-band attenuation.
From the previous section, one might assume that the literature of signal
Figure 2. Sampling and Reconstruction in Frequency Domain processing provides a complete solution to the reconstruction problem in
graphics; however, there is a serious difficulty with the ideal sine filter
that is not obvious from studying its frequency response. Figure 6
Figure 2 illustrates the consequences of the convolution in equation (3).
shows a simple figure reconstructed with the same filter used in Figure
The spectrum of a sampled signal F,(V) is the sum of an infinite
5. The rippling pattern radiating from the edges is called ringing. Ring-
sequence of shifted replicas of the original signal's spectrum, each
ing is strongly suggested by the form of the impulse response of the sinc
222

--- Page 3 ---

@ *
Computer Graphics, Volume 22, Number 4, August 1988
fiher, as shown in Figure 7: number of free parameters. First, the filter should be smooth in the
sense that its value and first derivative are continuous everywhere.
Discontinuities in k(x) will lead to high-frequency leakage in the fre-
quency response of the filter which can allow aliasing. In addition, the
1 - problem of sample-frequency ripple can be designed out of the filter by
requiring (for all x):
0.5-
k(x-n) = 1 (7)
0-
This means that if all the samples are a constant value, the reconstruction
will be a fiat constant signal. Figure 8 demonstrates this defect by using
I I I
an unnormalized Gaussian filter to reconstruct a 512 x 512 image from
-5 0 5
64 x 64 samples. In the frequency domain, sample ripple can be viewed
as an alias of the image's DC component. It can be shown that the con-
dition given by equation (7) means that the frequency response of these
Figure 7. mpulse Response of Ideal Sinc Filter
cubic filters will be zero at all integer multiples of the sampling fre-
quency except zero, eliminating all extraneous replicas of the DC com-
Classical digital filter design places a heavy emphasis on the frequency ponent.
response of a filter. That works well in the audio domain, but when con-
sidering the appearance of images, it is important to also pay attention to With these constraints, the number of free parameters are reduced from
the shape of the impulse response. eight to two, resulting in the following family of cubic filters;
The response of human viewers to various spatial effects of filters is not (12-9B - 6C) Ixl 3 + if Ixl<l
yet a well-understoed science and is largely subjective in nature. Filters I (_18+i2B+6C)Ix12+(6_2B)
that have some aliasing problems or certain types of base-band attenua-
1 J(-B-6C)Ixl 3 +(6B +30C)IxI 2 + if 1<_ Ixl <2 (8)
tion may turn out to give visually-pleasing results. Schreiber and Troxel
have discussed the spatial effects of reconstruction filters SCH85, and k(x) = 6 /0(-12B-48C)1xl +(8B+24C)
they mention some of the important defects that can occur when judging otherwise
the quality of an image subjectively: sample-frequency ripple, anisotro-
pic effects, ringing, blurring, and aliasing. Each of these effects will be
Some values of (B, C) correspond to well-known cubic splines. (1,0) is
considered in detail in the following section.
the cubic B-spliue, (0, C) is the one-parameter family of cardinal cubits
with (0, 0.5) being the Catmull-Rom spline, and (B, 0) are Duff's ten-
Unfortunately, it is often necessary to trade off one type of distortion for
sinned B-splines DLrF86.
another, and the design of a single filter perfect for all applications is
almost certainly impossible. As Figure 6 illustrated, perfect antialiasing
In two or more dimensions, visible artifacts can be caused by angle-
resulted in the serious defect of ringing. However, Brown realized that a
dependent behavior or anisotropic effects. Figure 9 illustrates this prob-
moderate amount of ringing can improve the subjective quality of an
lem by reconstructing with the separable filter k(x)k(y) using parameter
image by enhancing the appearance of shaqmess BRO69. He found
values of (0,0). Even though sample-frequency ripple has been designed
that a single transient lobe of ringing was effective at sharpening, but
out of k(x), in two dimensions the pixel structure is highly conspicuous
multiple transients (as in Figure 6) always degrade image quality.
because the impulse response and the sampling lattice are not radially
symmetric,
Many of the concepts presented so far have been illustrated in one
dimension for simplicity. However, image reconstruction takes place in
The phenomenon of ringing has already been seen in Figure 6. Filters
two dimensions and involves the convolution of a 2D lattice of samples
with a filter k(x,y). In this paper, we will consider only separable in the cubic filter family can also exhibit this problem as seen in Figure
filters, where the samples are convolved with the product k(x)kty). 10, where parameter values of (0,1) were used. Ringing results when
k(x) has negative side lobes, and although some ringing can enhance
Separable filters are compurationally more efficient than nonseparable
shaqaness, a filter that becomes negative is problematic. In Figure 10, a
because the filtering operation can be performed in separate passes verti-
typical problem is seen where portions of the image near an edge have
cally and horizontally. If the filter kernel is N samples wide, the recon-
struction can be performed with O(N 2) multiplications for the general become negative and have been damped to zero. This results in pro-
filter k(x,y) but with O(N) if the filter is separable. nounced black spots (e.g., at the top of the statue's head). Similar
clamping occurs to white, but is less noticeable because of the eye's non-
linear response to contrast. Schreiber and Troxel have suggested that
4. Piecewise Cubic Reconstruction Filters
subjectively even sharpening can only be produced by introducing ting-
Rather than discuss the issues of filter design abstractly, this paper will ing transients in a suitably nonlinear fashion SCH85. These conspicu-
apply them to the study of a family of filters defined by piecewise cubic ous clamping effects could also be eliminated by reducing the dynamic
polynomials. Cubic filters ate sufficiently complex to have a broad range of the image or raising the DC level of the image.
range of behaviors, but they are simple enough to be compntationally
attractive. Hou and Andrews have studied the fikering properties of the Parameter values of (3/2, -1/4) result in an image that is unnecessarily
cubic B-spline HOU78, and two studies have been made of the one- blurry, as seen in Figure 11. The cubic B-spline also suffers from this
parameter family of cardinal cubic splines KEYSI,PAR83. problem. In viewing many reconstructions with filters in this family,
ringing, anisotropy, and blurring are the dominant behaviors, and in a
The general form for a symmetric cubic filter is: small region of the parameter space, a satisfactory compromise seems to
exist which is seen in Figure 12, using parameter values of (1/3, 1/3).
Plxl3+Qlxl2+RIxI+S if Ixl<l This is quite good, considering that the image is being magnified from
x 64 pixels. There is some degree 0f sharpening, and almost no visi-
k(x)= ~Tlxl3+UIxIZ+VIxI+W ifl_<lxl<2 (6)
ble evidence of the sampling lattice.
to otherwise
To get a better idea of which regions of the parameter space yield which
type of behavior, a simple subjective test was designed. On a neutral
Several obvious constraints can be placed on this function to reduce the background, four images were displayed typifying the effects of ringing,
223

--- Page 4 ---

SIGGRAPH '88, Atlanta, August 1-5, 1988
blurring, anisotropy, and an example of die most satisfactory behavior. in Figure 13), quadratic convergence of fit is achieved. This line con-
In the center of the display, images reconsuucted fxom filters with ran- tains the cubic B-spline and the Catmull-Rom spline (which actually has
dom values of (B, C) were displayed, and the test subject was asked to cubic convergence). Within the interval of B = 5,33 to B = 0, good sub-
choose which of the four behaviors it exemplified. Nine expert jective behavior is found with a simple trade-off between blurring and
observers (researchers working in graphics or image processing) took tinging. Outside this interval, k(x) becomes bimodal or exhibits extreme
part and over 500 samples were taken. It would not be credible to sug- ringing. The filter (1/3, 1/3) used to generate Figure 12 is recommended
gest that a single ideal parameter pair can be deduced from subjective by the authors, but other observers may prefer more or less ringing.
testing. The motivation for this experiment was simply to draw approxi-
mate boundaries between regions of differing behavior as shown in Fig- 5. Postaliasing Revisited
ure 13. The test subjects were quite consistent with one another in their A systematic consideration of subjective appearance along with quantita-
judgements. tive analysis has yielded an excellent piecewise cubic filter. However,
the issue of postaliasing, defined in section 2, has been ignored. In fact
the (1/3, 1/3) filter has only fair antialiasing properties and was used to
generate Figure 4. Postaliasing is usually not strong enough to cause
'""'........ visible "jaggies" on edges unless a very poor filter is used (e.g., a box
filter); however, an image with periodic patterns can have conspicuous
postalias Moire effects unless careful precautions are taken. Synthetic
0.8-
images that contain brick walls, ocean waves, or the ubiquitous checker-
board pattern are examples of images that might have this difficulty.
There are several approaches to fixing this problem.
0.6-
B If the signal is bandlimited and samples carry information about the
parameter derivative as well as about signal amplitude, a better job of reconstruc-
tion can be done PET64. Given samples (at unit spacing) of a signal
0.4-
and of its derivative, a reconstruction can be done in the following form:
0.2- f(x)= n~= ~ If~g(x-n)+f,h(x-n) (1o)
x \ \ In an extension of the sampling theorem, if the signal contains no energy
0
I I I above the sampling frequency (twice the allowed bandwidth of sampling
0 0.2 0.4 0.6 0.8 without derivatives), then it can be perfectly reconstructed by the filter
C parameter kernels:
Figure 13. Regions of Dominant Subjective Behavior sinZ~
g(x)= n2x2 (ll)
sin2
To help choose a good filter rom the two-parameter space, some quanti- h(x) = (llb)
tative analysis can be done to remove one more degree of freedom. ~x
Keys and Park et al. studied the cardinal cubic splines because these
cubics exactly interpolate at the sample positions KEYSI,PAR83.
Using standard numerical analysis, Keys concluded that the Catmull- This is analogous to the ideal sine reconstruction formula in the standard
Rom spline was best. Park et al. reached the same conclusion using an case where no derivative information is present. A common approxima-
equivalent analysis in the frequency domain. Figure 14 illustrates this tion to these ideal reconstruction formulae is Hermite cubic interpolation:
technique;
/ (cid:127)
lxl3-31xl2+l if Ixl_<l
g (x) = otherwise (12)
{ (cid:127)
3-2x Ixl +x if Ixl~l
h (x) = otherwise (12b)
f (x)
Figure 15 shows the aliasing test pattern (stir starting with 128 x 128
samples) reconstructed with the Hermite cubic postfilter. The effect is
2J" X
~--- h --.-~. dramatic when compared to Figure 4, The postaliasing artifact in the
middle of the image is nearly gone, and the prealiasing paltern on the
right is less intense.
Figure 14. f(x) andf~(x)
The theory of derivative reconstruction may have some practical value in
computer graphics. For example, it may be possible to extend Whitted's
As the sample spacing h diminishes, the function and its reconstruction
ray-tracing shading model WHI80 to generate derivatives with respect
become closer. The difference f (x) -f,(x) can be expanded into a power
to the screen coordinates. This is not an easy problem, but we have
series in h to study how parameters affect various orders of behavior.
demonstrated the feasibility of this extension by deriving the formulae
Details of this type of analysis can be found in Keys' paper, and when
for Lambert and Phong shading on quadric surfaces. It is possible that
applied to the two-parameter family, the following is obtained:
the density of rays used to reconstruct an image could be reduced in this
manner by gathering more useful information from each visible surface
f (x)-f,(x) = (2C+B-1)hfr(x) + O(h 2) (9) calculation.
A second approach to improving postaliasing properties is suggested by
r(x) is a polynomial factor. When 2C +B = I (indicated by dotted line
the success of stochastic sampling on the prealiasing problem
224

--- Page 5 ---

'~' Computer Graphics, Volume 22, Number 4, August 1988
COO86,DIPg5,MIT87. However, preliminary experiments conducted A new family of cubic filters has been analyzed, and two interesting
by the authors with stochastic-phase reconstructfon have yielded very filters have been found. The (113, 1/3) filter yields excellent image qual-
poor results. The amount of noise needed to obscure postaliasing seri- ity, and the notch filter (3/2, -1/4) strongly suppresses postaliasing pat-
ously degraded image q?ality. terns.
Finally, it was observed in section 2 that signal energy very near the If derivative values can be generated by a procedural signal, an image
Nyquist frequency is most responsible for conspicuous Moire patterns. It with less aliasing is possible by reconstruction with Hermite intelpolation
is possible to cut out this component by notch-filter reconstruction. The or some other suitable filter.
frequency response of the two-parameter cubic filter in equation (8) is:
More work remains to be done. While the authors do not believe simple
filters will be found that improve much on the cubic filters derived here,
x(v) = -3~-3B- Isinc2(v)_sinc(2v) (13) there aae other avenues for progress. Adaptive filters might allow good
image quality with strong antialiasing only where it is needed in problem
areas. The effects of the reconstruction in the display and eye might be
2C _3sinc2(2v)+ 2sinc(2v)+sinc(4v)
allowed for given models of the visual system NET88.
+ B sinca(v) Finally, the problem of reconstruction from nonuniform sampling is not
entirely solved. Reasonable filters have been proposed MIT87, but
more analysis could be done: "Ideal" nonuniform reconstruction filters
am known which are analogous to the sinc filter used witli uniform sam-
This function goes to zero at v = 1/22 when B = 3/2. In fact, the fre-
pies. A greater challenge will be to understand the subjective issues
quency response is zero at all integer and half-integer multiples of the
involved in designing filters that are well suited to computer graphics.
sampling rate except zero. The filter (3/2, -1/4) is quadratically conver-
gent, and the result of reconstruction with it can be seen in Figure 16, in
7. Acknowledgements
which the postaliasing artifact is almost completely eliminated. Unfor-
tunately, this filter is quite blurry as was seen in Figure 11. The We would like to thank our colleagues who volunteered to help with
behavior of this notch filter can be seen in its frequency response in Fig- subjective testing. We would also like to thank Jim Bergen from the
ure 17 compared with the cubic B-spline filter (1, 0) in Figure 18. The David Sarnoff Laboratory, Jim Johnston from Bell Labs Signal Process-
log magnitudes of the frequency responses are plotted below: ing Research Department., and William Schreiber from MIT's Advanced
Television Research Program for their views on filter design. We would
also like to thank Larry O'Gorman, Brace Naylor, Rob Pike, David Tho-
mas and Pamela Zave and the SIGGRAPH reviewers for their helpful
comments.
0.1
8. References
0.01
BRO69 Brown, Earl F., "Television: The Subjective Effects of
Filter Ringing Transients", Journal of the SMPTE, Vol.
0.001
78, No. 4, April 1969, pp. 249-255.
0,0001
I i CAT80 Catmnll, Edwin, Alvy Ray Smith, "3-D Transformations
0 1 2 of Images in Scardine Order", Computer Graphics, Vol.
14, No. 3, pp. 279-285.
COO86 Cook, Robert L., "Stochastic Sampling in Computer
Figure 17. Frequency Response of Cubic Notch Filter
Graphics", ACM Trans. Graphics, VoL 5, No. t, January
1986.
COO87 Cook, Robert L., personal communication, August, 1987.
OH m CRO77 Crow, Franklin C., "The Aliasing Problem in Computer-
Generated Shaded Images", Comm. ACM, Vol. 20, No. 1 l,
0.01 - November 1977, pp. 799-805.
0.~1 - DIP85 Dippe, Mark A. Z. and Erling Henry Wold, "Antialiasing
Through Stochastic Sampling", Computer Graphics, Vol.
0.0~1 19, No. 3, July 1985, pp. 69-78.
I I
1 2
DIJF861 Duff, Tom, "Splines in Animation and Modeling", State of
the Art in Image Synthesis, SIGGRAPH g6 Course Notes.
Figure 18. Frequency Response of Cubic B-Spline Filter HOU78 Hou, Hsieh S., HaJ'ry C. Andrews, "Cubic Splines for
Image Interpolation and Digital Filtering", IEEE Trans.
Acoustics, Speech, and Signal Processing, Vol. ASSP-26,
No. 6, December 1978, pp. 508-517.
6. Conclusions
Designing reconstruction filters for computer graphics applications
KEY81 Keys, Robert, G, "Cubic Convolution Interpolation for
requires a balanced analysis of formal quantitative properties and subjec-
Digital Image Processing", 1EEE Trans. Acoustics, Speech,
tive image quality. There are many trade-offs, arid it may be impossible
and Signal Processing, Vol. ASSP-29, No. 6, December
to find a filter that yields good image quality and has good antialiasing
1981, pp. 1153-1160.
properties.
225

--- Page 6 ---

SIGGRAPH '88, Atlanta, August 1-5, 1988
[MER84] Mettz, Pierre, and Frank Grey, "A Theory of Scanning and
its Relation to the Characteristics of the Transmitted Sig-
nal in Telephotography and Television," Bell System Tech.
J., Vol. 13, pp. 464--515, July 193,4.
[MIT87] Mitchell, Don P., "Generating Antialiased Images at Low
Sampling Densities", Computer Graphics, Vol. 21, No. 4,
July 1987, pp. 65-72.
[NETg8] Nelravali, Arun N., Barry G. Haskell, Digital Pictures:
Representation and Compression, New York, Plenum,
1988.
[PAR83] Park, Stephen K., Robert A. Schowengerdt, "Image
Reconstruction by Parametric Cubic Convolution", Com-
puter Vision, Graphics, and Image Processing, Vol. 23,
No. 3, September 1983, pp. 258-272.
[PET64] Petersen, Daniel P., David Middleton, "Reconsa'aetion of
Multidimensional Stochastic Fields from Discrete Meas-
urements of Amplitude and Gradient", Information and
Control, Vol. 7, pp. 445-476.
[SCH85] Schreiber, William F., Donald E. Troxel, "Transformation
Between Continuous and Discrete Representations of
Images: A Perceptual Approach", IEEE Trans. Pattern Figure 4. Prealiasing and Postaliasing Example
Analysis and Machine Intelligence, Vol. PAMI-7, No. 2,
March 1985, pp. 178-186.
[SHA49] Shannon, Claude E., "Communication in the Presence of
Noise.", Proc. IRE Vol. 37, 19,49, pp. 10-21.
[wrn80] Whitted, Turner, "An Improved Illumination Model for
Shaded Display", Comm. ACM, Vol. 23, No. 6, June 1980,
pp. 343-349.
Figure 5. Nearly Ideal Postfiltering
226

--- Page 7 ---

~ ComputerG raphics, Volume 22, Number 4, August 1988
Figure 6. Ringing Caused By Sinc Postfilter Figure 9. Anisotropic Artifacts
Figure 8. Sample-Frequency Ripple Figure 10. Excessive Ringing and Clamping Artifacts
227

--- Page 8 ---

¢
SIGGRAPH '88, Atlanta, August 1-5, 1988
Figure 11. Excessive Blumng Figure 15. Using Derivative Reconstruction
Figure 12. Best-Looking Cubic Reconstruction Figure 16, Using Notch-Filter Reconstruction
228```

---

## Section 1.7: Fast and robust normal estimation for point clouds with sharp features

Source File: Fast and robust normal estimation for point clouds with sharp features.txt

### Paper Content:

```
# Fast and robust normal estimation for point clouds with sharp features.pdf
# Converted: 2025-07-18 12:59:01
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Fast and robust normal estimation for point clouds with sharp features.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Fast and robust normal estimation for point clouds with sharp features.txt


--- Page 1 ---

DOI: 10.1111/j.1467-8659.2012.03181.x
EurographicsSymposiumonGeometryProcessing2012 Volume31(2012),Number5
EitanGrinspunandNiloyMitra
(GuestEditors)
Fast and Robust Normal Estimation for Point Clouds
with Sharp Features
AlexandreBoulch1andRenaudMarlet1
1UniversitéParis-Est,LIGM(UMRCNRS),CenterforVisualComputing,EcoledesPontsParisTech
6-8av.BlaisePascal,77455Marne-la-Vallée,France
Abstract
Thispaperpresentsanewmethodforestimatingnormalsonunorganizedpointcloudsthatpreservessharpfea-
tures.ItisbasedonarobustversionoftheRandomizedHoughTransform(RHT).WeconsiderthefilledHough
transformaccumulatorasanimageofthediscreteprobabilitydistributionofpossiblenormals.Thenormalswe
estimatecorrespondstothemaximumofthisdistribution.Weuseafixed-sizeaccumulatorforspeed,statisticalex-
plorationboundsforrobustness,andrandomizedaccumulatorstopreventdiscretizationeffects.Wealsopropose
varioussamplingstrategiestodealwithanisotropy,asproducedbylaserscansduetodifferencesofincidence.
Ourexperimentsshowthatourapproachoffersanidealcompromisebetweenprecision,speed,androbustness:
itisatleastaspreciseandnoise-resistantasstate-of-the-artmethodsthatpreservesharpfeatures,whilebeing
almostanorderofmagnitudefaster.Besides,itcanhandleanisotropywithminorspeedandprecisionlosses.
CategoriesandSubjectDescriptors(accordingtoACMCCS): I.3.5[ComputerGraphics]:ComputationalGeometry
andObjectModeling—
1. Introduction
Numerousalgorithmsrelyonthequalityofnormalestima-
tioninpointclouds,suchaspoint-basedrendering[RL00],
surface reconstruction [ÖGG09], 3D piecewise-planar re-
construction[CLP10]andprimitiveextraction[SWK07].
Forthis,regressionmethodsarethemostcommon.Hoppe
Figure 1: Reconstructed normals of a corner with Least
etal.[HDD∗92]estimatenormalsapproximatingatangent
SquareRegression(left)andourmethod(right).
planewitharegressionthatiscomputedefficientlybyprin-
cipalcomponentanalysis(PCA).Othersurfaceshavebeen
usedtoo,e.g.,spheres[GG07]orjets(atruncatedTaylorex- Moving Least Squares (MLS) [ABCO∗01], adaptive ver-
pansion of a surface expression) such as quadrics [CP03]. sions[PKKG03],orrobustLocalKernelRegression(LKR)
Regression methods are robust to noise, although they can [ÖGG09]computeanimplicitsurfaceandestimatenormals
be improved in that respect with adaptive neighborhood asthegradientofthesurface.Theycanretrievesharpfea-
sizes[MNG04],buttheyaresensitivetooutliers.Morere- tures,buttheydependonareliablepriorestimationofinput
centworkhandlesbothnoiseandoutliers[GG07,HLZ∗09, normals.Bilateralfiltering[JDZ04]alsopreservessharpfea-
YLL∗07].However,allregression-basedtechniquestendto tureswhilesmoothingevenregions,butitcanbeslowand
smooth sharp features, and thus fail to correctly estimate thequalityreliesonthatofinputnormalstoo.
normals near edges (see Figure 1). The estimation quality
Otherapproachesdirectlyestimatenormalswithamethod
also depends a lot on the size of the neighborhood used
thatdoesnotoversmoothedges.Dey’smethod[DG06]re-
forregression:largerneighborhoodsareneededtodealwith
liesontheconstructionofaVoronoïdiagramandthesearch
noise,buttheymakesharpfeaturesevensmoother.
ofthefurthestvertexoftheVoronoïcell.Thismethodpre-
Anotherclassofmethodsisbasedonapreliminarynor- serves sharp features and can deal with density variation.
mal estimation, which is improved. Algorithms such as But it is quite sensitive to noise (and does not handle sur-
(cid:13)c 2012TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2012TheEurographicsAssociationandBlackwellPublish-
ingLtd. PublishedbyBlackwellPublishing,9600GarsingtonRoad,OxfordOX42DQ,
UKand350MainStreet,Malden,MA02148,USA.

--- Page 2 ---

1766 A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures
treatmentofanisotropy.ThisisillustratedinFigure3,where
avariationofpointdensityintwoplanescreatesestimation
errorsinthelow-densityareawheretheyintersect.
Wepresentanewmethodforestimatingnormalsthatad-
dresses the requirements of real data: sensitivity to sharp
features, robustness to noise, to outliers, and to sampling
anisotropy,aswellascomputationalspeedtoachievescala-
bility.Thismethodisbasedonafastandrobustadaptation
Figure 2: Laser scan of room, highlights of a corner with oftheRandomizedHoughTransform(RHT).
densityanisotropyandabevelededge.
The Hough transform [Hou62] has been originally in-
troduced to detect lines and arcs in bubble chamber pic-
tures. Duda and Hart [DH72] discretize the space and in-
troduced accumulators. The main idea of this transform is
tochangethedatarepresentationspacesuchthatthedesired
shape accumulates in a way that is easy to detect [IK88].
It has been successfully used in two dimensions to detect
otherprimitivessuchasellipses[TM78]orcorners[Dav88].
Figure 3: Reconstructed normals of a corner with sharp WiththegeneralizedHoughtransform(GHT)[Bal87],Bal-
densityvariation:Lietal.’salgorithm(left)andours(right). lard shows that the method can also be applied to detect
non analytic features in images. The Hough transform has
beenappliedinmanywaysfordetectionandclassification
faceboundaries).Alliezetal.[ACSTD07]addressthisissue
[Low04,GL09,BLK10,Oka09].Itgeneralizestohigherdi-
withaVoronoï-PCAmethodthatprovidessomecontrolover
mensionsandhasbeenusedinparticularfor3Dsegmenta-
smoothness. More recently, both noise and sharp features tion,recognitionandregistration[KPW∗10,PWP∗11].The
have been treated explicitly by Li et al. [LSK∗10], com-
generalalgorithmhasalsobeenmodifiedandimprovedfor
biningarobustlocalnoiseestimationandaRANSAC-like
speed, robustness and precision. As going through all the
methodthatisparameterizedbytheestimatednoisescale.It
datamaytaketoomuchtime,Kiryatietal.[KEB91]propose
handleswellnoiseandoutliers.Butitisnotveryfast(typi-
a probabilistic version, consisting in observing only sub-
callyaroundhalfanhourfor1.5millionpoints).Besides,it
sets.Xuetal.[XOK90,XO93]defineaRandomizedHough
doesnotaddressvariationofdensityatedges.
Transform,whereapointdoesnotvoteforalltheprimitives
Yet, sampling anisotropy is common in laser data, es- towhichitbelongs;thevoteisassociatedtoaprimitivecom-
pecially when scanning objects with access constraints or putedfromasubsetofpoints.Thecriteriontostoppicking
abruptvariations,suchasbuildings.First,thescanningde- moreprimitivesisauser-defined,globalcondition.
vicemaynotspacesamplesevenly.Forinstance,laserswith As our objective is the estimation of a single normal at
arotatingheadthatareusedtoscantheirsurroundingssys- eachpoint,weadapttheRandomizedHoughTransformto
tematically oversample the upper polar region (vertical di- search for only one primitive. One original aspect of our
rection),comparedtotheequatorialband(horizontaldirec- methodisthatweuseastopcriterioninheritedfromrobust
tions),typicallywithafactorofoneortwoordersofmag- statisticstoendexploringthespaceofprimitives.Thisen-
nitude. (The lower pole is generally occluded by the tri- suresbothspeedandrobustnesstopartialsampling.Robust-
podsupport.)Second,evenwhendevicesamplingismostly nesstodensityanisotropyisobtainedbyselectingprimitives
uniformly spaced, e.g., locally when focusing only on a asuniformlyaspossibleintheneighborhoodoftheconsid-
small area, the actual 3D spacing of the sampled data de- eredpoint.Wedonotaddresstheproblemoforientatingnor-
pendsontheincidenceofthelaseronthesurface.Asman- mals,whichcanbedoneseparately[HDD∗92,MdGD∗10].
made objects often present sharp features, a significant lo- Inthefollowing,wefirstgiveageneraloverviewofournor-
calanisotropyfrequentlyappearsinareasaroundedges.But malestimator(cf.§2)anddescribeourRobustRandomized
even a smooth object shows sampling variations when the HoughTransform(cf.§3).Thenweexplainhowtocompute
laser beam is almost tangent to the surface. This is illus- thenormalsdespitediscretization(cf.§4)andanisotropy(cf.
tratedinFigure2(left).Moreover,variationsofdensitycan §5),andfinallypresentourresults(cf.§6).
alsoappearinpointcloudsoriginatingfromphotogramme-
try,evenonsmoothsurfaces,becauseofreconstructioner-
2. Algorithm
rorsandimprecision,e.g.,duetospecularortexturelesssur-
faces.(Inourexperimentswithlasersandphotogrammetry, Beforedescribingouralgorithm,letusconsiderthefollow-
weobservedthatnoisewasnotisotropiceither;itdepends ingsimplesituation.LetPbeapointonapiecewiseplanar
onthesurface,inparticularontheviewpointincidence.)Yet, surfaceandletNPbeaneighborhoodofPonthissurface(a
most normal estimation methods do not have any specific subsurface).Therearetwobasiccases.
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 3 ---

A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures 1767
• IfPliesfarfromanyedgeorsharpfeature,thenpicking 3. RobustRandomizedHoughTransform
threepointsinNPdefinestheplanarpatchthatPlieson,
Thenumberofpossibletriplestopickcanbehuge,onthe
andthusthenormal(ifthepointsarenotcollinear). orderof|NP|3.Wethuswanttoconsideronlyasubset,and
• IfPliesnearanedgepartitioningtheneighborhoodNP
wewanttostopsamplingtriplesassoonasweareconfident
intoN1,P∪N2,P withP∈N1,P ,thenpickingthreepoints
enoughthatwecantakeadecisionbasedontheempirical
inNPdoesnotnecessarilydeterminetherightnormal.It
distributionintheaccumulator.Usingrobuststatisticstools,
defineseitherthecorrectnormal(ifallpointslieinN1,P ),
wedetermineageneralupperboundonthenumberoftriples
or the normal associated with the plane on the opposite
todraw,andpossiblyimproveitwhenenoughconcentration
sideoftheedge(ifallpointslieinN2,P ),ora“random”
inabincanbeguaranteed.ThisdefinesaRobustRandom-
plane(ifthepointsarenotonthesamesideoftheedge).
izedHoughTransform(RRHT)forestimatingnormals.
Inthesecondcase,asN1,P islikelytobelargerthanN2,P
In the following, we consider that the M bins of the ac-
since P∈N1,P is not exactly on the edge, the probability
cumulator follow a Bernoulli law with pm as parameter
of picking the dominant plane and thus the correct normal
(the theoretical mean). Let T be the number of planes to
ishigherthantheprobabilityofpickingthenormalonthe
oppositeedgeside.WhenPisveryclosetotheedge,drawn
pick and let (Xm,t)
m∈{1,...,M},t∈{1,...,T}
be the independent
andidenticallydistributedrandomvariablesassociatedwith
triplesarelikelytolieonbothsidesoftheedge.However,as
binmforplanet:Xm,t =1ifplanet votesforbinm,other-
itleadstoa“random”normal,thecorrectnormalstillisthe
wiseXm,t=0.Finally,wenotepˆmtheempiricalmeanofthe
o
ti
n
o
e
ns
w
w
it
h
h
er
t
e
he
P
h
i
i
s
g
c
h
l
e
o
s
s
t
e
p
t
r
o
o
s
b
e
a
v
b
e
i
r
l
a
it
l
y
e
.
d
T
g
h
e
i
s
s
,
g
in
e
c
n
l
e
u
r
d
a
i
l
n
iz
g
e
c
s
o
t
i
o
nc
s
i
i
d
tu
en
a-
t
proportionofvotesforbinm:pˆm=
T
1∑
t
T
=1
Xm,t.
edges.ThisideaappliesaswelltoapointcloudCinwhich Global upper bound. We want to stop drawing triples as
neighboringpointsNParedefinedforanypointP∈C. soonasweareconfidentenoughthattheempiricaldistribu-
Our method is a robust variant of this simple principle, tionisagoodapproximationoftheactualdistribution.For
to handle noise and outliers. For this, we sample as many this,wewanttoboundthedifferencebetweentheactualdis-
planesasnecessarytogainenoughconfidencethatwecan tributionpmandtheobserveddistributionpˆm.
identify the actual maximum of the probability density. In Let α ∈]0,1[ be a probability threshold expressing the
practice, we discretize the problem and fill a Hough accu- confidence when comparing pm to pˆm, and let δ∈]0,1[be
mulatoruntilanormalcanbeconfidentlychosenbasedon adistance.Wewishtoestimatetheminimalnumberofsam-
themostvotedbin,withthefollowingrefinements: plesT
min
suchthat,foreachbinm,theempiricalmeanpˆmis
• In the presence of noise and outliers, the discrete prob- atmostatdistanceδfrompm,withprobabilityatleastα:
ability distribution of possible normals is flatter, but the P( max |pˆm−pm|≤δ)≥α (1)
principalnormalremainstheonewiththehighestnumber m∈{1,...,M}
ofvotes.Toensureresistancetonoiseandoutliers,weuse
AsXm,t arei.i.d.,Hoeffding’sinequality[Hoe63]applies:
robuststatisticalboundsonthenumberoftriplestopick
(
b
c
e
f
c
.
a
§
u
3
s
)
e
.
i
N
ti
o
s
t
u
e
n
t
l
h
ik
at
el
n
y
e
a
a
n
r
d
co
w
l
o
li
u
n
l
e
d
a
g
ri
e
ty
ne
n
r
e
a
e
te
d
a
n
“
o
r
t
an
b
d
e
o
c
m
he
”
c
n
k
o
e
r
d
-
∀m,P(|pˆm−pm|≥δ)≤2exp(−
∑ t T =
2
1
δ
(
2
b
T
t
m
−
2 in
at)2
) (2)
malanyway.Onthecontrary,checkingcollinearityateach
drawingwouldbeunnecessarilytimeconsuming.
where[at,bt]istheintervalwhereXm,t lies,i.e.,[0,1].Thus:
• Ifthesurfaceiscurvedratherthanpiecewiseplanar,the P(|pˆm−pm|≥δ)≤2exp(−2δ2T min) (3)
discrete probability distribution of possible normals is
Asthisistrueforallm,wehave:
flatter too. However, as long as the surface can be lo-
cally approximated with planes, the right normal stays P( max |pˆm−pm|≥δ)
themostvotedone.Thesizeoftheneighborhoodimpacts m∈{1,...,M}
normalestimation:itshouldbesmallenoughforthepla- = P(∃m∈{1,...,M},|pˆm−pm|≥δ) (4)
nar approximation hypothesis to hold, and large enough M
fornoisetobeaveraged.(Neighborhoodsarediscussedin ≤ ∑ P(|pˆm−pm|≥δ) ≤ 2Mexp(−2δ2T min) (5)
Section5,togetherwithanisotropy.) m=1
• UsingaHoughaccumulatorintroducesdiscretizationar- Hence:
tifacts.First,asinglebincorrespondstoasmallrangeof P( max |pˆm−pm|<δ)≥1−2Mexp(−2δ2T min) (6)
normals(acone).Ratherthanassociatingafixednormal m∈{1,...,M}
toabin,weaveragethenormalsthatvoteforthebin.Sec- Now,consideringequation(1),T hastosatisfy:
min
ond,weactuallyestimatethenormalseveraltimesusing
1 2M
randomizedaccumulatorsandusethebestones(cf.§4). T min≥
2δ2
ln(
1−α
) (7)
• To deal with density anisotropy, we propose a spatially-
sensitivedrawingschemethatcomesintwoversions,con- We may thus choose T R = 2 1 δ2 ln( 1 2 − M α ) as an upper bound
tinuousandprecise,ordiscreteandfast(cf.§5). onthenumberoftriplestobedrawn.
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 4 ---

1768 A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures
Confidenceinterval. Ifwepickmostofthetimethesame n M
φ
bin,wewanttostoptheselectionoperation.Weuseacon-
5 23
fidenceintervaltoidentifyitastherightbintochoose.But
10 82
itisactuallyenoughtochooseabinthatgetssignificantly
15 171
morevotesthanothers,i.e.,morevotesthanthesecondmost
20 290
votedbin.Theabovecriteriononlyconcernstheaccuracyof
25 441
theglobaldistribution.Whatwewanttoestimatehereisthe
confidence interval of each pm. If the intervals of the two Figure4:Accumulatorshape&sizeforvariousn
φ
values.
mostvotedbinsdonotintersect,wearealmostsurethatthe
mostvotedbinwillnotchangeandwecanstoppickingmore
triples.
WithreferencetoBorrmannetal.’spaper,wedefinen as
φ
AccordingtotheCentralLimitTheorem,therandomvari- thenumberofslicesofthesphereinthez-axisandn
θ
=2n
φ
abledefinedas thenumberofbinsattheequator.Asweonlyestimatehere
pˆm−pm thenormaldirection,nottheorientation,weusehalfofthe
binsintheaccumulator.Figure4showsthevalueofthetotal
pm(pm−1)/T
numberofusedbinsMforseveralvaluesofn :
φ
convergesindistributpiontoastandardnormalrandomvari-
ableN(0,1).Letr∈]0,1[beaconfidencelevelandletz(r)
besuchthattheintegraloftheGaussiandensitybetween−z 4. Discretizationissues
andzisr.Theconfidenceintervalisthen:
Afterplanesampling,thechosennormalisgivenbythemost
pm(1−pm) pm(1−pm) votedbin.However,ratherthanproducingadiscretenormal
pˆm−z(r)
T
≤pm≤pˆm+z(r)
T (oneperbinintheaccumulator),weaverageallthenormals
r r
(8) thatvotedforthechosenbin.Fromtheimplementationpoint
Aspm∈[0,1],wehavepm(1−pm)≤ 1
4
andthus: ofview,samplednormalsdonotactuallyhavetobememo-
rized.Itisenoughtoonlyrecordincrementally,foreachbin,
z(r) 1 z(r) 1
pˆm− ≤pm≤pˆm+ (9) thesumofthenormalsthatvotedforthebin.Whenabinis
2 r T 2 r T chosen,thesumofitscontributingnormalsisrenormalized
Using,e.g.,theconfidencelevelr=95%,wehavez(r)≃2. tolieontheunitsphere,yieldingthefinalnormal.(Wehave
In this context, if m is the most voted bin and m is the alsoexperimentedmemorizingforeachbinthevotingtriples
1 2
secondmostvoted,wecanstopsamplingplanesassoonas ofpoints,andcomputingthefinalnormalastheregression
theconfidenceintervalsofthesetwobinsdonotintersect: planeofthepointsinthemostvotedbin:thenormalpreci-
sionisslightlybetter,buttherunningtimeismuchlonger.)
1
pˆm1 −pˆm2 ≥2
T
(10)
Usingadiscreteaccumulatorleadstootherissues.First,
r
thereisabinningeffect.Ifthemainpeakofthedistribution
ThistestmaylowertheglobalboundT Rbutdoesnotreplace
of normals lies near a bin boundary, votes will be almost
it(cf.§6).E.g.,forapointlyingveryclosetoanedge,the
equallydistributedbetweentwo(ormore)adjacentbins.Af-
twobinscorrespondingtothenormaloneachedgesideare
teralimitednumberofplanepickings,theactualpeakmay
likelytohavesimilarprobabilities.Confidenceintervalsthen
notbeinthemostvotedbin.Theeffectisstrongerfornoisy
requirealargeT before(10)issatisfied.Inextremecases,
data as the distribution is flatter and votes are distributed
intervalsalwaysoverlapandtheconditionisneverfulfilled.
into more bins, that receive less votes on average. Second,
binsarenotisotropic.Theaccumulator ofBorrmannetal.
Accumulator shape. As we only estimate the normal di-
[BELN11]guaranteestheareaofallbinstobenearlyequal,
rection,nottheorientation,pickingatripleofpointsdefines
buttheirshapeisdifferent.Forinstance,polarbinsaredisks
anormaldescribedbytwoangles(θ,φ),moduloπ.Itvotes
(caps) whereas equatorial bins are squares. A classic solu-
intoanaccumulatorthatpartitionshalftheunitsphereinto
tiontobindiscretizationwouldbeforanormaltoshareparts
similarbins.WeusethesphericalaccumulatorofBorrmann
of its vote in neighboring bins, but it does not answer the
etal.[BELN11]thatprovidesbinswithsimilarareaandal-
secondproblem.Bothissuescanbeaddressedbyrandomly
lowseasyandfastcomputationofbinindexes,giventhenor-
rotatingtheaccumulatorandrunningthealgorithmseveral
malangles.Thisaccumulatorfirstdividesthesphereaccord-
times.Themorerotations,themoreprecisetheestimation.
ing to the parallels, with same angle size. Then the slices
Inpractice,asfewas5rotationsareenoughtocompensate
(between two parallels) are divided into bins of nearly the
formostdiscretizationeffects(cf.§6).Fromtheimplemen-
samearea(cf.Fig.4).Wehavechosenthisaccumulatoraf-
tationpointofview,itismoreefficienttorotatethepoints,
teralsotestingageodesicsphereaccumulator,thatismore
ratherthanrotatingtheaccumulatoritself.
isotropicbutconsiderablyslowerforalimitedprecisionim-
provement. Aswerunthealgorithmseveraltimes,wegetseveralnor-
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 5 ---

A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures 1769
mals.Choosingtheappropriateonedependsofthesampled Letcbethecubediscretizationfactor:BP iscoveredbyc3
surfaceandonapplications.Weproposethreealternatives: smallcubes.Thebiggerc,themoreuniformthepicking,but
thehighertheprobabilitythatthesmallcubeisemptytoo.
• RRHT_m:theproducednormalisthemeanofthenor-
Inourexperimentswiththisstrategy,notedRRHT_Cubes,
malsweightedbytheirnumberofvotes.Comparedtothe
c=4providesagoodrobustnesstoanisotropywithoutslow-
otheralternatives, itminimizes the rootmean squareer-
ingdowntoomuchpointpicking.(Notethatthesemethods
ror,butitfailstoestimateaplausiblenormalnearedges
forpickingtriplesareactuallynotspecifictoournormales-
becauseofthesmoothingeffectofthemean.
timator.Theycouldbeappliedtoanyalgorithmusingaran-
• RRHT_b: the produced normal is the best one, i.e., the
domtripleselection,likeLietal.’sestimator.)
most voted one. Sharp features are well estimated, but
smoothsurfacesmayappeargrainy. We also consider a simple and fast version of our al-
• RRHT_c:wefirstclusternormalsthatareclosedtoeach gorithm, noted RRHT_Points, without any support for
other(withinananglethresholda )andthencompute anisotropicsampling.Itappliestopointcloudswithoutsig-
cluster
theaveragenormalofthemostvotedcluster.Althoughit nificant density variation: points are just picked randomly
introduces an extra parameter, it is a good compromise inNP.Moreprecisely,weuseacostlesscombinatorialor-
betweentheothertwoalternatives. dertomakesurethatwedonotpickseveraltimesthesame
triple,whichcouldbelikelyforasmallNP.
5. Dealingwithsamplinganisotropy
6. Experiments
Tripleselectionisthekeytorobustnesstodensityvariation.
GivenapointPanditsneighborhoodNP,selectingrandom Theparametersofouralgorithmaresummarizedbelow:
triplesintheneighborhoodwouldbesensitivetosampling
• Korr:numberofneighborsorneighborhoodradius,
density:tripleswouldbeselectedwithahigherprobability
inregionsofhighdensity.Wedefinethreevariants.
• T R:numberofprimitivestoexplore,
• n φ :parameterdefiningthenumberofbins,
Forrobustnesstodensityvariation,wedefineNP asthe • nrot:numberofaccumulatorrotations,
pointsinaballBParoundPwithagivenradiusrandchoose • c:presamplingordiscretizationfactor(anisotropyonly),
apresamplingfactorc.TopickapointinNP,werandomly • a cluster :toleranceangle(meanoverbestclusteronly).
pickasmallballBofradiusr/cinBP,thenpickarandom
ThechoiceofK orrdependsonthesurfacesamplingden-
pointinB.IfBisempty(nointersectionwiththesampled
surface), we just pick another small ball until we find one sityandnoise,w.r.t.thelevelofdetails.ParametersT R and
which is not empty. As shown below, this strategy, noted
nrot can be used to balance precision and execution time.
Unlessotherwisementioned,inallourexperimentswetake
RRHT_Unif,givesgoodresultsbutitisrelativelyslow.
T R=700,n φ=15,nrot=5,c=4anda cluster= π
4
.Inthis
Asweareinterestedinagoodcompromisebetweenpre- settings, confidence α=0.95 corresponds to δ=0.08 for
cision and speed, we propose a discretized version of this theglobalupperboundonthenumberofdrawings.Finally,
spatially-sensitivedrawingscheme.Wediscretizedtheball onsyntheticdatawealwaysuseK=500;onrealdatawe
into small cubes and we randomly pick small cubes rather useeitherK=500oraneighborhooddefinedbyaball.
thansmallballs.ThisonlyrequiresfastoperationsonCarte-
We compare our algorithm with some of the existing
siancoordinatesratherthanheaviertrigonometriccomputa-
ones:leastsquareregressionwithplanes[HDD∗92]imple-
tions.Yet,asallsmallcubesarenotfullyincludedintheBP
mentedinthePointCloudLibrary[PCL],jetfitting[CP03]
ball,weassignthemweightsaccordingtotheproportionof
from CGAL [CGA], Tamal Dey’s NormFet implementa-
theirintersectionwithBP(cf.Fig.5).Theseweightsareused
tion[DG06]andLietal.’salgorithm[LSK∗10].Theseare
to define the probability of picking any given small cube.
“direct”normalestimators.Nopostprocessingisconsidered
here.Inthefollowing,wealwaysusethesameparameters
forthesemethods:K=80forregressionandjetfitting(as
it is most sensitive to neighborhood size), K =500 for Li
etal.’salgorithm.Allotherparameters,ifany,aresettode-
fault.Weevaluateprecisionandspeedondatawithartificial
noise:acenteredGaussiannoisewithdeviationdefinedasa
percentageofthediagonaloftheaxis-alignedboundingbox.
Computationtime. Allexperimentshavebeenperformed
Discretizationofthesphereforc=4(left)andsmall onthesamecomputer,with2CPUsIntel(R)Xeon(R)X5472
cubeprobabilities(right):darkerismoreprobable. 3.00GHz, 4 threads each. Our algorithms are parallelized.
Figure5:Discretizationoftheneighborhoodball. Othershavebeenusedaswegotthem.OnlyLietal.’swas
adaptedtouseourdatastructure,takenfromPCL[PCL].
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 6 ---

1770 A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures
T R=700 T R=300
nrot=5 nrot=2
w/o with w/o with
Model(#vertices) interv. interv. interv. interv.
Armadillo(173k) 21s 20s 3s 3s
Dragon(438k) 55s 51s 8s 7s
Buddha(543k) 1.1 1 10s 10s
Circ.Box(701k) 1.5 1.3 13s 12s
Omotondo(998k) 2 1.2 18s 10s
Statuette(5M) 11 10 1.5 1.4
Room(6.6M) 14 8 2.3 1.6
Lucy(14M) 28 17 4 2.5
Computation time for a point cloud on sphere with RRHT_Points algorithm with K =100 points in the
0.2%noiseasafunctionofthenumberofpoints. neighborhood,withandwithoutconfidenceintervals.
Figure6:Computationtime(globalupperboundonly). Alltimesareinminutesunlesssecondsmentioned.
Table1:Computationtimesonrealdata.
Figure7showstheratiooftheexecutiontimeswithand
withouttheconfidenceintervalcriterion(cf.§3),forvarious
noiselevels.(Theratiocanbeslightlygreaterthan1because
testingthecriterionaddsasmalloverheadthatisnotrepaid
if not satisfied.) We can see that the criterion significantly
reducescomputation.Asexpected,theimpactdecreasesas
noise level grows because the peaks of the distribution are
flatterandseparatingthehighestpeakfromthesecondhigh-
esttakeslonger.Theimpactalsodependsonthemodel.For
curvedandcomplexshapes,ittakemoretimeforapeakto
confidentlyemergefromtherestofthedistribution.Table1
shows computation times of RRHT_Points for several real
Ratiooftheexecutiontimewithandwithoutthecon-
models.Astheyrequirelessneighborsbecausetheyarenot
fidenceintervalcriterionasafunctionofnoiselevel.
noisy,weuseK=100.Formodelswithalotofplanarsub-
Figure7:Impactoftheconfidenceintervalcriterion.
surfaces (like the room scan, cf. Figure 2), the addition of
theconfidenceintervalcriteriaisveryuseful.
Toillustrateseparatelythedifferentcontributions,wefirst Precision. Wecomparethevariousalgorithmswithtwoer-
display the computation time with the robust global upper rormeasures:
boundonly(cf.§3),disablingtheconfidenceintervalcrite-
• RootMeanSquare(RMS):
rion.Figure6showsthecomputationtimeasafunctionof
t s h a e m n p u le m d b o e n r a of sp p h o e in re ts , . w T e h a e d p d o e i d nt 0. c 2 l % oud no h is a e s . b A e s en w u e n c i a fo n rm se l e y , RMS= |C 1 | ∑ n P \ ,ref n P,est 2
s P∈C
even with five rotations, the two fast versions of our algo-
rithm(RRHT_CubesandRRHT_Points)arecomparableto • RootMeanSquarewiththreshold(RMS_τ):
jetfittingandtheVoronoï-basedmethod(NormFet),andare
1
fasterthanLietal.’salgorithm(withthesameneighborhood RMS_τ= ∑ v2
|C| P
size).RRHT_Unifismuchslowerthanexistingalgorithms, s P∈C
butfullyhandlesanisotropy.Thecomplexityisbasicallythe
where
sameforourmethodsandforLietal.’s:itisO(nlogn)where
n=|C|isthetotalnumberofpointsinthecloud.Bothusea v P= n
π
P \ ,ref n P,est
o
if
th
n
e
P
r
\ ,r
w
ef
i
n
se
P,est<τ
Kd-treeforneighborhoodsearch(thelognfactor)andrepeat (cid:26) 2
asimilaroperationforeveryvertex(thenfactor).However, n
P,ref
isthereference(theoretical)normalatPandn P,estthe
complexityconstantsdiffermuchinpractice.Notethatour estimatedone.RMSisacommonmeasureforprecision,but
parametersettinghereisverydemandingonprecisionasex- itisnotreallyarenderingmeasurebecauseitfavourssmooth
plainedbelow.Figure6isthusakindofworstcasescenario reconstruction everywhere, including at edges: a smoothed
for our method. Much faster computation results are given edge is better than having some points with a normal cor-
laterwithonlyaslightlyreducedprecision. responding to the other side of the edge. That is why we
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 7 ---

A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures 1771
Fromlefttoright:Jetfitting,Lietal.’snormalestimator,RRHT_Cubes_candRRHT_Unif_c.Colorscale:
bluetogreenis0to10-degreeerror,redcorrespondstoanerrorgreaterthan10degrees.
Figure8:Visualrenderingoftheprecisionforthreealgorithmsonacylinderof50000pointswith0.2%noise.
DashedisRRHT_m(mean),dottedisRRHT_b(best), Solid curves represent a fixed nrot for a varying T R,
solidisRRHT_c(meanoverbestcluster). dashedcurvesrepresentafixedT Randvaryingnrot.
Figure9:RMSandRMS_10for50k-pointclosedcylinder. Figure10:Precisionvsspeedforacubewith50kpoints.
introduce RMS_τ. This measure considers any angle error sures, unlike other methods. As expected, RRHT_m gives
above threshold τ as very bad (as bad as π). This penal- better results for RMS, because of its tendency to smooth,
2
izesunwantedsmoothing,i.e.,equallydivergentestimations. butnotforRMS_10.RRHT_coffersthebettercompromise.
Point clouds with normals are better rendered with lower NotethatDey’salgorithmisthemostefficientforverysmall
RMS_τ than lower RMS. In our examples we take τ=10 noise.Fig.12illustratesanapplicationtodatawithrealistic
degrees.Figure8isavisualrepresentationoftheRMS_10 noise.SeealsoFig.7fortheimpactofnoiseonrunningtime.
error.Reddotsarethebadlyestimatedpoints.RRHTrecon-
structssmoothsurfaceswithoutgrainyeffect,butitisnotas Parameter (in)sensitivity. Figure 10 represents precision
discriminativeasLietal.’sclosetotheedge. versuscomputationtime,forafixednrotorafixedT R,anda
noisycube.Inourexperiments,changingthemodelornoise
Robustnesstonoise. Figure9displaysprecisionasafunc- does not affect the curve aspect. Two points are marked,
tionofnoise.Ouralgorithmsarecompetitivewithbothmea- (T R =700,nrot =5) and (T R =300,nrot =2), which cor-
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 8 ---

1772 A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures
Fromlefttoright:RRHT_Points_c,RRHT_Cubes_candRRHT_Unif_c.Colorscale:bluetogreenis0to
10-degree error, red corresponds to an error greater than 10 degrees. Density is uniform on each face; if
densityis1onrightface,thenitis5fortheleftfaceand10fortheupperface.
Figure11:Visualrenderingoftheprecisiononacornerof20000pointswithdensityanisotropy.
RRHT_Cubes_cwitharadiussearchcorrespondingto
0.1%oftheboundingboxdiagonal. DashedisRRHT_m(mean),dottedisRRHT_b(best),
solidisRRHT_c(meanoverbestcluster).
Figure12:VisualrenderingoftheChâteaudeSceaux,point
cloudobtainedbyphotogrammetry. Figure13:RMS_10foracornerwithdensityanisotropyof
20000points(seeFigure11),functionofnoisepercentage.
respondtovaluesusedintable1.Thefirstonefavorspre-
thesameasafixedballofradiusr.Butvariationsofdensity
cision;thesecondone,speed.Otherexperimentsshowthat
inrealdatamakeneighborhoodswithafixedKinoperative.
timevarieslittlewithc(2–2.5slowerwhencvariesfrom2
E.g., the peak of density at the pole of a laser scan can be
to10),norRMSor RMS_10(25–30%precisiongain). As
suchthattheactualneighborhoodradiusrcorrespondingto
forn ,itshouldbelargeforprecision,smallforrobustness.
φ
afixednumberofpointsKisonthesameorderasthenoise
Experimentally,bestn valuesforbothRMSandRMS_10
φ
standard deviation. It is thus too small to cope with noise.
areintherange5–25,withn φ=15atmost10%fromopti-
Conversely, data sparsity may also lead to mistakes. E.g.,
mum.Timelessthandoubleswhenn variesfrom5to25.
φ
the leg of a chair in the room (cf. Fig.2) may be sampled
withjustafewpointsandafixednumberofpointsK will
Robustness to density anisotropy. Figure 11 shows the
easilyincludemanypointsonthefloororonsittingarea,as
normals computed on a corner sampled with face-specific
opposed to a fixed neighborhood radius r. For this reason,
variationsofdensity.Asexpected,nosupportforanisotropy
whenoperatingonrealdata,weuseaneighborhoodball.
(RRHT_Points)isverybadwhereasuniformballsampling
(RRHT_Unif)recoverswelltheedges.Thecubicdiscretiza- Figure 14 shows two dragons with estimated normals
tion(RRHT_Cubes)compromiseswellprecisionvstime(cf. wheretheneighborhoodradiusvariesfromlefttoright.As
Fig.6).Figure13showstheRMS_10errorfordifferental- wecanseeonthemodelwithoutnoise,iftheradiusisgreat
gorithms.Asexpected,wehaveabetterprecisionthanthe wetendtoloosesmalldetails,butgeneralshape,withsharp
othermethods,thatarenotdesignedtodealwithanisotropy. features,ismaintained.Thenoisydragonillustratesthefact
iftheradiusissmallerthanthenoise,itcannotestimatenor-
Intheaboveexperiments,neighborhoodsaredefinedwith
mals,buttheresultsgoesbetterwiththeincreasingradius.
a fixed number of points K. This allows comparison with
otheralgorithmsbecausetheyusethesamenotionofneigh- Figure2displayspartsofalaserscanofameetingroom
borhood.Also,thisdataisuniformlysampled(exceptforthe with a few computers. The density of the point cloud de-
anisotropiccorner);afixednumberofpointsKisthusnearly pends a lot of the incidence. The original point cloud has
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 9 ---

A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures 1773
Armadillowith0.2%noiseand+100%outliers(top),
+300% (bottom). Neighborhood radius r is 3% of
boundingboxdiagonal.Outliersareuniformlydrawn
atdistanceatmostr fromoriginalpointset.Outliers
aredroppedinresultforvisualrending(right).
Figure15:OutlierrobustnessofRRHTonArmadillo.
RRHT_Cubes_c with a neighborhood radius ranging 7. Conclusion
fromavaluecomparabletothenoisestandarddevia-
Wehaveproposedanovelmethodforestimatingnormalsfor
tion,ifany(left),togreaterradiusvalues(right).
pointcloudsthatpreservessharpfeaturesandthatisrobust
Figure14:Normalestimationforthedragonwithnoadded tonoiseandoutliers.Differentvariantsorparametersettings
noise(top)vs0.2%addednoise(bottom). offergoodcompromisesbetweenprecisionandcomputation
time.Wehaveshownthatourmethodisatleastasprecise
andnoise-resistantasstate-of-the-artmethodsthatpreserve
6.6millionsverticesandwitharadiusof0.05(5cmatthe sharp features, while being almost an order of magnitude
modelscale),thecomputationtimeforRRHT_Cubesis41 faster. It can also handle anisotropy with minor speed and
min with confidence intervals, as opposed to 48 min with- precisionlosses.Besides,itissimpleandeasytoprogram.
out,i.e.,a15%improvement.Twodetailsareunderlined:a
Asfuturework,itwouldbeinterestingtoimprovespeed
cornerwithdensityanisotropyandabevelededge.Notethat
withanadaptivechoiceofvariantsandparameters,andthe
thereisasubstantialdifferencewiththetimefiguresinTa-
time saved could in turn be traded against more precision.
ble1.Thereasonisthatsearchingfor100neighborsismuch
The idea would be to use cubic drawing by default but to
fasterthansearchingwithinagivenradius,whichcancorre-
locallyswitchtotheuniformdrawingschemewhereestima-
spondtothousandsofneighbors(ormore)whenclosetothe
tionisknowntobeinaccurate.Also,anadaptiveneighbor-
pole.Also,Table1usesRRHT_Points,notRRHT_Cubes;
hoodwouldhelpreducingthenumberofparameters.More-
nothavingtopickinthecubeisfaster.
over, some kinds of laser scans organize data; preliminary
experimentsshowthatmakingagoodusethisorganization
Robustnesstooutliers. Ourmethodisrobusttoalargeout- greatlyreducescomputationtime.
lierratio.WeillustratethatontheArmadillowithbothadded
noiseandoutliers(Figure15).TheRMSonthemodelwith
Acknowledgments
0.2% noise is 0.39; adding +100% outliers yields an extra
+0.04ontheRMS;adding+300%outliersyields+0.15on WearegratefultoPierreAlliezforhelpfuladvices.Thanks
theRMS.Whenthecontaminationratioincreases,thefirst to Bao Li and Tamal Dey for providing the code used for
points not well estimated are the sharp points, whose nor- comparison. Armadillo, Lucy, Dragon, Buddha, and Stat-
mal distribution is flatter than the distribution on a regular uette point clouds come from the Stanford 3D Scanning
surface:theyaremoresensitivetonoiseandoutliers. Repository;CircularBoxandOmotondofromAim@Shape.
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.

--- Page 10 ---

1774 A.Boulch&R.Marlet/FastandRobustNormalEstimationforPointCloudswithSharpFeatures
References [JDZ04] JONEST.R.,DURANDF.,ZWICKERM.: Normalim-
provementforpointrendering. IEEEComputerGraphicsand
[ABCO∗01] ALEXAM.,BEHRJ.,COHEN-ORD.,FLEISHMAN Applications24,4(2004),53–56.1
S.,LEVIND.,SILVAC.T.: Pointsetsurfaces. InIEEEVisual-
ization(October2001),IEEEComp.Soc.,pp.21–28.1 [KEB91] KIRYATI, ELDAR, BRUCKSTEIN: A probabilistic
Hough transform. PATREC: Pattern Recognition, Pergamon
[ACSTD07] ALLIEZ P., COHEN-STEINER D., TONG Y., DES- Press24(1991).2
BRUN M.: Voronoi-based variational reconstruction of unori-
entedpointsets.InProceedingsoftheFifthEurographicsSympo- [KPW∗10] KNOPPJ.,PRASADM.,WILLEMSG.,TIMOFTER.,
siumonGeometryProcessing,Barcelona,Spain,July4-6,2007 GOOL L. J. V.: Hough transform and 3D SURF for robust
(2007),BelyaevA.G.,GarlandM.,(Eds.),vol.257ofACMIn- threedimensionalclassification.In11thEuropeanConferenceon
ternationalConferenceProceedingSeries,EurographicsAssoci- ComputerVision(ECCV2010),Proc.,PartVI(2010),vol.6316
ation,pp.39–48.2 ofLNCS,Springer,pp.589–602.2
[Bal87] BALLARD D. H.: Generalizingthehoughtransformto
[Low04] LOWE D. G.: Distinctive image features from scale-
detectarbitraryshapes.InRCV87(1987),pp.714–725.2 invariantkeypoints. InternationalJournalofComputerVision
60,2(2004),91–110.2
[BE
N
L
Ü
N
C
1
H
1
T
]
ER
B
A
O
.
R
:
R
T
M
he
AN
3D
N
H
D
o
.
u
,
g
E
h
L
T
S
r
E
a
B
ns
E
f
R
o
G
rm
J
f
.
o
,
r
L
pl
I
a
N
n
G
e
E
d
M
et
A
e
N
ct
N
ion
K
i
.
n
, [LSK∗10] LI B., SCHNABEL R., KLEIN R., CHENG Z.-Q.,
pointclouds:Areviewandanewaccumulatordesign. 3DRes.
DANG G., JIN S.: Robustnormalestimationforpointclouds
withsharpfeatures. Computers&Graphics34,2(2010),94–
2,2(Mar.2011),32:1–32:13.4
106.2,5
[BL
te
K
ct
1
i
0
o
]
nof
B
m
AR
ul
I
t
N
ip
O
le
VA
ob
O
je
.
c
,
t
L
in
E
s
M
ta
P
n
I
c
T
e
S
s
K
u
Y
si
V
ng
.S
H
.
o
,
u
K
g
O
h
H
tr
L
a
I
n
P
sf
.:
or
O
m
n
s.
d
I
e
n
- [MdGD∗10] MULLENP.,DEGOESF.,DESBRUNM.,COHEN-
CVPR(2010),IEEE,pp.2233–2240.2
STEINERD.,ALLIEZP.: Signingtheunsigned:Robustsurface
reconstructionfromrawpointsets. Comput.Graph.Forum29,5
[CGA] CGAL, Computational Geometry Algorithms Library. (2010),1733–1741.2
http://www.cgal.org.5
[MNG04] MITRA N. J., NGUYEN A., GUIBAS L.: Estimating
[CLP10] CHAUVE A.-L., LABATUT P., PONS J.-P.: Robust surface normals in noisy point cloud data. In special issue of
piecewise-planar3Dreconstructionandcompletionfromlarge- InternationalJournalofComputationalGeometryandApplica-
scaleunstructuredpointdata. InCVPR(2010),IEEE,pp.1261– tions(2004),vol.14,pp.261–276.1
1268.1
[ÖGG09] ÖZTIRELI A. C., GUENNEBAUD G., GROSS M. H.:
[CP03] CAZALSF.,POUGETM.: Estimatingdifferentialquanti- Featurepreservingpointsetsurfacesbasedonnon-linearkernel
tiesusingpolynomialfittingofosculatingjets.InProceedingsof regression.Comput.Graph.Forum28,2(2009),493–501.1
the2003Eurographics/ACMSIGGRAPHsymposiumonGeome-
[Oka09] OKADA R.: Discriminative generalized Hough trans-
tryprocessing(2003),SGP’03,pp.177–187.1,5
formforobjectdectection. InICCV (2009),IEEE,pp.2000–
[Dav88] DAVIES E. R.: ApplicationofthegeneralizedHough 2005.2
transformationtocornerdetection.ComputersandDigitalTech- [PCL] PCL,PointCloudLibrary.http://pointclouds.org.5
niques,IEEEproceedings15,1(1988),49–54.2
[PKKG03] PAULYM.,KEISERR.,KOBBELTL.P.,GROSSM.:
[DG06] DEYT.K.,GOSWAMIS.: Provablesurfacereconstruc- Shape modeling with point-sampled geometry. ACM Trans.
tionfromnoisysamples. Comput.Geom.35,1-2(2006),124– Graph.22,3(July2003),641–650.1
141.1,5
[PWP∗11] PHAMM.-T.,WOODFORDO.J.,PERBETF.,MAKI
[DH72] DUDAR.,HARTP.E.:UseoftheHoughtransformation A., STENGER B., CIPOLLA R.: A new distance for scale-
todetectlinesandcurvesinpictures. CACM15(1972),11–15. invariant3Dshaperecognitionandregistration.InIEEEInterna-
2 tionalConferenceonComputerVision,ICCV2011,Barcelona,
[GG07] GUENNEBAUD G., GROSS M. H.: Algebraicpointset Spain,November6-13,2011(2011),IEEE,pp.145–152.2
surfaces.ACMTrans.Graph26,3(2007),23.1 [RL00] RUSINKIEWICZ S., LEVOY M.: QSplat: A multireso-
lution point rendering system for large meshes. In Proc. of
[GL09] GALL J., LEMPITSKY V.: Class-specifichoughforests
theComputerGraphicsConference(SIGGRAPH)(2000),ACM
forobjectdetection.InCVPR(2009),pp.1022–1029.2
Press,pp.343–352.1
[HDD∗92] HOPPEH.,DEROSET.,DUCHAMPT.,MCDONALD
J., STUETZLE W.: Surface reconstruction from unorganized
[SWK07] SCHNABEL R., WAHL R., KLEIN R.: Efficient
RANSACforpoint-cloudshapedetection. ComputerGraphics
points. SIGGRAPHComput.Graph.26,2(July1992),71–78.
Forum26,2(June2007),214–226.1
1,2,5
[HLZ∗09] HUANGH.,LID.,ZHANGH.,ASCHERU.,COHEN-
[TM78] TSUJI S., MATSUMOTO F.: Detectionofellipsesbya
modifiedHoughtransformation.IEEETransactionsonComput-
OR D.: Consolidationofunorganizedpointcloudsforsurface
ers27(1978),777–781.2
reconstruction. ACMTrans.Graph.28,5(Dec.2009),176:1–
176:7.1 [XO93] XUL.,OJAE.: RandomizedHoughTransform(RHT):
Basicmechanisms,algorithms,andcomputationalcomplexities.
[Hoe63] HOEFFDING W.: Probability inequalities for sums of
CVGIP:ImageUnderstanding57,2(Mar.1993),131–154.2
boundedrandomvariables. JournaloftheAmericanStatistical
Association58,301(March1963),13–30.3 [XOK90] XUL.,OJAE.,KULTANENP.:Anewcurvedetection
method:RandomizedHoughTransform(RHT). PatternRecog-
[Hou62] HOUGH P. V. C.: Methodandmeansforrecognizing
nitionLetters11,5(May1990),331–338.2
complexpatterns.U.S.Patent3.069.654(1962).2
[YLL∗07] YOONM.,LEEY.,LEES.,IVRISSIMTZISI.,SEIDEL
[IK88] ILLINGWORTHJ.,KITTLERJ.V.:AsurveyoftheHough H.-P.:Surfaceandnormalensemblesforsurfacereconstruction.
transform.CVGIP:ImageUnderstanding44,1(Oct.1988),87– Comput.AidedDes.39,5(May2007),408–420.1
116.2
(cid:13)c 2012TheAuthor(s)
(cid:13)c 2012TheEurographicsAssociationandBlackwellPublishingLtd.```

---

## Section 1.8: Deep learning for robust normal estimation in unstructured point clouds

Source File: Deep learning for robust normal estimation in unstructured point clouds.txt

### Paper Content:

```
# Deep learning for robust normal estimation in unstructured point clouds.pdf
# Converted: 2025-07-18 12:59:05
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Deep learning for robust normal estimation in unstructured point clouds.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Deep learning for robust normal estimation in unstructured point clouds.txt


--- Page 1 ---

DOI: 10.1111/cgf.12983
EurographicsSymposiumonGeometryProcessing2016 Volume35(2016),Number5
MaksOvsjanikovandDanielePanozzo
(GuestEditors)
Deep Learning for Robust Normal Estimation
in Unstructured Point Clouds
AlexandreBoulch1 RenaudMarlet2
1ONERA-TheFrenchAerospaceLab,F-91761Palaiseau,France 2LIGM,UMR8049,EcoledesPonts,UPE,Champs-sur-Marne,France
Abstract
Normalestimationinpointcloudsisacrucialfirststepfornumerousalgorithms,fromsurfacereconstructionandsceneun-
derstandingtorendering.Arecurrentissuewhenestimatingnormalsistomakeappropriatedecisionsclosetosharpfeatures,
nottosmoothedges,orwhenthesamplingdensityisnotuniform,topreventbias.Ratherthanresortingtomanually-designed
geometricpriors,weproposetolearnhowtomakethesedecisions,usingground-truthdatamadefromsyntheticscenes.For
this, we project a discretized Hough space representing normal directions onto a structure amenable to deep learning. The
resultingnormalestimationmethodoutperformsmostofthetimethestateoftheartregardingrobustnesstooutliers,tonoise
andtopointdensityvariation,inthepresenceofsharpedges,whileremainingfast,scalinguptomillionsofpoints.
1. Introduction [CP05] have also been used to better adapt to the neighborhood
andtotheshapeoftheunderlyingsurface.Optimalneighborhood
Numerousalgorithmshavebeendevelopedtoprocesspointclouds,
sizescanbecomputed,w.r.t.curvature,samplingdensityandnoise,
such as geometric primitive extraction [SWK07], surface recon-
to minimize the estimation error [MNG04]. However, while be-
struction [BDLGM14,CLP10], 3D navigation [FGMP14], and
ing robust to noise, these methods remain sensitive to outliers.
point-basedrendering[RL00,ZPVBG01,ABCO∗03],justtoname
Improvements have been proposed to address robustness to out-
afew.Formanyofthem,theperformancesignificantlydependson liersandnon-uniformity,usingadaptiveweights[HLZ∗09].Yet,all
thequalityofnormalsestimatedateachpoint.
regression-basedmethodstendtosmooththenormalsatsharpfea-
Normalestimationisawell-studiedtopic.Theproblemistoin- tures.Moreover,ahigherrobustnesstooutliersisusuallyobtained
ferthelocalorientationoftheunknownsurfaceunderlyingapoint using a larger neighborhood, which makes sharp features even
cloud.Agoodestimatorshouldnotbesensitivetooutliers,tonoise smoother. Minimizing the (cid:96) 1 [ASGCO10] or (cid:96) 0 norm [SSW15]
and to variations of point density, which are common due to the is robust to sharp features but quite slow. Moving least squares
waypointcloudsarecaptured,e.g.,asmergedlaserscans,fusion [ABCO∗03,PKKG03] and local kernel regression [ÖGG09] es-
ofdepthimages,orstructure-from-motion.Non-uniformsampling, timate normals as the gradient of an implicit surface, preserving
withpossibleanisotropicbias,occursordinarilyduetovaryingin- sharpfeatures,butrequiringreliablenormalpriorsasinput.
cidencesonscannedsurfaces.Moreover,asmanycapturedscenes
DeyandGoswami[DG04]proposeanoriginalapproachbased
includeman-madeobjects,theygenerallyfeaturesharpedgesand
ontheVoronoïcellsofthepointcloud.Thenormalischosenasthe
corners,thathavetobepreservedandnotsmoothed.Last,estima-
cell direction with the largest extension. It is robust to sharp fea-
tionshouldbefast,typicallytoscaletomillionsofpoints.
tures,butsensitivetonoise.Toaddressit,Alliezetal.[ACSTD07]
Weproposehereanovelmethodfornormalestimationinunor- treatcelldistortionduetonoiseusingaPCA-Voronoïapproachto
ganizedpointclouds,thatisrobusttonoise,tooutliersandtoden- createelongatedcellsetsgroupingadjacentVoronoïcells.
sityvariation,inthepresenceofsharpedges.Itisbasedonarobust Lietal.[LSK∗10]usesampleconsensus(SAC),efficientlytreat-
randomized Hough transform [BM12], but rather than designing ingnoisydataandsharpfeatures.Themaindrawbacksarealack
explicitcriteriatoselectanormalfromtheaccumulator,welearna ofadaptationtopointdensityvariationsandahighcomputational
functionfordoingitusingaconvolutionalneuralnetwork(CNN). time.Anotherlineofworkisbasedonthedeterminationofcon-
Toourknowledge,thisisthefirstapplicationofdeeplearningtech- sistentpointclustersinaneighborhoodtobetterestimatenormals
niquestothiskindoftask,forunstructured3Ddata.Itoutperforms nearedgesandcorners.Zhangetal.[ZCL∗13]extractsuchclusters
mostofthetimethestate-of-the-artofnormalestimation. using low-rank subspace clustering with prior knowledge. Their
methodyieldsaccuratenormals,butisveryslow.Usingthesame
idea, Liu et al. [LZC∗15] overcome this issue by using a differ-
2. Relatedwork
ent representation for subspaces, and clustering only a subset of
Hoppeetal.[HDD∗92]computethetangentplaneatagivenpoint the points before propagating the results to adjacent points. The
byregressiononneighboringpoints.Spheres[GG07]andquadrics methodismuchfasterwhilebeingasaccurateas[ZCL∗13].
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohn
Wiley&SonsLtd.PublishedbyJohnWiley&SonsLtd.

--- Page 2 ---

282 AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds
Houghtransform isapopulartoolforshapeextraction[Hou62]. andassociatedfilledaccumulatorap,selectingabinbˆ p,andthusa
Itisbasedonachangeofspacewherethedesiredshapeisrepre- normaln .Theproblembecomesadiscreteversionof(1):
bp
sented by a point. Shape hypotheses populate the bins of an ac-
cumulator mapped onto this space, and the most densely popu- {bˆ p} p∈P = argmin ∑ φ(n bp ,n∗ p ) (2)
latedbinsidentifytheshapestoextract[DH72,IK88].Originally
{bp}p∈Pp∈P
designed for simple 2D primitive extraction in images [TM78, Inpractice,n∗ pisunknownandbˆ pisestimatedateachpointwitha
Dav88,SW02], it has since then been used for various purposes classifierovertheaccumulatorap.
from3Dprimitiveextraction[BELN11],recognitionandclassifi-
cation[KPW∗10,PWP∗11],togeneralmodelselection[Bal81].To [BM12] uses a very simple classifier: the most probable bin
w.r.t. the empirical probability distribution of normals in Hough
improve speed and scalability, Kiryati et al. [KEB91] propose a
space.Aspointedoutinthepaperitself,thisselectionprocessis
probabilisticversionoftheHoughtransformwhereonlyasubsetof
subjecttospacediscretization.Toovercomethiseffect,theauthors
theinputpointsvoteintheaccumulator.Ahighcomputationaleffi-
estimateanormalseveraltimesat p,randomlyrotatingtheaccu-
ciencyisreachedwiththeRandomizedHoughTransform[XO93],
mulatortochangediscretizationboundaries.Thefinalnormalisa
wherethepointsdonotvoteforallthepossibleshapes.Shapehy-
functionofthesefewestimatednormals,e.g.,theaveragenormal
pothesesaremadebydrawingtheminimalnumberofpointstopa-
ofthemostvotedclusterofnormals.Whilethistrickreducesdis-
rameterizetheshape,andeachsuchdrawnhypothesiscorresponds
cretization effects, it significantly increases the computation time
toonevoteintheaccumulator.Itresultsinasharperaccumulator
andintroducesadditionalparameters.Inthispaper,wedirectlyad-
distributionandafastermodelselection.However,itmaybedif-
dressEq.(1)asacontinuousproblem.Wewanttoconstructafunc-
ficulttotunethenumberofhypothesestodrawbeforeamodelis
estimatedfromthevotes.TheRobustRandomizedHoughTrans-
tionψsuchthat,givenafilledaccumulatorap,weproduce:
form(RRHT)addressesit[BM12]. ψ(ap)=nˆp (3)
Thisregressionproblemismoredifficultthanclassificationinthat
Convolutionalneuralnetworks,startingwithLeNet5[LBD∗89],
theregressorresponsecoversacontinuousspace,notjustasetof
arearchitecturedasasequenceofconvolutionalandpoolinglay-
discretevalues.Weactuallywanttolearnfunctionψ,usingaCNN.
ers,followedbyfully-connectedlayers.Theyweremostlyusedin
imageclassification,outperformingothermethodsbyalargemar-
CNNsforestimatingnormalsinpointclouds. Deeplearningis
gin[KSH12].Increasinglayernumber[SLJ∗14]andsize[ZF14],
good at making decisions in complex settings, especially when a
and using dropout to treat overfitting [HSK∗12], they have been
large number of unknown factors have a nonlinear influence. In
successfully applied, e.g., to object detection [GDDM14], seg-
particular,CNNsareveryefficientontaskssuchasobjectclassifi-
mentation [LSD15b] and localization [SEZ∗14]. Work on nor-
cationanddetection,includingwhenobjectsareseverelyoccluded.
mal estimation with CNNs focus on using as input RGB images
CNNs can also address regression problems such as object pose
[LSD∗15a,WFG15],orpossiblyRGB-D[BRG16],butnotsparse
estimation[PCFG12].Thesesamepropertiesseemappropriateas
datasuchasunstructured3Dpointclouds.CNN-basedtechniques
well for the task of learning how to estimate normals, including
havebeenappliedto3Ddatathough,butwithavoxel-basedper-
in the presence of noise and when several normal candidates are
spective[WSK∗15],whichisnotaccurateenoughfornormales-
possiblenearsharpfeaturesoftheunderlyingsurface.
timation. Techniques to efficiently apply CNN-based methods to
sparsedatahavebeenproposedtoo[Gra15],buttheymostlyfocus The question, however, is how to interpret the local neighbor-
on efficiency issues, to exploit sparsity; applications are 3D ob- hoodofa3Dpointasanimage-likeinputthatcanbefedtoaCNN.
ject recognition, again with voxel-based granularity, and analysis Ifthepointcloudisstructured,asgivenbyadepthsensor,thedepth
ofspace-timeobjects.Anolder,neuron-inspiredapproach[JIS03] mapisanaturalchoiceasCNNinput.Butifthepointcloudisun-
ismorerelevanttonormalestimationin3Dpointcloudsbutitactu- structured, it is not clear what to do. In this case, we propose to
allyaddressesthemoredifficulttaskofmeshing.Itusesastochas- associateanimage-likerepresentationtothelocalneighborhoodof
tic regularization based on neighbors, but the so-called “learning a3DpointviaaHoughtransform.Inthisimage(cf.Section4),a
process”actuallyisjustalocaliterativeoptimization. pixelcorrespondstoanormaldirection,anditsintensitymeasures
thenumberofvotesforthatdirection;besides,pixeladjacencyre-
lates to closeness of directions. It is a planar map of the empiri-
3. Motivationandoverviewofourapproach
calprobabilityofthedifferentpossibledirections.Then,justasa
Learningnormalestimation. Normalestimationcanbeformu- CNNforordinaryimagescanexploitthelocalcorrelationofpixels
latedasadiscreteclassificationprobleminHoughspace.Letnˆpbe to denoise the underlying information, a CNN for these Hough-
anormalestimatedatpoint pofpointcloudP,andn∗ ptheground baseddirectionmaps(cf.Section5)mightalsobeabletohandle
truthnormal.Theproblemistofindasetofnormals{nˆp}
p∈P
s.t.: noise,identifyingaflatpeakaroundonedirection.Similarly,justas
aCNNforimagescanlearnarobustrecognizer,aCNNfordirec-
tionmapsmightbeabletomakeuncompromisingdecisionsnear
{nˆp} p∈P = argmin ∑ φ(np,n∗ p ) (1) sharpfeatures,whendifferentnormalsarecandidate,optingforone
{np}p∈Pp∈P
specificdirectionratherthantradingoffforanaverage,smoothed
whereφ(.,.)isadistancefunction,suchasthe(cid:96) distance.Thetra- normal.Moreover,outlierscanbeignoredinasimplewaybylim-
2
ditionalHough-basedapproachconsistsinfirstassociatinganor- itingthesizeoftheneighborhood,thusreducingorpreventingthe
maln toeachbinbofanaccumulatorandthen,foreachpoint p influenceofpointslyingfarfromamoredenselysampledsurface.
b
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 3 ---

AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds 283
Hough space
Hough
CNN
transform
Point Filled Selected normal Point and
accumulator 2 coordinates normal
Figure2:Distortionofbinswhen
Figure1:OurCNN-basednormalestimationframework. projectedonthesphere.
RRHT. ApplyingaHoughtransformtoestimatethenormalata
given3Dpointp,[BM12]proposeathree-stepalgorithm: PCA
Normal hypotheses
3D rotation
2D projection in
1. Hypothesisgeneration.Hypothesesaregeneratedbyrandomly (local 3D coordinates)
Hough space
selectingthreepointsintheneighborhoodof p,whichdefinesa 3D space
planeandthusapossiblenormaldirection.
2. VoteinHoughspace.Eachhypothesisvotesina2Dspherical
accumulator,parameterizedbysphericalcoordinatesθandφ.
3. Electionofanormal.Finally,theestimatednormalistheaver- Accumulation PCA
ageofdirectionsinthemostvotedbinoftheaccumulator. 2D rotation
Acontributionof[BM12]regardingrunningtimeisarobuststa-
Hough space
tisticalcriteriontosafelystoppickingnewhypotheses,after
(cid:24) (cid:18) (cid:19)(cid:25)
T∗= 1 ln 2M (4) Figure3:Rotationstoensurestabilityoftheaccumulatorpattern.
2ε2 1−α
aredrawn,whereMisthenumberofbinsoftheaccumulator,α∈
shape extraction. The more adapted to the shape (with little bias
]0,1[istheconfidencelevel,andε∈]0,1[isthemaximumdistance
when voting), the better. To this end, [BM12] exploit a spherical
betweentheempiricaldistributionandthetheoreticaldistribution.
accumulator,proposedearlierforplaneextraction[BELN11].
Theresultingmethodisrobusttonoise,outliersandsharpfeatures
but,asmentionedabove,itissensitivetobindiscretization. Inourcase,wewanttheaccumulatortobemappedtotheinput
ofaCNN.Wechoseasimplesquareimage-accumulator:a2Dreg-
OurCNN-basedmethod,picturedonFigure1,keepsthesamehy- ulargridofsizeM=m×m.Givenanormaln=(nx,ny,nz),the
pothesisgenerationschemeas[BM12],asdescribedabove(step1), coordinatesofthevotex,yintheaccumulatoraregivenby:
including the robust stopping criterion of Eq. (4). However, we
nx+1 ny+1
change the accumulator and voting (step 2) to create an image (x,y)=( ∗m, ∗m) (5)
2 2
structure amenable to deep learning. Besides, the estimation of a
Notethat,whenback-projectedonthesphere,thesizesofthebins
normalfromafilledaccumulator(step3)isnowtheapplicationof
arenotsimilar,asillustratedonFigure2.Thisaccumulatordesign
alearnedfunctionthatdirectlyyieldstwocoordinatesrepresenting
thus leads to distortions that could affect vote count and bin se-
the estimated direction. It significantly reduces the discretization
lection.However,correctingthecontributionofeachbinisjusta
effectandimprovesnormalselectionwhileremainingfast.
constantfactor,thatthenetworkcaneasilylearn.(Wecheckedthat
reweightingvotesexplicitlydoesnotleadtosignificantchanges.)
Ourcontributionsareasfollows:
Withthissimplescheme,theimage-accumulatorcanbefilledvery
• WeshowhowtoreliablymapaHoughaccumulatorfornormal
efficiently,notevenrequiringtrigonometriccomputations.
estimationintoanimagethatcanbeusedasinputofaCNN.
• WedemonstratethataCNNcanlearnhowtoestimateanormal Inourexperiments,thesizeofthisimage-accumulatorissetM=
fromsuchanimage-accumulator. 33×33=1089bins.However,givenitisfilledasifprojectedfrom
• We define an efficient way to take point density variation into an accumulator sphere, only a circular area is used, i.e., roughly
account(whichcouldactuallybeusedin[BM12]aswell). 1089×π/4bins.Thisis5timesmorethan[BM12],whereM=
• Weshowhowthesensitivitytothesizeoftheneighborhoodcan 171.Asaresult,thebindiscretizationeffectisgreatlyreducedwith
beaddressedinourframework. ourapproach.Besides,noaccumulatorrotationorshiftisrequired.
• Weprovideexperimentsshowingthatourmethodoutperforms
mostofthetimethestate-of-the-artofnormalestimation. Accumulatornormalization. Toreducepatternvariationsandfa-
cilitatelearning,wenormalizetheimage-accumulator,aspictured
on Figure 3. The 3D coordinate system is first rotated according
4. AnaccumulatorforCNNinput
toaPrincipalComponentAnalysis(PCA)ofNp,thepointsinthe
Accumulator design. The form of a Hough accumulator is well neighborhood of p: the rotation aligns the z-axis on the smallest
known to have a strong impact on the efficiency and quality of eigenvector. To further improve stability, we perform an in-plane
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 4 ---

AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds
Input
(33x33)
(a) (b) (c)
Figure4:Examplesofaccumulators.Graylevelsreflectthenumber
of votes (negative images for readability: darker for more votes).
Theredpointmarksthetruenormal,thegreenpoint,themostvoted
bin.Fromlefttoright:(a)pointonaroughbutfeaturelesssurface,
(b)pointclosetoanedge,(c)pointclosetoa3-planecorner.
rotationafter3Dpointsareprojectedontotheaccumulatorplane:
usingasecond,2DPCA,wealignthelargesteigenvectoralongthe
x-axis.Asimilareffectcouldbeachievedbydirectlyperforminga
single3Drotationaligningthesecondlargest3Deigenvaluealong
thex-axis.However,itisnotequivalentbecauseoftheprojection
fromapointonthespheretoaplane,anditislessstablethandoing
itafterprojectionontheaccumulatorplane.Examplesofprojected
androtatedaccumulatorsareshowninFigure4.
TheseusesofPCAdonotalwaysguaranteeconsistentaccumu-
latornormalization.Still,whenthePCA-inducedrotationsarenot
stable, their instability is irrelevant or potentially manageable by
thenetwork.Indeed,whenonasmoothbutpossiblynoisysurface,
themassofvotesfocusesaroundonemain3Ddirection.Thisre-
sultsinacenteredblobafter3DPCAand2Dprojection(cf.Fig-
ure4a).Itsorientationafter2DPCAandrotationcanbeunstable,
butitislittlerelevantbecausewhatreallymattersisthepresenceof
apeaktowardsthecenteroftheimage,whichanetworkcaneas-
ilylearninallorientations.Whenthepointisclosetoanedge,the
votesfocusononearcofthesphereofdirections.Themainaxisof
the3DPCAalignswiththearccenterandthemainaxisofthe2D
PCAalignswiththearcspanning(cf.Figure4b).Last,whenthe
pointisclosetoacorner,thereareasmanyfocalizationdirections
asmainfeaturelesssurfacessupportingthecorner(cf.Figure4c).
Inthiscase,themainaxisofthe3DPCAalignsmoreorlesswith
thegeneraldirectionofthecorner,butthe2DPCAcanbeunsta-
ble.Ifthereisaprominentsurface,theeigenvaluesofthe2DPCA
might be different enough to rotate its normal near the x-axis. If
thereisnosuchprominentsurface,therotationhaslittlemeaning.
ItisthemostdifficultsituationtheCNNhastolearn.However,the
non-localityandnonlinearityofthenetworkhavethepotentialto
copewiththevarietyofsituations,learningenoughelementsofin-
formationtogeneralizewell,justasaCNNfordetectioncancope
withoccludedobjects.Wealsoleverageontheabilitytogeneratea
largeamountoftrainingdataforvariousconfigurations.
5. ACNNfornormalestimation
CNNarchitecture. Ourtaskdoesnotrequireafancynetworkar-
chitecture. We chose a small-sized network for a low processing
time.ItisbasedonLeNet[LBD∗89],asimplenetworkyetproven
tobeadaptabletovariousestimationproblemsinimageprocessing.
It is illustrated on Figure 5. It is composed of four convolutional
layers,twomaxpoolingsandfourfullyconnectedlayers.Mostof
theparametersareinthefully-connected(FC)layers.
ULeR+3x3vnoC
05=pn
ULeR+3x3vnoC
05=pn
2x2 gnilooP
xaM
ULeR+3x3vnoC
05=pn
2x2 gnilooP
xaM
ULeR+3x3vnoC
69=pn
ot 6 8 5 4 4 0 3 2
ULeR+CF
ot 8 4 4 2 0 0 2 1
ULeR+CF
ot 42 2 0 1 1 5
ULeR+CF
ot 215 2
CF
tuptuO )setanidrooc
2(
284
Figure5:OurCNNarchitecturefornormalestimation.
Figure6:Illustrationoftrainingdata(minandmaxangles).
Theconvolutionalnatureofthisnetworkcontributestotheca-
pacity of handling noise by potentially smoothing out accidental
peaks.Nonlinearity,providedbymaxpoolingsandReLUs,gives
risetotheabilitytochoosebetweendifferentpeaksbasedontheir
localshape.Andtheglobalchoiceamongpossiblenormalsorigi-
natesfromanonpurelylocalanalysisofthedirectionmapgiven
bothbythemaxpoolingsandthefully-connectedlayers.Wedonot
pretenditisthebestarchitectureforthistask,yetthatitismeaning-
ful.(Wealsoexperimentedwithanarchitecturemadefromfully-
connectedlayers;itdoesnotperformaswell,cf.Section8.) We
trainthisnetworkusingmeansquareerror((cid:96) penalization): 2
{nˆp}=argmin ∑ (np−n∗ p )2 (6)
{np} p∈P
Training data. To train this network, we generate synthetic
ground-truthexamples.Wecreateuniformlysampledpointclouds
overcornerswithdifferentangles.Anglesareuniformlydrawnbe-
tween 80◦ and 160◦. Examples of such point clouds are shown
onFigure6.Wegeneratepointsetswith5000pointsandrandomly
pick1000pointsineachset,forwhichwecomputethecorrespond-
ingaccumulator.Thetrainingdatacontain100,000suchfilledac-
cumulators. Theselearningsamples(cornerswithvaryingangles)
represent themostcommonsituationsofsharpfeaturesinrealdata.
Theyareusedtolearnproperdecisionsnearbothedgesandcor-
ners.Wethenrelyontheabilityofthenetworktogeneralizeand
treat partial data, as is the case for occlusion in object detection.
Itallowsthenetworktotreatmorecomplexsituationsthanjust3-
sidecorners.Regressionfornoisydata,whichisthegeneralcase,
islearnedfrompointsfarenoughfromtheedgesandthecorner.
Tobemorerealisticandproviderobustness,wealsoaddnoiseto
thetrainingdata.Inourexperiments,weuseaGaussiannoisefor
learning the network. However, it is not intrinsic to the method,
as in [LSK∗10]; other noise models could have been used too.
We also use a Gaussian noise for testing on synthetic data, as
[LSK∗10,BM12,LZC∗15].However, littlebiasisto beexpected
becausethenoiselevelvariesforeachtrainingsample,withastan-
darddeviationrandomlydrawnin[0%,200%]ofthemeandistance
betweenthepointsinthecloud,whilethenoiselevelisfixedfor
evaluation.Fortestsonrealdata,thenoiseisaspresentinthedata.
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 5 ---

AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds 285
VariantofCNN3s Not Robust
Robust
w.r.t.densityvariation robust discretized
Runningtime (s) 51.5 57.3 52.2
Table1:RunningtimeonaDFC2015tiledetail(185kpoints).
However,pickingrandomneighboringpointsaccordingtothis
localscaleisquiteslow.Itrequirescomputinganarrayofthecu-
(a)Notrobust (b)Robustdiscretized mulativesumsoflocalscales,sortingit,andgivenarandomnum-
ber, searching the corresponding point in the array. To overcome
Figure7:DetailofaDFC2015aeriallidartilewithnormalesti-
thisissue,wediscretizethesearchspace:wecomputetheminand
mation. Roofs are more densely sampled than facade walls. The
cross-sectionillustratesthehandling(ornot)ofdensityvariation.
maxlocalscalesanddividethisrangeintoks equalintervals.(In
ourexperiments,ks=5performswell.)Wethencomputethescore
of each interval (the sum of all local scales of points in the seg-
90°
... ment),andrandomlypickasegmentaccordingtothisscore.Last,
10°
wepickapointinthisintervalwithauniformprobability.Robust
7.5° discretized normal estimation with this optimization is illustrated
on Figures 7b and 8c. The quality is almost as good as with the
5°
non-discretizedversion(RMSerror5.5◦vs5.6◦inFigures8b-8c),
2.5°
whilebeingsignificantlyfaster,ascanbeseenonTable1.
(a)Notrobust (b)Robust (c)Robustdiscretized 0°
This way to deal with non-uniform densities is much more ef-
Figure8:Differentsamplingdensityoneachface,andmethodvari- ficientthanwhatwasproposedin[BM12]where,tosamplea3D
antswithdifferentlevelsofrobustnesstothisdensityvariation. point,aballisfirstuniformlysampledinwhichthepointisthen
sampled.Whileitprovidesgoodrobustnesstodensityvariation,it
considerablyslowsdownnormalestimation.Notethatourwayof
Trainingprocess. Foreachpointpinthetrainingset,weconsider
handlingdensityvariationcouldalsobeusedtospeedup[BM12].
afixedneighborhoodsize(K=100neighborsinourexperiments)
inwhich wesampletriplets ofpoints, filling p’saccumulator ac-
cordingly(cf.Section4).Itcreatesagray-levelimage,inwhichwe 7. Multiscaleapproach
scalethepixelvaluessothatthepixelofthemostvotedbiniswhite
Manynormalestimationmethodsrelyonascaleparameter.Itusu-
(highestintensity).Asthecorrespondingground-truthnormalat p
allycorrespondstothesizeoftheneighborhoodtoconsider.Itcan
isknown, it providesinput-output examplesto trainthe network.
beinterpretedasthescaleatwhichthesceneshouldbeobserved,
Whenlearning,werandomlychoose75%ofthesedataforactual
or the distance under which regularization is allowed. However
training.Therestisusedtocheckthatlearningdoesnotoverfit.
it is often difficult to tune this parameter, in particular for points
clouds with high density variation where different neighborhood
6. Dealingwithdensityvariation sizes would be necessary for a robust and accurate estimation. A
solutionistousenon-parametricmethodssuchasproposedbyDey
Densityvariation,withpossibleanisotropicbias,isacommonphe-
andGoswami[DG04],butaccuracydropswhennoiseishigh.
nomenoninreal-worldpointclouds.Forexample,alidaracquires
datafromasingleviewpoint,typicallywithregularangularsteps To improve robustness near sharp edges, we propose a simple
for azimuth and elevation, sampling more densely surfaces with variantofourmethodusingamultiscaleapproach.Thefactisthe
lowincidence.Figure7illustratesthissituationwithadetailofan input of the CNN can be easily modified to create a multicanal
aerialacquisitionoftheDataFusionContest(DFC)2015[DFC15]. tensorinput,likeRGBchannelsforprocessingcolorimages.Here
Another,syntheticexampleisshownonFigure8.Pointsintheless ourchannelsaretheaccumulatorscomputedfordifferentneighbor-
dense regions and next to edges with denser regions are wrongly hoodsizes,whichthePCA-basednormalizationcanroughlyalign
giventhenormaloftheothersideoftheedges(cf.Fig.7aand8a). forconsistency.Inourexperiments,weexploretwomultiscaleap-
proaches, with 3 and 5 scales. Given a neighborhood size of K
Robustness to density variation can be efficiently obtained at
neighbors, the neighborhood sizes are K/2, K and 2K for the 3-
planehypothesisgenerationtime,intheHoughtransform.Forthis,
scaleschemeandK/4,K/2,K,2Kand4Kforthe5-scalescheme.
weassociateadifferentweighttoeachpointdependingonthelo-
caldensity.Wethenpicktripletsfrompointshavingaprobability NotethatwithamonoscaleHoughaccumulator,aswiththeone
proportionaltotheseweights.Apointinasparseareawillbegiven in[BM12],asampleof3Dpointsvotesforonedirectionregard-
ahigherweight;itwillthusbepickedmoreoftenthanapointina less of its location, whether it is close or far from the point con-
denserregion.Theweightcorrespondstoakindoflocal(surfacic) sideredfornormalestimation.Butusingsimultaneouslydifferent
scale.Weuseaslocalscalethesquaredistanceoftheconsidered scales provides a form of distance sensitivity: a 3D point sample
pointtoitskth nearestneighbor.Thissquaredistancerepresents may contribute to a direction at a given scale, but not at another
dens
the influence area of the point on the underlying surface. (In our scalebecausethecorrespondingneighborhoodsizeissmaller.As
experiment,weusek =5.)Thislocalscalenormalizationcom- illustratedintheexperimentsection,thisdistancesensitivityseems
dens
pensatesforthelowdensity,ascanbeseenonFigure8b. tobeenough(w.r.t.alocationsensitivity)toreachahighaccuracy.
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 6 ---

AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds
stniopk02,ebuC
stniopk02,rednilyC
stniopk02,nordehasocI
286
Figure9:ComparisonofvariousmethodsonsimplegeometricmodelswithvaryingGaussiannoise(standarddeviationexpresseda%of
the mean distance between points) and no density variation. NN 3FC: network with 3 fully-connected layers on the same Hough image-
accumulatorasours;CNNns:ourmethodwithnscales.
8. Evaluation However, this measure does not favor sharp behaviors. Indeed,
smoothing the normals of points close to an edge results in a
Ourmethodhas4mainparameters,whicharesetasfollows:
smaller RMS than choosing the normal of the wrong side of the
• theaccumulatorsizeM=33×33, edge.Alesscompromisingerrormeasure,bettersuitedw.r.t.“vi-
• thenumberofhypothesistopickT =1000, sual”applicationssuchasrendering,istocounttheproportionof
• theneighborhoodsizeK=|Np|=100 (fortraining&testing), goodpoints(PGP),i.e.,whoseerrorisunderagiventhreshold;as
• theneighborhoodsizeforestimatingalocalscalek
dens
=5. in[LZC∗15],westudy5◦and10◦maximumdeviation.
T =1000correspondstoanalloweddeviationfromthetheoretical
accumulatordistributionofε=0.073forα=0.95.K istheonly
Experimentsonsyntheticdata. Figure9showstheimpactofan
practicalparameter.Forcomparisonpurposes,wesetK=100for
increasing Gaussian noise on RMS and PGP for various simple
allmethods,except[DG04]whichdoesnottakeanyneighborhood
geometric models. We tested our method and its variants against
sizeasparameter.Forafaircomparison,testswith[BM12]useex- five methods from the literature: [DG04], [LSK∗10], [BM12],
actlythesameparametersasours(i.e.,T,K).FormultiscaleCNN, [HDD∗92]and[CP05].AllthesemethodsareavailableontheIn-
we use: for 3 scales (CNN 3s), K=50, 100, 200, and for 5 scales
ternet,orthecodeweregrantedtousbytheauthors.Wealsoadded
(CNN5s),K=32,64,128,256,512.
as baseline a simpler neural network made of 3 fully connected
Weconsidertwodifferentscoresforquantitativeevaluation:the layers with interleaved ReLUs (NN 3FC). For very low levels of
root mean square (RMS) deviation and the number of points for noise,thebestmethodis[DG04],butitrapidlydegradesasnoise
whichthedeviationislessthanagivenangle.TheRMSisastan-
increases.Theregressionmethods[HDD∗92,CP05]performbet-
darderrormeasure.Itprovidesagoodideaoftheoverallperfor- terathighnoise,whenthesurfacedetailsarelostinthenoiseand
manceofanalgorithm.Itisdefinedby: very difficult to retrieve. Between those two extreme cases, our
(cid:115) multiscaleapproachesperformbest.Largerneighborhoodsprovide
RMS= 1 ∑ (n (cid:92) ˆpn∗ p)2 (7) betterrobustnessforhighnoise,whilesmallscalesgivemaintain
|P| p∈P goodresultsforlownoise.ThecomparisontotheNN3FCbaseline
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 7 ---

AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds
stniopk02,ebuC
stniopk02,rednilyC
stniopk02,nordehasocI
287
Figure10:Comparisonofourmethod(CNN)andasimilarCNNestimationbasedondepthmapinput(DM)withmultiscale(ns).
showsthatourgoodresultscannotbeattributedtotheHoughrep- gression[HDD∗92]andwearemorerobusttonoisethansample
resentationonly.Whereasasimpleneuralnetworkregressortends consensus[LSK∗10]andordinaryHoughtransform[BM12].
toproducesmootherpredictions(closertotheplanarregression),
ourCNNapproachismorediscriminative.
ToevaluatethegainsoftheHoughtransformfornormalestima-
tionusingaCNN,weimplementedanotherbaselinemethod.Once
the point neighborhood has been oriented via 3D PCA, we com-
pute a depth map. The depth direction is the axis of the smallest
eigenvalue. This depth map has the same dimension as the accu- 90°
mulator in our method. We build a corresponding learning set as
...
10°
forourmethod,andtrainthesamenetworkarchitecture.Figure10
7.5°
showsthecomparisonbetweenourHough-basedmethodandthis
depth-map-basedbaseline.Ourmethodperformssignificantlybet- 5°
terregardingPGP,whilehavingaslightlyhigherRMS.Thedepth 2.5°
mapisnotasregularastheHoughaccumulatorforlearning.
0°
Wecouldnotcompareto[ZCL∗13]and[LZC∗15]astheircode
isnotavailable.Still,weexperimentedona100k-pointoctahedron
(seeFigure11),whichisoneofthesyntheticmodeltheseauthors
usedforvalidation.With50%noise,theyobtainslightlybetterre- [HDD∗92] [LSK∗10] [BM12] CNN1s
sults than ours (please refer to [LZC∗15]), but at high computa-
tional cost; their parallel version is still more than twice slower Figure11:Visualresultsoffourestimationalgorithmsonanocta-
than our implementation. Moreover, our method degrades better hedron(100kpoints)with50%noise(topline),150%noise(mid-
forhighnoise,notrequiringtotuneparameters.Comparedtoother dle)and200%noise(bottom).Colorscale,givenontheright,maps
baselinemethods,weestimatesharpfeaturesbetterthanusingre- adeviationangletoacolor(redisadeviationgreaterthan10◦).
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 8 ---

288 AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds
0%noise 100%noise 200%noise
Figure12:Proportionofnormalswitherrorlessthanagivenangle,on250k-pointdragonwithnoise0%(left),100%(middle),200%(right).
(outliers) (outliers) (nooutliers)
truenormals CNN1snormals CNN1snormals
Figure13:Robustnesstooutlierson250k-pointdragonwith100%noiseand1M-pointoutliersaddedinboundingbox.
Experiments on real data. We first consider the scan data of a
dragon sculpture, for which an accurate mesh is available (3.6M
[HDD*92] 69.8%
vertices,7.2Mtriangles).Thedragonfeaturessharpattributessuch [DG04] 64.6%
[CP05] 70.2%
asfangs,hornsandscales.Werandomlydraw250kpointsonmesh [LSK*10] 70.3%
[BM12] 72.5%
faces; these faces determine reference normals. We use this sub- CNN 1s 72.4%
CNN 1s da 72.7%
sampledpointcloudtocomparewith[BM12]andtostudythesen-
sitivity to the neighborhood size K, for different levels of noise. [HDD*92] 49.1%
[DG04] 42.4%
Unsurprisingly,whennonoiseisadded(besidestherandompick- [CP05] 49.3%
ingofpointsonthemesh),asmallneighborhoodprovidesabetter [ [ L B S M K 1 * 2 1 ] 0 ] 5 5 1 2 . . 6 5 % %
sensitivitytosharpfeatures.Inthiscase,weonlyperformslightly CNN 1s 52.5%
CNN 1s da 52.7%
better than [BM12]. But when noise increases, information level
dropsinsmallneighborhoodsandlargeronesprovideabetterro- Figure14:Officeroom,proportionofnormalswitherrorlessthan
bustness.Inthismorecomplicatedsetting,weperformsignificantly agivenangle.CNN1sda:ourrobustdensity-adaptivevariant.
betterthan[BM12].Toevaluaterobustnesstooutliers,wedraw1M
randompointsinthedragonboundingbox.With100%noise,RMS
erroris21.1◦,vs20.5◦withnooutliers(seeFigure13).
Wethenconsideralaserscanofanofficeroom.Thepointcloud
naturallyfeaturesedges,corners,aswellasdensityvariationswith
anisotropic bias. An exact ground truth is not known, but can be
approximatedfromtheimplicitmeshstructureofthedepthmap:
weconsiderasreferenceateachpointthemeannormalofthesur- Figure15:Officeroomdetail.Fromlefttoright:planarregression,
roundingfaces.Althoughthesesnormalscanbenoisyinverydense ourplainmethod,andourrobustdensity-adaptivemethod.
areas,theyareenoughforalgorithmcomparison.Giventhispseudo
groundtruth,Figure14showstheproportionofestimatednormals
withangularerrorbelowagiventhreshold.Duetothesmallnum-
berofpointsnearedges,comparedtopointsonwideplanarareas,
thedifferencebetweenthemethodsissmallonaverage.However,
normalscanbelocallywrong,ascanbeseenonFigure15,which
illustratesadetailofthescenewithdensityvariations.
Finally,weshowqualitativeresultonoutdoorscenes.Figures7
displaysadetailoftheDFC2015aeriallidartileinFigure16,with
robustnesstodensityvariations.Figure17illustratesshadingwith
normalsestimatedonasparsestructure-from-motionpointcloud. Figure16:DFC2015lidartilewithnormals(decimated2.3M).
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 9 ---

AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds 289
Pointcloud(textured) Detail1(untextured,shaded) Detail2(untextured,shaded) Detail3(untextured,shaded)
Figure17:OurCNN-basednormalestimationonastructure-from-motionpointcloudoftheChâteaudeSceaux(France),400kpoints.
Model Cube Armadillo DFCdetail Omotondo DFCtile approachwhichautomaticallyadaptstodifferentsizesofneighbor-
Size 20k 173k 185k 997k 2.3M hoods,requiringbasicallynochangeintheCNNframework.Asa
[HDD∗92] 0.3 2.1 1.9 12 25 result,wearemorerobustandmoreaccuratemostofthetimeon
[DG04] 3.2 55 41 441 1243 bothsyntheticandrealdata,althoughjustafewtimesslower.We
[CP05] 5.8 50 54 304 711 actuallymostoftenalsooutperformotherstate-of-the-artmethods,
[BM12] 1.9 13 11 44 147 even[ZCL∗13,LZC∗15]forhighnoisewhichanywayareslower.
[LSK∗10] 8.8 64 75 392 902
Perspectives include the study of geometric transformations in
CNN1s 4.5 33 34 183 423
Houghspacetofacilitatethelearningandimproveaccuracy.The
CNN3s 5.9 48 52 273 639
CNNarchitectureandtrainingdatacancertainlyalsobeimproved,
CNN5s 7.9 69 73 382 897
asthespaceofpossibilitiesisquitelarge.Actually,futureadvances
Table2:Computationtimes(inseconds)fordifferentmodelsand inresearchonCNNsshouldalsobenefittothisframework.
differentmethods.CNNvariantsarewithoutdensity-adaptivity.
Thismethodparticipatestoanewtrendingeometryprocessing
wheregeometricdecisionsarelearntfromground-truthdata,pos-
Computation timesare given in Table 2. We tested a cube with siblybiasedtowardsaspecifickindofscenes,ratherthantheresult
50%noiseandrealpointclouds:Armadillo,Omotondo,DFCdetail ofexplicit,manually-designedgeometriccomputations.
of Figure 7, whole DFC tile of Figure 16. Our running times are
competitive w.r.t. compared methods, except [HDD∗92] which is Implementation details. The Hough transform is coded with
much faster. As weshare thefirst stepof [BM12] (cf.Section 3) Eigen(eigen.tuxfamily.org).Neighborsearchinapointcloud
andasaccumulatorfillingisfast,thedifferenceresidesmainlyin uses nanoflann kd-tree (https://github.com/jlblancoc/
theCNNcomputations.Usingmorescalesincreasescomputation nanoflann). Our CNN framework relies on Torch-nn (https:
time, due to the more expensive search for neighborhoods larger //github.com/torch/nn). For experiments, we used a laptop
thanK=100(uptoK=512),despitetheuseofakd-tree. withInteli7quadcoreandGPUNVidiaGTX970m.
Limitations. Contrarytootherapproacheswhereitisinexpensive Acknowledgements. We would like to thank all the authors of
tochangethevalueofaparameter,wehavetoretrainthenetwork the different papers for providing their code or executable. Ar-
ifweneedtoadaptspecificallytotheinputdata.Itmainlyconcerns madillo and Omotondo come from the Aim@Shape repository.
theneighborhoodsizeK,whichcontrolsthesensitivitytodetails. AsianDragoncomesfromtheStanford3Dscanningrepository.
However,themultiscaleapproachreducestheinfluenceofthispa-
rameterbyanalyzingdifferentscalessimultaneously.
References
[ABCO∗03] ALEXA M., BEHR J., COHEN-OR D., FLEISHMAN S.,
9. Conclusion
LEVIND.,SILVAC.T.: Computingandrenderingpointsetsurfaces.
IEEETr.onVisualizationandComputerGraphics9,1(2003),3–15.1
Wehaveproposedanovelmethodfornormalestimationinunorga-
nizedpointcloudsusingaconvolutionalneuralnetwork.Although [ACSTD07] ALLIEZP.,COHEN-STEINERD.,TONGY.,DESBRUNM.:
Voronoi-based variational reconstruction of unoriented point sets. In
wereusedtheideaoftheHoughtransformof[BM12]aswellasits
SGP(2007),pp.39–48.1
robustandefficientsamplingstrategy,weintroducedawholerange
of new features. We use a different accumulator, which is planar
[ASGCO10] AVRON H., SHARF A., GREIF C., COHEN-OR D.: L1-
sparsereconstructionofsharppointsetsurfaces. TOG)29,5(2010),
rather than spherical and which is less discretized. Moreover, we
135:1–135:12.1
defineatotallydifferent,CNN-baseddecisionproceduretoselect
[Bal81] BALLARD D. H.: GeneralizingtheHoughtransformtodetect
anormalfromtheaccumulator.Besides,todealwithdensityvari- arbitraryshapes.PatternRecognition13,2(1981),111–122.2
ation, we introduce a fast approach to pick points according to a
[BDLGM14] BOULCHA.,DELAGORCEM.,MARLETR.:Piecewise-
distributionbasedonalocaldensityestimation.Finally,toimprove planar3Dreconstructionwithedgeandcornerregularization. CGF33,
robustness and reduce parameter tuning, we present a multiscale 5(2014),55–64.1
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.

--- Page 10 ---

290 AlexandreBoulch&RenaudMarlet/DeepLearningforRobustNormalEstimationinUnstructuredPointClouds
[BELN11] BORRMANND.,ELSEBERGJ.,LINGEMANNK.,NÜCHTER [LBD∗89] LECUN Y., BOSER B., DENKER J. S., HENDERSON D.,
A.: The3DHoughTransformforplanedetectioninpointclouds:A HOWARDR.E.,HUBBARDW.,JACKELL.D.: Backpropagationap-
reviewandanewaccumulatordesign.3DResearch2,2(2011).2,3 plied to handwritten zip code recognition. Neural computation 1, 4
(1989),541–551.2,4
[BM12] BOULCH A., MARLET R.: Fastandrobustnormalestimation
forpointcloudswithsharpfeatures. CGF31,5(2012),1765–1774. 1, [LSD∗15a] LI B., SHEN C., DAI Y., VAN DEN HENGEL A., HE M.:
2,3,4,5,6,7,8,9 Depthandsurfacenormalestimationfrommonocularimagesusingre-
gression on deep features and hierarchical CRFs. In CVPR (2015),
[BRG16] BANSALA.,RUSSELLB.,GUPTAA.: Marrrevisited:2D-3D
pp.1119–1127.2
alignmentviasurfacenormalprediction.InCVPR(2016).2
[LSD15b] LONGJ.,SHELHAMERE.,DARRELLT.:Fullyconvolutional
[CLP10] CHAUVEA.-L.,LABATUTP.,PONSJ.-P.: Robustpiecewise- networksforsemanticsegmentation.InCVPR(2015).2
planar3Dreconstructionandcompletionfromlarge-scaleunstructured
pointdata.InCVPR(2010),pp.1261–1268.1
[LSK∗10] LIB.,SCHNABELR.,KLEINR.,CHENGZ.,DANGG.,JIN
S.:Robustnormalestimationforpointcloudswithsharpfeatures.Com-
[CP05] CAZALSF.,POUGETM.:Estimatingdifferentialquantitiesusing puters&Graphics34,2(2010),94–106.1,4,6,7,9
polynomialfittingofosculatingjets.ComputerAidedGeometricDesign
22,2(2005),121–146.1,6,9
[LZC∗15] LIU X., ZHANG J., CAO J., LI B., LIU L.: Qualitypoint
cloudnormalestimationbyguidedleastsquaresrepresentation. Com-
[Dav88] DAVIESE.R.: ApplicationofthegeneralisedHoughtransform puters&Graphics51,C(2015),106–116.1,4,6,7,9
tocornerdetection.ComputersandDigitalTechniques,IEEProceedings
[MNG04] MITRAN.J.,NGUYENA.,GUIBASL.: Estimatingsurface
E135,1(1988),49–54.2
normalsinnoisypointclouddata. InternationalJournalofComputa-
[DFC15] IEEEGRSSDataFusionContest,2015.URL:http://www. tionalGeometry&Applications14,04n05(2004),261–276.1
grss-ieee.org/community/technical-committees/ [ÖGG09] ÖZTIRELIA.C.,GUENNEBAUDG.,GROSSM.H.: Feature
data-fusion.5 preservingpointsetsurfacesbasedonnon-linearkernelregression.CGF
[DG04] DEYT.K.,GOSWAMIS.:Provablesurfacereconstructionfrom 28,2(2009),493–501.1
noisysamples.InSoCG(2004),pp.330–339.1,5,6,9 [PCFG12] PENEDONESH.,COLLOBERTR.,FLEURETF.,GRANGIER
D.: ImprovingObjectClassificationusingPoseInformation. Research
[DH72] DUDAR.O.,HARTP.E.: UseoftheHoughtransformationto
reportIdiap-RR-30-2012,IdiapResearchInstitute,2012.2
detectlinesandcurvesinpictures.Comm.ACM15,1(1972),11–15.2
[PKKG03] PAULYM.,KEISERR.,KOBBELTL.P.,GROSSM.: Shape
[FGMP14] FERRI F., GIANNI M., MENNA M., PIRRI F.: Pointcloud
modelingwithpoint-sampledgeometry.TOG)22,3(2003),641–650.1
segmentationand3Dpathplanningfortrackedvehiclesinclutteredand
dynamicenvironments. In3rdIROSWorkshoponRobotsinClutter: [PWP∗11] PHAM M.-T., WOODFORD O. J., PERBET F., MAKI A.,
PerceptionandInteractioninClutter(2014).1 STENGERB.,CIPOLLAR.:Anewdistanceforscale-invariant3Dshape
recognitionandregistration.InICCV(2011),pp.145–152.2
[GDDM14] GIRSHICKR.,DONAHUEJ.,DARRELLT.,MALIKJ.:Rich
featurehierarchiesforaccurateobjectdetectionandsemanticsegmenta- [RL00] RUSINKIEWICZS.,LEVOYM.: QSplat:amultiresolutionpoint
tion.InCVPR(2014),pp.580–587.2 renderingsystemforlargemeshes.InSIGGRAPH(NewYork,NY,USA,
2000),pp.343–352.1
[GG07] GUENNEBAUD G., GROSS M.: Algebraic point set surfaces.
TOG)26,3(2007),23.1 [SEZ∗14] SERMANETP.,EIGEND.,ZHANGX.,MATHIEUM.,FER-
GUSR.,LECUNY.: Overfeat:Integratedrecognition,localizationand
[Gra15] GRAHAM B.: Sparse 3D convolutional neural networks. In detectionusingconvolutionalnetworks.InICLR(2014).2
BMVC(2015).2
[SLJ∗14] SZEGEDY C., LIU W., JIA Y., SERMANET P., REED S.,
[HDD∗92] HOPPE H., DEROSE T., DUCHAMP T., MCDONALD J., ANGUELOVD.,ERHAND.,VANHOUCKEV.,RABINOVICHA.:Going
STUETZLEW.: Surfacereconstructionfromunorganizedpoints. ACM deeperwithconvolutions.InCVPR(2014).2
SIGGRAPHComputerGraphics26,2(1992),71–78.1,6,7,9
[SSW15] SUNY.,SCHAEFERS.,WANGW.:Denoisingpointsetsvial0
[HLZ∗09] HUANGH.,LID.,ZHANGH.,ASCHERU.,COHEN-ORD.: minimization.CAGD35-36(2015),2–15.1
Consolidation of unorganized point clouds for surface reconstruction. [SW02] SHENF.,WANGH.:CornerdetectionbasedonmodifiedHough
TOG)28,5(2009),176.1 transform.PatternRecognitionLetters23,8(2002),1039–1049.2
[Hou62] HOUGHP.V.C.: Methodandmeansforrecognizingcomplex [SWK07] SCHNABELR.,WAHLR.,KLEINR.: EfficientRANSACfor
patterns.U.S.Patent3.069.654(1962).2 point-cloudshapedetection.CGF26,2(2007),214–226.1
[HSK∗12] HINTON G. E., SRIVASTAVA N., KRIZHEVSKY A., [TM78] TSUJIS.,MATSUMOTOF.: Detectionofellipsesbyamodified
SUTSKEVER I., SALAKHUTDINOV R. R.: Improving neural net- Houghtransformation.IEEETrans.onComputers27,8(1978).2
works by preventing co-adaptation of feature detectors. preprint
[WFG15] WANGX.,FOUHEYD.F.,GUPTAA.: Designingdeepnet-
arXiv:1207.0580(2012).2
worksforsurfacenormalestimation.InCVPR(2015),pp.539–547.2
[IK88] ILLINGWORTHJ.,KITTLERJ.:AsurveyoftheHoughtransform. [WSK∗15] WU Z., SONG S., KHOSLA A., YU F., ZHANG L., TANG
CVGIP44,1(1988),87–116.2
X.,XIAOJ.:3DShapeNets:Adeeprepresentationforvolumetricshape
[JIS03] JEONG W. K., IVRISSIMTZIS I. P., SEIDEL H. P.: Neural modeling.InCVPR(2015).2
meshes:statisticallearningbasedonnormals. InPacificConferenceon [XO93] XU L., OJA E.: Randomized Hough transform (RHT): basic
ComputerGraphics&Applications(CGA)(2003),pp.404–408.2 mechanisms,algorithms,andcomputationalcomplexities. CVGIP:Im-
[KEB91] KIRYATIN.,ELDARY.,BRUCKSTEINA.M.: Aprobabilistic ageunderstanding57,2(1993),131–154.2
Houghtransform.PatternRecognition24,4(1991),303–316.2 [ZCL∗13] ZHANGJ.,CAOJ.,LIUX.,WANGJ.,LIUJ.,SHIX.: Point
[KPW∗10] KNOPP J., PRASAD M., WILLEMS G., TIMOFTE R., cloudnormalestimationvialow-ranksubspaceclustering.Computers&
Graphics37,6(2013),697–706.1,7,9
VAN GOOL L.: Hough transform and 3D SURF for robust three di-
mensionalclassification.InECCV(2010),pp.589–602.2 [ZF14] ZEILERM.D.,FERGUSR.:Visualizingandunderstandingcon-
volutionalnetworks.InECCV(2014),pp.818–833.2
[KSH12] KRIZHEVSKYA.,SUTSKEVERI.,HINTONG.E.: Imagenet
classificationwithdeepconvolutionalneuralnetworks. InNIPS(2012). [ZPVBG01] ZWICKERM.,PFISTERH.,VANBAARJ.,GROSSM.:Sur-
2 facesplatting.InSIGGRAPH(2001),ACM,pp.371–378.1
(cid:13)c 2016TheAuthor(s)
ComputerGraphicsForum(cid:13)c 2016TheEurographicsAssociationandJohnWiley&SonsLtd.```

---

## Section 1.9: Physics-informed neural networks.pt2

Source File: Physics-informed neural networks.pt2.txt

### Paper Content:

```
# Physics-informed neural networks.pt2.pdf
# Converted: 2025-07-18 12:59:06
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Physics-informed neural networks.pt2.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Physics-informed neural networks.pt2.txt


--- Page 1 ---

Physics Informed Deep Learning (Part II): Data-driven
Discovery of Nonlinear Partial Differential Equations
Maziar Raissi1, Paris Perdikaris2, and George Em Karniadakis1
1Division of Applied Mathematics, Brown University,
Providence, RI, 02912, USA
2Department of Mechanical Engineering and Applied Mechanics,
University of Pennsylvania,
Philadelphia, PA, 19104, USA
Abstract
Weintroducephysics informed neural networks–neuralnetworksthatare
trained to solve supervised learning tasks while respecting any given law of
physics described by general nonlinear partial differential equations. In this
second part of our two-part treatise, we focus on the problem of data-driven
discovery of partial differential equations. Depending on whether the avail-
able data is scattered in space-time or arranged in fixed temporal snapshots,
we introduce two main classes of algorithms, namely continuous time and
discrete time models. The effectiveness of our approach is demonstrated us-
ing a wide range of benchmark problems in mathematical physics, including
conservationlaws, incompressiblefluidflow, andthepropagationofnonlinear
shallow-water waves.
Keywords:
Data-driven scientific computing, Machine learning, Predictive modeling,
Runge-Kutta methods, Nonlinear dynamics
1. Introduction
Deep learning has gained unprecedented attention over the last few years,
anddeservedlyso,asithasintroducedtransformativesolutionsacrossdiverse
scientific disciplines [1, 2, 3, 4]. Despite the ongoing success, there exist many
scientific applications that have yet failed to benefit from this emerging tech-
nology, primarily due to the high cost of data acquisition. It is well known
Preprint submitted to Journal Name November 30, 2017
7102
voN
82
]IA.sc[
1v66501.1171:viXra

--- Page 2 ---

that the current state-of-the-art machine learning tools (e.g., deep/convolu-
tional/recurrent neural networks) are lacking robustness and fail to provide
any guarantees of convergence when operating in the small data regime, i.e.,
the regime where very few training examples are available.
In the first part of this study, we introduced physics informed neural net-
works as a viable solution for training deep neural networks with few training
examples, forcaseswheretheavailabledataisknowntorespectagivenphys-
icallawdescribedbyasystemofpartialdifferentialequations. Suchcasesare
abundant in the study of physical, biological, and engineering systems, where
longstanding developments of mathematical physics have shed tremendous
insight on how such systems are structured, interact, and dynamically evolve
in time. We saw how the knowledge of an underlying physical law can in-
troduce structure that effectively regularizes the training of neural networks,
and enables them to generalize well even when only a few training examples
are available. Through the lens of different benchmark problems, we high-
lighted the key features of physics informed neural networks in the context
of data-driven solutions of partial differential equations [5, 6].
In this second part of our study, we shift our attention to the problem of
data-driven discovery of partial differential equations [7, 8, 9]. To this end,
let us consider parametrized and nonlinear partial differential equations of
the general form
u + [u;λ] = 0, x Ω, t [0,T], (1)
t
N ∈ ∈
where u(t,x) denotes the latent (hidden) solution, [ ;λ] is a nonlinear op-
erator parametrized by λ, and Ω is a subset of RD. T N hi · s setup encapsulates a
wide range of problems in mathematical physics including conservation laws,
diffusion processes, advection-diffusion-reaction systems, and kinetic equa-
tions. As a motivating example, the one dimensional Burgers’ equation [10]
corresponds to the case where [u;λ] = λ uu λ u and λ = (λ ,λ ).
1 x 2 xx 1 2
N −
Here, the subscripts denote partial differentiation in either time or space.
Now, the problem of data-driven discovery of partial differential equations
poses the following question: given a small set of scattered and potentially
noisy observations of the hidden state u(t,x) of a system, what are the pa-
rameters λ that best describe the observed data?
2

--- Page 3 ---

In what follows, we will provide an overview of our two main approaches
to tackle this problem, namely continuous time and discrete time models, as
well as a series of results and systematic studies for a diverse collection of
benchmarks. In the first approach, we will assume availability of scattered
and potential noisy measurements across the entire spatio-temporal domain.
In the latter, we will try to infer the unknown parameters λ from only two
data snapshots taken at distinct time instants. All data and codes used in
this manuscript are publicly available on GitHub at https://github.com/
maziarraissi/PINNs.
2. Continuous Time Models
We define f(t,x) to be given by the left-hand-side of equation (1); i.e.,
f := u + [u;λ], (2)
t
N
and proceed by approximating u(t,x) by a deep neural network. This as-
sumption along with equation (2) result in a physics informed neural network
f(t,x). This network can be derived by applying the chain rule for differ-
entiating compositions of functions using automatic differentiation [11]. It
is worth highlighting that the parameters of the differential operator λ turn
into parameters of the physics informed neural network f(t,x).
2.1. Example (Burgers’ Equation)
As a first example, let us consider the Burgers’ equation. This equation
arises in various areas of applied mathematics, including fluid mechanics,
nonlinear acoustics, gas dynamics, and traffic flow [10]. It is a fundamen-
tal partial differential equation and can be derived from the Navier-Stokes
equations for the velocity field by dropping the pressure gradient term. For
small values of the viscosity parameters, Burgers’ equation can lead to shock
formation that is notoriously hard to resolve by classical numerical methods.
In one space dimension the equation reads as
u +λ uu λ u = 0. (3)
t 1 x 2 xx
−
Let us define f(t,x) to be given by
f := u +λ uu λ u , (4)
t 1 x 2 xx
−
3

--- Page 4 ---

and proceed by approximating u(t,x) by a deep neural network. This will
result in the physics informed neural network f(t,x). The shared parameters
of the neural networks u(t,x) and f(t,x) along with the parameters λ =
(λ ,λ ) of the differential operator can be learned by minimizing the mean
1 2
squared error loss
MSE = MSE +MSE , (5)
u f
where
N
1 (cid:88)
MSE = u(ti,xi) ui 2,
u N | u u − |
i=1
and
N
1 (cid:88)
MSE = f(ti,xi) 2.
f N | u u |
i=1
Here, ti,xi,ui N denote the training data on u(t,x). The loss MSE cor-
{ u u }i=1 u
responds to the training data on u(t,x) while MSE enforces the structure
f
imposed by equation (3) at a finite set of collocation points, whose number
and location is taken to be the same as the training data.
To illustrate the effectiveness of our approach, we have created a train-
ing data-set by randomly generating N = 2,000 points across the entire
spatio-temporal domain from the exact solution corresponding to λ = 1.0
1
and λ = 0.01/π. The locations of the training points are illustrated in the
2
top panel of figure 1. This data-set is then used to train a 9-layer deep
neural network with 20 neurons per hidden layer by minimizing the mean
square error loss of (5) using the L-BFGS optimizer [12]. Upon training,
the network is calibrated to predict the entire solution u(t,x), as well as the
unknown parameters λ = (λ ,λ ) that define the underlying dynamics. A
1 2
visual assessment of the predictive accuracy of the physics informed neural
network is given in the middle and bottom panels of figure 1. The network is
able to identify the underlying partial differential equation with remarkable
accuracy, even in the case where the scattered training data is corrupted with
1% uncorrelated noise.
Tofurtherscrutinizetheperformanceofouralgorithm,wehaveperformed
a systematic study with respect to the total number of training data, the
noise corruption levels, and the neural network architecture. The results are
summarized in tables 1 and 2. The key observation here is that the proposed
4

--- Page 5 ---

1.0
0.5
0.0
0.5
−
1.0
−
0.0 0.2 0.4 0.6 0.8
t
x
u(t,x)
1.00
0.75
0.50
0.25
0.00
0.25
−
0.50
−
0.75
−
1.00
−
Data(2000points)
1
0
1
−
1 0 1
−
x
)x,t(u
t=0.25
1
0
1
−
1 0 1
−
x
)x,t(u
t=0.50
1
0
1
−
1 0 1
−
x
Exact Prediction
)x,t(u
t=0.75
Correct PDE u +uu 0.0031831u =0
t x xx
−
Identified PDE (clean data) u +0.99915uu 0.0031794u =0
t x xx
−
Identified PDE (1% noise) u +1.00042uu 0.0032098u =0
t x xx
−
Figure 1: Burgers equation: Top: Predicted solution u(t,x) along with the training data.
Middle: Comparison of the predicted and exact solutions corresponding to the three tem-
poral snapshots depicted by the dashed vertical lines in the top panel. Bottom: Correct
partial differential equation along with the identified one obtained by learning λ and λ .
1 2
methodology appears to be very robust with respect to noise levels in the
data, andyieldsareasonableidentificationaccuracyevenfornoisecorruption
upto10%. Thisenhancedrobustnessseemstogreatlyoutperformcompeting
approaches using Gaussian process regression as previously reported in [7] as
well as approaches relying on sparse regression that require relatively clean
data for accurately computing numerical gradients [13].
2.1.1. Example (Navier-Stokes Equation)
Our next example involves a realistic scenario of incompressible fluid flow
as described by the ubiquitous Navier-Stokes equations. Navier-Stokes equa-
tions describe the physics of many phenomena of scientific and engineering
5

--- Page 6 ---

% error in λ % error in λ
1 2
noise
0% 1% 5% 10% 0% 1% 5% 10%
N
u
500 0.131 0.518 0.118 1.319 13.885 0.483 1.708 4.058
1000 0.186 0.533 0.157 1.869 3.719 8.262 3.481 14.544
1500 0.432 0.033 0.706 0.725 3.093 1.423 0.502 3.156
2000 0.096 0.039 0.190 0.101 0.469 0.008 6.216 6.391
Table 1: Burgers’ equation: Percentage error in the identified parameters λ and λ for
1 2
different number of training data N corrupted by different noise levels. Here, the neural
network architecture is kept fixed to 9 layers and 20 neurons per layer.
% error in λ % error in λ
1 2
Neurons
10 20 40 10 20 40
Layers
2 11.696 2.837 1.679 103.919 67.055 49.186
4 0.332 0.109 0.428 4.721 1.234 6.170
6 0.668 0.629 0.118 3.144 3.123 1.158
8 0.414 0.141 0.266 8.459 1.902 1.552
Table 2: Burgers’ equation: Percentage error in the identified parameters λ and λ
1 2
for different number of hidden layers and neurons per layer. Here, the training data
is considered to be noise-free and fixed to N =2,000.
interest. They may be used to model the weather, ocean currents, water flow
in a pipe and air flow around a wing. The Navier-Stokes equations in their
full and simplified forms help with the design of aircraft and cars, the study
of blood flow, the design of power stations, the analysis of the dispersion of
pollutants, and many other applications. Let us consider the Navier-Stokes
equations in two dimensions1 (2D) given explicitly by
u +λ (uu +vu ) = p +λ (u +u ),
t 1 x y − x 2 xx yy (6)
v +λ (uv +vv ) = p +λ (v +v ),
t 1 x y y 2 xx yy
−
where u(t,x,y) denotes the x-component of the velocity field, v(t,x,y) the
y-component, and p(t,x,y) the pressure. Here, λ = (λ ,λ ) are the unknown
1 2
1It is straightforward to generalize the proposed framework to the Navier-Stokes equa-
tions in three dimensions (3D).
6

--- Page 7 ---

parameters. Solutions to the Navier-Stokes equations are searched in the set
of divergence-free functions; i.e.,
u +v = 0. (7)
x y
This extra equation is the continuity equation for incompressible fluids that
describes the conservation of mass of the fluid. We make the assumption
that
u = ψ , v = ψ , (8)
y x
−
for some latent function ψ(t,x,y).2 Under this assumption, the continuity
equation (7) will be automatically satisfied. Given noisy measurements
ti,xi,yi,ui,vi N
{ }i=1
of the velocity field, we are interested in learning the parameters λ as well as
the pressure p(t,x,y). We define f(t,x,y) and g(t,x,y) to be given by
f := u +λ (uu +vu )+p λ (u +u ),
t 1 x y x − 2 xx yy (9)
g := v +λ (uv +vv )+p λ (v +v ),
t 1 x y y 2 xx yy
−
(cid:2) (cid:3)
and proceed by jointly approximating ψ(t,x,y) p(t,x,y) using a single
neuralnetworkwithtwooutputs. Thispriorassumptionalongwithequations
(cid:2) (cid:3)
(8)and(9)resultsintoaphysicsinformedneuralnetwork f(t,x,y) g(t,x,y) .
The parameters λ of the Navier-Stokes operator as well as the parameters of
(cid:2) (cid:3) (cid:2) (cid:3)
the neural networks ψ(t,x,y) p(t,x,y) and f(t,x,y) g(t,x,y) can be
trained by minimizing the mean squared error loss
N
1 (cid:88)(cid:0) (cid:1)
MSE := u(ti,xi,yi) ui 2 + v(ti,xi,yi) vi 2 (10)
N | − | | − |
i=1
N
1 (cid:88)(cid:0) (cid:1)
+ f(ti,xi,yi) 2 + g(ti,xi,yi) 2 .
N | | | |
i=1
Hereweconsidertheprototypeproblemofincompressibleflowpastacircular
cylinder; a problem known to exhibit rich dynamic behavior and transitions
2This construction can be generalized to three dimensional problems by employing the
notion of vector potentials.
7

--- Page 8 ---

for different regimes of the Reynolds number Re = u D/ν. Assuming a
∞
non-dimensional free stream velocity u = 1, cylinder diameter D = 1, and
∞
kinematic viscosity ν = 0.01, the system exhibits a periodic steady state
behavior characterized by a asymmetrical vortex shedding pattern in the
cylinder wake, known as the Ka´rm´an vortex street [14].
To generate a high-resolution data set for this problem we have employed
the spectral/hp-element solver NekTar [15]. Specifically, the solution domain
is discretized in space by a tessellation consisting of 412 triangular elements,
andwithin eachelementthe solution isapproximatedas alinear combination
of a tenth-order hierarchical, semi-orthogonal Jacobi polynomial expansion
[15]. We have assumed a uniform free stream velocity profile imposed at the
left boundary, a zero pressure outflow condition imposed at the right bound-
ary located 25 diameters downstream of the cylinder, and periodicity for the
top and bottom boundaries of the [ 15,25] [ 8,8] domain. We integrate
− × −
equation (6) using a third-order stiffly stable scheme [15] until the system
reaches a periodic steady state, as depicted in figure 2(a). In what follows,
a small portion of the resulting data-set corresponding to this steady state
solution will be used for model training, while the remaining data will be
used to validate our predictions. For simplicity, we have chosen to confine
our sampling in a rectangular region downstream of cylinder as shown in
figure 2(a).
Given scattered and potentially noisy data on the stream-wise u(t,x,y)
and transverse v(t,x,y) velocity components, our goal is to identify the un-
known parameters λ and λ , as well as to obtain a qualitatively accurate
1 2
reconstructionoftheentirepressurefieldp(t,x,y)inthecylinderwake, which
by definition can only be identified up to a constant. To this end, we have
created a training data-set by randomly sub-sampling the full high-resolution
data-set. To highlight the ability of our method to learn from scattered and
scarce training data, we have chosen N = 5,000, corresponding to a mere
1% of the total available data as illustrated in figure 2(b). Also plotted are
representative snapshots of the predicted velocity components u(t,x,y) and
v(t,x,y) after the model was trained. The neural network architecture used
here consists of 9 layers with 20 neurons in each layer.
A summary of our results for this example is presented in figure 3. We
observe that the physics informed neural network is able to correctly identify
8

--- Page 9 ---

5
0
5
−
15 10 5 0 5 10 15 20 25
− − −
x
y
Vorticity
3
2
1
0
1
−
2
−
3
−
u(t,x,y) v(t,x,y)
y t y t
x x
Figure 2: Navier-Stokes equation: Top: Incompressible flow anddynamic vortexshedding
past a circular cylinder at Re = 100. The spatio-temporal training data correspond to
the depicted rectangular region in the cylinder wake. Bottom: Locations of training data-
points for the the stream-wise and transverse velocity components, u(t,x,y) and v(t,x,t),
respectively.
the unknown parameters λ and λ with very high accuracy even when the
1 2
training data was corrupted with noise. Specifically, for the case of noise-
free training data, the error in estimating λ and λ is 0.078%, and 4.67%,
1 2
respectively. The predictions remain robust even when the training data are
corrupted with 1% uncorrelated Gaussian noise, returning an error of 0.17%,
and 5.70%, for λ and λ , respectively.
1 2
A more intriguing result stems from the network’s ability to provide a
qualitatively accurate prediction of the entire pressure field p(t,x,y) in the
absence of any training data on the pressure itself. A visual comparison
against the exact pressure solution is presented in figure 3 for a represen-
9

--- Page 10 ---

2
1
0
1
−
2
−
2 4 6 8
x
y
Predicted pressure
2
1.4
1.3 1
1.2
0
1.1
1.0 1
−
0.9
2
−
2 4 6 8
x
y
Exact pressure
0.0
0.1
−
0.2
−
0.3
−
0.4
−
0.5
−
u +(uu +vu )= p +0.01(u +u )
Correct PDE t x y − x xx yy
v +(uv +vv )= p +0.01(v +v )
t x y y xx yy
−
u +0.999(uu +vu )= p +0.01047(u +u )
Identified PDE (clean data) t x y − x xx yy
v +0.999(uv +vv )= p +0.01047(v +v )
t x y y xx yy
−
u +0.998(uu +vu )= p +0.01057(u +u )
Identified PDE (1% noise) t x y − x xx yy
v +0.998(uv +vv )= p +0.01057(v +v )
t x y y xx yy
−
Figure 3: Navier-Stokes equation: Top: Predicted versus exact instantaneous pressure
field p(t,x,y) at a representative time instant. By definition, the pressure can be recov-
ered up to a constant, hence justifying the different magnitude between the two plots.
This remarkable qualitative agreement highlights the ability of physics-informed neural
networkstoidentifytheentirepressurefield, despitethefactthatnodataonthepressure
are used during model training. Bottom: Correct partial differential equation along with
the identified one obtained by learning λ ,λ and p(t,x,y).
1 2
tative pressure snapshot. Notice that the difference in magnitude between
the exact and the predicted pressure is justified by the very nature of the
Navier-Stokes system, as the pressure field is only identifiable up to a con-
stant. This result of inferring a continuous quantity of interest from auxiliary
measurements by leveraging the underlying physics is a great example of the
enhanced capabilities that physics informed neural networks have to offer,
and highlights their potential in solving high-dimensional inverse problems.
Our approach so far assumes availability of scattered data throughout the
entire spatio-temporal domain. However, in many cases of practical interest,
one may only be able to observe the system at distinct time instants. In the
next section, we introduce a different approach that tackles the data-driven
discovery problem using only two data snapshots. We will see how, by lever-
aging the classical Runge-Kutta time-stepping schemes, one can construct
discrete time physics informed neural networks that can retain high predic-
tive accuracy even when the temporal gap between the data snapshots is
10

--- Page 11 ---

very large.
3. Discrete Time Models
We begin by applying the general form of Runge-Kutta methods with q
stages to equation (1) and obtain
un+ci = un ∆t
(cid:80)q
a [un+cj;λ], i = 1,...,q,
− j=1 ij N (11)
un+1 = un ∆t
(cid:80)q
b [un+cj;λ].
− j=1 j N
Here, un+cj(x) = u(tn + c ∆t,x) for j = 1,...,q. This general form en-
j
capsulates both implicit and explicit time-stepping schemes, depending on
the choice of the parameters a ,b ,c . Equations (11) can be equivalently
ij j j
{ }
expressed as
un = un, i = 1,...,q,
i (12)
un+1 = un+1, i = 1,...,q.
i
where
un := un+ci +∆t
(cid:80)q
a [un+cj;λ], i = 1,...,q,
i j=1 ij N (13)
un+1 := un+ci +∆t (cid:80)q (a b ) [un+cj;λ], i = 1,...,q.
i j=1 ij − j N
We proceed by placing a multi-output neural network prior on
(cid:2) (cid:3)
un+c1(x),...,un+cq(x) . (14)
This prior assumption along with equations (13) result in two physics in-
formed neural networks
(cid:2) (cid:3)
un(x),...,un(x),un (x) , (15)
1 q q+1
and
(cid:2) (cid:3)
un+1(x),...,un+1(x),un+1(x) . (16)
1 q q+1
Given noisy measurements at two distinct temporal snapshots xn,un and
{ }
xn+1,un+1 of the system at times tn and tn+1, respectively, the shared
{ }
parameters of the neural networks (14), (15), and (16) along with the pa-
rameters λ of the differential operator can be trained by minimizing the sum
11

--- Page 12 ---

of squared errors
SSE = SSE +SSE , (17)
n n+1
where
(cid:88)
q
(cid:88)
Nn
SSE := un(xn,i) un,i 2,
n | j − |
j=1 i=1
and
q Nn+1
(cid:88) (cid:88)
SSE := un+1(xn+1,i) un+1,i 2.
n+1 | j − |
j=1 i=1
Here, xn = xn,i Nn, un = un,i Nn, xn+1 = xn+1,i Nn+1, and un+1 =
{ }i=1 { }i=1 { }i=1
un+1,i Nn+1.
{ }i=1
3.1. Example (Burgers’ Equation)
Let us illustrate the key features of this method through the lens of the
Burgers’ equation. Recall the equation’s form
u +λ uu λ u = 0, (18)
t 1 x 2 xx
−
and notice that the nonlinear operator in equation (13) is given by
[un+cj] = λ un+cjun+cj λ un+cj.
N 1 x − 2 xx
Given merely two training data snapshots, the shared parameters of the
neural networks (14), (15), and (16) along with the parameters λ = (λ ,λ )
1 2
of the Burgers’ equation can be learned by minimizing the sum of squared
errors in equation (17). Here, we have created a training data-set comprising
of N = 199 and N = 201 spatial points by randomly sampling the exact
n n+1
solution at time instants tn = 0.1 and tn+1 = 0.9, respectively. The training
data are shown in the top and middle panel of figure 4. The neural network
architecture used here consists of 4 hidden layers with 50 neurons each, while
the number of Runge-Kutta stages is empirically chosen to yield a temporal
error accumulation of the order of machine precision (cid:15) by setting3
q = 0.5log(cid:15)/log(∆t), (19)
3ThisismotivatedbythetheoreticalerrorestimatesforimplicitRunge-Kuttaschemes
suggesting a truncation error of (∆t2q) [16].
O
12

--- Page 13 ---

% error in λ % error in λ
1 2
noise
0% 1% 5% 10% 0% 1% 5% 10%
∆t
0.2 0.002 0.435 6.073 3.273 0.151 4.982 59.314 83.969
0.4 0.001 0.119 1.679 2.985 0.088 2.816 8.396 8.377
0.6 0.002 0.064 2.096 1.383 0.090 0.068 3.493 24.321
0.8 0.010 0.221 0.097 1.233 1.918 3.215 13.479 1.621
Table 3: Burgers’ equation: Percentage error in the identified parameters λ and λ for
1 2
different gap size ∆t between two different snapshots and for different noise levels.
where the time-step for this example is ∆t = 0.8. The bottom panel of
figure 4 summarizes the identified parameters λ = (λ ,λ ) for the cases of
1 2
noise-free data, as well as noisy data with 1% of Gaussian uncorrelated noise
corruption. For both cases, the proposed algorithm is able to learn the cor-
rect parameter values λ = 1.0 and λ = 0.01/π with remarkable accuracy,
1 2
despite the fact that the two data snapshots used for training are very far
apart, and potentially describe different regimes of the underlying dynamics.
A sensitivity analysis is performed to quantify the accuracy of our predic-
tions with respect to the gap between the training snapshots ∆t, the noise
levels in the training data, and the physics informed neural network archi-
tecture. As shown in table 3, the proposed algorithm is quite robust to both
∆t and the noise corruption levels, and it consistently returns reasonable
estimates for the unknown parameters. This robustness is mainly attributed
to the flexibility of the underlying implicit Runge-Kutta scheme to admit an
arbitrarily high number of stages, allowing the data snapshots to be very far
apart in time, while not compromising the accuracy with which the nonlin-
ear dynamics of equation (18) are resolved. This is the key highlight of our
discrete time formulation for identification problems, setting it apart from
competing approaches [7, 13]. Lastly, table 4 presents the percentage error
in the identified parameters, demonstrating the robustness of our estimates
with respect to the underlying neural network architecture.
3.1.1. Example (Kortewegde Vries Equation)
Our final example aims to highlight the ability of the proposed frame-
work to handle governing partial differential equations involving higher or-
der derivatives. Here, we consider a mathematical model of waves on shallow
13

--- Page 14 ---

1.0
0.5
0.0
0.5
−
1.0
−
0.0 0.2 0.4 0.6 0.8
t
x
u(t,x)
0.75
0.50
0.25
0.00
0.25
−
0.50
−
0.75
−
1.0
0.5
0.0
0.5
−
1.0
−
1 0 1
−
x
)x,t(u
t = 0.10
199 trainng data
0.5
0.0
0.5
−
1 0 1
−
x
)x,t(u
t = 0.90
201 trainng data
Exact Data
Correct PDE u +uu +0.003183u = 0
t x xx
Identified PDE (clean data) u +1.000uu +0.003193u = 0
t x xx
Identified PDE (1% noise) u +1.000uu +0.003276u = 0
t x xx
Figure4: Burgers equation: Top: Solutionu(t,x)alongwiththetemporallocationsofthe
two training snapshots. Middle: Training data and exact solution corresponding to the
two temporal snapshots depicted by the dashed vertical lines in the top panel. Bottom:
Correct partial differential equation along with the identified one obtained by learning
λ ,λ .
1 2
water surfaces; the Korteweg-de Vries (KdV) equation. This equation can
also beviewed asBurgers’ equationwith an added dispersive term. The KdV
equationhasseveralconnectionstophysicalproblems. Itdescribestheevolu-
tion of long one-dimensional waves in many physical settings. Such physical
14

--- Page 15 ---

% error in λ % error in λ
1 2
Neurons
10 25 50 10 25 50
Layers
1 1.868 4.868 1.960 180.373 237.463 123.539
2 0.443 0.037 0.015 29.474 2.676 1.561
3 0.123 0.012 0.004 7.991 1.906 0.586
4 0.012 0.020 0.011 1.125 4.448 2.014
Table 4: Burgers’ equation: Percentage error in the identified parameters λ and λ for
1 2
different number of hidden layers and neurons in each layer.
settings include shallow-water waves with weakly non-linear restoring forces,
long internal waves in a density-stratified ocean, ion acoustic waves in a
plasma, and acoustic waves on a crystal lattice. Moreover, the KdV equa-
tion is the governing equation of the string in the Fermi-Pasta-Ulam problem
[17] in the continuum limit. The KdV equation reads as
u +λ uu +λ u = 0, (20)
t 1 x 2 xxx
with (λ ,λ ) being the unknown parameters. For the KdV equation, the
1 2
nonlinear operator in equations (13) is given by
[un+cj] = λ un+cjun+cj λ un+cj
N 1 x − 2 xxx
and the shared parameters of the neural networks (14), (15), and (16) along
with the parameters λ = (λ ,λ ) of the KdV equation can be learned by
1 2
minimizing the sum of squared errors (17).
To obtain a set of training and test data we simulated (20) using con-
ventional spectral methods. Specifically, starting from an initial condition
u(0,x) = cos(πx) and assuming periodic boundary conditions, we have inte-
grated equation (20) up to a final time t = 1.0 using the Chebfun package
[18] with a spectral Fourier discretization with 512 modes and a fourth-order
explicit Runge-Kutta temporal integrator with time-step ∆t = 10 6. Using
−
this data-set, we then extract two solution snapshots at time tn = 0.2 and
tn+1 = 0.8, and randomly sub-sample them using N = 199 and N = 201
n n+1
to generate a training data-set. We then use these data to train a discrete
timephysics informed neural networkbyminimizingthesumofsquarederror
15

--- Page 16 ---

loss of equation (17) using L-BFGS [12]. The network architecture used here
comprises of 4 hidden layers, 50 neurons per layer, and an output layer pre-
dicting the solution at the q Runge-Kutta stages, i.e., un+cj(x), j = 1,...,q,
where q is computed using equation (19) by setting ∆t = 0.6.
The results of this experiment are summarized in figure 5. In the top
panel, we present the exact solution u(t,x), along with the locations of the
two data snapshots used for training. A more detailed overview of the exact
solution and the training data is given in the middle panel. It is worth notic-
ing how the complex nonlinear dynamics of equation (20) causes dramatic
differences in the form of the solution between the two reported snapshots.
Despite these differences, and the large temporal gap between the two train-
ing snapshots, our method is able to correctly identify the unknown param-
eters regardless of whether the training data is corrupted with noise or not.
Specifically, for the case of noise-free training data, the error in estimating
λ and λ is 0.023%, and 0.006%, respectively, while the case with 1% noise
1 2
in the training data returns errors of 0.057%, and 0.017%, respectively.
4. Summary and Discussion
We have introduced physics informed neural networks, a new class of uni-
versal function approximators that is capable of encoding any underlying
physical laws that govern a given data-set, and can be described by par-
tial differential equations. In this work, we design data-driven algorithms
for discovering dynamic models described by parametrized nonlinear partial
differential equations. The inferred models allow us to construct computa-
tionally efficient and fully differentiable surrogates that can be subsequently
used for different applications including predictive forecasting, control, and
optimization.
Although a series of promising results was presented, the reader may per-
haps agree that this two-part treatise creates more questions than it answers.
In a broader context, and along the way of seeking further understanding of
such tools, we believe that this work advocates a fruitful synergy between
machine learning and classical computational physics that has the potential
to enrich both fields and lead to high-impact developments.
16

--- Page 17 ---

1.0
0.5
0.0
0.5
−
1.0
−
0.0 0.2 0.4 0.6 0.8 1.0
t
x
u(t,x)
2.0
1.5
1.0
0.5
0.0
0.5
−
1.0
−
1.0
0.5
0.0
0.5
−
1.0
−
1 0 1
−
x
)x,t(u
t = 0.20
199 trainng data
2
1
0
1 0 1
−
x
)x,t(u
t = 0.80
201 trainng data
Exact Data
Correct PDE u +uu +0.0025u = 0
t x xxx
Identified PDE (clean data) u +1.000uu +0.0025002u = 0
t x xxx
Identified PDE (1% noise) u +0.999uu +0.0024996u = 0
t x xxx
Figure 5: KdV equation: Top: Solution u(t,x) along with the temporal locations of the
two training snapshots. Middle: Training data and exact solution corresponding to the
two temporal snapshots depicted by the dashed vertical lines in the top panel. Bottom:
Correct partial differential equation along with the identified one obtained by learning
λ ,λ .
1 2
Acknowledgements
This work received support by the DARPA EQUiPS grant N66001-15-
2-4055, the MURI/ARO grant W911NF-15-1-0562, and the AFOSR grant
FA9550-17-1-0013. All data and codes used in this manuscript are publicly
17

--- Page 18 ---

available on GitHub at https://github.com/maziarraissi/PINNs.
References
[1] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with
deep convolutional neural networks, in: Advances in neural information
processing systems, pp. 1097–1105.
[2] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015)
436–444.
[3] B. M. Lake, R. Salakhutdinov, J. B. Tenenbaum, Human-level concept
learning through probabilistic program induction, Science 350 (2015)
1332–1338.
[4] B. Alipanahi, A. Delong, M. T. Weirauch, B. J. Frey, Predicting the se-
quencespecificitiesofDNA-andRNA-bindingproteinsbydeeplearning,
Nature biotechnology 33 (2015) 831–838.
[5] M. Raissi, P. Perdikaris, G. E. Karniadakis, Numerical Gaussian pro-
cesses for time-dependent and non-linear partial differential equations,
arXiv preprint arXiv:1703.10230 (2017).
[6] M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of dif-
ferential equations using noisy multi-fidelity data, Journal of Computa-
tional Physics 335 (2017) 736–746.
[7] M. Raissi, G. E. Karniadakis, Hidden physics models: Machine
learning of nonlinear partial differential equations, arXiv preprint
arXiv:1708.00588 (2017).
[8] M. Raissi, P. Perdikaris, G. E. Karniadakis, Machine learning of linear
differential equations using Gaussian processes, Journal of Computa-
tional Physics 348 (2017) 683 – 693.
[9] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven
discovery of partial differential equations, Science Advances 3 (2017).
[10] C. Basdevant, M. Deville, P. Haldenwang, J. Lacroix, J. Ouazzani,
R. Peyret, P. Orlandi, A. Patera, Spectral and finite difference solu-
tions of the Burgers equation, Computers & fluids 14 (1986) 23–41.
18

--- Page 19 ---

[11] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, J. M. Siskind, Au-
tomatic differentiation in machine learning: a survey, arXiv preprint
arXiv:1502.05767 (2015).
[12] D. C. Liu, J. Nocedal, On the limited memory BFGS method for large
scale optimization, Mathematical programming 45 (1989) 503–528.
[13] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equa-
tions from data by sparse identification of nonlinear dynamical systems,
Proceedings of the National Academy of Sciences 113 (2016) 3932–3937.
[14] T. Von Ka´rma´n, Aerodynamics, volume 9, McGraw-Hill New York,
1963.
[15] G. Karniadakis, S. Sherwin, Spectral/hp element methods for computa-
tional fluid dynamics, Oxford University Press, 2013.
[16] A. Iserles, A first course in the numerical analysis of differential equa-
tions, 44, Cambridge University Press, 2009.
[17] T. Dauxois, Fermi, Pasta, Ulam and a mysterious lady, arXiv preprint
arXiv:0801.1590 (2008).
[18] T. A. Driscoll, N. Hale, L. N. Trefethen, Chebfun guide, 2014.
19```

---

## Section 1.10: Fast and Accurate Normal Estimation for Point

Source File: Fast and Accurate Normal Estimation for Point.txt

### Paper Content:

```
# Fast and Accurate Normal Estimation for Point.pdf
# Converted: 2025-07-18 12:59:06
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Fast and Accurate Normal Estimation for Point.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Fast and Accurate Normal Estimation for Point.txt


--- Page 1 ---

Fast and Accurate Normal Estimation for Point
Cloud via Patch Stitching
Jun Zhou∗, Wei Jin, Mingjie Wang, Xiuping Liu, Zhiyang Li, Zhaobin Liu
Abstract—This paper presents an effective normal es-
timation method adopting multi-patch stitching for an un-
P
structured point cloud. The majority of learning-based ap- 2
proachesencodealocalpatcharoundeachpointofawhole
modelandestimatethenormalsinapoint-by-pointmanner.
In contrast, we suggest a more efficient pipeline, in which
we introduce a patch-level normal estimation architecture P 3
to process a series of overlapping patches. Additionally, a
multi-normal selection method based on weights, dubbed P
1 Overlapping Parts
as multi-patch stitching, integrates the normals from the
Normals Stitching
overlappingpatches.Toreducetheadverseeffectsofsharp
cornersornoiseinapatch,weintroduceanadaptivelocal
featureaggregationlayertofocusonananisotropicneigh- Fig.1. Theillustrationofmulti-patch(normals)stitching.
borhood. We then utilize a multi-branch planar experts
module to break the mutual influence between underlying
piecewise surfaces in a patch. At the stitching stage, we size is greatly affected by noise. The multi-scale strategy may
use the learned weights of multi-branch planar experts
achieve higher accuracy but suffers from high computation
and distance weights between points to select the best
time.Besides,assumingthatamodelcontains100kpointsand
normal from the overlapping parts. Furthermore, we put
forward constructing a sparse matrix representation to re- 256 neighbors are sampled by each point, a large repetition
ducelarge-scaleretrievaloverheadsfortheloopiterations rate would occur (Each point would be used about 256 times
dramatically. Extensive experiments demonstrate that our evenly), leading to a computational redundancy. Although
methodachievesSOTAresultswiththeadvantageoflower
the IterNet [7] can quickly implement normal estimation
computational costs and higher robustness to noise over
with fewer parameters, the whole model needs to be input
mostoftheexistingapproaches.
simultaneously, making the algorithm inflexible.
IndexTerms—NormalEstimation,PatchStitching.
Motivated by these challenges, we adopt a patch-by-patch
normal estimation method to effectively reduce the repetition
I. INTRODUCTION rate of each point on the model without sacrificing the size
of the sampling range, and only a few patches are used for
DUEtotheadvancesin3Dacquisitiontechnologies,point-
whole model estimation (see Fig. 1). A sparse index matrix
based representations can be captured in various vision
is constructed to map the point IDs of the patches to the
applications. Generally, the scanned large-scale point cloud
entire point cloud, significantly reducing the time-cost of the
is noisy, incomplete, and contains sampling irregularity while
multi-normal stitching process. Besides, large-scale sampling
lacking the essential local geometry properties such as point
patches with isotropy may contain underlying piecewise sur-
normals.Anaccurateandfastnormalestimationalgorithmcan
faces and noise. Thus, the useless information within neigh-
significantlyimprovethedownstreamtaskssuchas3Dsurface
borsandmutualinterferencebetweenunderlyingsurfaceswill
reconstruction [1], point cloud consolidation [2], [3].
leadto entanglementbetween pointsatthe patch-levelnormal
Compared with traditional methods, the learning-based
estimationstage.Tosolvetheentanglementproblem,wefirstly
methods [4]–[6] can well handle the problem of parameter
employ an adaptive local feature aggregation layer to focus
tuning. Commonly, given a point, neighboring points should
on a meaningful neighborhood for each point at the feature
be extracted via K nearest neighbor (kNN) search and used
extraction stage. Then a multi-branch planar experts module
to infer the normal of the center point via the deep learning
is introduced to break down the mutual interference between
architecture. In this process, the selection of neighborhood
the underlying surfaces at the normal estimation stage. The
contributions of our work are summarized as follows:
ThisworkissupportedbytheNationalNaturalScienceFoundationof • A flexible and fast multi-patch stitching framework is
China(No.62002040,No.61976040).
proposed, and the network’s time complexity can be
J.Zhou,Z.Li,Z.LiuarewiththeSchoolofinformationscienceand
technology,DalianMaritimeUniversity,China ignored relative to the scale of point cloud.
W.JiniswithDalianNeusoftUniversityofInformation,China. • A sparse index matrix is introduced to speed up the
M.WangiswithMemorialUniversityofNewfoundland,Canada.
process that maps the point IDs of patches to the point
X.LiuiswithwithDalianUniversityofLiaoningProvince,China.
E-mail:jun90@dlmu.edu.cn cloud. The mapping table can be used at the stage of
1202
raM
13
]VC.sc[
2v66061.3012:viXra

--- Page 2 ---

Multi-branch planar experts
Residual DGCNN
K×3 K×3
3×3
Rotation
T-Net
K×64 K×64 K×64
Multi- Scale
Features
K×64
Branch Net 1
Branch Net 2
Branch Net 3
Concatenate
Weights Estimation Net
N̂1(K×3)
N̂2(K×3)
N̂3(K×3)
N̂
(K×3)
K×3
Weights
Sampled Patches
1 2 3 4 5 … 1 2
...
N
)ledoM(sDI_tnioP
Training Stage
Model
(Multi-head Point Transformer)
Distance weights The attention weights
between PointsMi
Weights Table ϕi(F):
Sparse Index Matrix Table 1 2 3 4 5 6 7 8 9 …M K×
1
Points_IDs(Patches)
M K ×
2
...
ρi(F):
Sparse Matrix N Best Normal Representation 1 2 Mu 3 lti- 4 No 5 rm 6 als 7 Ta 8 ble 9 …M K× Selection αi(F):
1
2
...
N
Concatenate
Output
Loss
P i∈RK×3 F∈RK×192
Local Features
Aggregation
S
Be Used Again！
Inference Stage：Patches Stitching
P 1 K×K
F new P 2
The Predicted normals from Different Patches
PM
Fig.2. Thepipelineofournormalestimationmethod.GivenapointcloudS,weemploythetrainednetworktoestimatepatch-levelnormalsfrom
thesampledpatches,andtheweightsofmulti-branchplanarexpertscanbeobtainedsimultaneously.Inthestageofpatchstitching,asparseindex
representationisconstructedtomapthepointsIDsoftheoverlappingpatchestotheIDsofthepointcloud.Thelearnedexpertsweightsandthe
patchdistanceweightsareusedtoselectthebestnormalfromtheoverlappingpartswiththesameIDs.
weight-based normals selection. methods are essentially necessary technologies to improve the
• A multi-branch planar experts module and an adaptive robustness of the point cloud normal estimation.
local feature aggregation layer are introduced to greatly
improved the robustness of our patch-level network. B. Learning-basednormalestimation
Several deep learning approaches have been proposed to
II. RELATED WORK estimate normal vectors from unstructured point clouds in
recent years. Commonly, the existing deep learning strat-
The task of normal estimation for point cloud is a long-
egy is always inspired by the traditional methods: a local
standing fundamental problem in geometry processing. In this
neighborhood should be selected first, and then a regressor
section, we will review traditional and popular deep-based
is proposed to evaluate the normal of a center point. As a
methods for normal estimation.
pioneer,BoulchandMarlet[17]firstapply2DCNNtoregress
the point normal. Then, inspired by PointNet [4], PCPNet [4]
A. Traditionalnormalestimation
directly uses local neighboring points to regress the point
A classic regression method for normal estimation uses cloud normal. Besides, based on 3D modified fisher vectors
principal component analysis (PCA) [8]. However, the perfor- (3DmFV)[18]presentations,Nesti-Net[5]isproposedtousea
mance of this approach usually depends on the patch sizes mixture-of-experts architecture, which relies on a data-driven
and the noise scales. Thus, some more effective sampling approach for selecting the optimal scale around each point
methods [9], [10] are proposed. Several variants are also and encourages sub-network specialization. The multi-scale
proposed for plane fitting, such as local spherical surfaces methods can effectively improve the results, but it caused a
fitting [11] and Jets [12]. However, the effect of sampling great time consumption. Recently, Zhou et al. [19] introduce
size is still existed in these methods, in which the isotropic an extra feature constraint mechanism that can effectively
neighborhoods (which always contain different underlying distinguish the piecewise surfaces near the sharp edges of a
piecewise surfaces) will affect the final fitting process. For patch.ThenDeepfit[6]incorporatesaneuralnetworktolearn
better discarding the neighbors belonging to different surfaces pointwise weights for weighted least squares surface fitting
on the patch, more robust statistics approaches are employed and make a significant performance. However, these methods
to select an adaptive region [13]–[16]. While many of these alsofollowthepipelineofPCPNet,whichevaluatesnormalin
methods hold theoretical guarantees on approximation and a point-by-point way. Lenssen et al. [7] employ an iteration
robustness, all the above methods require a careful setting normal estimation method to reduce time consumption. This
of parameters for different shape types. So the data-driven method has to input the entire model, thus reducing the

--- Page 3 ---

256 1024 256 1024
points points points points
None 0
1
High
Fig.3. Visualizationofattentionweightssampledfromtwokindsofmodels.Fromtoptobottom:patcheswithoutnoise,0.012Gaussiannoise,and
0.006Gaussiannoise.Foreachmodel,fromlefttoright:patcheswithdifferentsamplingsizes.Patchesarecoloredaccordingtoweightmagnitude
mappedtoaheatmaprangingfrom0to1.
flexibility of the network. Considering the time-cost and the graphconvolutionlayersareusedtoextractpointwisefeatures
flexibility of the network, we propose a fast patch stitching of the rotated patch Pˆ.
methodfornormalestimation.Ourmethodcanachievenormal Specifically, in the first layer, we construct a directed graph
estimationmoreefficientlybyastitchingway(seeFig.1)than G0 = (V0,E0) where V0 and E0 are unordered point set
most existing deep learning methods regardless of network’s {pˆ}K andkNNgraphofthepatchPˆ,respectively.Wedefine
i i=1
time complexity. edgefeaturesofthe0-thlayerase0 =h (pˆ,pˆ ),whereh
ij Θ0 i j Θ0
is
III. OVERVIEW h
Θ0
=ReLU(θ
0
·(pˆ
j
−pˆ
i
)+θ¯
0
·pˆ
i
). (1)
This paper presents a novel pipeline that uses a multi- Here, (pˆ,pˆ ) ∈ E0, Θ = (θ ,θ¯ ), and the output feature
patch stitching strategy to accelerate the process of normal i j 0 0 0
F0 ∈RK×64 at the vertex i is thus given by
estimation (see Fig. 2). It consists of the following two main
parts: patch-level normal estimation and multi-patch normal f0 = max h (pˆ,pˆ ). (2)
stitching. Firstly, a residual DGCNN (Sec. IV-B) is proposed
i
j:(i,j)∈E0
Θ0 i j
as our patch-level network, which incorporates an adaptive
Fromthesecondtoforthlayerofthebasicnetwork,aresidual
localfeature aggregationlayer (Sec.IV-C)and amulti-branch
mapislearnedtotakefeatureFl ∈RK×64asinputandoutput
planarexpertsmodule(Sec.IV-D).Then,asparseindexmatrix
residual feature representation Fl+1 for the next layer. The
isbuilttoefficientlymapthepointIDsofpatchestothewhole
learned feature of vertex i can be defined as
point cloud in the multiple patches stitching (Sec. V). Finally,
evaluating weight is used to select the best normal from the fl+1 = max h (fl,fl)+fl, (3)
overlapping patches.
i
j:(i,j)∈El+1
Θl+1 i j i
where l = 0,1,2,3, and the El+1 is updated via a dynamic
IV. PATCH-LEVEL NORMAL ESTIMATION graph strategy [20]. Finally, we concatenated the multi-scale
A. Pre-processing features as global feature F = F1(cid:76) F2(cid:76) F3 ∈ RK×192,
and {Θ ,Θ ,Θ ,Θ ,Θ } are the training parameters of the
The full pipeline is illustrated in Fig. 2. Given a 3D point t 0 1 2 3
basic network.
cloud S ∈ RN×3 and a query point p ∈ S, a local patch
i
P ∈RK×3canbeextractedviakNN.Inthetrainingstage,we
i
randomlysamplepatchesfromthemodelstoensuresufficient
C. Adaptivelocalfeatureaggregation
training samples. In the inference stage, the M patches that
cover the entire model S are extracted by using the farthest For point cloud normal estimation, an accurate pointwise
point sampling (FPS) algorithm and M (cid:28)N. weight prediction can help to softly select the most relevant
points and improve normal estimation performance. Inspired
bythis,weconsidercalculatingpointwiseattentionweightsin
B. TheBasicnetwork:residualDGCNN
a patch, and then we can adaptively extract richer feature at
Given a sampled patch P ∈ Rk×3 as input, our estimator eachpointbyaggregatingfeaturesofpointsfromitsweighted
i
can predict each point normal of the patch. Therefore, we neighborhood. Thus, an adaptive local feature aggregation
design a basic network inspired by DGCNN [20], which can layerisproposedbasedonamulti-headpointtransformer[21].
extractthepointwisefeatureofapatchandasimpleregressor Fig.3showssomevisualizationexamplesinwhichanaverage
can be followed to estimate the pointwise normals. Firstly, a multi-head attention map of the specified point is given.
quaternion spatial transformer (QST) is used to transform the Specifically, given the concatenated feature F ∈ RK×C
input patch to a canonical pose Pˆ =P ·T (P), and T (P) extracted from the basic network, a multi-head scaled dot-
Θt Θt
is a learned rigid rotation transformation. Then, four stacked product attention is introduced. In each head, we should first

--- Page 4 ---

sehctap
prahS
sehctaP
htoomS
enoN
esioN
enoN
esioN
Fig.4. Visualizationofangularerrorandplanarexpertsweights.Foreachmodel,fromlefttoright:inputmodelwithaspecifiedneighborhood(two
samplingscalesaregiven),angularerrormap,andthreeplanarexpertsweights.Foreachkindofmodel(sharpandsmooth),fromtoptobottom:
patcheswithoutnoiseand0.006Gaussiannoise.
learn a weight matrix (self-attention matrix) M ∈ RK×K multi-planar experts module for different kinds of patches
i
between the points: (smooth and sharp ones).
Specifically, the multi-branch planar experts module has
φ (F)Tρ (F)
M =softmax( i √ i ), (4) three planar branches and a weight estimation branch. The
i
K branch j can give output as Nˆ ∈ RK×3 where j = 1,2,3
j
where M is the output attention matrix of the head i. φ and and the weight estimation branch learns a probability weight
ρ i are lin i ear projections used to transform pointwise fea i ture. foreachbranchw ∈RK×3,and (cid:80)3 j=1 w i,j =1issatisfiedfor
Then our aggregation features head can be calculated by the each point i on the patch. The loss function L can be written
i
attention matrix as: as:
K 3
head i =M i ·α i (F), (5) L= K 1 (cid:88)(cid:88) w(i,j)∗min((cid:107)Nˆ j [i]+N[i](cid:107) 2 , (7)
i=1j=1
whereα isalsoalinearprojection.Fig.3qualitativelydepicts
i (cid:107)Nˆ [i]−N[i](cid:107) )
the average map of the multi-head attentions, which can be j 2
calculated by M = (cid:80) n Mi where n is denoted the number of where N andNˆ j are the ground truth normal of the patch P
the heads. Finally, the aggregated feature F new ∈RK×C can and the predicted normal from the branch j, respectively. At
be obtained: test time, we compute only one normal, which is associated
(cid:77) (cid:77) (cid:77) with the maximal w, and the maximum weight of each point
F =(head head ··· head )·W, (6)
new 1 2 n will be reused in the patch stitching stage.
(cid:76)
whereW isalearnableprojectionmatrixand isaconcate-
nation operator. Generally, n=8 in our network. V. MULTI-PATCH NORMAL STITCHING
As illustrated in Fig. 1, a group of overlapping patches
{P } issampledfrommodelS,andapatchstitching
i i=1,2,···,M
D. Multi-branchplanarexpertsfornormalestimation
operationneedstobeexecutedtorealizenormalselectionfrom
After obtaining the pointwise feature F from the local the overlapping patches. Although the number of sampled
new
feature aggregation layer, as shown in Fig. 2, a normal patches M (cid:28) N (e.g. 103 vs. 105), where N is the point
estimator module is employed in our patch-level network. An numberofmodelS,thetotalnumberofpoints(K×M)isstill
intuitive consideration is that the canonical patch Pˆ in the 3D enormous.Theloopsthatmapeachpointfromtheoverlapping
space is oriented to three main directions of the axes x, y and patchestoS andcompletethemulti-normalstitchingaretime-
z, the visualization illustrations can be seen in the third to consuming.
sixthcolumnsofFig.4(seethefirstmodel).Inthispaper,we In this paper, the loop process consists of sparse index
look for a simple way, namely, multi-branch planar experts, matrix construction and multi-normal selection (see Fig. 2).
tobreakdowntheseunderlyingsurfaces,therebyreducingthe First, we perform a sparse matrix of large-scale indicators to
interaction between surface features in the process of normal realizetheunifiedsortingoftheindexesandquicklyrealizethe
estimation. Fig. 4 qualitatively visualizes the performance of points IDs mapping from the multi-patch to model S. Based

--- Page 5 ---

TABLEI
COMPARISONOFTHERMSEANGLEERRORFORUNORIENTEDNORMALESTIMATIONOFOURMULTI-PATCHSTITCHINGMETHOD(THREESAMPLING
SIZES)TOCLASSICALGEOMETRICMETHODS,ANDDEEPLEARNINGMETHODS.
Ours PCA Jet HoughCNN PCPNET Nesti-Net Lenssen DeepFit
1024 512 256 small med large small med large ss ss ms ms(MoE) ss ss
None 7.09 6.72 6.42 8.31 12.29 16.77 7.60 12.35 17.35 10.23 9.68 9.62 6.99 6.72 6.51
Noiseσ
0.00125 9.11 9.06 9.04 12.00 12.87 16.87 12.36 12.84 17.42 11.62 11.46 11.37 10.11 9.95 9.21
0.006 16.24 16.22 16.42 40.36 18.38 18.94 41.39 18.33 18.85 22.66 18.26 18.37 17.63 17.18 16.72
0.012 21.06 21.49 22.20 52.63 27.50 23.50 53.21 27.68 23.41 33.39 22.80 23.28 22.28 21.96 23.12
Density
Gradient 8.37 8.09 7.85 9.14 12.81 17.26 8.49 13.13 17.80 12.47 13.42 11.70 9.00 7.73 7.31
Stripes 7.65 7.27 6.88 9.42 13.66 19.87 8.61 13.39 19.29 11.02 11.74 11.16 8.47 7.51 7.92
Average 11.59 11.48 11.47 21.97 16.25 18.87 21.95 16.29 19.02 16.90 14.56 14.34 12.41 11.84 11.80
TABLEII
COMPARISONOFTIMEANDSPACECOMPLEXITYBETWEENTHEPROPOSEDAPPROACH(DIFFERENTOVERLAPPINGRATEr)ANDOTHERDEEP
LEARNINGNORMALSESTIMATIONMETHODS.
Ours(r =12) Ours(r =16) Ours (r =24) DeepFit IterNet Nesti-Net PCPNet(ms)
Num. parameters 19.41M 19.41M 19.41M 3.5 M 7981 170.1 M 21.3 M
Run time 0.09 0.13 0.20 1.02 0.05 8.9 3.79
Relative time 1.8× 2.6× 4× 20.4× 1 × 111× 178×
on the sparse matrix table, we can quickly find the normals TABLEIII
and weights from overlapping patches and select the best nor- ARCHITECTUREOFOUREXPERTSWEIGHTSESTIMATOR.
mal by a weight evaluation process. Specifically, we assume
that Nˆ = {nˆ ,nˆ ,··· ,nˆ }, {pˆ ,pˆ ,··· ,pˆ } and Layertype Settings Outputs
i i,1 i,2 i,mi i,1 i,2 i,mi Input K×192 -
{Pˆ i,1 ,Pˆ i,2 ,··· ,Pˆ i,mi } denote the predicted normals from the Conv1d(192,64)
overlapping parts at point i of model S, the correspondence Layer1 BatchNorm K×64
LeakyRelu(0.2)
points from the overlapping patches and the correspondence
Conv1d(64,32)
patches respectively, where m i is the number of candidate Layer2 BatchNorm K×32
normals at the point i. LeakyRelu(0.2)
Conv1d(32,3)
The sampled patches have an open boundary, and most of Layer3 K×3
Softmax
the boundary points only have neighbors on one side, lead-
ing to inaccurate feature extraction. Therefore, we introduce
a distance weight to avoid choosing a candidate point far TABLEIV
ARCHITECTUREOFOURPLANEREXPERTSBRANCH.
from patch center. The distance weights w (i,j) of each
dist
candidate normal nˆ i,j is Layertype Settings Outputs
Input K×192 -
(cid:107)pˆ −cˆ (cid:107)2 Conv1d(192,64)
w (i,j)=exp(− i,j i,j 2) (8) BatchNorm
dist 2σ2 Layer1 LeakyRelu(0.2) K×64
Dropout(0.3)
where j =1,2,··· ,m and cˆ is the centroid coordinate of Conv1d(64,32)
i i,j
patchPˆ i,j .Thismeansthatthebestnormalshouldbeselected Layer2 Le B ak a y tc R h e N lu or ( m 0.2) K×32
fromthepointthatmorenearthecenterofthecorrespondence Dropout(0.3)
patch. Then, the weights of planar experts are reused in this Layer3 Conv1d(32,3) K×3
stage. We keep the highest weight from the three experts
on the patch and denote it as w (i,j). Finally, we can
pred
obtain the candidate weights via multiply these two weights VI. EXPERIMENTS
as w (i,j)=w (i,j)·w (i,j). The best normal
candidate dist pred
A. Datasetandtrainingdetails
can be obtained by selecting the maximum candidate weight:
For training and testing, the PCPNet dataset [4] is used.
nˆ
selected
(i)=Nˆ
i
[(cid:101)j], The training set consists of eight shapes with different noise
(9) levels(nonoise,σ =0.00125,σ =0.0065andσ =0.012),and
(cid:101)j = argmax w
candidate
(i,j),
j∈{1,2,···,mi} all shapes are given as triangle meshes and densely sampled
with100kpoints.Thetestsetconsistsof22shapes,including
wherenˆ (i)denotesthefinalpredictednormalatapoint figurines, CAD objects, and analytic shapes. For evaluation,
selected
i of model S. we use the same 5000 point subset per shape as in Guerrero

--- Page 6 ---

Ours(K=256)Ours(K=512)Ours(K=1024) DeepFit
13.65 15.35 16.59 14.01
11.85 11.95 12.42 13.06
10.89 10.61 10.62 11.32
).ged(
srorrE
elgnA
PCPNet(ms) PCPNet(ss) IterNet Nesti-Net regressingonlythecenterpointnormal,patch-levelregression
is more likely to be affected by the mutual information
between the underlying surfaces. The point pairs distributed
on different underlying surfaces may interact with each other.
Thus, we propose a multi-branch planar experts module to
23.61 26.91 17.70 16.33
disentangle the interference between the surfaces effectively.
Fig.4showsthatourplanarexpertscandistinguishtheunderly
14.27 15.48 13.54 12.77
surfaces and adaptively adjust the selected surface of each
0∘ branchaccordingtothenumbersofunderlyingplanes(seethe
13.27 12.35 11.80 11.98 red dotted window of Fig. 4). Also, see the third column of
Tab.VII,themulti-branchplanarexpertsmodulegivesa7.2%
90∘ 10.16 9.87 9.78 11.24 10.77 10.70 10.24 10.27 performanceboostcomparedtothebasicnetwork(4layersof
residual DGCNN followed only one branch regressor).
Fig.5. Angularerrorvisualizationresultsofourmethod(threesampling
sizes) compared to others. The colors of the points correspond to Adaptive multi-scale feature aggregation. Typically, the
angulardifference,mappedtoaheatmaprangingfrom0-90degrees. sampledpatchesincludenoiseandoutliersthatheavilyreduce
accuracy. To overcome this, Deepfit and IterNet learn local
weightsandfitlocalsurfaceviaweightedleast-squares.Differ-
et al. [4]. In the training stage, we use a batch size of 48, the
ently, we consider learning patch-level weights in the feature
Adam optimizer, and a base learning rate of 0.1. Our network
space and adaptively aggregate local features of each point
is trained on a single Nvidia RTX 2080 Ti GPU.
on the patch. Inspired by self-attention network, we introduce
In addition, we show our normal estimation branch and ex-
anadaptivemulti-scalefeatureaggregationlayer.Visualization
pertsweightsestimatorinTab.IIIandTab.IVrespectively.We
of the adaptive local weights in different sampling sizes and
use three normal estimation branches and an experts weights
noise scales are exhibited in Fig. 3. It can be seen that the
estimator to constitute our multi-branch experts module.
local weights are anisotropic near the edges and corners,
assigninghighweightsontheplanethatincludesthespecified
B. Normalestimationperformance
points. Besides, see the red dotted circle of Fig. 3, at the
RMSE angle error of our approaches and related methods
convex point, the learned weights drop rapidly with strong
on the PCPNet test set are presented in Tab. I. Compared
self-adaptability.Thevisualizationresultsalsoconfirmthatour
to HoughCNN [17] (single-scale) and PCPNet [4] (single-
aggregation layer is robust to noise (see last row of Fig. 3).
scale and multi-scale), Nest-Net [5] achieves higher accuracy.
In Tab. I, compared to the basic network with multi-branch
However, the MoE architecture has 7 sub-networks with
planar experts module, the adaptive local feature aggregation
a significantly larger number of parameters. Deepfit [6] is
layer gives a 7.7 % improvement dramatically.
based on PCPNet [4]. Considering the local latent surface
Patch number and sampling size. The RMSE error of
representation,thereisabigimprovement.However,thetime-
different sampling sizes and patch numbers are explored (see
consuming problem is still not solved. IterNet [7] iterates to
Tab. VIII). Firstly, it shows that a large sampling size will
estimate point normal and have a low RMSE error compared
improve the high noise cases, and the patch number affects
totheabovemethods,butthenetworkneedstoinputthewhole
the performance of the network in the density cases. Then,
model simultaneously, which is not a flexible way. In Tab. I,
the table also shows that a large patch number will increase
we report results using patch stitching with the following
the overlapping rate of each point and affects the efficiency
configurations: patch sizes are set to 1024, 512, 256, and
of the algorithm. Finally, our accuracy for different parameter
patch numbers are set to 2304, 6144, 9216, respectively. Our
choices is relatively stable (from 11.74 to 11.47 on average).
estimator achieves better performance for the three sampling
sizes. Fig. 5 also depicts the angular error in each point for
the different learning-based methods using a heatmap, and
the noise is increasing from top to bottom. It can be seen
D. Efficiency
that large-size sampling can improve the robustness to noise,
andourresultshaveconsistentadvantageforbothsmoothand Tab. II lists the number of model parameters and average
sharp models compared to other methods. execution times (ms per point) for estimating normals on a
Finally, we also use the proportion of good points metric
point cloud with 100k points via using an Nvidia GTX 2080
(PGP α), which computes the percentage of points with an
Ti (almost 11G is occupied). IterNet uses a fixed batch of
error less than α; e.g., PGP10 computes the percentage of
size 100k, and it has the lowest number of parameters and
pointswithangularerroroflessthan10degrees.TableVIand
running time. Benefits from patch stitching strategy, the time
Table V report the results of PGP10 and PGP5 respectively
consumption of our method is not affected by the number
for different methods compared to ours.
of model parameters (Ours has 19.41M parameters but 0.1-
0.2 ms/p running time). Our patch-level normal estimation
C. AblationStudy method is the second fast (about 1.8 × to IterNet) and highly
Multi-branch planar experts module. Generally, a sampled competitive compared with other learning-based methods that
patch contains multiple surfaces, see Fig. 4. Compare to estimate normal in a point-by-point manner.

--- Page 7 ---

TABLEV
NORMALESTIMATIONRESULTSCOMPARISONUSINGTHEPGP5METRIC(HIGHERISBETTER).
Ours PCA Jet PCPNET Nesti-Net Lenssen DeepFit
1024 512 256 small med large small med large ss ms ms(MoE) ss ss
None 0.8006 0.8115 0.823 0.7756 0.6192 0.5361 0.7905 0.6284 0.5395 0.7078 0.6986 0.8057 0.8730 0.7985
Noiseσ
0.00125 0.7493 0.7409 0.7365 0.4758 0.6157 0.5335 0.4132 0.6237 0.5377 0.6245 0.5932 0.6611 0.734 0.7379
0.006 0.5818 0.5694 0.5359 0.02998 0.42 0.4812 0.027 0.4152 0.4837 0.4486 0.366 0.5618 0.5416 0.5424
0.012 0.4417 0.4172 0.3568 0.0104 0.154 0.3719 0.0099 0.1462 0.3715 0.3156 0.2482 0.399 0.3634 0.3132
Density
Gradient 0.8003 0.8153 0.8275 0.7743 0.647 0.4894 0.7883 0.6442 0.4976 0.6065 0.6254 0.7749 0.8778 0.7912
Stripes 0.7641 0.7773 0.7881 0.7575 0.6174 0.4415 0.7753 0.6321 0.4598 0.6126 0.6231 0.7676 0.8657 0.7595
Average 0.6898 0.6886 0.678 0.4706 0.5122 0.4756 0.4674 0.5150 0.4816 0.5526 0.5257 0.6617 0.7092 0.6572
TABLEVI
NORMALESTIMATIONRESULTSCOMPARISONUSINGTHEPGP10METRIC(HIGHERISBETTER).
Ours PCA Jet PCPNET Nesti-Net Lenssen DeepFit
1024 512 256 small med large small med large ss ms ms(MoE) ss ss
None 0.902 0.9097 0.918 0.8686 0.7409 0.6606 0.8802 0.7509 0.6584 0.8364 0.8404 0.9120 0.9288 0.9074
Noiseσ
0.00125 0.8653 0.8667 0.8649 0.7712 0.7378 0.6598 0.7346 0.7447 0.6575 0.8013 0.8031 0.8384 0.8495 0.8559
0.006 0.7361 0.7332 0.7232 0.1101 0.6402 0.6301 0.1006 0.6397 0.6311 0.6667 0.6294 0.7164 0.7154 0.7202
0.012 0.6299 0.6156 0.5853 0.04063 0.394 0.5462 0.0377 0.3827 0.547 0.5546 0.5124 0.6123 0.585 0.553
Density
Gradient 0.9093 0.917 0.9239 0.8731 0.7624 0.6366 0.8848 0.7695 0.6401 0.7801 0.8062 0.9003 0.9319 0.9114
Stripes 0.888 0.8956 0.9022 0.8609 0.7379 0.5879 0.8743 0.7504 0.6001 0.7967 0.8076 0.8929 0.9243 0.8888
Average 0.8218 0.8230 0.8212 0.4706 0.5122 0.4756 0.4674 0.5150 0.4816 0.5526 0.5257 0.8120 0.8225 0.8061
TABLEVII TABLEVIII
EFFECTSOFTHEBASICNETWORK,MULTI-BRANCHPLANAREXPERTS, THERMSEANGLEERRORANDAVERAGERUNNINGTIMEWITHDIFF.
ANDADAPTIVELOCALFEATUREAGGREGATION.THERMSEANGLE SAMPLINGSIZESANDDIFF.NUMBERSOFPATCHES.THE”OVERLAP”
ERRORISUSEDONPCPNETDATASET.THESAMPLINGSIZEAND INDICATESTHEAVERAGENUMBEROFTIMESTHATEACHPOINT
PATCHNUMBERARESETTO1024AND1152,RESPECTIVELY. PARTICIPATESINTHECALCULATION.
Ours 1024pts 512pts 256pts
Ours Ours
Scale PCPNet(ss) (Multi-Branch
(Basic) (Multi-Branch) #patch 1152 2304 2304 6144 4608 9216
+Aggregation)
None 9.68 8.62 8.31 7.13 None 7.13 7.09 6.84 6.72 6.44 6.42
Noiseσ Noiseσ
0.00125 11.46 10.62 10.05 9.18 0.00125 9.18 9.11 9.10 9.06 9.06 9.04
0.006 18.26 17.83 17.15 16.28 0.006 16.28 16.25 16.31 16.22 16.44 16.42
0.012 22.80 23.35 21.85 21.12 0.012 21.12 21.06 21.52 21.49 22.27 22.20
Density Density
Gradient 13.42 9.99 9.38 8.32
Gradient 8.32 7.65 7.96 7.27 7.48 6.88
Stripes 11.74 11.82 9.57 8.43
Stripes 8.43 8.37 8.11 8.09 7.92 7.85
average 14.56 13.70 12.72 11.74
average 11.74 11.59 11.64 11.48 11.60 11.47
Times(ms/p) 0.08 0.18 0.09 0.21 0.10 0.19
Overlap 12 24 12 32 12 24
VII. CONCLUSION
Afast,accurate,androbustnormalestimationframeworkis
proposed, whose efficiency is not affected by network scale. [2] S.Wu,H.Huang,M.Gong,M.Zwicker,andD.Cohen-Or,“Deeppoints
consolidation,”ACMTransactionsonGraphics(ToG),vol.34,no.6,pp.
Firstly, the adaptive local feature aggregation layer and multi-
1–13,2015.
branchplanarexpertsmoduleareemployedtoimprovetheac- [3] J. Wang, K. Xu, L. Liu, J. Cao, S. Liu, Z. Yu, and X. D. Gu, “Con-
curacyofthepatch-levelnormalestimatordramatically.Then, solidationoflow-qualitypointcloudsfromoutdoorscenes,”Computer
GraphicsForum,vol.32,no.5,pp.207–216,2013.
in the inference stage, a sparse index matrix is constructed to
[4] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra, “Pcpnet
improve the efficiency of the multi-batch stitching process. learning local shape properties from raw point clouds,” Computer
Finally, a sufficient weight is used to evaluate the multi- GraphicsForum,vol.37,no.2,pp.75–85,2018.
[5] Y. Ben-Shabat, M. Lindenbaum, and A. Fischer, “Nesti-net: Normal
normal of the overlapping parts. The approach demonstrates
estimation for unstructured 3d point clouds using convolutional neural
the competitiveness compared to SOTA approaches. networks,”inProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition,pp.10112–10120,2019.
[6] Y. Ben-Shabat and S. Gould, “Deepfit: 3d surface fitting via neural
REFERENCES network weighted least squares,” arXiv preprint arXiv:2003.10826,
2020.
[1] M.Berger,A.Tagliasacchi,L.Seversky,P.Alliez,J.Levine,A.Sharf, [7] J. E. Lenssen, C. Osendorfer, and J. Masci, “Deep iterative surface
and C. Silva, “State of the art in surface reconstruction from point normal estimation,” in Proceedings of the IEEE/CVF Conference on
clouds,”EurographicsStarReports,vol.1,no.1,pp.161–185,2014. ComputerVisionandPatternRecognition,pp.11247–11256,2020.

--- Page 8 ---

[8] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle, vol.39,no.5,pp.408–420,2007.
“Surface reconstruction from unorganized points,” in Proceedings of [16] Y. Wang, H.-Y. Feng, F.-E´. Delorme, and S. Engin, “An adaptive
the 19th annual conference on Computer graphics and interactive normalestimationmethodforscannedpointcloudswithsharpfeatures,”
techniques,pp.71–78,1992. Computer-AidedDesign,vol.45,no.11,pp.1333–1348,2013.
[9] N.J.MitraandA.Nguyen,“Estimatingsurfacenormalsinnoisypoint [17] A.BoulchandR.Marlet,“Deeplearningforrobustnormalestimation
cloud data,” in Proceedings of the nineteenth annual symposium on inunstructuredpointclouds,”ComputerGraphicsForum,vol.35,no.5,
Computationalgeometry,pp.322–328,2003.
pp.281–290,2016.
[10] M. Pauly, R. Keiser, L. P. Kobbelt, and M. Gross, “Shape modeling
[18] Y. Ben-Shabat, M. Lindenbaum, and A. Fischer, “3dmfv: Three-
withpoint-sampledgeometry,”ACMTransactionsonGraphics(TOG),
dimensional point cloud classification in real-time using convolutional
vol.22,no.3,pp.641–650,2003.
neuralnetworks,”IEEERoboticsandAutomationLetters,vol.3,no.4,
[11] G. Guennebaud and M. Gross, “Algebraic point set surfaces,” ACM
pp.3145–3152,2018.
SIGGRAPH2007papers,pp.23–es,2007.
[19] J.Zhou,H.Huang,B.Liu,andX.Liu,“Normalestimationfor3dpoint
[12] F.CazalsandM.Pouget,“Estimatingdifferentialquantitiesusingpoly-
nomial fitting of osculating jets,” Computer Aided Geometric Design,
cloudsvialocalplaneconstraintandmulti-scaleselection,”Computer-
AidedDesign,vol.129,p.102916,2020.
vol.22,no.2,pp.121–146,2005.
[13] B. Li,R. Schnabel,R. Klein, Z.Cheng, G. Dang,and S.Jin, “Robust [20] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.
normal estimation for point clouds with sharp features,” Computers & Solomon, “Dynamic graph cnn for learning on point clouds,” Acm
Graphics,vol.34,no.2,pp.94–106,2010. TransactionsOnGraphics(tog),vol.38,no.5,pp.1–12,2019.
[14] B.Mederos,L.Velho,andL.H.deFigueiredo,“Robustsmoothingof [21] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
noisy point clouds,” in Proc. SIAM Conference on Geometric Design Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
andComputing,vol.2004,no.1,p.2,2003. inneuralinformationprocessingsystems,pp.5998–6008,2017.
[15] M.Yoon,Y.Lee,S.Lee,I.Ivrissimtzis,andH.-P.Seidel,“Surfaceand
normalensemblesforsurfacereconstruction,”Computer-AidedDesign,```

---

## Section 1.11: Shape from Shading-A Method for Obtaining Shape from One View

Source File: Shape from Shading-A Method for Obtaining Shape from One View.txt

### Paper Content:

```
# Shape from Shading-A Method for Obtaining Shape from One View.pdf
# Converted: 2025-07-18 12:59:06
# Method: pdfplumber
# Domain: pixel2physics
# Source: /home/user/vekt/papers/pixel2physics/pdfs/layer2/Shape from Shading-A Method for Obtaining Shape from One View.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2/Shape from Shading-A Method for Obtaining Shape from One View.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/37602086
Shape from Shading: A Method for Obtaining the Shape of a Smooth Opaque
Object from One View
Article · October 2004
Source: OAI
CITATIONS READS
582 1,860
1 author:
Berthold K. P. Horn
Massachusetts Institute of Technology
241 PUBLICATIONS 36,884 CITATIONS
SEE PROFILE
All content following this page was uploaded by Berthold K. P. Horn on 05 April 2014.
The user has requested enhancement of the downloaded file.

--- Page 201 ---

View publication stats```

---

## Section 1.12: layer2_compendium

Source File: layer2_compendium.txt

### Paper Content:

```
```

---

# COMPENDIUM STATISTICS

Target Directory: ./layer2/
Compendium Name: layer2_compendium.txt
Total Chapters: 0
Total Papers: 12
Total Size: 404K
Generated: Sat 19 Jul 18:02:13 NZST 2025

Ready for layer2 research analysis!
