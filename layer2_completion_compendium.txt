# RESEARCH PAPERS COMPENDIUM: LAYER2_COMPLETION

Generated: Sat 19 Jul 18:23:00 NZST 2025
Purpose: Research foundation for layer2_completion domain
Source: ./layer2_completion/

---

# TABLE OF CONTENTS

Chapter 1: PAPERS IN LAYER2_COMPLETION

---

# CHAPTER 1: PAPERS IN LAYER2_COMPLETION

Directory: ./layer2_completion/
Papers: 43 research papers

## Section 1.1: 3D_Point_Cloud_Analysis_for_Detection_an

Source File: 3D_Point_Cloud_Analysis_for_Detection_an.txt

### Paper Content:

```
# 3D_Point_Cloud_Analysis_for_Detection_an.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/3D_Point_Cloud_Analysis_for_Detection_an.pdf
# Output: ../layer2_completion/txt/3D_Point_Cloud_Analysis_for_Detection_an.txt


--- Page 1 ---

HAL Id: hal-01622056
https://hal-mines-albi.archives-ouvertes.fr/hal-01622056
Submitted on 24 Oct 2017
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
3D Point Cloud Analysis for Detection and
Characterization of Defects on Airplane Exterior Surface
Igor Jovančević, Huy-Hieu Pham, Jean-José Orteu, Rémi Gilblas, Jacques
Harvent, Xavier Maurice, Ludovic Brèthes
To cite this version:
Igor Jovančević, Huy-Hieu Pham, Jean-José Orteu, Rémi Gilblas, Jacques Harvent, et al.. 3D Point
Cloud Analysis for Detection and Characterization of Defects on Airplane Exterior Surface. Journal
of Nondestructive Evaluation, Springer Verlag, 2017, 36 (4), pp.74. ฀10.1007/s10921-017-0453-1฀. ฀hal-
01622056฀


--- Page 2 ---

3D point cloud analysis for detection and characterization of
defects on airplane exterior surface
Igor Jovanˇcevi´c1 · Huy-Hieu Pham1 · Jean-Jos´e Orteu1 · R´emi Gilblas1 ·
Jacques Harvent1 · Xavier Maurice2 · Ludovic Br`ethes2
Received: date / Accepted: date
Abstract Three-dimensional surface defect inspection
remains a challenging task. This paper describes a novel
automatic vision-based inspection system that is capa-
ble of detecting and characterizing defects on an air-
plane exterior surface. By analyzing 3D data collected
with a 3D scanner, our method aims to identify and ex-
tract the information about the undesired defects such
as dents, protrusions or scratches based on local sur-
face properties. Surface dents and protrusions are iden-
tiﬁed as the deviations from an ideal, smooth surface.
Given an unorganized point cloud, we ﬁrst smooth noisy
data by using Moving Least Squares algorithm. The
curvature and normal information are then estimated
at every point in the input data. As a next step, Re-
gion Growing segmentation algorithm divides the point
cloud into defective and non-defective regions using the
local normal and curvature information. Further, the
convex hull around each defective region is calculated
in order to englobe the suspicious irregularity. Finally,
we use our new technique to measure the dimension,
depth, and orientation of the defects. We tested and
validated our novel approach on real aircraft data ob-
tained from an Airbus A320, for diﬀerent types of de-
fect. The accuracy of the system is evaluated by com-
paring the measurements of our approach with ground
truth measurements obtained by a high-accuracy mea-
suring device. The result shows that our work is robust,
eﬀective and promising for industrial applications.
Igor Jovanˇcevi´c
igorjovan@gmail.com
1
Universit´e de Toulouse; CNRS, INSA, UPS, Mines Albi,
ISAE; Institut Cl´ement Ader (ICA);
Campus Jarlard, F-81013 Albi, France.
2
KEONYS, 5 avenue de l’escadrille Normandie-Niemen,
31700 Blagnac, France.
Keywords aircraft · defect detection · defect charac-
terization · non destructive evaluation · 3D scanner ·
unorganized point cloud


--- Page 3 ---

2
Igor Jovanˇcevi´c1 et al.
List of symbols
In this paper, we use the following notation:
PN = {p1, p2, ..., pN}
A set of N points, pi is
the ith data point.
pi = (xi, yi, zi)
A point in three-dimensional
space.
P K = p1, p2, ..., pK
The set of points which are
located in the k-neighborhood
of a query point pi.
p
The centroid of the data
e.g., given a set of points
PN, we have:
p = 1
N (
N
P
i=1
xi,
N
P
i=1
yi,
N
P
i=1
zi)
ni
A surface normal estimated
at a point pi.
·
The dot product.
⇥
The cross product.
k ◦k
The Euclidean norm of ◦.
1 Introduction
In the aviation industry, one of the most important
maintenance tasks is aircraft surface inspection. The
main purpose of fuselage inspection process is to de-
tect the undesired defects such as dents, protrusions
or cracks. This is a diﬃcult task for a human opera-
tor, especially when dealing with small defects hardly
or not at all visible to the naked eye. In order to speed-
up the inspection process and reduce human error, a
multi-partners research project is being carried on to
develop a collaborative mobile robot named Air-Cobot,
with integrated automatic vision-based aircraft inspec-
tion capabilities.
Currently, coordinate measuring machines (CMMs)
are widely used in the ﬁeld of three-dimensional (3D)
inspection. However, the inspection systems based on
CMM machines have extremely low scanning speed;
these systems are not suitable for working with the
large objects such as an airplane. Instead, the recent
advances of laser scanning technologies now allow the
development of new devices to acquire the 3D data.
Various types of 3D scanner have been developed for
the inspection applications and the use of laser sen-
sors in 3D part measurement process has introduced a
signiﬁcant improvement in data acquisition process re-
garding time and cost [18]. Therefore, Air-Cobot uses
a 3D scanner that is capable of collecting point cloud
within a short time at high rate of accuracy and under
diﬀerent illumination conditions. In order to get infor-
mation about the airplane exterior surface, we need to
develop a robust inspecting technique for processing the
scanned point cloud data.
In this paper, we present a robust approach for de-
tecting and characterizing undesired deformation struc-
tures from 3D data. It mainly consists of two processes:
detection process and characterization process. Firstly,
the point cloud is pre-processed to remove measurement
errors and outliers. The proposed approach then analy-
ses the point cloud for identifying the defects and their
positions. For this purpose, we focus on developing a
segmentation algorithm in which the defect regions are
segmented based on local features including local curva-
ture and normal information. After isolating the defec-
tive regions, they are analyzed to ﬁnd their dimensions,
depths and orientations.
Our proposed method has the following advantages:
(1) provides a robust framework which is able to de-
tect and extract detailed information about the defects;
(2) detects various types of defects without any prior
knowledge of size or shape; (3) fully automates inspec-
tion process.
The rest of the paper is organized as follows: Sect. 2
contains a review of the related work. The dataset, con-
text, and our approach are explained in Sect. 3. Sect. 4
shows a few empirical experiments of the proposed ap-
proach and discusses about experimental results. Fi-
nally, in Sect. 5, some future directions are presented
and the paper is concluded.
2 Related work
Over the last few decades, visual inspection has re-
ceived a great interest from the aviation industry. The
majority of the existing systems have been developed
for aircraft surface inspection. For instance, C. Seher
et al. [46] have developed a prototype robot for non-
destructive inspection (NDI) based on 3-D stereoscopic
camera. M. Siegel et al. [48,49] have introduced the
surface crack detection algorithm for aircraft skin in-
spection. This algorithm is based on determining re-
gion of interest (ROI) and edge detection technique. B.
S. Wong et al. [57] have also developed an algorithm
based on ROI and edge detection, but using a digital
X-ray sensor. R. Mumtaz et al. [32] proposed a new
image processing technique using neural network for
classifying crack and scratch on the body of the air-
craft. Wang et al. [54] developed a mobile platform for
aircraft skin crack classiﬁcation by fusing two diﬀer-
ent data modalities: CCD camera image and ultrasonic
data. They designed features which they further used
to train multi-class support vector machine in order to
accomplish classiﬁcation of cracks. In the literature, to


--- Page 4 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
3
our knowledge, there is no much work that concerns the
point cloud analysis for aircraft inspection. However, we
can ﬁnd some similar studies for diﬀerent purposes. For
instance, V. Borsu et al. [4] analyzed the surface of the
automotive body panel and determined the positions
and type of deformations of interest. P. Tang et al.[52]
have developed a ﬂatness defect detection algorithm by
ﬁtting a plane against point clouds and calculating the
residuals of each point. Recently, R. Marani et al. [30]
have presented a system based on a laser triangulation
scanner that allows to identify the surface defects on
tiny objects, solving occlusion problems.
The main purpose of our work is the defects de-
tection and characterization by analyzing the surface
structure in point cloud data. Speciﬁcally, this study
is closely related to surface segmentation. Deriving de-
fected surfaces from a set of 3D point clouds is not a
trivial task as the cloud data retrieved from 3D sensor
are usually incomplete, noisy, and unorganized. Many
authors have introduced approaches and algorithms for
segmenting 3D point cloud. We refer the reader to [56,
23,34] for a global review of 3D cloud segmentation
strategies. In the literature, region-based method is one
of the most popular approaches for 3D data segmenta-
tion. This segmentation technique is proposed by Besl
and Jain in 1988 [2]. It is a procedure that groups
points or subregions into larger regions based on ho-
mogeneity measures of local surface properties [12,19,
26,20,53,40,7,43,37,35]. Many of edge-based segmen-
tation methods have been used to segment point cloud
data. The principle of these methods is based on the
determination of contours and then identiﬁcation of re-
gions limited by these contours [13,50,44]. Some local
information of point cloud should be calculated such as
normal directions [3,1], geometric and topological in-
formation [22]. In addition, the authors also use model-
based approaches [45,36] and graph-based approaches
[11,51,58].
3 Methodology for defect detection and
characterization
3.1 Overview of the proposed system
Fig. 1 illustrates all the steps of our approach. We use a
personal computer for processing point clouds acquired
from a structured light 3D scanner. First, a defect de-
tection module identiﬁes and localizes the presence of
defects or deformations on the airplane exterior surface.
Then, we analyse the status of all the defected regions
and extract information about the defects size, depth
and orientation. We termed this second phase as defect
characterization process.
The 3D data processing program must ensure robust-
Fig. 1 Overview of proposed system architecture
ness for industrial applications. In other words, it must
be able to detect diﬀerent types of defects with diﬀerent
properties.
3.2 Data acquisition
Our approach is applied to inspect the fuselage of real
Airbus A320 airplane. The dataset is captured using a
3D scanner mounted on Air-Cobot (see Fig. 2a and 5b).
The process is fully automatic and performs inspection
of the body as the Air-Cobot moves following a pre-
determined trajectory like Fig. 2b. In order to test the
Fig. 2 (a) Air-Cobot and Airbus A320 airplane; (b) Illustra-
tion of moving map of Air-Cobot
robustness of our approach, we collected data of vari-
ous types of defects such as undesired dents or scratches
under diﬀerent light and weather conditions. Few exam-
ples of our dataset are shown in Fig. 3.
3.3 Defect detection process
In this section, we introduce the defect detection pro-
cess as illustrated in Fig. 4. The process is divided into


--- Page 5 ---

4
Igor Jovanˇcevi´c1 et al.
(a)
(b)
(c)
(d)
Fig. 3 (a) Point cloud of surface without defect; (b) point
cloud with large and small dents (c) point cloud with small
dents;(d) point cloud with a long scratch
ﬁve steps. First, 3D point cloud is acquired using a 3D
scanner. Next, it is smoothed by Moving Least Squares
(MLS) algorithm. Further, we calculate the normal and
curvature information of each point in the point cloud.
We employ Region-Growing for segmenting the point
cloud into two sets of points: (1) defected regions and
(2) non-defected regions. Finally, these two sets are ac-
cordingly labeled for visualization.
Fig. 4 Overview of the detection phase
Step 1 ( data): With the advances of 3D scanning
technologies, various types of 3D sensors have been de-
veloped for acquiring 3D data of high quality. This tech-
nology is very useful for material inspection and qual-
ity control. It allows to collect a lot of 3D data about
the object surface and its size. Diﬀerent 3D scanners
such as FARO Focus 3Dr, Trimbler, Artec Evar, or
Handyscan 3Dr can be used for our work. After ana-
lyzing the data quality of diﬀerent types of scanner, we
decided to use Artec Eva 3D scanner (see Fig. 5a). It
scans quickly, in high resolution (0.5 mm) and accuracy
(0.1 mm). Artec 3D scanner is also very versatile. It is
recommended to keep the distance between the scanner
and the object in the range 0.4 −1m. The scanner has
ﬁeld of view up to 536⇥371mm (for furthest range) and
frame rate of 16 frames per second. It should be noted,
however, that the fundamental part of our system does
not need to be changed if we want to use another type
of 3D scanner.
(a)
(b)
Fig. 5 (a) Artec Eva 3D scanner; (b) Air-Cobot with the
scanner mounted on a pantograph
Step 2 (Pre-processing): Although the quality of
3D scanners has been improved greatly, we still get in-
evitable measurement errors and outliers in point cloud.
The goal of this step is to smooth and re-sample point
cloud data. This pre-processing step is important be-
cause it gives more accurate local information. We use
Moving Least Squares (MLS) for smoothing the sur-
face. MLS is a method of reconstructing a surface from
a set of unorganized point data by higher order poly-
nomial interpolations in the neighborhood of a ﬁxed
point. This technique was proposed by Lancaster and
Salkauskas in 1981 [27] and developed by Levin [28,29].
We are approximating our cloud with a polynomial of
second degree in Rn, since airplane fuselage is closest to
this type of surface. The mathematical model of MLS
algorithm is described as follows:
Consider a function f : Rn 7! R and a set of points
S = {xi, fi|f(xi) = fi} where xi 2 Rn and fi 2 R. The
Moving Least Square approximation of the point xi is
the error functional:
fMLS(xi) =
X
i
(k f(xi) −fi k)2Θ(k x −xi k)
(1)


--- Page 6 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
5
We achieve the weighted least-square error at bf where:
bf = min(fMLS(xi)) = min(k f(xi)−fi k)2Θ(k x−xi k)
In equation (1), the function Θ is called weighting func-
tion. Authors have proposed diﬀerent choices for this
function. For example, in [29] the author used a Gaus-
sian function: Θ(d) = e
−d2
h2 . By applying the MLS al-
gorithm, we can remove the small errors and further
estimate the intrinsic properties of the surface such as
normal and curvature (see Fig. 6).
(a)
(b)
Fig. 6 Surface normal estimation on the: (a) original point
cloud before resampling and (b) after resampling using Mov-
ing Least Squares algorithm
Step 3 (Normals and Curvature Estimation):
In 3D geometry, a surface normal at a point is a vec-
tor that is perpendicular to the surface at that point.
The surface normals are important information for un-
derstanding the local properties of a geometric surface.
Many diﬀerent normal estimation techniques exist in
Fig. 7 Illustration of surface normals
the literature [24,8,31]. One of the simplest methods to
estimate the normal of a point on the surface is based
on estimating the normal of a plane tangent to the sur-
face [41].
Given a point cloud PN , we consider the neighboring
points P K of a query point pq. By using a least-square
plane ﬁtting estimation algorithm as introduced in [47],
we can determine the tangent plane S represented by
a point x and a normal vector nx. For all the points
pi 2 P K, the distance from pi to the plane S is deﬁned
as :
di = (pi −x) · nx
S is a least-square plane if di = 0.
If we set x as a centroid of P K:
x = p = 1
K
K
X
i=0
(pi)
in order to estimate nx, we need to analyze the eigen-
values λj and eigenvectors vj (j = 0, 1, 2) of the 3 ⇥3
covariance matrix A formed by the points pi 2 P K :
A = 1
K
K
X
i=0
(pi −p).(pi −p)T
(2)
The eigenvector v0 corresponding to the smallest eigen-
value λ0 is the approximation of n [41].
Another surface property that we are using in de-
fect detection is curvature. In computer graphics, there
are many ways to deﬁne the curvature of a surface at
a point such as Gaussian curvature (K = k1k2), or
Mean Curvature (H = k1 + k2
2
) [10] where k1 and k2
are the principal curvatures of the surface. In the lit-
erature, these methods are widely used for calculating
curvature information [39]. Some other techniques have
been proposed by the authors in [59,25]. The above
approaches are accurate but very sensitive to noise and
unable to estimate the curvature from a set of points di-
rectly (mesh representation required). We estimate the
curvature information at a speciﬁc point by analysing
the eigenvalues of covariance matrix deﬁned in equation
2.
The curvature value of a point Pj is estimated as:
c{Pj} =
λ0
λ0 + λ1 + λ2
(3)
where λ0 = min (λj=0,1,2) [38].
To resume, we estimate surface normals and curva-
ture of each point in the cloud. This information is used
in the next step.
Step 4 (Segmentation): In order to detect the
damaged regions on airplane exterior surface, we need
to segment the 3D points cloud data into regions that
are homogeneous in terms of calculated surface char-
acteristics, more speciﬁcally normal vector angles and
curvature diﬀerences. By this way, we can divide orig-
inal point cloud into two principal parts: damaged re-
gions and non-damaged regions. The objective of this
step is to partition a point cloud into sub-point clouds
based on normal and curvature information which are
calculated in step 3.


--- Page 7 ---

6
Igor Jovanˇcevi´c1 et al.
Let P represent the entire input point cloud, the
region-based segmentation divides P into n sub-point
clouds R1, R2, R3, ...Ri..., Rn such that:
(1)
nS
i=1
Ri = P
(2) Ri is connected region (i = 1, n )
(3) Ri \ Rj = ↵for all i and j, i 6= j
(4) LP(Ri) = True for i = 1, n
(5) LP(Ri [ Ri) = False for any adjacent regions Ri
and Rj
LP(Ri) is a logical predicate deﬁned on the points p 2
Ri. Condition (4) indicates that the diﬀerences in sur-
face properties (normal and curvature in our case) in
a segmented region must be below certain threshold.
Condition (5) regulates the diﬀerence between adjacent
regions which should be above the threshold. The algo-
rithm starts with random points (Pseeds) representing
distinct regions and grow them until they cover the en-
tire cloud. For region growing, we need a rule for check-
ing the homogeneity of a region after each growth step.
In this paper, we have used surface normals and curva-
tures to merge the points that are close enough in terms
of the smoothness constraint. The picked point is added
to the set called seeds. In each iteration a seed point is
chosen from the set of unlabeled points. Seed point is
always selected as a point with the lowest curvature
in the current set of unlabeled points. For every seed
point, the algorithm ﬁnds neighboring points (30 in our
case). Every neighbor is tested for the angle between its
normal and normal of the current seed point. If the an-
gle is less than a threshold value, then current point is
added to the current region. Further, every neighbour
is tested for the curvature value. If the curvature is less
than threshold value cth, then the point is added to the
seeds [42]. The criteria is shown in Eq. 4:
arccos(n, nk) αth,
(4)
where n and nk are normals of the seed point p and
current tested point pk, respectively.
By this way, the output of this algorithm is the set
of clusters, where each cluster is a set of points that
are considered to be a part of the same smooth surface.
We ﬁnish by obtaining one vast cluster which is consid-
ered background and a lot of small clusters only in the
defected regions. Admittedly, we obtained several clus-
ters within the same defect, but we solve this by simply
merging adjacent clusters. Our defects are never close
to each other so this merging step is safe to be done.
The segmentation algorithm presented in step 4 can
be described as following:
Algorithm 1: Point cloud segmentation based on
surface normal and curvature
Input: Point cloud P = p1, p2...., pN; Point normals N; Point
curvatures C ; Angle threshold αth; Curvature threshold cth;
Neighbour ﬁnding function F(·)
Process:
1: Region list {R}  −↵
2: Available points list {L}  −{1..|P|}
3: While {L} is not empty do
4:
Current region {Rc}  −↵
5:
Current seeds {Sc}  −↵
6:
Point with minimum curvature in {L} = Pmin
7:
{Sc}  −{Sc} [ Pmin
8:
{Rc}  −{Rc} [ Pmin
9:
{L}  −{L} \ Pmin
10:
For i = 0 to size ({Sc}) do
11:
Find nearest neighbors of current seed point
{Bc}  −F(Sc{i})
12:
For j = 0 to size ({Bc}) do
13:
Current neighbor point Pj  −Bc{j}
14:
If Pj 2 L and
arccos (
!!(N{Sc{i}}, N{Sc{j}})
!!) < αth then
15:
{Rc}  −{Rc} S Pj
16:
{L}  −{L} \ Pj
17:
If c{Pj} < cth then
18:
{Sc}  −{Sc} [ Pj
19:
End if
20:
End if
21:
End for
22:
End for
23:
Global segment list {R}  −{R} S{Rc}
24: End while
25: Return the global segment list {R}
Outputs: a set of homogeneous regions R = {Ri}.
Step 5 (Labeling): The previous algorithm allows
determining the regions which contain points that be-
long to defects. The defects are labeled by the algorithm
in order to show them on the original point cloud. The
resulting labeling is shown in red color as in Fig. 8:
3.4 Defect characterization process
Next step is to characterize the defects by estimating
their size and depth. For that, we use the result of the
defect detection process.


--- Page 8 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
7
(a)
(b)
(c)
Fig. 8 (a) Part of the fuselage; (b) Acquired point cloud
(visualized with MeshLab).; (c) The detected defects on the
original mesh are shown in red color
The purpose of this process is to extract and show
the most important information about each detected
defect. In our study, we propose an approach that allows
estimating three main information about a defect, in-
cluding size (bounding box), the maximum depth, and
the principal orientation of a defect. Orientation is use-
ful in the case of scratch-like defects (ex. Fig. 12a).
Our global approach can be viewed as a 4-step pro-
cess (Fig. 9): (1) projection of the 3D point cloud onto
the fronto-parallel 2D image plane (2) data prepara-
tion, (3) reconstruction, and (4) extracting information
about the defects. Further on we will explain each of
the steps.
Fig. 9 Global approach of characterization process
3.4.1 Step C1 : 3D/2D projection
We are reducing our problem from 3D to 2D by pro-
jecting our 3D cloud onto the fronto-parallel 2D image
plane placed on a certain distance from the cloud. We
do this in order to reduce computational cost and also
to facilitate operations such as neighbors search in char-
acterization phase. We do not lose information because
our clouds are close to planes. After this process, each
3D point can be referenced by its 2D projection (pixel).
Planar geometric projection is mapping 3D points
of a 3D object to a two-dimensional plane called pro-
jection plane. It is done by passing lines (projectors)
through 3D points and calculating their intersections
with projection plane. Depending on the center of pro-
jection (COP), there are two principal kinds of projec-
tion: parallel and perspective projection [6]. When the
COP is placed on a ﬁnite distance from the projection
plane, perspective projection is obtained. In the case of
parallel projection, the COP is considered to be at inﬁn-
ity and projectors are parallel. Orthographic projection
is a subclass of parallel projection which is obtained
when the projectors are orthogonal to the projection
plane. If the scale is introduced in a uniform manner, it
is said that scaled orthographic projection is performed.
Scale is added in a way that the whole object is uni-
formly decreased/increased after being projected. This
type of projection is also called weak perspective pro-
jection. It assumes that relative depths of object points
are negligible compared to the average distance between
the object and COP.
In our work, we are performing a scaled orthographic
projection of our point cloud. The projection plane is
placed on a certain distance d from the cloud and ori-
ented approximately parallel to the cloud. The point
cloud points are represented by their (x, y, z) coordi-
nates in scanner reference system. We are expressing
these points in the new coordinate system which en-
ables the projection to be straightforward. This new
coordinate system is placed in the mean point of the
cloud with mean normal of the cloud as its z axis (O
0
rf
in Fig. 11). Finally, this system is translated for length
d along its z axis. The process consists of 3 steps.
Step C1.1 (Find the mean normal of the point
cloud)
The notion of centroid can apply to vectors. Let V
be a set of N normal vectors in all the points of the
cloud:
V = {n1, n2...nN} with ni = [xni, yni, zni]T
The mean normal is calculated as:
n = 1
N
N
P
i=1
ni = (xn, yn, zn)
The mean normal is then normalized:


--- Page 9 ---

8
Igor Jovanˇcevi´c1 et al.
bn =
n
knk =
 
xn
knk, yn
knk, zn
knk
!
where knk =
p
x2
n + y2
n + z2
n .
Step C1.2 (Calculate the rotation and trans-
formation matrix)
When the point cloud is created, it is deﬁned in the
reference system of the scanner Orf. We deﬁne a new
reference system O
0
rf in which zO0
rf = bn where zO0
rf
is a unit vector along z axis of the new reference sys-
tem O
0
rf. The origin of O
0
rf is unchanged. Further, we
ﬁnd the rotation matrix which aligns two unit vectors
zOrf = [0, 0, 1] and zO0
rf = bn. This task can be solved
as follows.
It should be noted that the 3D rotation which aligns
these two vectors is actually a 2D rotation in a plane
with normal zOrf ⇥bn by an angle Θ between these two
vectors:
R =
2
64
cos θ −sin θ 0
sin θ
cos θ 0
0
0
1
3
75
Since cos θ = zOrf · bn and sin θ = kzOrf ⇥bnk, we
further obtain:
R =
2
64
zOrf · bn
−kzOrf ⇥bnk 0
kzOrf ⇥bnk
zOrf · bn
0
0
0
1
3
75 =
2
64
x1 y1 0
x2 y2 0
0 0 1
3
75
With R we deﬁned a pure z-rotation which should
be performed in the reference frame whose axes are
(zOrf ,
bn−(zOrf ·bn)zOrf
kbn−(zOrf ·bn)zOrf k, zOrf ⇥
bn−(zOrf ·bn)zOrf
kbn−(zOrf ·bn)zOrf k). It can
be easily veriﬁed that this is an orthonormal basis. If
we denote zOrf with A and bn with B, the axes are illus-
trated in Fig. 10 where BPA is the projection of vector
B onto the vector A.
Fig. 10
Constructing the new orthonormal base. Thick blue
vectors denote x and y vectors of new reference frame (not
yet normalized).
Matrix for changing basis is then:
C = (zOrf ,
bn −(zOrf · bn)zOrf
kbn −(zOrf · bn)zOrf k, zOrf ⇥bn)−1.
Further, we multiply all the cloud points with C−1RC.
With C we change the basis, with R we perform the ro-
tation in the new basis and C−1 brings the coordinates
back to the original basis. After this operation we have
our cloud approximately aligned with xy plane of the
original frame and approximately perpendicular to the
z axis of the same frame.
Step C1.3 (Orthographic projection and trans-
lation in image plane)
Once the cloud is rotated, orthographic projection
on the xy plane means just keeping x and y coordinates
of each point.
u = x; v = y
Some of these values can be negative. In that case,
we are translating all the 2D values in order to obtain
positive pixel values and ﬁnally create an image. Let
pneg = (upneg, vpneg) be the most negative 2D point in
the set of projected points. We are translating all the
points as follows:
ui = ui + kupnegk
vi = vi + kvpnegk
The projection process is illustrated in Fig. 11. Ex-
amples of two point clouds and their projections are
shown in Fig. 12. The projection is better visible in the
Fig. 13a.
Fig. 11
Orthographic projection from 3D point cloud to 2D
plane
As the last step in C1 phase, in the image space, we
perform resampling of projected pixels (Fig. 13). Af-
ter projection, pixels are scattered (Fig. 13a). Resam-
pling is done in order to have regular grid of projected


--- Page 10 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
9
(a)
(b)
(c)
(d)
Fig. 12 (a),(c) 3D mesh of original point cloud; (b),(d) 2D
image after projecting
points. Regular grid, shown in Fig. 13b, makes neigh-
bors search faster by directly addressing neighboring
pixels with their image coordinates instead of search-
ing among scattered points.
(a)
(b)
Fig. 13
(a) scattered pixels after projection (b) regular grid
after resampling
The same as the whole input point cloud, the de-
fected regions are separately projected onto another 2D
image. An example is shown in Fig. 14. Note that these
images have the same size as the projection of the orig-
inal point cloud.
3.4.2 Step C2 : Data preparation
The second step of the characterization process is the
preparation of data. There are three diﬀerent types of
data which are essential for this process: (1) the orig-
inal point cloud, (2) identiﬁed points belonging to the
defect-regions, and (3) the polygon surrounding each
defect. The point cloud and all the defect-regions are
available from Sec. 3.3.
In order to obtain the surrounding polygon of a defect,
we start from the binary image with all projected defect
points after the projection process (Fig. 14b). Note that
the input data can contain one or several defects. For
the defects located in close proximity, we group these
defects into one by using the mathematical morphology
operation called dilation [14]. This operator also allows
to enlarge the boundaries of defect-regions (Fig. 14c).
After dilating the defect-regions, we identify con-
nected components [15] on binary image (see Fig. 14d).
Each of the connected components corresponds to a
damage. Further, contours are extracted for each de-
fect (see Fig. 14e). The convex hull [16] of the defect is
then determined as in Fig. 14f and taken as the polygon
surrounding the points which belong to the defect.
3.4.3 Step C3 : Reconstruction
Our main idea in this section is to reconstruct the ideal
surface of the 3D data. This ideal surface is further
used as a reference to extract the information about
the status of defect by comparing the variance between
the z-coordinate value of each point in the ideal surface
and the corresponding point in the original data. The
concept is illustrated in Fig. 15.
In order to reconstruct the ideal surface of the 3D
data, we use a method called Weighted Least Squares
(WLS) [33]. We are ﬁtting a quadratic bivariate poly-
nomial f(u, v) : R2 −! R to a set of cloud points which
are out of the polygonal defect area. We justify this by
the shape of the airplane fuselage which is close to the
quadratic surface.
We start with a set of N points (ui, vi) 2 R2 with
their z-values zi 2 R. All these values are obtained in
the projection phase. We search for a globally-deﬁned
function f(u, v) = z, that best approximates the sam-
ples. The goal is to generate this function such that
the distance between the scalar data values zi and the
function evaluated at the points f(ui, vi) is as small as
possible. This is written as:
min =
N
X
i=1
θ(k (u, v) −(ui, vi) k) k f(ui, vi) −zi k
(5)


--- Page 11 ---

10
Igor Jovanˇcevi´c1 et al.
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 14 (a) Labeled defects after detection; (b) Binary image after projecting defects onto the plane; (c) Defect regions after
dilation; (d)Identifying each connected component as one defect; (e) Contours of the enlarged defects; and (f) Convex hull of
each defect
Fig. 15 An illustration of the approach for calculating defect depth
where (u, v) is a ﬁxed point, for ex. center of mass of
the defect region. We can ﬁnd many choices for the
weighting function θ(d) in the literature such as a Gaus-
sian [29] or the Wendland function [55]. It is a function
which is favorizing the points which are in the proxim-
ity of the defect, while assigning lower weights to the
points far away from the ﬁxed point (u, v).
3.4.4 Step C4 : Extracting information about the
defects
The lowest point
For each point in a defect region, we estimate the
values ∆z(pi) = zP (ideal) −z(pi). Here, pi is a point
belonging to a defect region. We do not consider pi as
a defect point if
..∆z(pi)
.. is lower than a predeﬁned
threshold. The lowest point of the defect is determined
by max{
..∆z(pi)
..} among all the points from that de-
fect region. The sign of ∆z(pi) determines if defect is a
dent or a protrusion. A dent is detected when ∆z(pi)
is positive and a protrusion is detected when ∆z(pi) is
negative.
The dimension and orientation of defect
In order to show the size and the orientation of the
defect, we construct an oriented bounding-box (OBB)
[17]. We rely on Principal Component Analysis (PCA)
[21]. Let X be a ﬁnite set of N points in R2. Our prob-
lem consists of ﬁnding a rectangle of minimal area en-
closing X.
(a)
(b)
Fig. 16
Illustration of the PCA bounding-box of a set of
points X 2 R2


--- Page 12 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
11
The main idea of PCA is to reduce the dimensional-
ity of a data set based on the most signiﬁcant directions
or principal components. For performing a PCA on X,
we compute the the eigenvectors of its covariance ma-
trix and choose them as axes of the orthonormal frame
eξ (see Fig. 16b). The ﬁrst axis of eξ is the direction of
largest variance and the second axis is the direction of
smallest variance [9]. In our case, given a ﬁnite set of
points in the defect-regions, we ﬁrst calculate the center
of mass of the defect and then apply the PCA algorithm
for determining eξ. We continue by searching the end
points along two axes of eξ. These points allow us to
draw an oriented bounding-box of the defect as we can
see for ex. in Fig. 17c .
4 Experiments and discussion
The proposed method has been tested on 15 point clouds,
both with and without defective regions. The items
which have been used to test and illustrate our ap-
proach are: radome, static port with its surrounding
area and some parts of the fuselage. This set is con-
sidered representative since the radome (airplane nose)
has a signiﬁcant curvature (Fig. 22a) while static port
(Fig. 22c) and fuselage (Fig. 20a) are the surfaces rel-
atively ﬂat. We obtained promising results which will
further be illustrated. We acquired point clouds using
the Artec Eva 3D scanner at Air France Industries tar-
mac and Airbus hangar in diﬀerent lighting conditions.
We acquired scans of aircraft surface with multiple de-
fects. The same parameters of the detection algorithm
are used for most of the input clouds. The scanner was
placed 60 −100 cm from the surface. Speciﬁcally, we
choose angle threshold αth = 0.25 and the curvature
threshold cth = 0.3. The original point clouds, detected
defects and the corresponding characterization results
for each defect are shown in Fig. 17, Fig. 18, Fig. 19,
Fig. 20, Fig. 21, and Fig. 22.
The parameters we use in our algorithm play an im-
portant role in detecting the defects. The most impor-
tant one is the angle threshold αth. In our experiments,
we have used αth in the range {0.2 ⇠1} degrees. In
most cases, we have set αth = 0.25. When we reduced
the value of angle threshold αth, the sensitivity of the
algorithm increased. Fig. 23 shows the inﬂuence of the
value αth on the area of detected defect.
For curvature threshold cth, we test the algorithm
on our dataset and we set it to cth = 0.3. This study
also indicates that the performance of the program is
inﬂuenced by various factors, as scanning mode, scan-
ning distance, density of point cloud and dimensions of
the defects (depth, area).
(a)
(b)
 
 
(c)
(d)
Fig. 17 Scratch on fuselage. (a) Original point cloud; (b)
Defects detected; (c) Information about defect 1; (d) Infor-
mation about defect 2


--- Page 13 ---

12
Igor Jovanˇcevi´c1 et al.
 
Defect 1 
Defect 4 
Defect 2 
Defect 3 
(a)
(b)
 
 
Defect 1 
Max depth: 1.803 mm 
Size: 21.244 x 44.245 mm 
Orientation: 180 deg 
 
 
 
 
Defect 2 
Max depth: 2.397 mm 
Size: 27.255 x 42.269 mm 
Orientation: 169.695 deg 
 
 
(c)
(d)
 
 
Defect 3 
Max depth: 0.852 mm 
Size: 16.681 x 21.384 mm 
Orientation: 239.036 deg 
 
 
 
 
Defect 4 
Max depth: 0.835 mm 
Size: 11.242 x 14.781 mm 
Orientation: 194.036 deg 
 
 
(e)
(f)
Fig. 18 Four impacts on fuselage. (a) Original point cloud;
(b) Defects detected; (c) Information about defect 1; (d) In-
formation about defect 2; (e) Information about defect 3; (f)
Information about defect 4
 
 
Defects 
(a)
(b)
 
 
Defects 
Max depth: 2.864 mm 
Size: 55.161 x 69.284 mm 
Orientation: 176.186 deg 
 
 
(c)
Fig. 19 One large impact on fuselage. (a) Original point
cloud; (b) Defects detected (c) Information about the largest
defect


--- Page 14 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
13
(a)
(b)
 
 
(c)
(d)
 
 
(e)
(f)
Fig. 20 Four defects on fuselage. (a) Original point cloud;
(b) Defects detected; (c) Information about defect 1; (d) In-
formation about defect 2; (e) Information about defect 3; (f)
Information about defect 4
 
Defect 1 
Defect 2 
Defect 3 
(a)
(b)
 
 
Defect 1 
Max depth: 1.437 mm 
Size: 20.140 x 29.217 mm 
Orientation: 180 deg 
 
 
 
 
Defect 2 
Max depth: 1.523 mm 
Size: 17.78 x 25.394 mm 
Orientation: 172.875 deg 
 
 
(c)
(d)
 
 Defect 3 
Max depth: 2.079 mm 
Size: 30.276 x 31.039 mm 
Orientation: 156.038 deg 
 
 
(e)
Fig. 21 (a) Original point cloud; (b) Defects detected; (c)
Information about defect 1; (d) Information about defect 2;
(e) Information about defect 3
(a)
(b)
(c)
(d)
Fig.
22 Examples of point clouds without defects: (a)
Radome; (c) Static port ; (b) and (d) Detection result


--- Page 15 ---

14
Igor Jovanˇcevi´c1 et al.
(αth = 0.2)
(αth = 0.25)
(αth = 0.3)
(αth = 0.35)
(αth = 0.4)
(αth = 0.45)
(αth = 0.5)
(αth = 1.0)
Fig. 23 The inﬂuence of the value αth on the detection re-
sults


--- Page 16 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
15
4.1 Evaluation using dial gauge ground truth
In practice, the fuselage inspection is done manually by
a quality manager who ﬁrst examines the surface using
a low angle light in order to detect defects. Next, the
zone around the detected defect is demarcated with a
marker pen. The zone is further examined using a dial
gauge, also named dial indicator. This instrument is
shown in Fig. 24a and its functioning principle is illus-
trated in Fig. 24b. The probe is traversing the defective
area until surface contact occurs.
Obvious drawback of this method is that it depends
on the expertise and mood of the person operating the
equipment. Another ﬂaw appears in the case of larger
defects, such as those in Fig. 18c and Fig. 18d. Having
a measuring stand with a ﬁxed standardized diameter,
the gauge can dive into the defect and report a lower
depth than the real one (Fig. 25). An advantage of our
method is that it can characterize defects of any size.
(a)
(b)
Fig. 24 (a) AIRBUS standardized dial gauge; (b) Illustra-
tion of dial gauge functioning
Fig. 25 Imprecision in measuring depth in the case of large
defects. Red: depth measured by dial gauge; Blue: real depth.
(a)
(b)
 
 
Defect 5 
Max depth: 0.271 mm 
Size: 3.997 x 1.628 mm 
Orientation: 180 deg 
 
 
 
 
Defect 6 
Max depth: 0.474 mm 
Size: 3.453 x 5.974 mm 
Orientation: 180 deg 
 
 
(c)
(d)
Fig. 26 (a) Part of the fuselage; (b) The detected defects
are shown in red color; (c) Information about defect 5 (Dial
gauge max depth: 0.31mm); (d) Information about defect 6
(Dial gauge max depth: 0.48mm)
In the case of small defects, we compared our method
with the result obtained by AIRBUS experts using their
standardized dial gauge (diameter of the measuring stand
34mm) shown in Fig. 24a. Fig. 26a shows the same
part of the fuselage as the one in Fig. 8, with indicated
two additional defects (5 and 6), hardly visible to an
eye. For detecting these shallow defects, αth had to be
decreased. For this reason, sensitivity of our detection
phase increased. Consequently, we produced some false
detections as well (Fig. 26b).
Fig. 26c and 26d show that the estimated maxi-
mal depths obtained by our approach are 0.27mm and
0.47mm while standardized AIRBUS dial gauge results
are 0.31mm and 0.48mm respectively. The average dis-
crepancy is around 8%.
For the reason of small diameter measuring stand,
we could not obtain accurate results with the same
dial gauge for neither of the defects larger than 34mm.
Therefore, we carried on the measuring in laboratory
conditions. Our setup is shown in Fig. 27. Part of the
fuselage was ﬁxed on XY mobile table used for precise
cutting of composite materials. The part was placed
as parallel as possible with the table in order to min-
imize inclination. Dial gauge (with 0.01mm gradua-
tions) without limiting measuring stand was ﬁxed by
using magnetic base. Rectangular grid was drawn around
each defect and the part was slowly moved along X and


--- Page 17 ---

16
Igor Jovanˇcevi´c1 et al.
Y axis of the table. In all the intersections points of the
grid, the depth is measured by the dial gauge.
(a)
(b)
Fig. 27 Measuring the depth of defects with Dial gauge; (a)
Measuring setup; (b) Dial gauge
This way we obtained 10cm long proﬁle lines. Val-
ues read along middle lines are shown in Fig. 28 to-
gether with our results. In order to take into account
possible inclination of the fuselage part, the depth is
obtained by measuring the diﬀerence between the low-
est point (black squares in Fig. 28) and the line ob-
tained as average of end values on the proﬁle (red lines
in Fig. 28). The discrepancies between the Dial gauge
measurements and our measured values (Fig. 18 c and
d) are e = |1.8 −1.7| = 0.1mm (6%) and e = |2.44 −2.4| =
0.04mm (2%). The values obtained by the three mea-
surement methods are given in Table 1. This table con-
ﬁrms our doubt that, in case of large defects (defects
1 and 2), AIRBUS gauge depth values are underesti-
mated due to the measuring stand issue. The other tests
that have been carried out so far on large defects have
shown that the discrepancy is on average 5% and al-
ways below 10%. As per defects 3 and 4 from the same
cloud (Fig. 18 e and f), it was impossible to measure
them with dial gauge because those are two holes. How-
ever, having similar values for these two defects ( 0.85
and 0.84) is coherent since they are two identical screw
holes produced in the manufacturing phase.
(a)
(b)
Fig. 28 (a) Proﬁle for defect 1 (Fig. 18c); (b) Proﬁle for
defect 2 (Fig. 18d)
It should be noted that dial gauge method does not
take into account the curvature of the fuselage which
can aﬀect the characterization process of defects above
certain size. Contrary, with the ideal surface reconstruc-
tion explained in Sec. 3.4.3, our approach considers this
aspect of the problem.
Table 1 Maximal depth of large defects shown in Fig. 18
Our method
Dial gauge
AIRBUS dial gauge
Defect 1
1.80
1.70
1.42
Defect 2
2.40
2.44
1.73
4.2 Execution time
Execution time of the whole process is not easily quan-
tiﬁable because it depends on density and size of the
cloud (number of points) as well as on the number of
defects. It should be noted that characterization process
is performed for each detected defect sequentially. Also,
in our process we are converting the input cloud from
the scanner format to the format suitable for process-
ing, which also takes time. However the total processing
time which varies between 20s and 120s on our dataset,
is acceptable for our application since the 3D inspec-
tion is planned to be done during more detailed and
longer check, usually in the hangar. These values were
obtained by testing non-optimized code on the PC with:
2.4 GHz Core(TM) i7 CPU, 8GB RAM with Microsoft
Visual Studio 2013. The method was developed in C++
with the support of Point Cloud Library v.1.7.0 cite
[42] and OpenCV v.3.0. library [5]. Approximately for
a cloud with 30000 points, detection phase takes around
8 −9s while characterization step takes 2 −3s for each
defect. Our time rises up to 120s because some of our
clouds contain redundant information, caused by the
longer exposure time. It is experimentally established
that this scanning mode is not useful and ”one shot”
scanning mode is recommended. Typical cloud obtained
with ”one shot” scanning mode contains 30000 points.
Therefore typical processing time is 20s, if we assume
that typical number of detected defects is 3 −5.
5 Conclusions
In this paper, an original framework for the detection
and characterization of defects in point cloud data has
been presented. Proposed methodology is divided into
two main processes. The ﬁrst process is the defects de-
tection. In this process, the Point Cloud is segmented
to identify the defect regions and non-defect regions.
A computer vision algorithm which is able to detect
various undesired deformations on airplane surface was


--- Page 18 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
17
developed using Region-Growing method with the local
information about surface including points normal and
curvature. In the next process, we developed a tech-
nique for characterizing the defects. This technique al-
lows us to provide information about each defect such as
the size, the depth and the orientation. Experiments are
conducted on real data captured by 3D scanner on the
fuselage of Airbus A320 airplane. This is a set of clouds
encompassing various characteristics. The experimen-
tal results demonstrate that our approach is scalable,
eﬀective and robust to clouds with noise and can de-
tect diﬀerent types of deformation such as protrusions,
dents or scratches. In addition, the proposed processes
work completely automatically. Finally, a limitation of
our approach is processing-time. In the future, we plan
to reduce program execution time by optimizing our
code. Thus, we believe that our results are promising
for application in an inspection system. Not only lim-
ited to the context of airplane surface inspection, our
approach can be applied in wide range of industrial ap-
plications. Our approach is also limited to plane-like
surfaces. Strongly curved surfaces, such as wings and
engine cowling, cause our characterization approach to
fail. We propose cloud ﬁtting to the available Computer
Aided Design model of the airplane, in order to calcu-
late ideal surface more precisely.
Acknowledgements This work is part of the AIR-COBOT
project (http://aircobot.akka.eu) approved by the Aerospace
Valley world competitiveness cluster. The authors would like
to thank the French Government for the ﬁnancial support
via the Single Inter-Ministry Fund (FUI). The partners of
the AIR-COBOT project (AKKA TECHNOLOGIES, Airbus
Group, ARMINES, 2MoRO Solutions, M3 SYSTEMS and
STERELA) are also acknowledged for their support. Nicolas
Simonot and Patrick Metayer from AIRBUS/NDT are also
acknowledged for their help in providing dial gauge measure-
ments.
References
1. Benhabiles, H., Lavou´e, G., Vandeborre, J., Daoudi, M.:
Learning boundary edges for 3d mesh segmentation. In:
Computer Graphics Forum, vol. 30, pp. 2170–2182. Wiley
Online Library (2011)
2. Besl, P.J., Jain, R.C.: Segmentation through variable-
order surface ﬁtting. Pattern Analysis and Machine In-
telligence, IEEE Transactions on 10(2), 167–192 (1988)
3. Bhanu, B., Lee, S., Ho, C., Henderson, T.: Range data
processing: Representation of surfaces by edges. Univer-
sity of Utah, Department of Computer Science (1985)
4. Borsu, V., Yogeswaran, A., Payeur, P.: Automated sur-
face deformations detection and marking on automotive
body panels.
In: Automation Science and Engineering
(CASE), 2010 IEEE Conference on, pp. 551–556. IEEE
(2010)
5. Bradski, G.: Dr. Dobb’s Journal of Software Tools
6. Carlbom, I., Paciorek, J.: Planar geometric projections
and viewing transformations. ACM Computing Surveys
(CSUR) 10(4), 465–502 (1978)
7. Deng, H., Zhang, W., Mortensen, E., Dietterich, T.,
Shapiro, L.: Principal curvature-based region detector for
object recognition.
In: Computer Vision and Pattern
Recognition, 2007. CVPR’07. IEEE Conference on, p. 18.
IEEE (2007)
8. Dey, T.K., Li, G., Sun, J.: Normal estimation for
point clouds: A comparison study for a voronoi based
method.
In: Pointbased graphics, 2005. Eurograph-
ics/IEEE VGTC symposium proceedings, pp. 39–46.
IEEE (2005)
9. Dimitrov, D., Knauer, C., Kriegel, K., Rote, G.: On the
bounding boxes obtained by principal component anal-
ysis.
In: 22nd European Workshop on Computational
Geometry, pp. 193–196. Citeseer (2006)
10. Dyn, N., Hormann, K., Kim, S., Levin, D.: Optimizing 3d
triangulations using discrete curvature analysis. Mathe-
matical methods for curves and surfaces 28(5), 135–146
(2001)
11. Filin, S.: Surface clustering from airborne laser scanning
data. International Archives of Photogrammetry Remote
Sensing and Spatial Information Sciences 34(3/A), 119–
124 (2002)
12. Fua, P., Sander, P.: Segmenting unstructured 3d points
into surfaces. In: Computer Vision - ECCV’92, pp. 676–
680. Springer (1992)
13. Golovinskiy, A., Funkhouser, T.: Randomized cuts for 3d
mesh analysis.
ACM transactions on graphics (TOG)
27(5), 145 (2008)
14. Gonzalez, R.C., Woods, R.E.: Digital image processing.
Prentice Hall, Third Edition, pp. 669-671 (2002)
15. Gonzalez, R.C., Woods, R.E.: Digital image processing.
Prentice Hall, Third Edition, pp. 667-669 (2002)
16. Gonzalez, R.C., Woods, R.E.: Digital image processing.
Prentice Hall, Third Edition, pp. 655-657 (2002)
17. Gottschalk, S., Lin, M.C., Manocha, D.: Obbtree: A hi-
erarchical structure for rapid interference detection. In:
Proceedings of the 23rd annual conference on Computer
graphics and interactive techniques, pp. 171–180. ACM
(1996)
18. Haddad, N.A.: From ground surveying to 3d laser scan-
ner: A review of techniques used for spatial documen-
tation of historic sites. Journal of King Saud Universi-
tyEngineering Sciences 23(2), 109–118 (2011)
19. Hoover, A., JeanBaptiste, G., Jiang, X., Flynn, P.J.,
Bunke, H., Goldgof, D.B., Bowyer, K., Eggert, D.W.,
Fitzgibbon, A., Fisher, R.B.: An experimental compar-
ison of range image segmentation algorithms.
Pattern
Analysis and Machine Intelligence, IEEE Transactions on
18(7), 673–689 (1996)
20. Jin, H., Yezzi, A.J., Soatto, S.: Region-based segmenta-
tion on evolving surfaces with application to 3d recon-
struction of shape and piecewise constant radiance. In:
Computer Vision - ECCV 2004, pp. 114–125. Springer
(2004)
21. Jolliﬀe, I.: Principal component analysis. Wiley Online
Library (2002)
22. Katz, S., Tal, A.: Hierarchical mesh decomposition using
fuzzy clustering and cuts, vol. 22. ACM (2003)
23. Khan, W.: Image segmentation techniques: A survey.
Journal of Image and Graphics 1(4), 166–170 (2013)
24. Klasing, K., Althoﬀ, D., Wollherr, D., Buss, M.: Com-
parison of surface normal estimation methods for range
sensing applications. In: Robotics and Automation, 2009.
ICRA’09. IEEE International Conference on, pp. 3206–
3211. IEEE (2009)


--- Page 19 ---

18
Igor Jovanˇcevi´c1 et al.
25. Koenderink, J.J., van Doorn, A.J.: Surface shape and cur-
vature scales. Image and vision computing 10(8), 557–
564 (1992)
26. K¨oster, K., Spann, M.: Mir: an approach to robust
clustering-application to range image segmentation. Pat-
tern Analysis and Machine Intelligence, IEEE Transac-
tions on 22(5), 430–444 (2000)
27. Lancaster, P., Salkauskas, K.: Surfaces generated by mov-
ing least squares methods. Mathematics of computation
37(155), 141–158 (1981)
28. Levin, D.: The approximation power of moving leasts-
quares. Mathematics of Computation of the American
Mathematical Society 67(224), 1517–1531 (1998)
29. Levin, D.: Mesh-independent surface interpolation.
In:
Geometric modeling for scientiﬁc visualization, pp. 37–
49. Springer (2004)
30. Marani, R., Roselli, G., Nitti, M., Cicirelli, G., D’Orazio,
T., Stella, E.: A 3d vision system for high resolution sur-
face reconstruction. In: Sensing Technology (ICST), 2013
Seventh International Conference on, pp. 157–162. IEEE
(2013)
31. Mitra, N.J., Nguyen, A.: Estimating surface normals in
noisy point cloud data. In: Proceedings of the nineteenth
annual symposium on Computational geometry, pp. 322–
328. ACM (2003)
32. Mumtaz, R., Mumtaz, M., Mansoor, A.B., Masood, H.:
Computer aided visual inspection of aircraft surfaces. In-
ternational Journal of Image Processing (IJIP) 6(1), 38
(2012)
33. Nealen, A.: An as-short-as-possible introduction to the
least squares, weighted least squares and moving least
squares methods for scattered data approximation and
interpolation. URL: http://www. nealen. com/projects
130, 150 (2004)
34. Nguyen, A., Le, B.: 3d point cloud segmentation: A sur-
vey. In: Robotics, Automation and Mechatronics (RAM),
2013 6th IEEE Conference on, pp. 225–230. IEEE (2013)
35. Nurunnabi, A., Belton, D., West, G.: Robust segmenta-
tion in laser scanning 3d point cloud data. In: Digital Im-
age Computing Techniques and Applications (DICTA),
2012 International Conference on, p. 18. IEEE (2012)
36. Nurunnabi, A., West, G., Belton, D.: Robust methods for
feature extraction from mobile laser scanning 3d point
clouds
37. Pauling, F., Bosse, M., Zlot, R.: Automatic segmentation
of 3d laser point clouds by ellipsoidal region growing.
In: Australasian Conference on Robotics and Automation
(ACRA) (2009)
38. Pauly, M., Gross, M., Kobbelt, L.P.: Eﬃcient simpliﬁca-
tion of point-sampled surfaces.
In: Proceedings of the
conference on Visualization’02, pp. 163–170. IEEE Com-
puter Society (2002)
39. Peng, J., Li, Q., Kuo, C.J., Zhou, M.: Estimating gaus-
sian curvatures from 3d meshes. In: Electronic Imaging
2003, pp. 270–280. International Society for Optics and
Photonics (2003)
40. Rabbani, T., Van Den Heuvel, F., Vosselmann, G.: Seg-
mentation of point clouds using smoothness constraint.
International Archives of Photogrammetry, Remote Sens-
ing and Spatial Information Sciences 36(5), 248–253
(2006)
41. Rusu, R.B.: Semantic 3d object maps for everyday ma-
nipulation in human living environments. Ph.D. thesis,
Technische Universit¨at M¨unchen (2009)
42. Rusu, R.B., Cousins, S.: 3D is here: Point Cloud Library
(PCL). In: IEEE International Conference on Robotics
and Automation (ICRA). Shanghai, China (2011)
43. Rusu, R.B., Marton, Z.C., Blodow, N., Dolha, M., Beetz,
M.: Towards 3d point cloud based object maps for house-
hold environments. Robotics and Autonomous Systems
56(11), 927–941 (2008)
44. Sappa, A.D., Devy, M.: Fast range image segmentation by
an edge detection strategy. In: 3D Digital Imaging and
Modeling, 2001. Proceedings. Third International Con-
ference on, pp. 292–299. IEEE (2001)
45. Schnabel, R., Wahl, R., Klein, R.: Eﬃcient ransac for
point cloud shape detection. In: Computer graphics fo-
rum, vol. 26, pp. 214–226. Wiley Online Library (2007)
46. Seher, C., Siegel, M., Kaufman, W.M.: Automation tools
for nondestructive inspection of aircraft: Promise of tech-
nology transfer from the civilian to the military sector.
In: Fourth Annual IEEE DualUse Technologies and Ap-
plications Conference (1994)
47. Shakarji, C.M., et al.: Leasts-quares ﬁtting algorithms of
the nist algorithm testing system. Journal of ResearchNa-
tional Institute of Standards and Technology 103, 633–
641 (1998)
48. Siegel, M., Gunatilake, P.: Remote inspection technolo-
gies for aircraft skin inspection. In: Proceedings of the
1997 IEEE Workshop on Emergent Technologies and Vir-
tual Systems for Instrumentation and Measurement, Ni-
agara Falls, CANADA, pp. 79–78 (1997)
49. Siegel, M., Gunatilake, P., Podnar, G.: Robotic assistants
for aircraft inspectors. Industrial Robot: An International
Journal 25(6), 389–400 (1998)
50. Simari, P., Nowrouzezahrai, D., Kalogerakis, E., Singh,
K.: Multi-objective shape segmentation and labeling. In:
Computer Graphics Forum, vol. 28, pp. 1415–1425. Wiley
Online Library (2009)
51. Strom, J., Richardson, A., Olson, E.: Graph-based seg-
mentation for colored 3d laser point clouds. In: Intelligent
Robots and Systems (IROS), 2010 IEEE/RSJ Interna-
tional Conference on, pp. 2131–2136. IEEE (2010)
52. Tang, P., Akinci, B., Huber, D.: Characterization of three
algorithms for detecting surface ﬂatness defects from
dense point clouds. In: IS&T/SPIE Electronic Imaging,
pp. 72,390N–72,390N. International Society for Optics
and Photonics (2009)
53. T´ov´ari, D., Pfeifer, N.: Segmentation based robust inter-
polation - a new approach to laser data ﬁltering. Interna-
tional Archives of Photogrammetry, Remote Sensing and
Spatial Information Sciences 36(3/19), 79–84 (2005)
54. Wang, C., Wang, X., Zhou, X., Li, Z.: The aircraft
skin crack inspection based on diﬀerent-source sen-
sors and support vector machines.
Journal of Nonde-
structive Evaluation 35(3), 46 (2016).
DOI 10.1007/
s10921-016-0359-3. URL http://dx.doi.org/10.1007/
s10921-016-0359-3
55. Wendland, H.: Piecewise polynomial, positive deﬁnite
and compactly supported radial functions of minimal de-
gree. Advances in computational Mathematics 4(1), 389–
396 (1995)
56. Wirjadi, O.: Survey of 3D image segmentation methods,
vol. 35. ITWM (2007)
57. Wong, B.S., Wang, X., Koh, C.M., Tui, C.G., Tan, C.,
Xu, J.: Crack detection using image processing tech-
niques for radiographic inspection of aircraft wing spar.
InsightNonDestructive Testing and Condition Monitor-
ing 53(10), 552–556 (2011)
58. Yang, J., Gan, Z., Li, K., Hou, C.: Graph-based segmen-
tation for rgb-d data using 3d geometry enhanced super-
pixels. Cybernetics, IEEE Transactions on 45(5), 927–
940 (2015)


--- Page 20 ---

3D point cloud analysis for detection and characterization of defects on airplane exterior surface
19
59. Zhang, X., Li, H., Cheng, Z., Zhang, Y.: Robust curva-
ture estimation and geometry analysis of 3d point cloud
surfaces. J. Inf. Comput. Sci 6(5), 1983–1990 (2009)
```

---

## Section 1.2: Accelerating surface remeshing through GPU-based

Source File: Accelerating surface remeshing through GPU-based.txt

### Paper Content:

```
# Accelerating surface remeshing through GPU-based.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Accelerating surface remeshing through GPU-based.pdf
# Output: ../layer2_completion/txt/Accelerating surface remeshing through GPU-based.txt


--- Page 1 ---

Computer Aided Geometric Design 104 (2023) 102216
Contents lists available at ScienceDirect
Computer Aided Geometric Design
journal homepage: www.elsevier.com/locate/cagd
Accelerating surface remeshing through GPU-based 
computation of the restricted tangent face ✩
Yuyou Yao, Jingjing Liu, Wenming Wu, Gaofeng Zhang, Benzhu Xu, 
Liping Zheng ∗
School of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230601, China
a r t i c l e 
i n f o
a b s t r a c t
Article history:
Received 29 July 2022
Received in revised form 5 May 2023
Accepted 12 May 2023
Available online 18 May 2023
Keywords:
Restricted tangent face
Centroidal Voronoi tessellation
Parallel computation
GPU acceleration
Surface remeshing
High-quality mesh surfaces are crucial for geometric processing in a variety of applications. 
To generate these meshes, polyhedral remeshing techniques truncate Voronoi cells of the 
original surface and yield precise intersections, but the calculation is complicated. Some 
methods apply auxiliary points to construct Voronoi diagrams to simplify these techniques, 
thereby extracting co-planar facets to approximate the original surface. However, extracting 
these approximate facets from the constructed Voronoi diagram makes it ineﬃcient and 
non-parallelizable. To this end, we propose an eﬃcient GPU method for manifold surface 
remeshing, where the restricted tangent face (RTF) is utilized to approximate the original 
surface. By intersecting the pre-clipped Voronoi cell with the tangent plane, this method 
directly calculates the RTF of each point without any auxiliary points or traversing Voronoi 
cells. Moreover, to restrict the movement of points, we introduce a projection method 
based on the KNN strategy, where each point is projected onto the triangular facet 
in the original surface. Owing to the independence and non-interference of the RTF 
computation and projection of each point, our method is implemented in parallel on the 
GPU. Experimental results on various mesh surfaces demonstrate the superior performance 
of our method in the viability, effectiveness, and eﬃciency.
© 2023 Elsevier B.V. All rights reserved.
1. Introduction
Triangle surface mesh is frequently used for three-dimensional (3D) data representation in geometrical modeling (Decker 
et al., 2021), architectural modeling (Yang et al., 2018), and scientiﬁc visualization (Sullivan and Kaszynski, 2019) due to 
its effectiveness and simplicity. Previous work produces raw meshes by 3D scanning reconstruction (Chen et al., 2021) 
and computer vision method (Shazeer et al., 2018), etc. However, low-quality raw meshes are unsuitable for subsequent 
geometric processing in a variety of applications owing to small angles, short edges, irregular vertices (Liang and Zhang, 
2011; Engwirda and Ivers, 2014), etc. As an eﬃcient technique for improving the quality of meshes, surface remeshing has 
received much attention from researchers in the past two decades.
Previous work (Du et al., 2003; Alliez et al., 2005; Ye et al., 2019) for generating meshes has achieved considerable 
advancements due to the emergence of polyhedral meshing algorithms (Yan et al., 2013; Yan and Wonka, 2015) based on 
the centroidal Voronoi tessellation (CVT) (Du et al., 1999). These methods either construct Voronoi diagrams on a surface 
✩Editor: Jin Huang.
* Corresponding author.
E-mail address: zhenglp@hfut.edu.cn (L. Zheng).
https://doi.org/10.1016/j.cagd.2023.102216
0167-8396/© 2023 Elsevier B.V. All rights reserved.


--- Page 2 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 1. Exempliﬁcation of the geodesic distance, RVD (3D), RPF (2D), and RTF (2D). (a) the geodesic distance between two points on a surface (highlighted 
with purple line); (b) RVD, intersecting Voronoi cells with the original surface; (c) RPF, extracting the co-facets of the power diagram constructed with 
shadow points; and (d) RTF, restricted tangent planes without auxiliary points or Voronoi diagram construction. Notably, (b) is generated by the code 
provided in Lévy and Liu (2010). (For interpretation of the colors in the ﬁgure(s), the reader is referred to the web version of this article.)
(e.g., the geodesic CVT (GCVT)) (Fu and Zhou, 2009) or truncate Voronoi cells of the original surface (e.g., the restricted CVT 
(RCVT), the restricted Voronoi diagram (RVD), etc.) (Yan et al., 2009, 2010), along with CVT optimization techniques (e.g., 
Lloyd’s method or the quasi-Newton method) (Lloyd, 1982; Liu et al., 2009) to produce remeshing surfaces. They can be 
regarded as exact-CVT-oriented methods as they calculate the geodesic distance or the intersection on the original surface, 
as shown in Fig. 1(a)(b). However, it is extremely diﬃcult and time-consuming (several seconds per iteration, see Fig. 12) to 
calculate the precise geodesic distance or the intersection of Voronoi cells with the surface.
Several researchers (Zimmer et al., 2012; Chen et al., 2018) have recently concentrated on yielding high-quality meshes 
by employing approximate planes, e.g., the restricted power face (RPF) (Xu et al., 2019) in Fig. 1(c). They construct 3D 
Voronoi diagrams using extra sampling points (e.g., shadow points), ﬁltering the co-planar facets of these boundary Voronoi 
cells as approximate planes (Abdelkader et al., 2020). These techniques, which are considered approximate-CVT-oriented 
methods, compute the CVT on these approximation planes during each iteration, producing remeshing results. Compared to 
Yan et al. (2013), they drastically reduce the necessary calculations and achieve better computational performance owing to 
the avoidance of truncating Voronoi cells on the original surface. Nevertheless, since they construct some inner Voronoi cells 
based on these extra sampling points, approximate-CVT-oriented methods consume excessive time and memory. Moreover, 
ﬁltering these approximate planes by traversing the Voronoi diagrams could be more effective.
Therefore, we concentrate on the limitations of previous work on surface remeshing and take into account parallelization 
for speedups. In this paper, we propose an eﬃcient GPU method for manifold surface remeshing without truncating Voronoi 
cells of the original surface. To achieve it, we adapt the concept of approximate CVT and ﬁt the primordial surface using 
restricted tangent faces (RTF). Unlike existing methods (Xu et al., 2019), we are only interested in these planar facets of 
randomly initialized sampling points, and auxiliary points (Abdelkader et al., 2020) are unnecessary. Speciﬁcally, an eﬃcient 
method is presented to directly calculate the tangent face of each sampling point instead of traversing the Voronoi diagram 
(Xu et al., 2019; Abdelkader et al., 2020). This method is well-designed, and the tangent face of each sampling point is 
calculated independently, which is easily implemented in parallel on the GPU. Thus, the sampling points are relocated to 
the barycenters of the respective tangent faces. Nevertheless, this may result in optimized points deviating from the original 
surface. To restrict the movement of sampling points and couple with the tangent face calculations on the GPU, we introduce 
a projection strategy based on k-nearest-neighbors (KNN), where each sampling point is projected onto the triangular facet 
on the original surface. The tangent facet computation and projection strategy are alternately and iteratively performed to 
generate the RTF, producing high-quality meshes. We contribute the following:
• a GPU-based method to directly calculate the RTF of sampling points without constructing or traversing Voronoi dia-
grams with auxiliary points.
• a GPU-based projection strategy via KNN method to restrict sampling points moving on the original surface.
The remainder of this paper is organized as follows. Sect. 2 brieﬂy review some CVT-based methods for surface remesh-
ing. The preliminary of the Voronoi diagram, CVT and RPF is provided in Sect. 3. In Sect. 4, we introduce our method for 
generating high-quality meshes. Sect. 5 presents some remeshing results, and some conclusions are given in Sect. 6.
2. Related work
High-quality meshes are critical in various applications, i.e., geometrical modeling (Decker et al., 2021), scientiﬁc visual-
ization (Sullivan and Kaszynski, 2019), etc. Several techniques have been introduced to improve the quality of a given mesh, 
one of which is surface remeshing (Khan et al., 2022). The raw mesh is fed as input to a series of remeshing algorithms, 
generating another high-quality mesh. Existing work introduces a mount of remeshing algorithms, i.e., local modiﬁcation-
based remeshing (Wang et al., 2018), segmentation-based remeshing (Khan et al., 2018a), and Delaunay triangulation (DT) 
based remeshing (Chen and Holst, 2011). An exhaustive review of remeshing techniques could be referred to Khan et al. 
(2022). Here we only give a brief review of the CVT-based remeshing methods, including two-dimensional (2D) CVT-oriented 
methods in Sect. 2.1, exact-CVT-oriented methods in Sect. 2.2, and approximate-CVT-oriented methods in Sect. 2.3.
2


--- Page 3 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
2.1. 2D-CVT-oriented remeshing
The CVT (Du et al., 1999) deﬁnes a particular partition of a given domain into several subregions, where each site 
corresponds to the barycenter of its subregion. Due to its excellent geometric properties, the CVT has been applied for 
surface remeshing, and a series of algorithms have been proposed (Du et al., 2003; Leung et al., 2015; Herholz et al., 2017; 
Ye et al., 2019; Yan et al., 2014; Abdelkader et al., 2020).
Alliez et al. (2005) introduce a CVT-based method for isotropic remeshing for triangulated surface meshes. They use 
a global conformal planar parameterization and apply Lloyd’s method in the parametric space using a density function 
designed to compensate for the area distortion due to ﬂattening. However, the diﬃculty is that the density function is not 
speciﬁc, which may result in an undesirable remeshing result. Similarly, an adaptive meshing approach for heterogeneous 
materials is proposed in You et al. (2015), where the CVT and mesh extraction are on the basis of a density function 
associated with the material distributions. In contrast, this method is only suitable for 2D heterogeneous objects. Besides, 
two CVT variants for a general planar domain are presented in Khan et al. (2018b), which utilize an extended domain rather 
than the origin domain for computing the centroids. Still, this method is so limited that it only applies to the planar domain. 
Additionally, to speed up surface remeshing, Rong et al. (2010) introduce a GPU-assisted construction and optimization for 
CVT on 2D parameter spaces, which achieves better computation eﬃciency.
Although the 2D CVT could be easily constructed and optimized, the 2D CVT-based remeshing methods are either chal-
lenging to extend to 3D objects (Khan et al., 2018b) or rely on a speciﬁc density function (Alliez et al., 2005), resulting in 
the generated meshes with undesirable quality.
2.2. Exact-CVT-oriented remeshing
Different from the CVT computation in 2D parametric space, some methods try to directly compute the CVT on the 3D 
original space and then generate the remeshing results. These methods calculate the CVT on a surface (i.e., the GCVT) or 
truncate the 3D Voronoi diagram of the original surface (i.e., the RVD). Recently, many researchers have worked to design an 
eﬃcient method for calculating the geodesic distance in the GCVT or the intersection of 3D Voronoi cells with the surface 
in the RVD.
In view of the GCVT, the crux is calculating the geodesic distance of any two points on the surface. Rong et al. (2011)
extend the concept of CVT from 2D space to 3D spherical space and provide a framework to compute the CVT. Zhuang et al. 
(2014) introduce an interactive method for mesh generation by calculating anisotropic geodesics. To improve the compu-
tational eﬃciency, they present a fast local subdivision method to calculate anisotropic geodesics from existing Euclidean 
geodesics, producing a comparable mesh for surfaces with sharp features. Wang et al. (2015) present two intrinsic meth-
ods to compute the centroid for any geodesic Voronoi diagram (GVD). Though the computational eﬃciency of the GVD is 
improved, there is no signiﬁcant improvement in the generated surface quality. The 2D Possion disk sampling method (Yan 
and Wonka, 2013) is extended with consideration of geodesic distance by Fu and Zhou (2009). Liu et al. (2016) provide a 
manifold differential evolution (MDE) method for globally optimizing the energy of GCVT on a surface, yielding meshes of 
high quality. However, an essential limitation of the GCVT-based remeshing method is that the geodesic distance calculation 
is too complicated, resulting in ineﬃcient geodesic path computation.
Considering the RVD, the key is the intersection between a Voronoi cell and the primitive surface, as shown in Fig. 1(b). 
Du et al. (2003) introduce constrained CVT (CCVT) on a surface where each vertex is constrained on the surface and coin-
cides with the barycenter of the Voronoi cell on the surface. However, the local minimum problem of the energy function 
in this method is still a challenge. Yan et al. (2009) propose another CVT-based method for surface remeshing, called RVD, 
which computes the exact RVD based on the quasi-Newton method. To eliminate the triangles with small or obtuse angles, 
they further extend the exact RVD energy function with a penalty function (Yan and Wonka, 2015) to avoid the existence 
of short edges. However, the limitation of this work is the lacking of termination guarantees for more complex models or 
sharp features. Additionally, by discretizing the original surface into voxels, Leung et al. (2015) compute the exact Euclidean 
distance transform (EDT), 3D CVT, and RVD on the GPU, which is highly eﬃcient. Nevertheless, this method is limited by 
the graphical memory, especially for more complicated models. Recently, a ﬁeld-aligned method (Du et al., 2018) based on 
RVD has been introduced for surface remeshing. They minimize an energy function that combines both CVT and the penalty 
enforced by a six-way rotational symmetry ﬁeld, but the interpolation is ineﬃcient. Several extensions of the RVD have 
been presented for surface remeshing in some particular applications, such as the thin-plate models (Wang et al., 2020), 
signed distance ﬁeld (Hou et al., 2022) and triangulated surfaces (Sainlot et al., 2017), etc. However, the intersection be-
tween Voronoi cells and the primal surface is necessary for these RVD-based remeshing methods, which leads to complex 
computation and more time consumption.
To summarize, exact-CVT-oriented remeshing methods rely on calculating the geodesic distance between any two points 
on the surface or the intersection of a Voronoi cell with the surface. Nevertheless, both of these calculations are complex 
and time-consuming, leading to ineﬃcient remeshing of these methods.
3


--- Page 4 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
2.3. Approximate-CVT-oriented remeshing
Tangent plane intersection (TPI) (Zimmer et al., 2012) approximates the intersection between Voronoi cells and the 
original surface, signiﬁcantly reducing the computation complexity in RVD (Yan et al., 2009). Recently, computing the CVT 
on an approximation planar has received much attention for surface reconstruction or remeshing.
Chen et al. (2018) extend the CVT to point clouds and generate meshes from high-quality input points. The critical of 
this method is that the CVT computation is on the point cloud by restricting each Voronoi cell to the underlying surface. 
However, the underlying surface is approximated by a set of best-ﬁt planes closely related to the number of sampling points. 
Especially for curved regions, this method produces a poor approximation, resulting in low-quality remeshing results. Xu 
et al. (2019) introduce the RPF to approximate the intersection in RVD (Yan et al., 2009) to produce meshes. They deﬁne 
a set of shadow points outside the surface boundary, and each is assigned a speciﬁc weight. Based on the sampling points 
and shadow points, they constructed a power diagram from which the RPF of each sampling point could be extracted. 
However, this method is affected by the location of shadow points, which may cause low-quality or failure remeshing 
results. Besides, extracting the RPF from the power diagram requires traversing all vertices, which is too complicated and 
unparalleled. Recently, a robust remeshing approach called VoroCrust (Abdelkader et al., 2020) introduces a placement 
strategy of points for surface remeshing without Voronoi clipping. The co-planar facets generated by the points on different 
sides of the boundary are utilized as approximation planes. Nevertheless, extracting the co-planar from the constructed 
Voronoi diagram, similar to Xu et al. (2019), requires more time and cannot be parallelized. Moreover, this method fails to 
eliminate short edges and hole ﬁlling, affecting the quality of meshes.
In summary, existing methods employ auxiliary points and sampling points, to construct Voronoi/power diagrams, from 
which the co-planar facets are extracted as the approximate planes (Xu et al., 2019; Abdelkader et al., 2020). However, this 
method is complicated, time-consuming, and non-parallelizable. Others take high-quality point clouds without outliers or 
noises as inputs to generate meshes (Chen et al., 2018). However, this suffers from the number of sampling points and may 
generate low-quality meshes, especially for those with curved regions, as shown in Fig. 11. With this regard, we borrow the 
idea of approximate CVT (Xu et al., 2019) and introduce an eﬃcient GPU method for generating high-quality meshes with 
truncating Voronoi cells, where the RTF is utilized to ﬁt the original surface. To be speciﬁc, we directly calculate a CVT on 
the RTF and optimize the relevant sampling point to its barycenter. Moreover, we introduce a KNN based projection strategy 
to project each sampling point onto the triangular facet in the original surface instead of a best-ﬁt plane affected by the 
number of sampling points (Chen et al., 2018). Owing to the independence of the RTF computation and projection of each 
sampling point, our method is implemented in parallel on the GPU.
3. Preliminary
In this section, we provide an overview of the power diagram, CVT and RPF in sequence.
3.1. Voronoi diagram & power diagram
The Voronoi diagram, also called “Voronoi tessellation”, deﬁnes a 
spatial subdivision of a given N-dimensional domain  ⊂RN into sev-
eral subregions. Given a set S = {si}n
i=1 of n distinct points (also called 
“sites” or “generators”) in , the Voronoi diagram divides the domain 
 into n disjoint regions V (S) = {V (si)}n
i=1 based on the Euclidean 
distance, as shown in the inset (left). Each region V (si) of the point si, 
called Voronoi region, is deﬁned as:
V (si) = {s ∈|∥s −si∥≤∥s −s j∥,∀j ̸= i}
(1)
where d(s, si) = ∥s −si∥denote the Euclidean distance of two points s and si.
The Power diagram (Aurenhammer, 1987), as an extension of the Voronoi diagram, introduces weights W = {wi}n
i=1 to 
points. That is, each point si is associated with a parameter wi. Each region P(si), called power cell, is redeﬁned as:
P(si) = {s ∈|∥s −si∥2 −wi ≤∥s −s j∥2 −w j,∀j ̸= i}
(2)
where dp(s, si) = ∥s−si∥2−wi is redeﬁned as the power distance of points s and si. Notably, the power diagram degenerates 
to the Voronoi diagram when the weights of all points are equal (Aurenhammer et al., 1998).
3.2. CVT
By imposing centroidal constraint to ordinary Voronoi diagram and power diagram, two extensions of the Voronoi dia-
gram can be generated: CVT (Liu et al., 2009) and centroidal power diagram (CPD) (Zheng et al., 2021). Taking the CVT as 
an example, each point si is located at the barycenter s∗
i of its Voronoi region V (si), and the barycenter s∗
i can be calculated 
as:
4


--- Page 5 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
s∗
i =

V (si) ρ(s)sds

V (si) sds
(3)
Equivalently, CVT can be obtained by minimizing the following term:
Q (S) =
n

i=1

V (si)
ρ(s)∥s −si∥2ds
(4)
Lloyd’s (Liu et al., 2009) and quasi-Newton methods (De Goes et al., 2012) are the most popular implementations of 
CVT/CPD. The former is the simplest way to generate CVT, which moves each site to the mass center of its Voronoi cell 
in each iteration. The latter applies a quasi-Newton-like solver to compute the CVT/CPD. Compared to Lloyd’s method with 
linear convergence, the quasi-Newton method with super-linear convergence is more eﬃcient (Xin et al., 2016).
3.3. RPF
Given a point si on surface M, associated with a shadow point ss
i = si + d · nsi , where nsi is the normal of si and d is 
the offset. The weight of the point si is wi = 0, while ws
i = ∥si −ss
i ∥2 is set for the corresponding shadow site ss
i . The point 
si is therefore typically located on the equal power distance facet, and Xu et al. (2019) introduce the RPF Fi:
Fi = {τ|τ ⊂P(si),τ ⊂P(ss
i )}
(5)
Obviously, the RPF Fi is deﬁned as the co-planar facet of their power cells P(si) and P(ss
i ), as illustrated in Fig. 1(c). 
Based on the CVT framework implemented with CGAL (Fabri and Pion, 2009), they provide a method to compute the RPF (Xu 
et al., 2019). To be speciﬁc, they ﬁrst compute the regular triangulation of all sampling points and shadow points, thereby 
constructing the power diagram based on the duals of regular triangulation. To the best of our knowledge, the power 
diagram construction in this way is not parallelizable. Moreover, the co-planar facet, i.e., RPF, of the respective power cells 
of each sampling point and the relevant shadow point is highly dependent on the power diagram construction. Extraction 
of these co-planar facets is performed by traversing all vertices of each power cell only if the power diagram construction 
is done, which is ineﬃcient.
Contrary to previous work (Xu et al., 2019), these shadow points and their weights in RPF are not necessary in our 
work. We are only concerned with cutting the tangent planes to generate approximative facets. Therefore, in this paper, we 
simplify the notion of RPF as RTF, which could be directly computed by truncating Voronoi cells of the tangent planes, as 
shown in Fig. 1(d). In the absence of misunderstandings, we also utilize Fi as the RTF of sampling point si. In this paper, 
we provide an eﬃcient GPU-based method to directly compute the RTF without any auxiliary points (e.g., these shadow 
points in RPF) or power diagram construction, which is described later.
4. Our methodology
The main components of our methodology are 1) the GPU-based computation of the RTF (Sect. 4.2) and 2) the projection 
strategy to restrict the sampling points moving on the original surface (Sect. 4.3). Note that the KNN method serves as the 
foundation for both modules. Additionally, the CVT-based optimization for the RTF and ﬁnal mesh extraction (Sect. 4.4) are 
provided in this section.
4.1. Overall idea
Based on the ideas of TPI (Zimmer et al., 2012) and RPF (Xu et al., 2019), we propose an eﬃcient GPU method for surface 
remeshing, where the restricted tangent faces are introduced to approximate the original surface. Unlike Xu et al. (2019), 
our method computes RTF directly from the sampling points and their normal directions without these shadow points or 
power diagram construction. In addition, a projection strategy is introduced to restrict the movement of sampling points on 
the original surface to generate a more accurate mesh. Notably, the RTF computation and projection strategy are based on 
the KNN method. Thanks to the open-source KNN method (Garcia et al., 2008), the RTF computation and projection strategy 
are well-designed and implemented in parallel with GPU acceleration. Thus, they are iteratively performed by computing a 
CVT, and the ﬁnal meshes are generated from the optimized sampling points.
Given that the input mesh surface M = {Vm, T m} is composed by a set of vertices Vm = {vm
i }
nm
v
i=1 and a set of triangles 
T m = {tm
i }
nm
t
i=1, where nm
v and nm
t
are the number of vertices and triangle facets. Each triangle facet ti is represented by three 
ordered vertices, with indices 0, 1 and 2. Any combination of two vertices forms an edge of ti. The sampling points set is 
represented as S = {si}n
i=1, and the corresponding RTF set of all sampling points is denoted as F = {Fi}n
i=1. The RTF Fi of 
sampling point si is composed by a set of vertices, that is, Fi = V f
i = {v j}
nFi
j=1, where nFi represents the number of RTF 
vertices.
5


--- Page 6 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 2. Overall remeshing process of the Kitten model with our method. (a) input mesh (12.5K vertices, 25.0K faces); (b) initial tangent faces of sampling 
points (10.0K); (c) optimized restricted tangent faces; (c) reconnected restricted tangent faces; and (e) remeshing result (10.0K vertices, 20.0K faces).
Algorithm 1: Remeshing method based on RTF computation.
Input: origin mesh M, number of sampling points n, error termination ϵ, max iteration number maxiter
Output: remeshing result M′
1 Initialization: S = {si}n
i=1 are randomly sampled from M
2 while δ > ϵ & nbiter < maxiter do
3
estimating normals N = {ni}n
i=1 of sampling points S
// Normal estimation, Sect. 4.2
4
for si ∈S in parallel do
5
computing the RTF Fi of the point si
// RTF computation, Algorithm 2, Sect. 4.2
6
calculating the barycenter bi of the RTF Fi
// RTF barycenter, Eq. (7), Sect. 4.2
7
computing bp
i by projecting the bi to surface M
// projection strategy, Algorithm 3
8
δi = |si −bp
i |
// distance error of current point
9
si ←bp
i
// point optimization, Sect. 4.4
10
end
11
δ = maxi=1,2,···,n{δi}
// max distance error
12 end
13 Extraction: generating mesh M′ from the optimized points S
// remeshing, Sect. 4.4
It should be mentioned that the sampling points are randomly sampled from the input surface M in an initialization 
stage. The pseudo code of our method for surface remeshing is provided in Algorithm 1, and the remeshing process is given 
in Fig. 2. Our method consists of the following steps:
a) computing the RTF and barycenter of each sampling point directly; (Sect. 4.2)
b) projecting the barycenter (optimized sampling point) onto the input surface; (Sect. 4.3)
c) extracting triangle meshes from the optimized sampling points. (Sect. 4.4)
In fact, the sampling initialization in our method could be easily extended to weighted sampling or anisotropic sampling 
(Chen et al., 2018) to capture the surface curvatures.
4.2. RTF computation
Previous work (Xu et al., 2019) computes the RPF based on the CVT framework with CGAL (Fabri and Pion, 2009) on the 
CPU. The RPF is ﬁltered from the 3D power diagram by traversing the vertices of each power cell. Notably, as the shadow 
point is determined based on the normal of each sampling point in RPF (Xu et al., 2019), we observe that the RPF of a 
sampling point resembles its tangent plane. Thus, we simplify the calculation of RPF by computing the RTF in this paper. 
Based on the 3D clipped framework (Ray et al., 2018), we design a GPU-based method to compute the RTF based on the 
sampling point and its normal direction. Fortunately, the computation of the barycenter, volume, or integral of Voronoi cell 
is unnecessary in our method, and we are only interested in the intersection of RTF. Thus, the computational eﬃciency of 
the 3D clipped Voronoi diagram framework is further improved in our method.
To be speciﬁc, we use a pre-clipping for each sampling point si to compute a convex cell Vi by clipping an initialized 
bounding box. The pre-clipping is based on the orthogonal duals with the k-nearest-neighbors of si. Then, we directly 
compute the equation of a half-space plane, called the RTF plane, by the sampling point si and its normal direction ni. 
Thus, the convex cell Vi is further clipped by the RTF plane to obtain the exact RPF vertices Fi = {v f
i }
nFi
i=1, that is the 
RTF-clipping process. The ﬂow of our RTF computation method is shown in Fig. 3, and the pseudo-code is presented in 
Algorithm 2. The detailed description of each step in the RTF computation method is given as follows.
KNN & Pre-clipping. The RTF computation method is on the basis of the 3D clipped Voronoi diagram framework (Ray 
et al., 2018; Liu et al., 2022), in which a key technique is the KNN method. The k-nearest-neighbors Sk
i of each sampling 
point si are used to clip a convex cell Vi in the pre-clipping process. Some eﬃcient implementations of the KNN method 
are available, such as the multi-threading KNN on the CPU (Yan and Qixiang, 2009) or the parallelized KNN on the GPU 
6


--- Page 7 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 3. The RTF computation process of a sampling point si on surface. Top: an overview of the RTF computation; (a) a convex cell initialized with bounding 
box, (b) the pre-clipped convex cell, (c) the half-space plane (RTF plane), and (d) the obtained RTF (red colored). Bottom: exempliﬁcation of the pre-clipping
in 2D and the half-space plane; I(1) sampling points, I(2) k-nearest-neighbors of si, I(3) pre-clipping process, and II(1) half-space plane PFi .
Algorithm 2: GPU-based computation for RTF.
Input: sampling points S, points normal N = {ni}n
i=1
Output: RTF F = {Fi}n
i=1
1 for si ∈S in parallel do
2
Vi ←−BoundingBox(si)
// initialize a convex cell
3
Sk
i ←−k-nearest-neighbors
// KNN
4
Vi ←−clipping by the orthogonal duals of Sk
i
// pre-clipping
5
PF
i
←−plane equation of Fi based on Eq. (6)
// RTF equation
6
Fi ←−clipping by the RTF plane PFi
// RTF-clipping
7 end
(Garcia et al., 2008). Owing to the purpose of parallelized computing RPF on the GPU, we use the simple and open-source 
Brute-and-Force strategy (Garcia et al., 2008).
Once the k-nearest-neighbors Sk
i of a sampling point si are obtained, the pre-clipping process is performed as follows. 
The bounding box of the primordial surface is used to establish a convex cell Vi of the sampling point si. After obtaining 
the k-nearest-neighbors Sk
i , a pre-clipping step is then carried out to clip the convex cell Vi by the orthogonal duals of the 
sampling point si and its Sk
i . Fig. 3(I(1)–I(3)) illustrates the pre-clipping process in two dimensions, that is similar to Ray 
et al. (2018). However, the k-nearest-neighbors Sk
i are often close to the RTF plane PFi (but usually not on the RTF plane), 
which may lead to superﬂuous clippings and result in a thin polyhedral cell. These meaningless clips do not impact the RTF 
results, but they lengthen computation times.
In our method, a two-virtual-point clipping pro-
ceeds before the pre-clipping stage, aiming to pro-
duce a suitable convex cell, which is further clipped 
by the orthogonal duals of its neighbors, as shown 
in the illustration. For each sampling point si on 
the original surface, associated with a normal ni, 
the two virtual points sv1 and sv2 are deﬁned as: 
sv1 = si + d · ni and sv2 = si −d · ni, where d is the 
offset of virtual points. In fact, due to the orthogo-
nal duals of virtual points and the RTF being parallel, the value of d has no impact on the RTF results, and we set d to 20 
in our work. By taking advantage of two-virtual-point clipping, our method avoids the unnecessary intersection (red circle 
in the illustration (a)) in the subsequent pre-clipping stage, improving the RTF computation eﬃciency.
Normal estimation & RTF plane equation. The normal estimation of each sampling point si on the surface, which is 
intimately connected to the RTF plane PFi , is a crucial step in our methodology. In actuality, the RTF plane equation PFi
could be calculated using nothing more than the normal direction and position of sampling point si. The underlying idea of 
the normal estimation is to establish an approximation plane using the k nearest neighbors, whose normal represents the 
estimated normal of the sampling point si.
7


--- Page 8 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 4. The RTF extraction and symbolic representation in our method. Left: the RTF extraction process; (a) a convex cell Vi, (b) vertices of the RTF Fi, and 
(c) the barycenter bi of Fi. Right: the symbolic representation in GPU; (d) the vertex storage of Vi, (e) the vertex storage of Fi, and (f) the storage of RTF 
barycenter bi.
In our method, we immediately utilize the Point Cloud Library (PCL) (Rusu and Cousins, 2011) to determine the normal 
direction of each sampling point in S since PCL has been designed to perform the function of normal estimation. After 
acquiring the normal directions of sampling points S, we could immediately calculate the related RTF plane equation, as 
shown in Fig. 3 II(1). To be speciﬁc, let ni = (x′
i, y′
i, z′
i) represent the normal of the sampling point si = (xi, yi, zi), and the 
equation of PFi could be calculated as follow:
PFi :
x′
i · x + y′
i · y + z′
i · z −ni · si = 0
(6)
RTF-clipping & Symbolic representation. A further intersection (called RTF-clipping) of the pre-clipped convex cell from 
the preceding section with the RTF plane yields the precise RTF result that is similar to the pre-clipping process. In contrast 
to the truncation of Voronoi cells of the input mesh producing partial surface (Yan et al., 2009), the intersection in our 
method creates a bounded plane considerably more eﬃciently.
The data structure of the 3D clipped Voronoi diagram is fully described in Ray et al. (2018). A convex polyhedron is 
typically composed of a set of vertices and half-space plane equations. Each vertex is represented by a dual triangle, i.e., IDs 
of the half-space planes. However, there is neither an explicit storage structure for each facet in a Voronoi cell nor an order 
storage for the vertices of the Voronoi cell. Thus, additional post-processing is necessary for extracting vertices of a speciﬁc 
facet. Fortunately, we observe that these intersections are ordered in the latest clipping, which could be directly extracted 
without traversing all vertices of the Voronoi cell. By making use of this observation, a well-designed clipping procedure 
is described in our method, where the RTF-clipping is viewed as the last stage in the RTF computation process. As a result, 
instead of traversing all vertices in the convex polyhedron Vi, the vertices {v j}
nFi
j=1 of an RTF Fi could be directly derived 
from the latest incision, as shown in Fig. 4. Moreover, the barycenter bi of the RTF PFi could be easily calculated as:
bi =
1
nFi
·
nFi

j=1
v j
(7)
4.3. KNN-based projection strategy
As previously explained, we parallelly calculate the RTF barycenters Bi of sampling points S = {bi}n
i=1 on the original 
surface M. Thus, in each iteration, the sampling points are updated to their relevant barycenters. This, however, can only 
produce unconstrained points that may diverge from the original surface M, causing remeshing outputs with signiﬁcant 
errors or failures. With this regard, we borrow the notion of pulling back in Chen et al. (2018) and design a KNN-based 
projection strategy to prevent the sampling points from moving off the original surface M.
To achieve this, we directly employ the triangular facets on the original surface rather than the approximate underlying 
surface derived using a best-ﬁtting plane in Chen et al. (2018). Similar to the previous RTF computation, our objective is 
to independently project each barycenter bi onto the original surface M, which could be accomplished in parallel by GPU 
acceleration. Due to the successful application of the KNN method in the above text, we also utilize it in the projection 
strategy. Speciﬁcally, we ﬁrstly seek for the k-nearest-neighbor vertices Nk
i of each RTF barycenter bi from the original 
surface M, as shown in Fig. 5(a). Then, the triangular facets from M with at least one vertex v p in Nk
i are easily acquired. 
Thus, the barycenter bi is projected onto the triangular facets, and the nearest point is chosen as the projected point bp
i . 
The pseudo of the KNN-based projection strategy is provided in Algorithm 3. Given the process of projecting the barycenter 
bi onto a triangular facet t j, two main cases are considered in our method:
8


--- Page 9 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 5. Illustration of the projection strategy using the k-nearest-neighbor vertices Nk
i , where the target facet is represented by the purple colored triangle 
t j. (a) the k-nearest-neighbor vertices (purple dots) from original surface M of the barycenter bi (blue dot); (b) the barycenter bi is projected onto the 
triangle t j, generating the projected point bp′
ij ; and (c) the barycenter bi is projected outside the triangle t j (yellow dot), and a further projection is utilized 
to produce the projected point bp
ij (red dot) with nearest projection distance (red line).
Algorithm 3: KNN-based projection strategy.
Input: original mesh surface M, RTF barycenter B = {bi}n
i=1
Output: projected points Bp = {bp
i }n
i=1
1 for bi ∈B in parallel do
2
Nk
i ←k-nearest-neighbor vertices from original surface M
3
for each triangle t j(∃v p ∈t j, v p ∈Nk
i ) do
4
bp
ij ←the projected point on the triangle t j of bi
5
dp
ij ←the projection distance d(bi, bp
ij)
6
end
7
sorting all projection distances {dp
ij} in ascending order
8
bp
i ←bp
ij with the minimal projection distance
9 end
a) the barycenter bi is projected inside the triangular facet t j (including the boundaries), as shown in Fig. 5(b);
b) the barycenter bi is projected outside the triangular facet t j, as shown in Fig. 5(c).
The projected point bp
ij of the RTF barycenter bi on the triangular facet t j is straightforward in case a). While in case b), 
the early projection bp′
ij of the barycenter bi lies outside of the triangular facet t j, as shown in the yellow dot in Fig. 5(c). 
Therefore, a further projection of bp′
ij onto the edges of the triangular facet t j is required, as shown in the green lines in 
Fig. 5(c). At this point, we could calculate the normal of t j through its three vertices, then combine it with one edge e ∈t j, 
to obtain an exact plane P vir. The point bp′
ij is further projected onto the plane P vir, which ensures the projection point 
falls on the edge e or its extension, as shown in the green and red dots in Fig. 5(c). Only these points on the edges of the 
triangular facet are taken into account, and the projection point bp
ij of the RTF barycenter bi is determined to be the point 
with the minimal projection distance, as shown the red dot in Fig. 5(c).
Consequently, each RTF barycenter bi could be effectively restricted onto the original surface M by the two cases a) and 
b) above. Theoretically, there may be a corner case where the projection of the barycenter bi onto the triangular facet t j in 
Fig. 5(c) is not on any edge of t. At this time, the barycenter bi is away from any vertex of the triangular facet t j. In our 
method, this corner case is well avoided due to the usage of the KNN method. The reason is that several nearest neighbors 
Nk
i of the barycenter bi are calculated so that each triangular facet t j containing a vertex from Nk
i is not too far from the 
current barycenter bi. Notably, the vertex v p ∈Nk
i is taken as the projection point bp
ij in the corner case to circumvent 
exceptions in the implementation of our method.
4.4. Optimization & mesh extraction
To generate high-quality meshes on the basis of the RTF computation, optimization and mesh extraction are two addi-
tional steps in our method, which are described in the following.
Optimization. For CVT optimization, the existing methods mainly involve Lloyd’s method (Lloyd, 1982) with linear con-
vergence and the quasi-Newton method (Liu et al., 2009) with super-linear convergence, e.g., the L-BFGS method (Xin et al., 
2016). For instance, Alliez et al. (2005) optimize each sampling point in 2D parametric space using Lloyd’s method. Yan 
et al. (2009) compute the CVT based on the quasi-Newton method, which is faster than Lloyd’s method. However, in our 
method, there may be gaps or misaligned facets in the RTF of sampling points, as shown in Fig. 2. These misaligned facets 
9


--- Page 10 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 6. Exempliﬁcation of RTF vertices reconnection in our method. (a) and (b) three co-circular sampling points; (c–e) four co-circular sampling points; (f) 
triangular meshes of equal quality in four co-circular sampling points; and (g–h) triangular meshes of different quality in four co-circular sampling points.
lead to diﬃculties in the computation of the laplacian operator matrix. Therefore, we turn to the simple and easy Lloyd’s 
method to compute CVT, that is, the sampling point optimization.
To be more speciﬁc, the optimization of sampling points in our method works as follows: 1) the RTF Fi computation 
of each sampling point si by Algorithm 2; 2) the barycenter bi calculation of each RTF Fi based on Equation (7); 3) the 
projection bp
i of the barycenter bi onto the original surface M by Algorithm 3; and 4) the update of each sampling point 
si based on bp
i . The four steps are performed in parallel on the GPU (lines 5–7 in Algorithm 1), and we also calculate the 
distance error δi between the sampling point si and its projected point bp
i
(line 8 in Algorithm 1) in each iteration. This 
optimization process is repeated until the convergence, or the maximum iterations are reached.
Mesh extraction. After the optimization process, we could obtain regularly distributed sampling points. Then, a mesh 
extraction process is used to produce the triangle meshes. However, as shown in Fig. 2(c), the misaligned facets in the 
optimized RTF make it almost impossible to directly generate the desired triangle mesh as the connectivity is ambiguous. 
One straightforward way is the reconnection strategy presented by Xu et al. (2019), where circumcenters for being dual are 
computed by clustering the neighbors of sampling points, from which the triangle meshes could be generated. Speciﬁcally, 
as shown in Fig. 6(b) and (d), these red dots are merged as two dual circumcenters, and each of them is correlated to several 
RTFs. Typically, a circumcenter is associated with three RTFs, and the corresponding clustered neighbor sampling points are 
reconnected as a triangle in our output, as shown in Fig. 6(a–b). In the degenerate cases, there are four co-circular sampling 
points on the same plane, as shown in Fig. 6(c–e). As shown in Fig. 6(g–h), two triangular divisions could be obtained. The 
division yielding higher-quality triangles is selected as the output in our method. In particular, when the quality of triangles 
in two different divisions are of the same quality, either of these two divisions is feasible in our method, as shown in 
Fig. 6(f).
Another robust way for mesh generation is the RVD-based mesh extraction method in Boltcheva and Lévy (2017), which 
is provided in Geogram (Lévy and Filbois, 2015). This method takes as input a ﬁltered pointset and connects the input 
points with triangles by computing their restricted Voronoi diagram. An essential requirement of this method is a high-
quality point set, which coincides with the output of the optimization process in our method. Therefore, the regularly 
distributed sampling points generated by the optimization process taken as the inputs of the RVD-based mesh extraction 
method is also an eﬃcient way to produce triangular meshes of high quality.
5. Evaluation
In this section, we present various computational results to demonstrate the potency and usefulness of our remeshing 
method. The proposed algorithms are implemented using C++. The 3D clipped Voronoi diagram framework (Ray et al., 2018) 
is applied to compute the pre-clipped convex cell, and the normal directions are estimated using PCL 1.8.1. All experiments 
are performed on a Windows 10 computer with 3.6 GHz Intel (R) Core (TM) i7-9700K GPU with 16 GB memory and an 
NVIDIA GeForce RTX 2080 Ti with 11 GB memory, using CUDA version 10.0.
GPU speciﬁc. The ability of our method to compute each RTF independently is friendly for a GPU implementation. Here 
we provide some implementation details of our method. Similar to the 3D clipped Voronoi diagram (Ray et al., 2018), the 
sets for vertices and plane equations are stored in shared memory arrays of constant size max #T and #P, respectively. The 
value of #T is set to 96, and #P is 64 in our experiments. The number of threads by blocks is set to 16, and the maximum 
iterations are 160. Moreover, the parameter k of k-nearest-neighbor is crucial in our method, which is described in detail 
in our experiments. Here, the default values are given as follows: knor = 16 in the normal estimation stage, kclip = 32 in the 
pre-clipping stage, and kproj = 16 in the KNN-based projection stage.
10


--- Page 11 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 7. Remeshing results of various models. From top to bottom: input meshes, optimized RTF and output meshes. From left to right: remeshing of the 
Rocker model with 8.0K vertices, Kitten model with 12.0K vertices, Moai model with 15.0K vertices, Botijo model with 20.0K vertices, Bunny model with 
25.0K vertices and Egea model with 30.0K vertices. The percentage of RTFs with fewer than ﬁve or more than seven vertices in each result is reported: 
1.400% (112/8000 in Rocker), 2.02% (242/12000 in Kitten), 2.23% (334/15000 in Moai), 1.96% (392/20000 in Botijo), 2.36% (591/25000 in Bunny), and 3.56% 
(1067/30000 in Egea).
Fig. 8. Remeshing results of the Fertility model with different numbers of sampling points. From left to right: input model with 44.0K vertices, result with 
10.0K sampling points (18.294 seconds), result with 20.0K sampling points (35.434 seconds), and result with 30.0K sampling points (62.906 seconds).
5.1. Qualitative evaluation
We now provide some computational results to evaluate the feasibility of our remeshing method. Firstly, we conduct an 
experiment on several models with different numbers of vertices, including the Rocker, Kitten, Moai, Botijo, Bunny and Egea
models. The remeshing results of these models with our method are presented in Fig. 7. Then, we also test our method on 
one model (that is, the Fertility model), where the number of sampling points is set to 10.0K, 20.0K, and 30.0K. All sampling 
points are randomly sampled from the input surface (as shown in Fig. 8(a)), and the output meshes are illustrated in 
Fig. 8(b–d). According to these results in Fig. 7 and Fig. 8, we can observe that our method is feasible for surface remeshing, 
and the obtained results are of higher quality than the raw meshes.
11


--- Page 12 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 9. Computational eﬃciency of our method with various amount of sampling points. (a) computational timing in each iteration with the Fertility model; 
(b) the corresponding triangle quality of the results in (a).
Table 1
Quality of output meshes with our method, corresponding to these results in Fig. 7.
Model
In/Out
#V 1
#F 2
Q min
Q avg
θmin
θmin.avg
θ<30◦
θ>90◦
dH
3
T (s)
Moai
Input
23.2K
46.4K
0.005
0.709
2.672
38.048
0.244
0.356
–
–
Output
15.0K
30.0K
0.603
0.921
31.477
53.173
0.000
0.001
0.023
22.167
Botijo
Input
10.7K
21.4K
0.026
0.669
1.571
35.332
0.336
0.399
–
–
Output
20.0K
40.0K
0.597
0.914
32.029
52.670
0.000
0.001
0.041
39.864
Bunny
Input
35.3K
70.6K
0.008
0.715
4.825
36.946
0.068
0.138
–
Output
25.0K
50.0K
0.572
0.895
27.112
51.187
0.001
0.003
0.015
57.639
Egea
Input
50.0K
100.0K
0.030
0.631
1.246
36.424
0.029
0.104
–
–
Output
30.0K
60.0K
0.578
0.897
26.853
51.028
0.003
0.005
0.075
76.481
1 number of vertices; 2 number of facets; 3 Hausdorff distance (×10−2).
Notably: the quality of Rocker, Kitten is referred to Table 2.
5.2. Quantitative evaluation
The quantitative results of some remeshing models using our method are reported in this section. We introduce mesh 
quality metrics to evaluate the computational eﬃciency, followed by a quality analysis and computational timings.
Mesh quality metric. The criteria in Khan et al. (2022) is utilized to assess the quality of generated meshes in our 
experiments. The quality of a triangle t in the mesh surface M is determined as Q t =
6
√
3 ·
At
St Et , where the area, half-
perimeter, and the length of the longest edge of the triangle t are represented by At, St, and Et, respectively. Thus, Q min
and Q avg, which indicate the minimal and average triangle quality of the mesh surface, are utilized to quantify the mesh 
quality. Similarly, the average of the minimum angles in all triangles is θavg, and the minimum angle is θmin. The percentage 
of triangles with minimum angles less than 30◦is designated as θ<30◦, while the percentage of those with maximum angles 
of more than 90◦is θ>90◦. We also evaluate the quality of meshes by the approximation error in terms of Hausdorff distance 
error dH, which is normalized by the diameter of the bounding box. Moreover, the eﬃciency of the remeshing method is 
crucial to the subsequent geometric processing. Execution time is also taken into account as an important metric in our 
experiments to assess the eﬃciency of our method.
Quality analysis. Table 1 illustrates the quality of some output meshes generated by our method, corresponding to these 
remeshing results in Fig. 7. We can observe from the results in Table 1 that our method is effective for surface remeshing, 
and the quality of output meshes has signiﬁcantly improved. Particularly, for the Rocker, Kitten, and Botijo models, the 
percentage of triangles with minimum angles smaller than 30◦is decreased to 0, further demonstrating that our method 
yields high-quality remeshing results. However, for the special structures in these complex mesh surfaces, such as the ears 
in the Bunny model, our method yields slightly inferior triangles, which leads to a slightly lower quality of the remeshing 
results compared to these smooth mesh surfaces.
Eﬃciency analysis. The quantity of sampling points has an impact on the computational eﬃciency of our method. To 
demonstrate computational timings and the triangular quality of the remeshing results, we put the Fertility model to the 
test. Fig. 9(a) displays the computational timing proﬁles in each iteration, comprising the normal estimation (Normal), RTF 
and its barycenter computation (RTF), barycenter projection (Projection), and total time. As demonstrated in Fig. 9(b), we 
also offer the corresponding triangular quality of the remeshing results. According to the remeshing results in Fig. 9(a–b), 
we can observe that the triangular quality of the related results improves as the number of sampling points gradually grows, 
and so does the computational timing increases.
12


--- Page 13 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 10. The impact of KNN parameter k at different stages of our method on the Torus (10.0K sampling points) and Bunny (20.0K sampling points) models, 
where knor is used for the normal estimation stage, kclip is applied for the pre-clipping stage, and kproj is utilized for the projection stage. Top: the impact 
of different values of knor, kclip, and kproj on the quality of generated triangular meshes. Bottom: the impact of different values of knor, kclip, and kproj on 
the running time of each iteration in the corresponding stages.
Parameter analysis. A crucial technique is the KNN method in our implementation, which is frequently used in different 
stages, including the normal estimation knor, the pre-clipping kclip, and the projection strategy kproj. To analyze the effect of 
different values of these parameters on the generated meshes, a comprehensive experiment is performed, where the number 
of sampling points is set to 10.0K for the Torus model and 20.0K for the Bunny model. Notably, when analyzing the effect of 
one parameter, the others are set to the default values in this experiment. Fig. 10 presents the quality of generated meshes 
and the relevant running time of each iteration on different values of knor, kclip, and kproj.
Overall, remeshing with more sampling points results in longer iteration running times. In view of the parameter knor, 
the requirement of our method is that more sampling points are used to capture the characteristics of complicated models. 
Thus, the generated meshes are typical of comparable quality. Considering the effect of the parameter kclip, a small value 
of the parameter kclip results in an inaccurate convex polyhedron and the respective RTF, leading to a poor-quality mesh. 
However, the quality of the generated meshes signiﬁcantly improves and stabilizes as the value of the parameter kclip
increases. Finally, when the parameter kproj is taken into account, setting a small value of kproj during the projection stage 
may lead to the aggregation of sampling points, especially at corners or edges, resulting in the generation of meshes with 
poor quality.
5.3. Comparison
To further demonstrate the viability and effectiveness of our method, we compare against two exact CVT-oriented 
remeshing techniques, including the maximal Poisson-disk sampling (MPS) (Guo et al., 2015) and the RVD (Yan et al., 
2009), as well as two approximate CVT-oriented remeshing techniques, including the RPF (Xu et al., 2019) and the re-
stricted Voronoi cell (RVC) (Chen et al., 2018) in terms of the quality of generated triangular meshes. Moreover, in view of 
the computational performance per iteration, we compare against the RVD and RPF running on a CPU, the multi-threading 
RVC on a multi-core CPU, and the clipped Voronoi diagram (CVD) running on a GPU, respectively.
Notably, despite the fact that the RVD is generally utilized within the quasi-Newton method to speed up the CVT energy 
calculations, we are solely interested in the intersection between Voronoi cells and the original surface. Using the code 
provided in Lévy and Liu (2010), we reimplemented the RVD algorithm, and Lloyd’s method is applied to relocate these 
sampling points. Similarly, we reimplemented the CVD method for surface remeshing by replacing the Voronoi diagram 
computation in RVD (Yan et al., 2009) using the GPU-based construction method in Liu et al. (2022). Moreover, the RPF 
algorithm (Xu et al., 2019) in this comparison is reimplemented based on the normal estimation on PCL (Rusu and Cousins, 
2011), as we only focus on the computation of these approximate facets, that is RPFs.
Remeshing results & quality analysis. We compare the generated meshes with our method against a number of other 
methods, including MPS (Guo et al., 2015), RVD (Yan et al., 2009), RPF (Xu et al., 2019), RVC (Chen et al., 2018). Fig. 11
presents the results of several remeshing techniques, and Table 2 reports the pertinent mesh qualities. The implementation 
details for this comparative experiment are as follows: three different raw meshes are taken as inputs for these remeshing 
13


--- Page 14 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 11. Comparison results of different remeshing methods, where the colored triangles indicate the facets with lower quality (θmin < 33◦). From top to 
bottom: remeshing results of the Rocker, Botijo, and Kitten models. From left to right: input mesh, remeshing results of MPS (Guo et al., 2015), RVD (Yan 
et al., 2009), RPF (Xu et al., 2019), RVC (Chen et al., 2018), Ours with less number of vertices (Ours1) and Ours with more number of vertices (Ours2).
methods, i.e., Rocker, Botijo, and Kitten. We initialize some uniformly distributed auxiliary points inside the mesh model to 
assist the 3D Voronoi diagram construction in the RVD, which is further intersected with the original surface. We apply the 
original mesh surfaces to the RVC, using their vertices as the original sampling points, even though the RVC requires point 
clouds as inputs to reconstruct surfaces. Notably, the Botijo model with 10.7K vertices in Table 1 is selected as input for the 
RVC as that with 3.0K in Table 2 results in the surface calculated abnormally. Furthermore, we show two results with our 
method under a different number of vertices, and the detailed information is provided in Table 2.
The results in Fig. 11 and Table 2 demonstrate the effectiveness of our method for surface remeshing. Compared to MPS 
(Guo et al., 2015), our method generates meshes of higher quality, but the reported time of MPS is better than ours, as seen 
in Fig. 11. Instead of truncating boundary Voronoi cells in the RVD (Yan et al., 2009), the original surface is roughly ﬁtted by 
calculating the tangent planes. Thus, our method signiﬁcantly reduces the computational complexity and generates meshes 
with similar or better triangular quality with less time consumption. As illustrated in the ﬁrst row in Fig. 11, the RVC 
yields signiﬁcant approximation error as it pulls the sampling points back to the best-ﬁtting planes to represent the surface, 
which is inﬂuenced by the number of input points. Unlike the RVC, the sampling points are projected onto the triangular 
facet in the original surface, producing meshes of higher quality, especially smaller Hausdorff distance errors. Nevertheless, 
inﬂuenced by the accuracy of the normal estimation, there are still a few triangles with small or obtuse angles in the 
remeshing results with our method, which leads to low-quality triangles (as shown in colored facets in Fig. 11).
Eﬃciency comparison. An essential highlight of our method and its main strength is computational performance. We 
perform an experiment to track the computation times of different methods with various numbers of sample points on 
Kitten and Botijo models. These methods could be typically classiﬁed into those running on a CPU, including RVD (Yan 
et al., 2009) and RPF (Xu et al., 2019), and those running on a multi-core CPU or GPU, including RVC (Chen et al., 2018), 
CVD (Liu et al., 2022) and our method. Despite the fact that several optimization techniques have been used to optimize 
the position per sampling point, we only pay attention to the computational performance of these methods in one itera-
tion. The corresponding record timings are shown in Fig. 12, where the number of sampling points is gradually increased 
from 10.0K to 30.0K. In view of the computation process of one iteration in these methods, the RVD calculates the exact 
14


--- Page 15 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Table 2
Quality of the output results with different remeshing methods in Fig. 11, including the MPS (Guo et al., 2015), RVD (Yan et al., 
2009), RPF (Xu et al., 2019), and Ours, where the red values indicate the best results and the green values represent the second-best 
results.
Model
Method
#V 1
#F 2
Q min
Q avg
θmin
θmin.avg
θ<30o
θ>90o
dH
3
T(s)
Rocker
Input
9.4K
18.8K
0.004
0.695
0.240
36.456
0.272
0.350
–
–
MPS
5.8K
11.6K
0.460
0.795
24.862
44.203
0.009
0.177
0.084
0.235
RVD
5.8K
11.6K
0.588
0.882
32.926
50.411
0.000
0.018
0.035
117.25
RPF
5.8K
11.6K
0.442
0.871
27.266
49.069
0.001
0.019
0.034
17.863
RVC
5.8K
11.6K
0.436
0.894
24.966
51.489
0.003
0.002
0.103
9.867
Ours1
5.8K
11.6K
0.578
0.884
29.019
51.753
0.000
0.002
0.057
5.680
Ours2
8.0K
16.0K
0.592
0.902
29.347
52.035
0.000
0.001
0.023
8.289
Botijo
Input
3.0K
6.0K
0.026
0.590
1.571
30.722
0.494
0.554
–
–
MPS
5.0K
10.0K
0.415
0.797
25.103
44.255
0.008
0.170
0.091
0.219
RVD
5.0K
10.0K
0.568
0.884
30.405
50.650
0.000
0.018
0.055
188.791
RPF
5.0K
10.0K
0.489
0.872
26.659
49.295
0.003
0.020
0.083
29.624
RVC
5.0K
10.0K
0.448
0.826
25.447
46.255
0.003
0.009
0.102
11.371
Ours1
5.0K
10.0K
0.571
0.894
29.433
51.590
0.000
0.001
0.077
7.534
Ours2
10.0K
20.0K
0.599
0.913
32.391
52.528
0.000
0.000
0.058
15.724
Kitten
Input
12.5K
25.0K
0.008
0.663
5.123
35.823
0.327
0.467
–
–
MPS
10.3K
20.6K
0.442
0.793
22.859
43.898
0.014
0.178
0.024
0.469
RVD
10.2K
20.4K
0.524
0.883
29.438
50.478
0.001
0.019
0.029
223.517
RPF
10.0K
20.0K
0.459
0.874
27.284
49.507
0.001
0.021
0.022
27.318
RVC
10.0K
20.0K
0.570
0.878
32.715
50.439
0.000
0.025
0.058
18.263
Ours1
10.0K
20.0K
0.572
0.908
31.147
52.139
0.000
0.001
0.007
13.334
Ours2
12.0K
24.0K
0.599
0.912
33.672
52.620
0.000
0.000
0.006
16.651
1 number of vertices; 2 number of facets; 3 Hausdorff distance (×10−2).
Fig. 12. Comparison of computational eﬃciency of various methods under different number of sampling points ranging from 10.0K to 30.0K. (a) computa-
tional eﬃciency of methods running on a CPU, including the RVD (Yan et al., 2009) and RPF (Xu et al., 2019); and (b) computational eﬃciency of methods 
running on a multi-core CPU or GPU, including the CVD (Liu et al., 2022) and RVC (Chen et al., 2018). Note that the computational eﬃciency of our method 
reported in (a) and (b) is the same.
intersection of boundary Voronoi cells and the original surface, which is complicated and time-consuming. Based on the in-
tersection calculation algorithm in RVD, the CVD achieves better computational eﬃciency with the usage of GPU-accelerated 
3D Voronoi diagram construction. Nevertheless, this does not circumvent the calculation of these exact intersections in RVD, 
which is still time-consuming. The RPF extracts the co-planar facets by traversing the constructed power diagram, achieving 
better computational eﬃciency compared to the RVD. The RVC calculates the CVT and pulls the sampling points back to a 
best-ﬁtting plane and speedups by multi-threading. The RTF computation and the projection strategy in our method are im-
plemented in parallel by taking full advantage of GPU acceleration. Therefore, our method achieves the best computational 
eﬃciency compared to other methods.
More qualitative comparison. Furthermore, we compare our method with that proposed by Leung et al. (2015) from a 
qualitative perspective. The two methods share several features: CVT-driven, extrinsic, and GPU-friendly. However, there are 
also signiﬁcant differences between the two methods, and Table 3 illustrates the qualitative comparison results of the two 
methods at the methodological level. To be speciﬁc, in terms of CVT dimension and RVD computation, Leung et al. (2015)
compute the 3D CVT and RVD, whereas our method computes the 2D CVT (the plane in 3D space) and RVD computation is 
unnecessary, which means that our method is more straightforward and more eﬃcient. Considering the CVT computation 
and GPU acceleration, Leung et al. (2015) compute the exact EDT and CVT on GPU by discretizing the original surface 
15


--- Page 16 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Table 3
Qualitative comparison to the method proposed by Leung et al. (Leung et al., 2015).
Method
Input
CVT
RVD
Eﬃciency
Dimension
Computation
Accuracy
Leung et al. (Leung et al., 2015)
mesh/non-mesh
3D
voxel-based
poor
necessary
fast
Ours
mesh
2D
clipping-based
good
unnecessary
faster
Fig. 13. Remeshing results of our method on a complex model David Head with 30.0K sampling points. (a) optimized RTF result; (b) the corresponding 
triangular mesh.
into voxels with a constant resolution, which is limited by the graphical memory and does not provide suﬃcient accuracy 
to guarantee the correctness of the CVT for complex models. In contrast, our method provides a more accurate 2D CVT 
by clipping the tangent in a resolution-independent manner to approximate the original surface. However, the method 
proposed by Leung et al. (2015) has advantages in remeshing various surface representations, including implicit, meshes, 
and point clouds.
5.4. More results
Complex models are commonly used in practical applications, e.g., modeling in computer games, physics simulations, 
etc. We conduct an experiment on a complex model, i.e., the David Head, to further evaluate the capability of our method. 
The number of sampling points is set to 30.0K in this experiment, and the remeshing results are presented in Fig. 13. To 
illustrate the RTF and corresponding triangular meshes in detail, we provide additional details in the remeshing results of 
the David Head, e.g., the left eye and the hair on top of the head. From the results in Fig. 13, it could be observed that 
our method is feasible for complex models, and we could obtain the desired triangular meshes on these relatively smooth 
regions. Nevertheless, regions with complex features are still challenging for our method, as the normal estimation through 
these sampling points cannot well-capture these complicated regions, e.g., the left eye of the David Head.
6. Conclusion
In this paper, we provide an effective GPU method for producing high-quality remeshing results. Based on the principle 
of the planar approximation, a parallel method is presented to compute the RTF of each sampling point directly, and each 
sampling point is further optimized to the barycenter of its RTF. Additionally, we present a KNN-based projection strategy 
to restrict the movement of sampling points on the original surface during the optimization process. Experimental results 
on a variety of mesh surfaces demonstrate the feasibility and eﬃciency of our method.
Limitations and future work. Despite the fact that our method offers various techniques for producing high-quality remesh-
ing results, there are still several diﬃculties in our work. The main limitation is that the proposed method does not have 
theoretical guarantees, although experimental results demonstrate its validity. Another limitation is the challenge of remesh-
ing complex geometries, especially with few sampling points. A small number of sampling points can hardly capture some 
details of a complex model, e.g., hands of Homer model, etc., which leads to incorrect RTF or even failure remeshing, as 
shown in Fig. 14(a). Besides, our method introduces the RTF, coupled with the CVT optimization, to yield regularly dis-
tributed sampling points, thus generating high-quality meshes. However, the surface curvature is not considered in our 
work, making it challenging to produce remeshing results with sharp feature preservation or surface curvature adaptation, 
as shown in Fig. 14(b). Finally, some special topological structures are commonly used in practical applications, e.g., thin-
plate or close-plate models. Due to these thin-plate or close-plate structures, the normal estimation of sampling points may 
be incorrect, resulting in an incorrect RTF result, as shown in Fig. 14(c).
16


--- Page 17 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Fig. 14. Exempliﬁcations of several limitations in our work. (a) failure in capturing details of the Homer model; (b) remeshing without sharp feature 
preservation of the Fandisk model; and (c) shortcomings in remeshing with close-plate of the Close Hemisphere model.
For future work, we are thinking of merging the proposed RTF with power diagrams for non-uniform remeshing. We 
believe that by giving each sampling point a weight parameter to regulate the area size of the relevant RTF, the surface 
curvature could be characterized. Additionally, verifying the normal orientation of each sampling point is another way to 
potentially solve defects in thin-plate or close-plate surface remeshing.
CRediT authorship contribution statement
Yuyou Yao: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualiza-
tion, Writing – original draft. Jingjing Liu: Data curation, Software, Validation, Visualization, Writing – original draft, Writing 
– review & editing. Wenming Wu: Formal analysis, Methodology, Writing – review & editing. Gaofeng Zhang: Formal anal-
ysis, Resources, Writing – review & editing. Benzhu Xu: Resources, Software, Supervision. Liping Zheng: Conceptualization, 
Funding acquisition, Investigation, Methodology, Project administration, Resources, Supervision, Writing – review & editing.
Declaration of competing interest
The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have 
appeared to inﬂuence the work reported in this paper.
Data availability
Data will be made available on request.
Acknowledgements
The models used in this paper are from the AIM@Shape and courtesy of the Stanford 3D Scanning Repository. We would 
like to thank Zhonggui Chen and Jianwei Guo for providing their executable programs, and we also thank Bruno Lévy and 
Yang Liu for the source code of truncating Voronoi cells. This work was supported in part by a grant from the National 
Natural Science Foundation of China (No. 61972128) and the National Key Research and Development Plan of China (No. 
2022YFC3900800).
References
Abdelkader, A., Bajaj, C.L., Ebeida, M.S., Mahmoud, A.H., Mitchell, S.A., Owens, J.D., Rushdi, A.A., 2020. VoroCrust: Voronoi meshing without clipping. ACM 
Trans. Graph. 39, 1–16.
Alliez, P., De Verdière, É.C., Devillers, O., Isenburg, M., 2005. Centroidal Voronoi diagrams for isotropic surface remeshing. Graph. Models 67, 204–231.
Aurenhammer, F., 1987. Power diagrams: properties, algorithms and applications. SIAM J. Comput. 16, 78–96.
Aurenhammer, F., Hoffmann, F., Aronov, B., 1998. Minkowski-type theorems and least-squares clustering. Algorithmica 20, 61–76.
Boltcheva, D., Lévy, B., 2017. Surface reconstruction by computing restricted Voronoi cells in parallel. Comput. Aided Des. 90, 123–134.
Chen, L., Holst, M., 2011. Eﬃcient mesh optimization schemes based on optimal Delaunay triangulations. Comput. Methods Appl. Mech. Eng. 200, 967–984.
Chen, Y., Ma, Z., Zhou, H., Zhang, W., 2021. 3D print-scan resilient localized mesh watermarking. In: 2021 IEEE International Workshop on Information 
Forensics and Security. WIFS. IEEE, pp. 1–6.
Chen, Z., Zhang, T., Cao, J., Zhang, Y.J., Wang, C., 2018. Point cloud resampling using centroidal Voronoi tessellation methods. Comput. Aided Des. 102, 12–21.
De Goes, F., Breeden, K., Ostromoukhov, V., Desbrun, M., 2012. Blue noise through optimal transport. ACM Trans. Graph. 31, 1–11.
Decker, N., Lyu, M., Wang, Y., Huang, Q., 2021. Geometric accuracy prediction and improvement for additive manufacturing using triangular mesh shape 
data. J. Manuf. Sci. Eng. 143.
Du, Q., Faber, V., Gunzburger, M., 1999. Centroidal Voronoi tessellations: applications and algorithms. SIAM Rev. 41, 637–676.
Du, Q., Gunzburger, M.D., Ju, L., 2003. Constrained centroidal Voronoi tessellations for surfaces. SIAM J. Sci. Comput. 24, 1488–1506.
Du, X., Liu, X., Yan, D.-M., Jiang, C., Ye, J., Zhang, H., 2018. Field-aligned isotropic surface remeshing. Comput. Graph. Forum 37, 343–357.
17


--- Page 18 ---

Y. Yao, J. Liu, W. Wu et al.
Computer Aided Geometric Design 104 (2023) 102216
Engwirda, D., Ivers, D., 2014. Face-centred Voronoi reﬁnement for surface mesh generation. Proc. Eng. 82, 8–20.
Fabri, A., Pion, S., 2009. CGAL: the computational geometry algorithms library. In: Proceedings of the 17th ACM SIGSPATIAL International Conference on 
Advances in Geographic Information Systems, pp. 538–539.
Fu, Y., Zhou, B., 2009. Direct sampling on surfaces for high quality remeshing. Comput. Aided Geom. Des. 26, 711–723.
Garcia, V., Debreuve, E., Barlaud, M., 2008. Fast k nearest neighbor search using GPU. In: 2008 IEEE Computer Society Conference on Computer Vision and 
Pattern Recognition Workshops. IEEE, pp. 1–6.
Guo, J., Yan, D.-M., Jia, X., Zhang, X., 2015. Eﬃcient maximal Poisson-disk sampling and remeshing on surfaces. Comput. Graph. 46, 72–79.
Herholz, P., Haase, F., Alexa, M., 2017. Diffusion diagrams: Voronoi cells and centroids from diffusion. Comput. Graph. Forum 36, 163–175.
Hou, W., Zong, C., Wang, P., Xin, S., Chen, S., Liu, G., Tu, C., Wang, W., 2022. SDF-RVD: restricted Voronoi diagram on signed distance ﬁeld. Comput. Aided 
Des. 144, 103166.
Khan, D., Yan, D.-M., Ding, F., Zhuang, Y., Zhang, X., 2018a. Surface remeshing with robust user-guided segmentation. Comput. Vis. Media 4, 113–122.
Khan, D., Yan, D.-M., Wang, Y., Hu, K., Ye, J., Zhang, X., 2018b. High-quality 2D mesh generation without obtuse and small angles. Comput. Math. Appl. 75, 
582–595.
Khan, D., Plopski, A., Fujimoto, Y., Kanbara, M., Jabeen, G., Zhang, Y.J., Zhang, X., Kato, H., 2022. Surface remeshing: a systematic literature review of methods 
and research directions. IEEE Trans. Vis. Comput. Graph. 28, 1680–1713.
Leung, Y.-S., Wang, X., He, Y., Liu, Y.-J., Wang, C.C., 2015. A uniﬁed framework for isotropic meshing based on narrow-band Euclidean distance transformation. 
Comput. Vis. Media 1, 239–251.
Lévy, B., Filbois, A., 2015. Geogram: a library for geometric algorithms. In: VII International Conference on Adaptive Modeling and Simulation. ADMOS 2015. 
CIMNE, p. 45.
Lévy, B., Liu, Y., 2010. Lp centroidal Voronoi tessellation and its applications. ACM Trans. Graph. 29, 1–11.
Liang, X., Zhang, Y., 2011. Hexagon-based all-quadrilateral mesh generation with guaranteed angle bounds. Comput. Methods Appl. Mech. Eng. 200, 
2005–2020.
Liu, X., Ma, L., Guo, J., Yan, D.-M., 2022. Parallel computation of 3D clipped Voronoi diagrams. IEEE Trans. Vis. Comput. Graph. 28, 1363–1372.
Liu, Y., Wang, W., Lévy, B., Sun, F., Yan, D.-M., Lu, L., Yang, C., 2009. On centroidal Voronoi tessellation—energy smoothness and fast computation. ACM Trans. 
Graph. 28, 1–17.
Liu, Y.-J., Xu, C.-X., Yi, R., Fan, D., He, Y., 2016. Manifold differential evolution (MDE) a global optimization method for geodesic centroidal Voronoi tessella-
tions on meshes. ACM Trans. Graph. 35, 1–10.
Lloyd, S., 1982. Least squares quantization in PCM. IEEE Trans. Inf. Theory 28, 129–137.
Ray, N., Sokolov, D., Lefebvre, S., Lévy, B., 2018. Meshless Voronoi on the GPU. ACM Trans. Graph. 37, 1–12.
Rong, G., Liu, Y., Wang, W., Yin, X., Gu, D., Guo, X., 2010. GPU-assisted computation of centroidal Voronoi tessellation. IEEE Trans. Vis. Comput. Graph. 17, 
345–356.
Rong, G., Jin, M., Shuai, L., Guo, X., 2011. Centroidal Voronoi tessellation in universal covering space of manifold surfaces. Comput. Aided Geom. Des. 28, 
475–496.
Rusu, R.B., Cousins, S., 2011. 3D is here: Point Cloud Library (PCL). In: 2011 IEEE International Conference on Robotics and Automation. IEEE, pp. 1–4.
Sainlot, M., Nivoliers, V., Attali, D., 2017. Restricting Voronoi diagrams to meshes using corner validation. Comput. Graph. Forum 36, 81–91.
Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., et al., 2018. Mesh-tensorﬂow: deep 
learning for supercomputers. Adv. Neural Inf. Process. Syst. 31.
Sullivan, C., Kaszynski, A., 2019. PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). J. Open Sour. 
Softw. 4, 1450.
Wang, P., Xin, S., Tu, C., Yan, D., Zhou, Y., Zhang, C., 2020. Robustly computing restricted Voronoi diagrams (RVD) on thin-plate models. Comput. Aided 
Geom. Des. 79, 101848.
Wang, X., Ying, X., Liu, Y.-J., Xin, S.-Q., Wang, W., Gu, X., Mueller-Wittig, W., He, Y., 2015. Intrinsic computation of centroidal Voronoi tessellation (CVT) on 
meshes. Comput. Aided Des. 58, 51–61.
Wang, Y., Yan, D.-M., Liu, X., Tang, C., Guo, J., Zhang, X., Wonka, P., 2018. Isotropic surface remeshing without large and small angles. IEEE Trans. Vis. Comput. 
Graph. 25, 2430–2442.
Xin, S.-Q., Lévy, B., Chen, Z., Chu, L., Yu, Y., Tu, C., Wang, W., 2016. Centroidal power diagrams with capacity constraints: computation, applications, and 
extension. ACM Trans. Graph. 35, 1–12.
Xu, M., Xin, S., Tu, C., 2019. An eﬃcient surface remeshing algorithm based on centroidal power diagram. In: Proceedings of the 2019 7th International 
Conference on Information Technology: IoT and Smart City, pp. 536–542.
Yan, C., Qixiang, C., 2009. A novel parallel processing for continuous k-nearest neighbor queries. In: 2009 International Conference on Environmental Science 
and Information Application Technology. IEEE, pp. 593–596.
Yan, D.-M., Wonka, P., 2013. Gap processing for adaptive maximal Poisson-disk sampling. ACM Trans. Graph. 32, 1–15.
Yan, D.-M., Wonka, P., 2015. Non-obtuse remeshing with centroidal Voronoi tessellation. IEEE Trans. Vis. Comput. Graph. 22, 2136–2144.
Yan, D.-M., Lévy, B., Liu, Y., Sun, F., Wang, W., 2009. Isotropic remeshing with fast and exact computation of restricted Voronoi diagram. Comput. Graph. 
Forum 28, 1445–1454.
Yan, D.-M., Wang, W., Lévy, B., Liu, Y., 2010. Eﬃcient computation of 3D clipped Voronoi diagram. In: International Conference on Geometric Modeling and 
Processing. Springer, pp. 269–282.
Yan, D.-M., Wang, W., Lévy, B., Liu, Y., 2013. Eﬃcient computation of clipped Voronoi diagram for mesh generation. Comput. Aided Des. 45, 843–852.
Yan, D.-M., Bao, G., Zhang, X., Wonka, P., 2014. Low-resolution remeshing using the localized restricted Voronoi diagram. IEEE Trans. Vis. Comput. Graph. 20, 
1418–1427.
Yang, X., Koehl, M., Grussenmeyer, P., 2018. Mesh-to-BIM: from segmented mesh elements to BIM model with limited parameters. In: The International 
Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. 42. Copernicus Publications, pp. 1213–1218.
Ye, Z., Yi, R., Yu, M., Liu, Y.-J., He, Y., 2019. Geodesic centroidal Voronoi tessellations: theories, algorithms and applications. arXiv preprint. arXiv:1907.00523.
You, Y., Kou, X., Tan, S., 2015. Adaptive meshing for ﬁnite element analysis of heterogeneous materials. Comput. Aided Des. 62, 176–189.
Zheng, L., Yao, Y., Wu, W., Xu, B., Zhang, G., 2021. A novel computation method of hybrid capacity constrained centroidal power diagram. Comput. Graph. 97, 
108–116.
Zhuang, Y., Zou, M., Carr, N., Ju, T., 2014. Anisotropic geodesics for live-wire mesh segmentation. Comput. Graph. Forum 33, 111–120.
Zimmer, H., Campen, M., Herkrath, R., Kobbelt, L., 2012. Variational tangent plane intersection for planar polygonal meshing. In: AAG, pp. 319–332.
18
```

---

## Section 1.3: Adaptive mixtures of local experts

Source File: Adaptive mixtures of local experts.txt

### Paper Content:

```
# Adaptive mixtures of local experts.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Adaptive mixtures of local experts.pdf
# Output: ../layer2_completion/txt/Adaptive mixtures of local experts.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/233806999
Adaptive Mixtures of Local Experts
Article  in  Neural Computation · March 1991
DOI: 10.1162/neco.1991.3.1.79
CITATIONS
4,394
READS
12,464
4 authors, including:
Robert Jacobs
University of Rochester
137 PUBLICATIONS   18,316 CITATIONS   
SEE PROFILE
Michael Jordan
University of California, Berkeley
998 PUBLICATIONS   237,349 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Robert Jacobs on 02 June 2014.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

Communicated by Jeffrey Elman 
Adaptive Mixtures of Local Experts 
Robert A. Jacobs 
Michael I. Jordan 
Drpartiiient of Brain arid Cogriitiz~e Sciences, Massachusetts Znstitlrte of Technology, 
Cainbridge, MA 02139 USA 
Steven J. Nowlan 
Geoffrey E. Hinton 
Departinerit of Coinpiiter Sciericc, Uiiiz~ersity of Toroilto, 
Toronto, Canada M5S 1A4 
We present a new supervised learning procedure for systems composed 
of many separate networks, each of which learns to handle a subset of 
the complete set of training cases. The new procedure can be viewed 
either as a modular version of a multilayer supervised network, or as 
an associative version of competitive learning. It therefore provides 
a new link between these two apparently different approaches. We 
demonstrate that the learning procedure divides up a vowel discrimi- 
nation task into appropriate subtasks, each of which can be solved by 
a very simple expert network. 
1 Making Associative Learning Competitive 
If backpropagation is used to train a single, multilayer network to per- 
form different subtasks on different occasions, there will generally be 
strong interference effects that lead to slow learning and poor gener- 
alization. If we know in advance that a set of training cases may be 
naturally divided into subsets that correspond to distinct subtasks, inter- 
ference can be reduced by using a system composed of several different 
"expert" networks plus a gating network that decides which of the ex- 
perts should be used for each training case.' Hampshire and Waibel 
(1989) have described a system of this kind that can be used when the 
division into subtasks is known prior to training, and Jacobs ef al. (1990) 
have described a related system that learns how to allocate cases to ex- 
perts. The idea behind such a system is that the gating network allocates 
a new case to one or a few experts, and, if the output is incorrect, the 
weight changes are localized to these experts (and the gating network). 
'This idea was first presented by Jacobs and Hinton at the Connectionist Summer 
School in Pittsburgh in 1988. 
N w m l  Corripiitntiori 3, 79-87 (1991) 
@ 1991 Massachusetts Institute of Technology 


--- Page 3 ---

80 
Robert A. Jacobs et al. 
So there is no interference with the weights of other experts that special- 
ize in quite different cases. The experts are therefore local in the sense 
that the weights in one expert are decoupled from the weights in other 
experts. In addition they will often be local in the sense that each expert 
will be allocated to only a small local region of the space of possible input 
vectors. 
Unfortunately, both Hampshire and Waibel and Jacobs et al. use an 
error function that does not encourage localization. They assume that the 
final output of the whole system is a linear combination of the outputs of 
the local experts, with the gating network determining the proportion of 
each local output in the linear combination. So the final error on case c 
is 
where 0," is the output vector of expert i on case c, p: is the proportional 
contribution of expert i to the combined output vector, and d" is the 
desired output vector in case c. 
This error measure compares the desired output with a blend of the 
outputs of the local experts, so, to minimize the error, each local expert 
must make its output cancel the residual error that is left by the combined 
effects of all the other experts. When the weights in one expert change, 
the residual error changes, and so the error derivatives for all the other 
local experts change.2 This strong coupling between the experts causes 
them to cooperate nicely, but tends to lead to solutions in which many 
experts are used for each case. It is possible to encourage competition by 
adding penalty terms to the objective function to encourage solutions in 
which only one expert is active (Jacobs et 41. 1990), but a simpler remedy 
is to redefine the error function so that the local experts are encouraged 
to compete rather than cooperate. 
Instead of linearly combining the outputs of the separate experts, we 
imagine that the gating network makes a stochastic decision about which 
single expert to use on each occasion (see Fig. 1). The error is then the 
expected value of the squared difference between the desired and actual 
output vectors 
Notice that in this new error function, each expert is required to pro- 
duce the whole of the output vector rather than a residual. As a result, 
the goal of a local expert on a given training case is not directly affected 
by the weights within other local experts. There is still some indirect 
2For Hampshire and Waibel, this problem does not arise because they do not learn the 
task decomposition. They train each expert separately on its own preassigned subtask. 


--- Page 4 ---

Adaptive Mixtures of Local Experts 
Network 
Expert 
Expert 
Expert 
Network 
Network 
Network 
Input 
Figure 1: A system of expert and gating networks. Each expert is a feed- 
forward network and all experts receive the same input and have the same 
number of outputs. The gating network is also feedforward, and typically 
receives the same input as the expert networks. It has normalized outputs 
I?, = exp(.r,)/ C, exp(.r,), where .T] is the total weighted input received by out- 
put unit J of the gating network. The selector acts like a multiple input, single 
output stochastic switch; the probability that the switch will select the output 
from expert J is pJ. 
coupling because if some other expert changes its weights, it may cause 
the gating network to alter the responsibilities that get assigned to the ex- 
perts, but at least these responsibility changes cannot alter the sign of the 
error that a local expert senses on a given training case. If both the gating 
network and the local experts are trained by gradient descent in this new 
error function, the system tends to devote a single expert to each training 
case. Whenever an expert gives less error than the weighted average of 
the errors of all the experts (using the outputs of the gating network to 
decide how to weight each expert's error) its responsibility for that case 


--- Page 5 ---

82 
Robert A. Jacobs et al. 
will be increased, and whenever it does worse than the weighted average 
its responsibility will be decreased. 
The error function in equation 1.2 works in practice but in the sim- 
ulations reported below we used a different error function which gives 
better performance: 
The error defined in equation 1.3 is simply the negative log probability 
of generating the desired output vector under the mixture of gaussians 
model described at the end of the next section. To see why this error 
function works better, it is helpful to compare the derivatives of the two 
error functions with respect to the output of an expert. From equation 1.2 
we get 
while from equation 1.3 we get 
(1.4) 
(1.5) 
In equation 1.4 the term 11; is used to weight the derivative for expert i. 
In equation 1.5 we use a weighting term that takes into account how well 
expert i does relative to other experts. This is a more useful measure of 
the relevance of expert i to training case c, especially early in the train- 
ing. Suppose, for example, that the gating network initially gives equal 
weights to all experts and lid' - of11 > 1 for all the experts. Equation 1.4 
will adapt the best-fitting expert the slowest, whereas equation 1.5 will 
adapt it the fastest. 
2 Making Competitive Learning Associative 
It is natural to think that the "data" vectors on which a competitive net- 
work is trained play a role similar to the input vectors of an associative 
network that maps input vectors to output vectors. This correspondence 
is assumed in models that use competitive learning as a preprocessing 
stage within an associative network (Moody and Darken 1989). A quite 
different view is that the data vectors used in competitive learning cor- 
respond to the output vectors of an associative network. The competitive 
network can then be viewed as an inputless stochastic generator of output 
vectors and competitive learning can be viewed as a procedure for mak- 
ing the network generate output vectors with a distribution that matches 
the distribution of the "data" vectors. The weight vector of each com- 
petitive hidden unit represents the mean of a multidimensional gaussian 


--- Page 6 ---

Adaptive Mixtures of Local Experts 
83 
distribution, and output vectors are generated by first picking a hidden 
unit and then picking an output vector from the gaussian distribution 
determined by the weight vector of the chosen hidden unit. The log 
probability of generating any particular output vector 0'' is then 
where I is an index over the hidden units, pL is the "weight" vector of 
the hidden unit, k is a normalizing constant, and p ,  is the probability of 
picking hidden unit i, so the pz are constrained to sum to 1. In the statis- 
tics literature (McLachlan and Basford 1988), the p ,  are called "mixing 
proportions." 
"Soft" competitive learning modifies the weights (and also the vari- 
ances and the mixing proportions) so as to increase the product of the 
probabilities (i.e., the likelihood) of generating the output vectors in the 
training set (Nowlan 1990a). "Hard" competitive learning is a simple 
approximation to soft competitive learning in which we ignore the pos- 
sibility that a data vector could be generated by several different hidden 
units. Instead, we assume that it must be generated by the hidden unit 
with the closest weight vector, so only this weight vector needs to be 
modified to increase the probability of generating the data vector. 
If we view a competitive network as generating output vectors, it is 
not immediately obvious what role input vectors could play. However, 
competitive learning can be generalized in much the same way as Barto 
(1985) generalized learning automata by adding an input vector and mak- 
ing the actions of the automaton be conditional on the input vector. We 
replace each hidden unit in a competitive network by an entire expert 
network whose output vector specifies the mean of a multidimensional 
gaussian distribution. So the means are now a function of the current 
input vector and are represented by activity levels rather than weights. 
In addition, we use a gating network which allows the mixing propor- 
tions of the experts to be determined by the input vector. This gives us 
a system of competing local experts with the error function defined in 
equation 1.3. We could also introduce a mechanism to allow the input 
vector to dynamically determine the covariance matrix for the distribu- 
tion defined by each expert network, but we have not yet experimented 
with this possibility. 
3 Application to Multispeaker Vowel Recognition 
The mixture of experts model was evaluated on a speaker independent, 
four-class, vowel discrimination problem (Nowlan 1990b). The data con- 
sisted of the first and second formants of the vowels [i], [I], [a], and [A1 
(usually denoted [A]) from 75 speakers (males, females, and children) ut- 
tered in a hVd context (Peterson and Barney 1952). The data forms two 


--- Page 7 ---

84 
Robert A. Jacobs et al. 
'T 
/ 
I 
3.2 
2 . 4  
1.6 
0 . 8  
I 
0.3 
0.6 
0.9 
1.2 
1.5 
Figure 2: Data for vowel discrimination problem, and expert and gating net- 
work decision lines. The horizontal axis is the first formant value, and the 
vertical axis is the second formant value (the formant values have been lin- 
early scaled by dividing by a factor of 1000). Each example is labeled with its 
corresponding vowel symbol. Vowels [il and [I] form one overlapping pair of 
classes, vowels [a] and [A] form the other pair. The lines labeled Net 0, 1, and 2 
represent the decision lines for 3 expert networks. On one side of these lines the 
output of the corresponding expert is less than 0.5, on the other side the output 
is greater than 0.5. Although the mixture in this case contained 4 experts, one 
of these experts made no significant contribution to the final mixture since its 
mixing proportion p, was effectively 0 for all cases. The line labeled Gate 0:2 in- 
dicates the decision between expert 0 and expert 2 made by the gating network. 
To the left of this line pz > PO, to the right of this line po > pz. The boundary 
between classes [a] and [A] is formed by the combination of the left part of Net 
2's decision line and the right part of Net 0's decision line. Although the system 
tends to use as few experts as it can to solve a problem, it is also sensitive to 
specific problem features such as the slightly curved boundary between classes 
[a1 and [A]. 
pairs of overlapping classes, and different experts learn to concentrate 
on one pair of classes or the other (Fig. 2). 
We compared standard backpropagation networks containing a sin- 
gle hidden layer of 6 or 12 units with mixtures of 4 or 8 very simple 
experts. The architecture of each expert was restricted so it could form 
only a linear decision surface, which is defined as the set of input vec- 
tors for which the expert gives an output of exactly 0.5. All models were 
trained with data from the first 50 speakers and tested with data from 
the remaining 25 speakers. The small number of parameters for each ex- 
pert allows excellent generalization performance (Table l), and permits 


--- Page 8 ---

Adaptive Mixtures of Local Experts 
85 
Average number 
System 
Train % correct Test % correct 
of epochs 
SD 
4 Experts 
88 
90 
1124 
23 
BP 6 Hid 
88 
90 
2209 
83 
BP 12 Hid 
88 
90 
2435 
124 
8 Experts 
88 
90 
1083 
12 
Table 1: Summary of Performance on Vowel Discrimination Task. Results are 
based on 25 simulations for each of the alternative models. The first column of 
the table indicates the system simulated. The second column gives the percent 
of training cases classified correctly by the final set of weights, while the third 
column indicates the percent of testing cases classified correctly. The last two 
columns contain the average number of epochs required to reach the error 
criterion, and the standard deviation of the distribution of convergence times. 
Although the squared error was used to decide when to stop training, the 
criterion for correct performance is based on a weighted average of the outputs 
of all the experts. Each expert assigns a probability distribution over the classes 
and these distributions are combined using proportions given by the gating 
network. The most probable class is then taken to be the response of the system. 
The identical performance of all the systems is due to the fact that, with this 
data set, the set of misclassified examples is not sensitive to small changes in 
the decision surfaces. Also, the test set is easier than the training set. 
a graphic representation of the process of task decomposition (Figure 3). 
The number of hidden units in the backpropagation networks was cho- 
sen to give roughly equal numbers of parameters for the backpropagation 
networks and mixture models. All simulations were performed using a 
simple gradient descent algorithm with fixed step size t. To simplify 
the comparisons, no momentum or other acceleration techniques were 
used. The value of f for each system was chosen by performing a lim- 
ited exploration of the convergence from the same initial conditions for 
a range of t. Batch training was used with one weight update for each 
pass through the training set (epoch). Each system was trained until an 
average squared error of 0.08 over the training set was obtained. 
The mixtures of experts reach the error criterion significantly faster 
than the backpropagation networks ( p  >> 0.9991, requiring only about half 
as many epochs on average (Table 1). The learning time for the mixture 
model also scales well as the number of experts is increased: The mixture 
of 8 experts has a small, but statistically significant (11 > 0.951, advantage 
in the average number of epochs required to reach the error criterion. 
In contrast, the 12 hidden unit backpropagation network requires more 
epochs (11 > 0.95) to reach the error criterion than the network with 6 


--- Page 9 ---

86 
Robert A. Jacobs et al. 
-o’21 
-0.4 
0 
- 0 . 6  
-0.84 
-0.56 
-0.28 
0 
0 . 2 8  
Figure 3: The trajectories of the decision lines of some experts during one 
simulation. The horizontal axis is the first formant value, and the vertical axis 
is the second formant value. Each trajectory is represented by a sequence of 
dots, one per epoch, each dot marking the intersection of the expert’s decision 
line and the normal to that line passing through the origin. For clarity, only 5 
of the 8 experts are shown and the number of the expert is shown at the start 
of the trajectory. The point labeled TO indicates the optimal decision line for 
a single expert trained to discriminate [i] from [I]. Similarly, T 1  represents the 
optimal decision line to discriminate [a] from [A]. The point labeled X is the 
decision line learned by a single expert trained with data from all 4 classes, and 
represents a type of average solution. 
hidden units (Table 1). All statistical comparisons are based on a t test 
with 48 degrees of freedom and a pooled variance estimator. 
Figure 3 shows how the decision lines of different experts move 
around as the system learns to allocate pieces of the task to different 
experts. The system begins in an unbiased state, with the gating net- 
work assigning equal mixing proportions to all experts in all cases. As 
a result, each expert tends to get errors from roughly equal numbers of 
cases in all 4 classes, and all experts head towards the point X, which 
represents the optimal decision line for an expert that must deal with 
all the cases. Once one or more experts begin to receive more error 
from cases in one class pair than the other, this symmetry is broken and 
the trajectories begin to diverge as different experts concentrate on one 
class pair or the other. In this simulation, expert 5 learns to concentrate 
on discriminating classes [i] and [I] so its decision line approaches the 
optimal line for this discrimination (TO). Experts 4 and 6 both concentrate 
on discriminating classes [a] and [A], so their trajectories approach the 


--- Page 10 ---

Adaptive Mixtures of Local Experts 
87 
optimal single line (Tl) and then split to form a piecewise linear approx- 
imation to the slightly curved optimal decision surface (see Fig. 2). Only 
experts 4, 5, and 6 are active in the final mixture. This solution is typical 
- 
in all simulations with mixtures of 4 or 8 experts all but 2 or 3 experts 
had mixing proportions that were effectively 0 for all cases. 
Acknowledgments 
Jordan and Jacobs were funded by grants from Siemens and the McDon- 
nell-Pew program in Cognitive Neuroscience. Hinton and Nowlan were 
funded by grants from the Ontario Information Technology Research 
Center and the Canadian Natural Science and Engineering Research Coun- 
cil. Hinton is a fellow of the Canadian Institute for Advanced Research. 
References 
Barto, A. G. 1985. Learning by statistical cooperation of self-interested neuron- 
like computing elements. Human Neurobiol. 4, 229-256. 
Hampshire, J., and Waibel, A. 1989. The meta-pi network: Building distributed 
knowledge representations for robust pattern recognition. Tech. Rep. CMU-CS- 
89-166, Carnegie Mellon University, Pittsburgh, PA. 
Jacobs, R. A., and Jordan, M. I. 1991. Learning piecewise control strategies in a 
modular connectionist architecture, in preparation. 
Jacobs, R. A., Jordan, M. I., and Barto, A. G. 1991. Task decomposition through 
competition in a modular connectionist architecture: The what and where 
vision tasks. Cog. Sci., in press. 
McLachlan, G. J., and Basford, K. E. 1988. Mixture Models: Znference and Appli- 
cations to Clustering. Marcel Dekker, New York. 
Moody, J., and Darken, C. 1989. Fast learning in networks of locally-tuned 
processing units. Neural Comp. 1(2), 281-294. 
Nowlan, S. J. 1990a. Maximum likelihood competitive learning. In Advances 
in Neural lnformution Processing System 2, D. S. Touretzky, ed., pp. 574-582. 
Morgan Kaufmann, San Mateo, CA. 
Nowlan, S. J. 1990b. Competing experts: A n  experimental investigation of associative 
mixture models. Tech. Rep. CRG-TR-90-5, University of Toronto, Toronto, 
Canada. 
Peterson, G. E., and Barney, H. L. 1952. Control methods used in a study of the 
vowels. I. Acoust. SOC. Am. 24, 175-184. 
Received 27 July 1990; accepted 1 November 90. 


--- Page 11 ---

This article has been cited by:
1. Matthew M. Walsh, John R. Anderson. 2013. Electrophysiological Responses
to Feedback during the Application of Abstract Rules. Journal of Cognitive
Neuroscience 25:11, 1986-2002. [Abstract] [Full Text] [PDF] [PDF Plus]
2. Bruno Damas, José Santos-Victor. 2013. Online Learning of Single- and
Multivalued Functions with an Infinite Mixture of Linear Experts. Neural
Computation 25:11, 3044-3091. [Abstract] [Full Text] [PDF] [PDF Plus]
3. Qingguo Tang, Rohana J. Karunamuni. 2013. Minimum distance estimation in
a finite mixture regression model. Journal of Multivariate Analysis 120, 185-204.
[CrossRef]
4. Filipe Rodrigues, Francisco Pereira, Bernardete Ribeiro. 2013. Learning from
multiple annotators: Distinguishing good from random labelers. Pattern
Recognition Letters 34:12, 1428-1436. [CrossRef]
5. Saeed Reza Kheradpisheh, Fatemeh Behjati-Ardakani, Reza Ebrahimpour. 2013.
Combining classifiers using nearest decision prototypes. Applied Soft Computing .
[CrossRef]
6. Mansour Sheikhan, Amir Ali Sha’bani. 2013. PSO-optimized modular neural
network trained by OWO-HWO algorithm for fault location in analog circuits.
Neural Computing and Applications 23:2, 519-530. [CrossRef]
7. Young-Seol Lee, Sung-Bae Cho. 2013. Activity recognition with android
phone using mixture-of-experts co-trained with labeled and unlabeled data.
Neurocomputing . [CrossRef]
8. Rahul Kala, Anupam Shukla, Ritu Tiwari. 2013. Breast Cancer Diagnosis Using
Optimized Attribute Division in Modular Neural Networks. Journal of Information
Technology Research 4:1, 34-47. [CrossRef]
9. I. A. Kruglov, O. A. Mishulina. 2013. Neural network modeling of vector
multivariable functions in ill-posed approximation problems. Journal of Computer
and Systems Sciences International 52:4, 503-518. [CrossRef]
10. Steven J. SimskeIntroduction and Overview 1-41. [CrossRef]
11. Subrajeet Mohapatra, Dipti Patra, Sanghamitra Satpathy. 2013. An ensemble
classifier system for early diagnosis of acute lymphoblastic leukemia in blood
microscopic images. Neural Computing and Applications . [CrossRef]
12. Bogusław CyganekObject Recognition 408-486. [CrossRef]
13. Heeyoul Choi, Seungjin Choi, Yoonsuck Choe. 2013. Parameter Learning for
Alpha Integration. Neural Computation 25:6, 1585-1604. [Abstract] [Full Text]
[PDF] [PDF Plus]
14. Abbas Khalili, Shili Lin. 2013. Regularization in Finite Mixture of Regression
Models with Diverging Number of Parameters. Biometrics 69:2, 436-446.
[CrossRef]


--- Page 12 ---

15. David J. Nott, Lucy Marshall, Mark Fielding, Shie-Yui Liong. 2013. Mixtures
of experts for understanding model discrepancy in dynamic computer models.
Computational Statistics & Data Analysis . [CrossRef]
16. J I Park, N Kim, M K Jeong, K S Shin. 2013. Multiphase support vector regression
for function approximation with break-points. Journal of the Operational Research
Society 64:5, 775-785. [CrossRef]
17. Mehrdad Javadi, Seyed Ali Asghar Abbaszadeh Arani, Atena Sajedin, Reza
Ebrahimpour. 2013. Classification of ECG arrhythmia by a modular neural network
based on Mixture of Experts and Negatively Correlated Learning. Biomedical Signal
Processing and Control 8:3, 289-296. [CrossRef]
18. Davy Sannen, Hendrik Brussel. 2013. The active grading ensemble framework
for learning visual quality inspection from multiple humans. Pattern Analysis and
Applications 16:2, 223-234. [CrossRef]
19. RASTISLAV J. R. STRUHARIK, LADISLAV A. NOVAK. 2013. HARDWARE
IMPLEMENTATION OF DECISION TREE ENSEMBLES. Journal of Circuits,
Systems and Computers 1350032. [CrossRef]
20. Michał Woźniak, Manuel Graña, Emilio Corchado. 2013. A survey of multiple
classifier systems as hybrid systems. Information Fusion . [CrossRef]
21. Jeff A. Tracey, Jun Zhu, Erin Boydston, Lisa Lyren, Robert N. Fisher, Kevin
R. Crooks. 2013. Mapping behavioral landscapes for animal movement: a finite
mixture modeling approach. Ecological Applications 23:3, 654-669. [CrossRef]
22. Rahul Kala, Anupam Shukla, Ritu TiwariBreast Cancer Diagnosis Using
Optimized Attribute Division in Modular Neural Networks 34-47. [CrossRef]
23. P. Hall, Y. Xia, J.-H. Xue. 2013. Simple tiered classifiers. Biometrika . [CrossRef]
24. Tarek Helmy, S. M. Rahman, Muhammad Imtiaz Hossain, Abdulaziz
Abdelraheem. 2013. Non-linear Heterogeneous Ensemble Model for Permeability
Prediction of Oil Reservoirs. Arabian Journal for Science and Engineering .
[CrossRef]
25. Mehmet Gönen, Ethem Alpaydın. 2013. Localized algorithms for multiple kernel
learning. Pattern Recognition 46:3, 795-807. [CrossRef]
26. Andriy Norets, Satoru Takahashi. 2013. On the surjectivity of the mapping
between utilities and choice probabilities. Quantitative Economics 4:1, 149-155.
[CrossRef]
27. Konrad Jackowski. 2013. Fixed-size ensemble classifier system evolutionarily
adapted to a recurring context with an unlimited pool of classifiers. Pattern Analysis
and Applications . [CrossRef]
28. Haibo He, Yuan Cao, Yi Cao, Jinyu Wen. 2013. Ensemble learning for wind profile
prediction with missing values. Neural Computing and Applications 22:2, 287-294.
[CrossRef]


--- Page 13 ---

29. Zhigang Liu, Wenfan Li, Wanlu Sun. 2013. A novel method of short-term load
forecasting based on multiwavelet transform and multiple neural networks. Neural
Computing and Applications 22:2, 271-277. [CrossRef]
30. Yanan Fan, David J. Nott, Scott A. Sisson. 2013. Approximate Bayesian
computation via regression density estimation. Stat 2:1, 34-48. [CrossRef]
31. Siew Li Tan, David J. Nott. 2013. Variational approximation for mixtures of linear
mixed models. Journal of Computational and Graphical Statistics 130123061318003.
[CrossRef]
32. Reza Ebrahimpour, Seyed Ali Asghar Abbaszadeh Arani, Saeed Masoudnia. 2013.
Improving combination method of NCL experts using gating network. Neural
Computing and Applications 22:1, 95-101. [CrossRef]
33. Rodrigo Arnaldo Scarpel. 2013. Forecasting air passengers at São Paulo
International Airport using a mixture of local experts model. Journal of Air
Transport Management 26, 35-39. [CrossRef]
34. Adam Lammert, Louis Goldstein, Shrikanth Narayanan, Khalil Iskarous. 2013.
Statistical methods for estimation of direct and differential kinematics of the vocal
tract. Speech Communication 55:1, 147-161. [CrossRef]
35. Sungmin Myoung. 2013. Modified Mixture of Experts for the Diagnosis of
Perfusion Magnetic Resonance Imaging Measures in Locally Rectal Cancer
Patients. Healthcare Informatics Research 19:2, 130. [CrossRef]
36. Eri Itoh, Shinji Suzuki. 2013. Evaluation on novel architecture for harmonizing
manual and automatic flight controls under atmospheric turbulence. Aerospace
Science and Technology 24:1, 241-254. [CrossRef]
37. Li-Jie Zhao, Tian-You Chai, De-Cheng Yuan. 2012. Selective ensemble extreme
learning machine modeling of effluent quality in wastewater treatment plants.
International Journal of Automation and Computing 9:6, 627-633. [CrossRef]
38. Mattias Villani, Robert Kohn, David J. Nott. 2012. Generalized smooth finite
mixtures. Journal of Econometrics 171:2, 121-133. [CrossRef]
39. Michael K. Pitt, Ralph dos Santos Silva, Paolo Giordani, Robert Kohn. 2012. On
some properties of Markov chain Monte Carlo simulation methods based on the
particle filter. Journal of Econometrics 171:2, 134-151. [CrossRef]
40. Patrick P.K. Chan, Daniel S. Yeung, Wing W.Y. Ng, Chih Min Lin, James N.K.
Liu. 2012. Dynamic fusion method using Localized Generalization Error Model.
Information Sciences 217, 1-20. [CrossRef]
41. Eduardo F. Mendes, Wenxin Jiang. 2012. On Convergence Rates of Mixtures of
Polynomial Experts. Neural Computation 24:11, 3025-3051. [Abstract] [Full Text]
[PDF] [PDF Plus]
42. Saeed Masoudnia, Reza Ebrahimpour, Seyed Ali Asghar Abbaszadeh Arani. 2012.
Combining features of negative correlation learning with mixture of experts in
proposed ensemble methods. Applied Soft Computing 12:11, 3539-3551. [CrossRef]


--- Page 14 ---

43. Rodrigo C. Barros, Márcio P. Basgalupp, André C. P. L. F. Carvalho, Marcos G.
Quiles. 2012. Clus-DTI: improving decision-tree classification with a clustering-
based decision-tree induction algorithm. Journal of the Brazilian Computer Society
18:4, 351-362. [CrossRef]
44. Tim Shallice, Richard P. Cooper. 2012. The Organisation of Mind: Response to
commentators. Cortex 48:10, 1383-1387. [CrossRef]
45. Jae Joon Ahn, Young Min Kim, Keunje Yoo, Joonhong Park, Kyong Joo Oh.
2012. Using GA-Ridge regression to select hydro-geological parameters influencing
groundwater pollution vulnerability. Environmental Monitoring and Assessment
184:11, 6637-6645. [CrossRef]
46. Marek Kurzynski, Michal Wozniak. 2012. Combining classifiers under probabilistic
models: experimental comparative analysis of methods. Expert Systems 29:4,
374-393. [CrossRef]
47. J. F. De Paz, M. Navarro, C. I. Pinzon, V. Julian, D. I. Tapia, J. Bajo. 2012.
Mathematical model for a temporal-bounded classifier in security environments.
Logic Journal of IGPL 20:4, 712-721. [CrossRef]
48. Saeed Masoudnia, Reza Ebrahimpour, Seyed Ali Asghar Abbaszadeh Arani. 2012.
Incorporation of a Regularization Term to Control Negative Correlation in Mixture
of Experts. Neural Processing Letters 36:1, 31-47. [CrossRef]
49. Hamid Shayegh Boroujeni, Nasrollah Moghadam Charkari. 2012. Robust moving
shadow detection with hierarchical mixture of MLP experts. Signal, Image and
Video Processing . [CrossRef]
50. Reza Ebrahimpour, Naser Sadeghnejad, Atena Sajedin, Nima Mohammadi.
2012. Electrocardiogram beat classification via coupled boosting by filtering and
preloaded mixture of experts. Neural Computing and Applications . [CrossRef]
51. David J. Nott, Siew Li Tan, Mattias Villani, Robert Kohn. 2012. Regression
Density Estimation With Variational Methods and Stochastic Approximation.
Journal of Computational and Graphical Statistics 21:3, 797-820. [CrossRef]
52. B. VermaNeural Network Based Classifier Ensembles 229-239. [CrossRef]
53. CHUANYU SUN, XIAO-LIN WU, KENT A. WEIGEL, GUILHERME J.
M. ROSA, STEWART BAUCK, BRENT W. WOODWARD, ROBERT D.
SCHNABEL, JEREMY F. TAYLOR, DANIEL GIANOLA. 2012. An ensemble-
based approach to imputation of moderate-density genotypes for genomic selection
with application to Angus cattle. Genetics Research 94:03, 133-150. [CrossRef]
54. Andriy Norets, Justinas Pelenis. 2012. Bayesian modeling of joint and conditional
distributions. Journal of Econometrics 168:2, 332-346. [CrossRef]
55. Kristine Monteith, Tony Martinez. 2012. AGGREGATE CERTAINTY
ESTIMATORS. Computational Intelligence no-no. [CrossRef]
56. Saeed Masoudnia, Reza Ebrahimpour. 2012. Mixture of experts: a literature survey.
Artificial Intelligence Review . [CrossRef]


--- Page 15 ---

57. Pejman Tahmasebi, Ardeshir Hezarkhani. 2012. A fast and independent
architecture of artificial neural network for permeability prediction. Journal of
Petroleum Science and Engineering 86-87, 118-126. [CrossRef]
58. Maya Wardeh, Frans Coenen, Trevor Bench Capon. 2012. PISA: A framework for
multiagent classification using argumentation. Data & Knowledge Engineering 75,
34-57. [CrossRef]
59. Reza Ebrahimpour, Naser Sadeghnejad, Seyed Ali Asghar Abbaszadeh Arani, Nima
Mohammadi. 2012. Boost-wise pre-loaded mixture of experts for classification
tasks. Neural Computing and Applications . [CrossRef]
60. MATTEO RE, GIORGIO VALENTINIEnsemble Methods 20124949, .
[CrossRef]
61. M. J. Frank, D. Badre. 2012. Mechanisms of Hierarchical Reinforcement Learning
in Corticostriatal Circuits 1: Computational Analysis. Cerebral Cortex 22:3,
509-526. [CrossRef]
62. Charles Kemp, Patrick Shafto, Joshua B. Tenenbaum. 2012. An integrated account
of generalization across objects and features. Cognitive Psychology 64:1-2, 35-73.
[CrossRef]
63. Ricardo Ñanculef, Carlos Valle, Héctor Allende, Claudio Moraga. 2012.
Training Regression Ensembles by Sequential Target Correction and Resampling.
Information Sciences . [CrossRef]
64. Nima Hatami. 2012. Thinned-ECOC ensemble based on sequential code
shrinking. Expert Systems with Applications 39:1, 936-947. [CrossRef]
65. Xiuqin Bai, Weixin Yao, John E. Boyer. 2012. Robust fitting of mixture regression
models. Computational Statistics & Data Analysis . [CrossRef]
66. Souhaib Ben Taieb, Gianluca Bontempi, Amir F. Atiya, Antti Sorjamaa. 2012. A
review and comparison of strategies for multi-step ahead time series forecasting
based on the NN5 forecasting competition. Expert Systems with Applications .
[CrossRef]
67. Sungmin Myoung, Ji Hong Chang, Kijun Song. 2012. A Mixture of Experts Model
for the Diagnosis of Liver Cirrhosis by Measuring the Liver Stiffness. Healthcare
Informatics Research 18:1, 29. [CrossRef]
68. P. Giordani, X. Mun, R. Kohn. 2012. Efficient Estimation of Covariance Matrices
using Posterior Mode Multiple Shrinkage. Journal of Financial Econometrics 11:1,
154-192. [CrossRef]
69. Scott Doyle, Michael D Feldman, Natalie Shih, John Tomaszewski, Anant
Madabhushi. 2012. Cascaded discrimination of normal, abnormal, and confounder
classes in histopathology: Gleason grading of prostate cancer. BMC Bioinformatics
13:1, 282. [CrossRef]
70. Anil K. Ghosh, Fred Godtliebsen. 2011. On hybrid classification using model
assisted posterior estimates. Pattern Recognition . [CrossRef]


--- Page 16 ---

71. Jong-Won Yoon, Sung-Ihk Yang, Sung-Bae Cho. 2011. Adaptive mixture-of-
experts models for data glove interface with multiple users. Expert Systems with
Applications . [CrossRef]
72. Andrew R. Webb, Keith D. CopseyReferences 591-636. [CrossRef]
73. Tomasz Woloszynski, Marek Kurzynski. 2011. A probabilistic model of classifier
competence for dynamic ensemble selection. Pattern Recognition 44:10-11,
2656-2668. [CrossRef]
74. Ahmad Banakar, Mohammad Fazle Azeem. 2011. Local recurrent sigmoidal–
wavelet neurons in feed-forward neural network for forecasting of dynamic systems:
Theory. Applied Soft Computing . [CrossRef]
75. Cristian I. Pinzón, Juan F. De Paz, Martí Navarro, Javier Bajo, Vicente Julián, Juan.
M. Corchado. 2011. Real-time CBR-agent with a mixture of experts in the reuse
stage to classify and detect DoS attacks. Applied Soft Computing 11:7, 4384-4398.
[CrossRef]
76. SULTAN UDDIN AHMED, MD. SHAHJAHAN, KAZUYUKI MURASE.
2011. A LEMPEL-ZIV COMPLEXITY-BASED NEURAL NETWORK
PRUNING ALGORITHM. International Journal of Neural Systems 21:05,
427-441. [CrossRef]
77. Yunwu Xiong, Rony Wallach, Alex Furman. 2011. Modeling multidimensional flow
in wettable and water-repellent soils using artificial neural networks. Journal of
Hydrology . [CrossRef]
78. Yan Yang, Jinwen Ma. 2011. Asymptotic Convergence Properties of the EM
Algorithm for Mixture of Experts. Neural Computation 23:8, 2140-2168.
[Abstract] [Full Text] [PDF] [PDF Plus] [Supplementary Content]
79. Iffat A. Gheyas, Leslie S. Smith. 2011. A novel neural network ensemble
architecture for time series forecasting. Neurocomputing . [CrossRef]
80. Shuhaida Ismail, Ani Shabri, Ruhaidah Samsudin. 2011. A hybrid model of
self-organizing maps (SOM) and least square support vector machine (LSSVM)
for time-series forecasting. Expert Systems with Applications 38:8, 10574-10578.
[CrossRef]
81. Lei XuLearning Algorithms for RBF Functions and Subspace Based Functions
1034-1065. [CrossRef]
82. Lei Xu, Shun-ichi AmariCombining Classifiers and Learning Mixture-of-Experts
243-252. [CrossRef]
83. Pramod P. Nair. 2011. A multigradient algorithm using a mixture of experts
architecture for land cover classification of multisensor images. International
Journal of Remote Sensing 1-9. [CrossRef]
84. Bruno Baruque, Santiago Porras, Emilio Corchado. 2011. Hybrid Classification
Ensemble Using Topology-preserving Clustering. New Generation Computing
29:3, 329-344. [CrossRef]


--- Page 17 ---

85. Ivette Luna, Rosangela Ballini. 2011. Top-down strategies based on adaptive
fuzzy rule-based systems for daily time series forecasting. International Journal of
Forecasting 27:3, 708-724. [CrossRef]
86. Sotirios P. Chatzis, Yiannis Demiris. 2011. The copula echo state network. Pattern
Recognition . [CrossRef]
87. Reza Ebrahimpour, Hossein Nikoo, Saeed Masoudnia, Mohammad Reza Yousefi,
Mohammad Sajjad Ghaemi. 2011. Mixture of MLP-experts for trend forecasting
of time series: A case study of the Tehran stock exchange. International Journal of
Forecasting 27:3, 804-816. [CrossRef]
88. Mehdi Salkhordeh Haghighi, Abedin Vahedian, Hadi Sadoghi Yazdi. 2011.
Extended decision template presentation for combining classifiers. Expert Systems
with Applications 38:7, 8414-8418. [CrossRef]
89. Dong-Hun Seo, Won-Don Lee. 2011. A New Ensemble System using Dynamic
Weighting Method. The Journal of the Korean Institute of Information and
Communication Engineering 15:6, 1213-1220. [CrossRef]
90. Haibo HeEnsemble Learning 108-139. [CrossRef]
91. David Martínez-Rego, Oscar Fontenla-Romero, Amparo Alonso-Betanzos. 2011.
Efficiency of local models ensembles for time series prediction. Expert Systems with
Applications 38:6, 6884-6894. [CrossRef]
92. Shiliang Sun, Xin Xu. 2011. Variational Inference for Infinite Mixtures of Gaussian
Processes With Applications to Traffic Flow Prediction. IEEE Transactions on
Intelligent Transportation Systems 12:2, 466-475. [CrossRef]
93. Martina Sattlecker, Rebecca Baker, Nick Stone, Conrad Bessant. 2011. Support
vector machine ensembles for breast cancer type prediction from mid-FTIR micro-
calcification spectra. Chemometrics and Intelligent Laboratory Systems . [CrossRef]
94. David P. Williams. 2011. Label Alteration to Improve Underwater Mine
Classification. IEEE Geoscience and Remote Sensing Letters 8:3, 488-492. [CrossRef]
95. Mahdi Tabassian, Reza Ghaderi, Reza Ebrahimpour. 2011. Knitted fabric defect
classification for uncertain labels based on Dempster–Shafer theory of evidence.
Expert Systems with Applications 38:5, 5259-5267. [CrossRef]
96. Isobel Claire Gormley, Thomas Brendan MurphyMixture of Experts Modelling
with Social Science Applications 101-121. [CrossRef]
97. Feng Li, Mattias Villani, Robert KohnModelling Conditional Densities Using
Finite Smooth Mixtures 123-144. [CrossRef]
98. Duy Nguyen-Tuong, Jan Peters. 2011. Model learning for robot control: a survey.
Cognitive Processing . [CrossRef]
99. Xu Zhao, Yun Fu, Yuncai Liu. 2011. Human Motion Tracking by Temporal-
Spatial Local Gaussian Process Experts. IEEE Transactions on Image Processing
20:4, 1141-1151. [CrossRef]


--- Page 18 ---

100. Chen Wu. 2011. Deriving collective intelligence from reviews on the social Web
using a supervised learning approach. Expert Systems with Applications . [CrossRef]
101. Parisa Rashidi, Diane J. Cook. 2011. Activity knowledge transfer in smart
environments. Pervasive and Mobile Computing . [CrossRef]
102. Rainer Schlittgen. 2011. A weighted least-squares approach to clusterwise
regression. AStA Advances in Statistical Analysis . [CrossRef]
103. Lei Xu, Yanda Li. 2011. Machine learning and intelligence science: Sino-foreign
interchange workshop IScIDE2010 (A). Frontiers of Electrical and Electronic
Engineering in China 6:1, 1-5. [CrossRef]
104. Y. Sun, M.B. Brown, M. Prapopoulou, N. Davey, R.G. Adams, G.P. Moss. 2011.
The application of stochastic machine learning methods in the prediction of skin
penetration. Applied Soft Computing 11:2, 2367-2375. [CrossRef]
105. Davy Sannen, Hendrik Van Brussel. 2011. A Multilevel Information Fusion
Approach for Visual Quality Inspection. Information Fusion . [CrossRef]
106. Abdelhamid Bouchachia. 2011. Incremental learning with multi-level adaptation.
Neurocomputing . [CrossRef]
107. Sally Wood. 2011. Bayesian Mixtures of Autoregressive Models. Journal of
Computational and Graphical Statistics 1-22. [CrossRef]
108. M. Olteanu, J. Rynkiewicz. 2011. Asymptotic properties of mixture-of-experts
models. Neurocomputing . [CrossRef]
109. A. Wefky, F. Espinosa, A. Prieto, J.J. Garcia, C. Barrios. 2011. Comparison of
neural classifiers for vehicles gear estimation. Applied Soft Computing . [CrossRef]
110. Phaedon-Stelios Koutsourelakis, Elias Bilionis. 2011. Scalable Bayesian Reduced-
Order Models for Simulating High-Dimensional Multiscale Dynamical Systems.
Multiscale Modeling & Simulation 9:1, 449-485. [CrossRef]
111. Georgios Kalantzis, Luis A. Vasquez-Quino, Travis Zalman, Guillem Pratx, Yu
Lei. 2011. Toward IMRT 2D dose modeling using artificial neural networks: A
feasibility study. Medical Physics 38:10, 5807. [CrossRef]
112. Esma Kilic, Ethem Alpaydin. 2011. Learning the areas of expertise of classifiers in
an ensemble. Procedia Computer Science 3, 74-82. [CrossRef]
113. Ahmad Banakar. 2011. Lyapunov Stability Analysis of Gradient Descent-
Learning Algorithm in Network Training. ISRN Applied Mathematics 2011, 1-12.
[CrossRef]
114. R. A. Mat Noor, Z. Ahmad, M. Mat Don, M. H. Uzir. 2010. Modelling
and control of different types of polymerization processes using neural networks
technique: A review. The Canadian Journal of Chemical Engineering 88:6,
1065-1084. [CrossRef]
115. Zhouyu Fu, A Robles-Kelly, Jun Zhou. 2010. Mixing Linear SVMs for
Nonlinear Classification. IEEE Transactions on Neural Networks 21:12, 1963-1975.
[CrossRef]


--- Page 19 ---

116. Abbas Khalili. 2010. New estimation and feature selection methods in mixture-of-
experts models. Canadian Journal of Statistics 38:4, 519-539. [CrossRef]
117. Salah Bouktif, Faheem Ahmed, Issa Khalil, Giuliano Antoniol. 2010. A novel
composite model approach to improve software quality prediction. Information and
Software Technology 52:12, 1298-1311. [CrossRef]
118. Chrisantha Fernando, Richard Goldstein, Eörs Szathmáry. 2010. The Neuronal
Replicator Hypothesis. Neural Computation 22:11, 2809-2857. [Abstract] [Full
Text] [PDF] [PDF Plus] [Supplementary Content]
119. Luís M. Silva, J. Marques de Sá, Luís A. Alexandre. 2010. The MEE Principle
in Data Classification: A Perceptron-Based Analysis. Neural Computation 22:10,
2698-2728. [Abstract] [Full Text] [PDF] [PDF Plus]
120. D.S. Young, D.R. Hunter. 2010. Mixtures of regressions with predictor-dependent
mixing proportions. Computational Statistics & Data Analysis 54:10, 2253-2266.
[CrossRef]
121. José Luis Aznarte, José Manuel Benitez. 2010. Equivalences Between Neural-
Autoregressive Time Series Models and Fuzzy Systems. IEEE Transactions on
Neural Networks 21:9, 1434-1444. [CrossRef]
122. Francis Colas, Julien Diard, Pierre Bessière. 2010. Common Bayesian Models for
Common Cognitive Issues. Acta Biotheoretica 58:2-3, 191-216. [CrossRef]
123. Ladan Shams, Ulrik R. Beierholm. 2010. Causal inference in perception. Trends
in Cognitive Sciences 14:9, 425-432. [CrossRef]
124. Alexandre Carvalho, Georgios Skoulakis. 2010. Time Series Mixtures of
Generalized t Experts: ML Estimation and an Application to Stock Return Density
Forecasting. Econometric Reviews 29:5, 642-687. [CrossRef]
125. Michael C. Lee, Lilla Boroczky, Kivilcim Sungur-Stasik, Aaron D. Cann, Alain
C. Borczuk, Steven M. Kawut, Charles A. Powell. 2010. Computer-aided diagnosis
of pulmonary nodules using a two-step approach for feature selection and classifier
ensemble construction. Artificial Intelligence in Medicine 50:1, 43-53. [CrossRef]
126. Yasutake Takahashi, Yoshihiro Tamura, Minoru Asada, Mario Negrello. 2010.
Emulation and behavior understanding through shared values. Robotics and
Autonomous Systems 58:7, 855-865. [CrossRef]
127. Min Liu, Mingyu Dong, Cheng Wu. 2010. A New ANFIS for Parameter Prediction
With Numeric and Categorical Inputs. IEEE Transactions on Automation Science
and Engineering 7:3, 645-653. [CrossRef]
128. Jun Namikawa, Jun Tani. 2010. Learning to imitate stochastic time series in a
compositional way by chaos. Neural Networks 23:5, 625-638. [CrossRef]
129. Seyoung Kim, Padhraic Smyth, Hal Stern. 2010. A Bayesian Mixture Approach to
Modeling Spatial Activation Patterns in Multisite fMRI Data. IEEE Transactions
on Medical Imaging 29:6, 1260-1274. [CrossRef]


--- Page 20 ---

130. Salim Chitroub. 2010. Classifier combination and score level fusion: concepts and
practical aspects. International Journal of Image and Data Fusion 1:2, 113-135.
[CrossRef]
131. Elif Derya Übeyli, Konuralp Ilbay, Gul Ilbay, Deniz Sahin, Gur Akansel. 2010.
Differentiation of Two Subtypes of Adult Hydrocephalus by Mixture of Experts.
Journal of Medical Systems 34:3, 281-290. [CrossRef]
132. K. A. Le Cao, E. Meugnier, G. J. McLachlan. 2010. Integrative mixture of experts
to combine clinical factors and gene markers. Bioinformatics 26:9, 1192-1198.
[CrossRef]
133. Jean-Pierre Stockis, Jürgen Franke, Joseph Tadjuidje Kamgaing. 2010. On
geometric ergodicity of CHARME models. Journal of Time Series Analysis 31:3,
141-152. [CrossRef]
134. I. Zaier, C. Shu, T.B.M.J. Ouarda, O. Seidou, F. Chebana. 2010. Estimation of ice
thickness on lakes using artificial neural network ensembles. Journal of Hydrology
383:3-4, 330-340. [CrossRef]
135. J. McLean Sloughter. 2010. Probabilistic Wind Speed Forecasting Using
Ensembles and Bayesian Model Averaging. Journal of the American Statistical
Association 105:489, 25-35. [CrossRef]
136. József Fiser, Pietro Berkes, Gergő Orbán, Máté Lengyel. 2010. Statistically optimal
perception and learning: from behavior to neural representations. Trends in
Cognitive Sciences 14:3, 119-130. [CrossRef]
137. Chulsang Yoo, Jooyoung Park. 2010. A mixture-density-network based approach
for finding rating curves: Facing multi-modality and unbalanced data distribution.
KSCE Journal of Civil Engineering 14:2, 243-250. [CrossRef]
138. Shile Zhang, Bin Li, Xiangyang Xue. 2010. Semi-automatic dynamic auxiliary-
tag-aided image annotation☆. Pattern Recognition 43:2, 470-477. [CrossRef]
139. Rajib Nayak, James Gomes. 2010. Generalized hybrid control synthesis for affine
systems using sequential adaptive networks. Journal of Chemical Technology &
Biotechnology 85:1, 59-76. [CrossRef]
140. Isaac Martín Diego, Alberto Muñoz, Javier M. Moguerza. 2010. Methods for
the combination of kernel matrices within a support vector framework. Machine
Learning 78:1-2, 137-174. [CrossRef]
141. Petr Kadlec, Bogdan Gabrys. 2009. Architecture for development of adaptive on-
line prediction models. Memetic Computing 1:4, 241-269. [CrossRef]
142. Mattias Villani, Robert Kohn, Paolo Giordani. 2009. Regression density estimation
using smooth adaptive Gaussian mixtures. Journal of Econometrics 153:2, 155-173.
[CrossRef]
143. Reza Ebrahimpour, Ehsanollah Kabir, Mohammad Reza Yousefi. 2009. Improving
mixture of experts for view-independent face recognition using teacher-directed
learning. Machine Vision and Applications . [CrossRef]


--- Page 21 ---

144. Kyuwan Choi, Hideaki Hirose, Yoshio Sakurai, Toshio Iijima, Yasuharu Koike.
2009. Prediction of arm trajectory from the neural activities of the primary motor
cortex with modular connectionist architecture. Neural Networks 22:9, 1214-1223.
[CrossRef]
145. J. Bolton, P. Gader. 2009. Random Set Framework for Context-Based Classification
With Hyperspectral Imagery. IEEE Transactions on Geoscience and Remote Sensing
47:11, 3810-3821. [CrossRef]
146. RASTISLAV J. R. STRUHARIK, LADISLAV A. NOVAK. 2009. EVOLVING
DECISION TREES IN HARDWARE. Journal of Circuits, Systems and Computers
18:06, 1033-1060. [CrossRef]
147. Eri Itoh, Shinji Suzuki. 2009. Architecture for Harmonizing Manual and
Automatic Flight Controls. Journal of Aerospace Computing, Information, and
Communication 6:10, 553-569. [CrossRef]
148. Christian Plagemann, Sebastian Mischke, Sam Prentice, Kristian Kersting,
Nicholas Roy, Wolfram Burgard. 2009. A Bayesian regression approach to terrain
mapping and an application to legged robot locomotion. Journal of Field Robotics
26:10, 789-811. [CrossRef]
149. Clodoaldo A. M. Lima, André L. V. Coelho, Fernando J. Zuben. 2009. Pattern
classification with mixtures of weighted least-squares support vector machine
experts. Neural Computing and Applications 18:7, 843-860. [CrossRef]
150. Hsiu-Ting Yu. 2009. S. FRÜHWIRTH-SCHNATTER (2006) Finite Mixture
and Markov Switching Models. Psychometrika 74:3, 559-560. [CrossRef]
151. Paolo Soda, Giulio Iannello, Mario Vento. 2009. A multiple expert system for
classifying fluorescent intensity in antinuclear autoantibodies analysis. Pattern
Analysis and Applications 12:3, 215-226. [CrossRef]
152. Enso Ikonen, Kaddour Najim. 2009. Multiple Model-Based Control Using Finite
Controlled Markov Chains. Cognitive Computation 1:3, 234-243. [CrossRef]
153. Elif Derya Übeyli. 2009. Modified mixture of experts employing eigenvector
methods and Lyapunov exponents for analysis of electroencephalogram signals.
Expert Systems 26:4, 339-354. [CrossRef]
154. Elif Derya Übeyli. 2009. Modified Mixture of Experts for Diabetes Diagnosis.
Journal of Medical Systems 33:4, 299-305. [CrossRef]
155. Andrea L. Gebhart, Richard N. Aslin, Elissa L. Newport. 2009. Changing
Structures in Midstream: Learning Along the Statistical Garden Path. Cognitive
Science 33:6, 1087-1116. [CrossRef]
156. Adam Tashman, Robert Frey. 2009. Modeling risk in arbitrage strategies using
finite mixtures. Quantitative Finance 9:5, 495-503. [CrossRef]
157. Giovanni Pezzulo, Gianguglielmo Calvi. 2009. Computational explorations of
perceptual symbol systems theory. New Ideas in Psychology . [CrossRef]


--- Page 22 ---

158. E.D. Ubeyli. 2009. Eigenvector Methods for Automated Detection of
Electrocardiographic Changes in Partial Epileptic Patients. IEEE Transactions on
Information Technology in Biomedicine 13:4, 478-485. [CrossRef]
159. Zainal Ahmad, Rabiatul ′Adawiah Mat Noor, Jie Zhang. 2009. Multiple neural
networks modeling techniques in process control: a review. Asia-Pacific Journal of
Chemical Engineering 4:4, 403-419. [CrossRef]
160. J. Carreau, Y. Bengio. 2009. A Hybrid Pareto Mixture for Conditional Asymmetric
Fat-Tailed Distributions. IEEE Transactions on Neural Networks 20:7, 1087-1101.
[CrossRef]
161. Hiroshi Imamizu, Mitsuo Kawato. 2009. Brain mechanisms for predictive control
by switching internal models: implications for higher-order cognitive functions.
Psychological Research Psychologische Forschung 73:4, 527-544. [CrossRef]
162. Sheng-Hsun Hsu, JJ Po-An Hsieh, Ting-Chih Chih, Kuei-Chu Hsu. 2009. A
two-stage architecture for stock price forecasting by integrating self-organizing
map and support vector regression. Expert Systems with Applications 36:4,
7947-7951. [CrossRef]
163. Aydın Ulaş, Murat Semerci, Olcay Taner Yıldız, Ethem Alpaydın. 2009.
Incremental construction of classifier and discriminant ensembles. Information
Sciences 179:9, 1298-1318. [CrossRef]
164. Yaoyao Zhu, Xiaolei Huang, Wei Wang, Daniel Lopresti, Rodney Long, Sameer
Antani, Zhiyun Xue, George Thoma. 2009. Balancing the Role of Priors in Multi-
Observer Segmentation Evaluation. Journal of Signal Processing Systems 55:1-3,
185-207. [CrossRef]
165. J. Molina-Vilaplana, J. L. Contreras-Vidal, M. T. Herrero-Ezquerro, J. Lopez-
Coronado. 2009. A model for altered neural network dynamics related to
prehension movements in Parkinson disease. Biological Cybernetics 100:4, 271-287.
[CrossRef]
166. Manuel Carcenac. 2009. A modular neural network for super-resolution of human
faces. Applied Intelligence 30:2, 168-186. [CrossRef]
167. Elif Derya Übeyli. 2009. Features for analysis of electrocardiographic changes
in partial epileptic patients. Expert Systems with Applications 36:3, 6780-6789.
[CrossRef]
168. N. Gradojevic, R. Gencay, D. Kukolj. 2009. Option Pricing With Modular Neural
Networks. IEEE Transactions on Neural Networks 20:4, 626-637. [CrossRef]
169. P. Rojanavasu, Hai Huong Dam, H.A. Abbass, C. Lokan, O. Pinngern. 2009. A
Self-Organized, Distributed, and Adaptive Rule-Based Induction System. IEEE
Transactions on Neural Networks 20:3, 446-459. [CrossRef]
170. Kai A. Krueger, Peter Dayan. 2009. Flexible shaping: How learning in small steps
helps. Cognition 110:3, 380-394. [CrossRef]


--- Page 23 ---

171. Ori Rosen, David S. Stoffer, Sally Wood. 2009. Local Spectral Analysis via
a Bayesian Mixture of Smoothing Splines. Journal of the American Statistical
Association 104:485, 249-262. [CrossRef]
172. S FROLOV, A BAPTISTA, Y ZHANG, C SEATON. 2009. Estimation of
ecologically significant circulation features of the Columbia River estuary and
plume using a reduced-dimension Kalman filter. Continental Shelf Research 29:2,
456-466. [CrossRef]
173. Michael J. Procopio, Jane Mulligan, Greg Grudic. 2009. Learning terrain
segmentation with classifier ensembles for autonomous robot navigation in
unstructured environments. Journal of Field Robotics 26:2, 145-175. [CrossRef]
174. Serkan Tapkın, Ozdemir Akyılmaz. 2009. A new approach to neural trip
distribution models: NETDIM. Transportation Planning and Technology 32:1,
93-114. [CrossRef]
175. Mike Oaksford, Nick Chater. 2009. The uncertain reasoner: Bayes, logic, and
rationality. Behavioral and Brain Sciences 32:01, 105-120. [CrossRef]
176. M.D. Muhlbaier, A. Topalis, R. Polikar. 2009. Learn$^{++}$ .NC: Combining
Ensemble of Classifiers With Dynamically Weighted Consult-and-Vote for
Efficient Incremental Learning of New Classes. IEEE Transactions on Neural
Networks 20:1, 152-168. [CrossRef]
177. Sergios Theodoridis, Konstantinos KoutroumbasNonlinear Classifiers 151-260.
[CrossRef]
178. S MABU, K SHIMADA, K HIRASAWA, J HU. 2009. Study of multi-branch
structure of Universal Learning Networks. Applied Soft Computing 9:1, 393-403.
[CrossRef]
179. Xin Li, Yunfei Zheng. 2009. Patch-Based Video Processing: A Variational Bayesian
Approach. IEEE Transactions on Circuits and Systems for Video Technology 19:1,
27-40. [CrossRef]
180. J NAMIKAWA, J TANI. 2008. A model for learning to segment temporal
sequences, utilizing a mixture of RNN experts together with adaptive variance.
Neural Networks 21:10, 1466-1475. [CrossRef]
181. COLIN FYFE, WESAM BARBAKH, WEI CHUAN OOI, HANSEOK KO.
2008. TOPOLOGICAL MAPPINGS OF VIDEO AND AUDIO DATA.
International Journal of Neural Systems 18:06, 481-489. [CrossRef]
182. Fernanda L. Minku, Teresa B. Ludermir. 2008. Clustering and co-evolution to
construct neural network ensembles: An experimental study. Neural Networks 21:9,
1363-1379. [CrossRef]
183. Luís M. Silva, J. Marques de Sá, Luís A. Alexandre. 2008. Data classification with
multilayer perceptrons using a generalized error function. Neural Networks 21:9,
1302-1310. [CrossRef]


--- Page 24 ---

184. H. Cevikalp, R. Polikar. 2008. Local Classifier Weighting by Quadratic
Programming. IEEE Transactions on Neural Networks 19:10, 1832-1838.
[CrossRef]
185. Pietro Zito, Haibo Chen, Margaret C. Bell. 2008. Predicting Real-Time
Roadside CO and $\hbox{NO}_{2}$ Concentrations Using Neural Networks.
IEEE Transactions on Intelligent Transportation Systems 9:3, 514-522. [CrossRef]
186. M.A. Wiering, H. van Hasselt. 2008. Ensemble Algorithms in Reinforcement
Learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)
38:4, 930-936. [CrossRef]
187. KOSTAS FRAGOS, SPIROS PANETSOS. 2008. DISAMBIGUATION
OF 
GREEK 
POLYSEMOUS 
WORDS 
USING 
HIERARCHICAL
PROBABILISTIC NETWORKS AND A CHI-SQUARE FEATURE
SELECTION STRATEGY. International Journal on Artificial Intelligence Tools
17:04, 687-701. [CrossRef]
188. Reza Ebrahimpour, Ehsanollah Kabir, Mohammad Reza Yousefi. 2008. Teacher-
directed learning in view-independent face recognition with mixture of experts
using overlapping eigenspaces. Computer Vision and Image Understanding 111:2,
195-206. [CrossRef]
189. Dongbing Gu. 2008. Distributed EM Algorithm for Gaussian Mixtures in Sensor
Networks. IEEE Transactions on Neural Networks 19:7, 1154-1166. [CrossRef]
190. M.M. Islam, Xin Yao, S.M. Shahriar Nirjon, M.A. Islam, K. Murase. 2008.
Bagging and Boosting Negatively Correlated Neural Networks. IEEE Transactions
on Systems, Man, and Cybernetics, Part B (Cybernetics) 38:3, 771-784. [CrossRef]
191. Sally A Wood, Robert Kohn, Remy Cottet, Wenxin Jiang, Martin Tanner. 2008.
Locally Adaptive Nonparametric Binary Regression. Journal of Computational and
Graphical Statistics 17:2, 352-372. [CrossRef]
192. Giovanni Pezzulo. 2008. Coordinating with the Future: The Anticipatory Nature
of Representation. Minds and Machines 18:2, 179-225. [CrossRef]
193. Elif Derya Übeyli. 2008. Implementing wavelet transform/mixture of experts
network for analysis of electrocardiogram beats. Expert Systems 25:2, 150-162.
[CrossRef]
194. E UBEYLI. 2008. Wavelet/mixture of experts network structure for EEG signals
classification. Expert Systems with Applications 34:3, 1954-1962. [CrossRef]
195. Leo Galway, Darryl Charles, Michaela Black. 2008. Machine learning in digital
games: a survey. Artificial Intelligence Review 29:2, 123-161. [CrossRef]
196. J. Peres, R. Oliveira, S. Feyo de Azevedo. 2008. Bioprocess hybrid parametric/
nonparametric modelling based on the concept of mixture of experts. Biochemical
Engineering Journal 39:1, 190-206. [CrossRef]
197. Jun Tani, Ryu Nishimoto, Jun Namikawa, Masato Ito. 2008. Codevelopmental
Learning Between Human and Humanoid Robot Using a Dynamic Neural-


--- Page 25 ---

Network Model. IEEE Transactions on Systems, Man, and Cybernetics, Part B
(Cybernetics) 38:1, 43-59. [CrossRef]
198. Berk Gokberk, Helin Dutagaci, Ayd¿n Ulas, Lale Akarun, B☆lent Sankur. 2008.
Representation Plurality and Fusion for 3-D Face Recognition. IEEE Transactions
on Systems, Man, and Cybernetics, Part B (Cybernetics) 38:1, 155-173. [CrossRef]
199. Shalom Darmanjian, Jose C. Principe. 2008. Boosted and Linked Mixtures of
HMMs for Brain-Machine Interfaces. EURASIP Journal on Advances in Signal
Processing 2008, 1-13. [CrossRef]
200. R POLIKAR, A TOPALIS, D PARIKH, D GREEN, J FRYMIARE, J
KOUNIOS, C CLARK. 2008. An ensemble based data fusion approach for early
diagnosis of Alzheimer’s disease. Information Fusion 9:1, 83-95. [CrossRef]
201. T TANIGUCHI, T SAWARAGI. 2008. Incremental acquisition of multiple
nonlinear forward models based on differentiation process of schema model. Neural
Networks 21:1, 13-27. [CrossRef]
202. Yong Liu,, Xin Yao,. 2008. Nature Inspired Neural Network Ensemble Learning.
Journal of Intelligent Systems 17:supplement, 5. [CrossRef]
203. Minh Ha Nguyen, H.A. Abbass, R.I. McKay. 2008. Analysis of
CCME: Coevolutionary Dynamics, Automatic Problem Decomposition, and
Regularization. IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews) 38:1, 100-109. [CrossRef]
204. Sébastien Hélie, Gyslain Giguère, Denis Cousineau, Robert Proulx. 2007. Using
knowledge partitioning to investigate the psychological plausibility of mixtures of
experts. Artificial Intelligence Review 25:1-2, 119-138. [CrossRef]
205. Spyros N. Raptis, Elpida S. Tzafestas. 2007. Robot inverse kinematics via neural
and neurofuzzy networks: architectural and computational aspects for improved
performance. Journal of Information and Optimization Sciences 28:6, 905-933.
[CrossRef]
206. Sheng-Uei Guan, Chunyu Bao, TseNgee Neo. 2007. Reduced Pattern Training
Based on Task Decomposition Using Pattern Distributor. IEEE Transactions on
Neural Networks 18:6, 1738-1749. [CrossRef]
207. Manuel Carcenac. 2007. A modular neural network applied to image transformation
and mental images. Neural Computing and Applications . [CrossRef]
208. Mark Eastwood, Bogdan Gabrys. 2007. The Dynamics of Negative Correlation
Learning. The Journal of VLSI Signal Processing Systems for Signal, Image, and Video
Technology 49:2, 251-263. [CrossRef]
209. Shun-ichi Amari. 2007. Integration of Stochastic Models by Minimizing α-
Divergence. Neural Computation 19:10, 2780-2796. [Abstract] [PDF] [PDF Plus]
210. J SU. 2007. Camera calibration based on receptive fields. Pattern Recognition 40:10,
2837-2845. [CrossRef]
211. Mitsuo Kawato, Daniel WolpertInternal Models for Motor Control 291-307.
[CrossRef]


--- Page 26 ---

212. M. Aziz Muslim, Masumi Ishikawa, Tetsuo Furukawa. 2007. Task segmentation
in a mobile robot by mnSOM: a new approach to training expert modules. Neural
Computing and Applications 16:6, 571-580. [CrossRef]
213. Zhuoxin Sun, Ori Rosen, Allan R. Sampson. 2007. Multivariate Bernoulli
Mixture Models with Application to Postmortem Tissue Studies in Schizophrenia.
Biometrics 63:3, 901-909. [CrossRef]
214. L XU. 2007. A unified perspective and new results on RHT computing, mixture
based learning, and multi-learner based problem solving☆. Pattern Recognition 40:8,
2129-2153. [CrossRef]
215. Reza Ebrahimpour, Ehsanollah Kabir, Mohammad Reza Yousefi. 2007. Face
Detection Using Mixture of MLP Experts. Neural Processing Letters 26:1, 69-82.
[CrossRef]
216. Hiroshi Imamizu, Norikazu Sugimoto, Rieko Osu, Kiyoka Tsutsui, Kouichi
Sugiyama, Yasuhiro Wada, Mitsuo Kawato. 2007. Explicit contextual information
selectively contributes to predictive switching of internal models. Experimental
Brain Research 181:3, 395-408. [CrossRef]
217. Mark D. Skowronski, John G. Harris. 2007. Noise-Robust Automatic Speech
Recognition Using a Predictive Echo State Network. IEEE Transactions on Audio,
Speech and Language Processing 15:5, 1724-1730. [CrossRef]
218. D. M. Gavrila, S. Munder. 2007. Multi-cue Pedestrian Detection and Tracking
from a Moving Vehicle. International Journal of Computer Vision 73:1, 41-59.
[CrossRef]
219. C LIMA, A COELHO, F VONZUBEN. 2007. Hybridizing mixtures of experts
with support vector machines: Investigation into nonlinear dynamic systems
identification. Information Sciences 177:10, 2049-2074. [CrossRef]
220. Abdelhamid Bouchachia. 2007. Learning with partly labeled data. Neural
Computing and Applications 16:3, 267-293. [CrossRef]
221. Giovanna Jona Lasinio, Fabio Divino, Annibale Biggeri. 2007. Environmental
risk assessment in the Tuscany region: a proposal. Environmetrics 18:3, 315-332.
[CrossRef]
222. Yung-Keun Kwon, Byung-Ro Moon. 2007. A Hybrid Neurogenetic Approach
for Stock Forecasting. IEEE Transactions on Neural Networks 18:3, 851-864.
[CrossRef]
223. R VANDERMERWE, T LEEN, Z LU, S FROLOV, A BAPTISTA. 2007.
Fast neural network surrogates for very high dimensional physics-based models in
computational oceanography. Neural Networks 20:4, 462-478. [CrossRef]
224. Yasutoshi Nomura, Hitoshi Furuta, Michiyuki Hirokane. 2007. An Integrated
Fuzzy Control System for Structural Vibration. Computer-Aided Civil and
Infrastructure Engineering 22:4, 306-316. [CrossRef]
225. M SKOWRONSKI, J HARRIS. 2007. Automatic speech recognition using a
predictive echo state network classifier. Neural Networks 20:3, 414-423. [CrossRef]


--- Page 27 ---

226. Ludmila Kuncheva, Juan Rodriguez. 2007. Classifier Ensembles with a Random
Linear Oracle. IEEE Transactions on Knowledge and Data Engineering 19:4,
500-508. [CrossRef]
227. Devi Parikh, Robi Polikar. 2007. An Ensemble-Based Incremental Learning
Approach to Data Fusion. IEEE Transactions on Systems, Man and Cybernetics,
Part B (Cybernetics) 37:2, 437-450. [CrossRef]
228. Colin Fyfe. 2007. Two topographic maps for data visualisation. Data Mining and
Knowledge Discovery 14:2, 207-224. [CrossRef]
229. David J. Miller, Siddharth Pal. 2007. Transductive Methods for the Distributed
Ensemble Classification Problem. Neural Computation 19:3, 856-884. [Abstract]
[PDF] [PDF Plus]
230. Mark S. Handcock, Adrian E. Raftery, Jeremy M. Tantrum. 2007. Model-based
clustering for social networks. Journal of the Royal Statistical Society: Series A
(Statistics in Society) 170:2, 301-354. [CrossRef]
231. Anelia Angelova, Larry Matthies, Daniel Helmick, Pietro Perona. 2007. Learning
and prediction of slip from visual information. Journal of Field Robotics 24:3,
205-231. [CrossRef]
232. Elif Derya Übeyli. 2007. Comparison of different classification algorithms in
clinical decision-making. Expert Systems 24:1, 17-31. [CrossRef]
233. Xia Hong, Sheng Chen, Chris J. Harris. 2007. A Kernel-Based Two-Class Classifier
for Imbalanced Data Sets. IEEE Transactions on Neural Networks 18:1, 28-41.
[CrossRef]
234. P HARTONO, S HASHIMOTO. 2007. Learning from imperfect data. Applied
Soft Computing 7:1, 353-363. [CrossRef]
235. Justin C. Sanchez, José C. Principe. 2007. Brain–Machine Interface Engineering.
Synthesis Lectures on Biomedical Engineering 2:1, 1-234. [CrossRef]
236. Sheng-Uei Guan,, Kai Wang,. 2007. Hierarchical Incremental Class Learning with
Output Parallelism. Journal of Intelligent Systems 16:2, 167. [CrossRef]
237. Nicolas Amaury, Emilio Benfenati, Severin Bumbaru, Antonio Chana, Marian
Craciun, Jacques R. Chrétien, Giuseppina Gini, Gongde Guo, Frank Lemke, Viorel
Minzu, Johann-Adolf Müller, Daniel Neagu, Marco Pintore, Silviu Augustin
Stroia, Paul TrundleHybrid systems 149-183. [CrossRef]
238. Junfeng Pan, Qiang Yang, Yiming Yang, Lei Li, Frances Li, George Li. 2007. Cost-
Sensitive-Data Preprocessing for Mining Customer Relationship Management
Databases. IEEE Intelligent Systems 22:1, 46-51. [CrossRef]
239. Kumiko Nishi, Ichiro Takeuchi. 2007. Casualty Insurance Pure Premium
Estimation Using Two-Stage Regression Tree. Transactions of the Japanese Society
for Artificial Intelligence 22, 183-190. [CrossRef]
240. Matthew C. Casey, Khurshid Ahmad. 2006. A competitive neural model of small
number detection. Neural Networks 19:10, 1475-1489. [CrossRef]


--- Page 28 ---

241. Sheng-Uei Guan, Chunyu Bao, Ru-Tian Sun. 2006. Hierarchical Incremental
Class Learning with Reduced Pattern Training. Neural Processing Letters 24:2,
163-177. [CrossRef]
242. I. Guler, E.D. Ubeyli. 2006. Automated Diagnostic Systems With Diverse and
Composite Features for Doppler Ultrasound Signals. IEEE Transactions on
Biomedical Engineering 53:10, 1934-1942. [CrossRef]
243. Y OHIGASHI, T OMORI. 2006. Modeling of autonomous problem solving
process by dynamic construction of task models in multiple tasks environment.
Neural Networks 19:8, 1169-1180. [CrossRef]
244. Kazuyuki Samejima, Ken'Ichi Katagiri, Kenji Doya, Mitsuo Kawato. 2006.
Multiple model-based reinforcement learning for nonlinear control. Electronics and
Communications in Japan (Part III: Fundamental Electronic Science) 89:9, 54-69.
[CrossRef]
245. J. Verbeek. 2006. Learning nonlinear image manifolds by global alignment of local
linear models. IEEE Transactions on Pattern Analysis and Machine Intelligence 28:8,
1236-1250. [CrossRef]
246. A SHARMA, K PALIWAL, G ONWUBOLU. 2006. Class-dependent PCA,
MDC and LDA: A combined classifier for pattern classification. Pattern
Recognition 39:7, 1215-1229. [CrossRef]
247. G BARRETO, L SOUZA. 2006. Adaptive filtering with the self-organizing map:
A performance comparison. Neural Networks 19:6-7, 785-798. [CrossRef]
248. I AUTIO. 2006. Using natural class hierarchies in multi-class visual classification.
Pattern Recognition 39:7, 1290-1299. [CrossRef]
249. G.-B. Huang, L. Chen, C.-K. Siew. 2006. Universal Approximation Using
Incremental Constructive Feedforward Networks With Random Hidden Nodes.
IEEE Transactions on Neural Networks 17:4, 879-892. [CrossRef]
250. W WANG, P GELDER, J VRIJLING, J MA. 2006. Forecasting daily streamflow
using hybrid ANN models. Journal of Hydrology 324:1-4, 383-399. [CrossRef]
251. S-P Kim, J C Sanchez, Y N Rao, D Erdogmus, J M Carmena, M A Lebedev, M
A L Nicolelis, J C Principe. 2006. A comparison of optimal MIMO linear and
nonlinear models for brain–machine interfaces. Journal of Neural Engineering 3:2,
145-161. [CrossRef]
252. V.R. Khare, X. Yao, B. Sendhoff. 2006. Multi-network evolutionary systems and
automatic decomposition of complex problems. International Journal of General
Systems 35:3, 259-274. [CrossRef]
253. Michael Rimer, Tony Martinez. 2006. Classification-based objective functions.
Machine Learning 63:2, 183-205. [CrossRef]
254. Robi PolikarPattern Recognition . [CrossRef]
255. Mingyang Xu, Michael W. Golay. 2006. Data-guided model combination by
decomposition and aggregation. Machine Learning 63:1, 43-67. [CrossRef]


--- Page 29 ---

256. J. Zhang, Q. Jin, Y. Xu. 2006. Inferential Estimation of Polymer Melt Index
Using Sequentially Trained Bootstrap Aggregated Neural Networks. Chemical
Engineering & Technology 29:4, 442-448. [CrossRef]
257. Erhan Oztop, Mitsuo Kawato, Michael Arbib. 2006. Mirror neurons and imitation:
A computationally guided review. Neural Networks 19:3, 254-271. [CrossRef]
258. D LOYOLAR. 2006. Applications of neural network methods to the processing of
earth observation satellite data. Neural Networks 19:2, 168-177. [CrossRef]
259. D SOLOMATINE, M SIEK. 2006. Modular learning models in forecasting natural
phenomena. Neural Networks 19:2, 215-224. [CrossRef]
260. J. Cho, J.C. Principe, D. Erdogmus, M.A. Motter. 2006. Modeling and Inverse
Controller Design for an Unmanned Aerial Vehicle Based on the Self-Organizing
Map. IEEE Transactions on Neural Networks 17:2, 445-460. [CrossRef]
261. J ZHANG. 2006. Improved on-line process fault diagnosis through information
fusion in multiple neural networks. Computers & Chemical Engineering 30:3,
558-571. [CrossRef]
262. Marcus FreanConnectionist Architectures: Optimization . [CrossRef]
263. Yang Ge, Wenxin Jiang. 2006. On Consistency of Bayesian Inference with Mixtures
of Logistic Regression. Neural Computation 18:1, 224-243. [Abstract] [PDF]
[PDF Plus]
264. Xin Yao, Yong Xu. 2006. Recent Advances in Evolutionary Computation. Journal
of Computer Science and Technology 21:1, 1-18. [CrossRef]
265. Alexandre X. Carvalho, Martin A. Tanner. 2006. Modeling nonlinearities with
mixtures-of-experts of time series models. International Journal of Mathematics and
Mathematical Sciences 2006, 1-23. [CrossRef]
266. Giuliano Armano, Alessandro Orro, Eloisa Vargiu. 2006. MASSP3: A System for
Predicting Protein Secondary Structure. EURASIP Journal on Advances in Signal
Processing 2006, 1-10. [CrossRef]
267. J. Peres, F. Freitas, MAM Reis, S. Feyo de Azevedo, R. OliveiraHybrid modular
mechanistic/ANN modelling of a wastewater phosphorus removal process 21,
1717-1722. [CrossRef]
268. Tadahiro TANIGUCHI, Tetsuo SAWARAGI. 2006. Construction of Behavioral
Concepts through Social Interactions based on Reward Design: Schema-Based
Incremental Reinforcement Learning. Journal of Japan Society for Fuzzy Theory and
Intelligent Informatics 18:4, 629-640. [CrossRef]
269. Steven Walczak, Madhavan Parthasarathy. 2006. Modeling online service
discontinuation with nonparametric agents. Information Systems and e-Business
Management 4:1, 49-70. [CrossRef]
270. Sergios Theodoridis, Konstantinos KoutroumbasNonlinear Classifiers 121-211.
[CrossRef]
271. Biswanath BhattacharyaReferences . [CrossRef]


--- Page 30 ---

272. Z AHMAD, J ZHANG. 2005. Combination of multiple neural networks using
data fusion techniques for enhanced nonlinear process modelling. Computers &
Chemical Engineering 30:2, 295-308. [CrossRef]
273. S SCHAAL, N SCHWEIGHOFER. 2005. Computational motor control in
humans and robots. Current Opinion in Neurobiology 15:6, 675-682. [CrossRef]
274. J. Arenas-Garcia, V. Gomez-Verdejo, A.R. Figueiras-Vidal. 2005. New Algorithms
for Improved Adaptive Convex Combination of LMS Transversal Filters. IEEE
Transactions on Instrumentation and Measurement 54:6, 2239-2249. [CrossRef]
275. G.-B. Huang, K.Z. Mao, C.-K. Siew, D.-S. Huang. 2005. Fast Modular Network
Implementation for Support Vector Machines. IEEE Transactions on Neural
Networks 16:6, 1651-1663. [CrossRef]
276. Geoffrey HintonArtificial Intelligence: Neural Networks . [CrossRef]
277. Elif Derya Übeyli. 2005. A Mixture of Experts Network Structure for Breast
Cancer Diagnosis. Journal of Medical Systems 29:5, 569-579. [CrossRef]
278. Peter D Neilson, Megan D Neilson. 2005. An overview of adaptive model theory:
solving the problems of redundancy, resources, and nonlinear interactions in human
movement control. Journal of Neural Engineering 2:3, S279-S312. [CrossRef]
279. G. Armano, L. Milanesi, A. Orro. 2005. Multiple Alignment Through Protein
Secondary-Structure Information. IEEE Transactions on Nanobioscience 4:3,
207-211. [CrossRef]
280. DAVID 
WINDRIDGE, 
JOSEF 
KITTLER. 
2005. 
PERFORMANCE
MEASURES 
OF 
THE 
TOMOGRAPHIC 
CLASSIFIER 
FUSION
METHODOLOGY. International Journal of Pattern Recognition and Artificial
Intelligence 19:06, 731-753. [CrossRef]
281. K. Chen. 2005. On the Use of Different Speech Representations for Speaker
Modeling. IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications
and Reviews) 35:3, 301-314. [CrossRef]
282. J.I. Arribas, J. Cid-Sueiro. 2005. A Model Selection Algorithm for a Posteriori
Probability Estimation With Neural Networks. IEEE Transactions on Neural
Networks 16:4, 799-809. [CrossRef]
283. I. Mora-Jimenez, J. Cid-Sueiro. 2005. A Universal Learning Rule That Minimizes
Well-Formed Cost Functions. IEEE Transactions on Neural Networks 16:4,
810-820. [CrossRef]
284. Tugba Taskaya-Temizel, Matthew C. Casey. 2005. A comparative study
of autoregressive neural network hybrids. Neural Networks 18:5-6, 781-789.
[CrossRef]
285. John A. Drakopoulos, Ahmad Abdulkader. 2005. Training neural networks with
heterogeneous data. Neural Networks 18:5-6, 595-601. [CrossRef]
286. Takumi Ichimura, Shinichi Oeda, Machi Suka, Katsumi Yoshida. 2005. A
learning method of immune multi-agent neural networks. Neural Computing and
Applications 14:2, 132-148. [CrossRef]


--- Page 31 ---

287. Masahiko Morita, Kouhei Matsuzawa, Shigemitsu Morokami. 2005. A model
of context-dependent association using selective desensitization of nonmonotonic
neural elements. Systems and Computers in Japan 36:7, 73-83. [CrossRef]
288. Abedalrazq Khalil, Mohammad N. Almasri, Mac McKee, Jagath J. Kaluarachchi.
2005. Applicability of statistical learning algorithms in groundwater quality
modeling. Water Resources Research 41:5, n/a-n/a. [CrossRef]
289. BRENT FERGUSON, RANADHIR GHOSH, JOHN YEARWOOD. 2005.
MODULAR NEURAL NETWORK DESIGN FOR THE PROBLEM OF
ALPHABETIC CHARACTER RECOGNITION. International Journal of
Pattern Recognition and Artificial Intelligence 19:02, 249-269. [CrossRef]
290. Alexandre X. Carvalho, Martin A. Tanner. 2005. Modeling nonlinear time series
with local mixtures of generalized linear models. Canadian Journal of Statistics 33:1,
97-113. [CrossRef]
291. G ARMANO, M MARCHESI, A MURRU. 2005. A hybrid genetic-neural
architecture for stock indexes forecasting. Information Sciences 170:1, 3-33.
[CrossRef]
292. A HAGEN. 2005. Recent advances in the multi-stream HMM/ANN hybrid
approach to noise robust ASR. Computer Speech & Language 19:1, 3-30. [CrossRef]
293. Seiji Ishihara, Harukazu Igarashi. 2005. A Task Decomposition Algorithm Based
on the Distribution of Input Pattern Vectors for Classification Problems. IEEJ
Transactions on Electronics, Information and Systems 125:7, 1043-1048. [CrossRef]
294. A.X. Carvalho, M.A. Tanner. 2005. Mixtures-of-Experts of Autoregressive Time
Series: Asymptotic Normality and Model Specification. IEEE Transactions on
Neural Networks 16:1, 39-56. [CrossRef]
295. David WindridgeMorphologically Debiased Classifier Fusion: A Tomography-
Theoretic Approach 134, 181-266. [CrossRef]
296. J. Lan, J. Cho, D. Erdogmus, J.C. Principe, M.A. Motter, J. Xu. 2005. Local Linear
PID Controllers for Nonlinear Control. Control and Intelligent Systems 33:1. .
[CrossRef]
297. R PAINE, J TANI. 2004. Motor primitive and sequence self-organization in
a hierarchical recurrent neural network. Neural Networks 17:8-9, 1291-1309.
[CrossRef]
298. C.K. Loo, M. Rajeswari, M.V.C. Rao. 2004. Novel Direct and Self-Regulating
Approaches to Determine Optimum Growing Multi-Experts Network Structure.
IEEE Transactions on Neural Networks 15:6, 1378-1395. [CrossRef]
299. Roelof K. Brouwer. 2004. A hybrid neural network for input that is both categorical
and quantitative. International Journal of Intelligent Systems 19:10, 979-1001.
[CrossRef]
300. A.S. Cofino, J.M. Gutierrez, M.L. Ivanissevich. 2004. Evolving modular networks
with genetic algorithms: application to nonlinear time series. Expert Systems 21:4,
208-216. [CrossRef]


--- Page 32 ---

301. SHIMON COHEN, NATHAN INTRATOR. 2004. ON DIFFERENT
MODEL SELECTION CRITERIA IN A FORWARD AND BACKWARD
REGRESSION HYBRID NETWORK. International Journal of Pattern
Recognition and Artificial Intelligence 18:05, 847-865. [CrossRef]
302. CHRISTOPH KÖNIG, GIUSEPPINA GINI, MARIAN CRACIUN, EMILIO
BENFENATI. 2004. MULTICLASS CLASSIFIER FROM A COMBINATION
OF LOCAL EXPERTS: TOWARD DISTRIBUTED COMPUTATION FOR
REAL-PROBLEM CLASSIFIERS. International Journal of Pattern Recognition
and Artificial Intelligence 18:05, 801-817. [CrossRef]
303. Jayanta Basak, Ravi Kothari. 2004. A Classification Paradigm for Distributed
Vertically Partitioned Data. Neural Computation 16:7, 1525-1544. [Abstract]
[PDF] [PDF Plus]
304. F.A. Mussa-Ivaldi, S.A. Solla. 2004. Neural Primitives for Motion Control. IEEE
Journal of Oceanic Engineering 29:3, 640-650. [CrossRef]
305. L. Xu. 2004. Advances on BYY Harmony Learning: Information Theoretic
Perspective, Generalized Projection Geometry, and Independent Factor
Autodetermination. IEEE Transactions on Neural Networks 15:4, 885-902.
[CrossRef]
306. S.K. Warfield, K.H. Zou, W.M. Wells. 2004. Simultaneous Truth and Performance
Level Estimation (STAPLE): An Algorithm for the Validation of Image
Segmentation. IEEE Transactions on Medical Imaging 23:7, 903-921. [CrossRef]
307. M.-W. Chang, C.-J. Lin, R.C.-H. Weng. 2004. Analysis of Switching Dynamics
With Competing Support Vector Machines. IEEE Transactions on Neural Networks
15:3, 720-727. [CrossRef]
308. J Brown. 2004. How laminar frontal cortex and basal ganglia circuits interact to
control planned and reactive saccades. Neural Networks 17:4, 471-510. [CrossRef]
309. S.-K. Ng, G.J. McLachlan. 2004. Using the EM Algorithm to Train Neural
Networks: Misconceptions and a New Algorithm for Multiclass Classification.
IEEE Transactions on Neural Networks 15:3, 738-749. [CrossRef]
310. M.A. Moussa. 2004. Combining Expert Neural Networks Using Reinforcement
Feedback for Learning Primitive Grasping Behavior. IEEE Transactions on Neural
Networks 15:3, 629-638. [CrossRef]
311. WEI HUANG, K. K. LAI, Y. NAKAMORI, SHOUYANG WANG. 2004.
FORECASTING FOREIGN EXCHANGE RATES WITH ARTIFICIAL
NEURAL NETWORKS: A REVIEW. International Journal of Information
Technology & Decision Making 03:01, 145-165. [CrossRef]
312. J. Su, J. Wang, Y. Xi. 2004. Incremental Learning With Balanced Update on
Receptive Fields for Multi-Sensor Data Fusion. IEEE Transactions on Systems, Man
and Cybernetics, Part B (Cybernetics) 34:1, 659-665. [CrossRef]


--- Page 33 ---

313. Hyun-Chul Kim, Daijin Kim, Sung Yang Bang, Sang-Youn Lee. 2004.
Face recognition using the second-order mixture-of-eigenfaces method. Pattern
Recognition 37:2, 337-349. [CrossRef]
314. Andrew Estabrooks, Taeho Jo, Nathalie Japkowicz. 2004. A Multiple Resampling
Method for Learning from Imbalanced Data Sets. Computational Intelligence 20:1,
18-36. [CrossRef]
315. Sheng-Uei Guan,, TseNgee Neo,, Chunyu Bao,. 2004. Task Decomposition Using
Pattern Distributor. Journal of Intelligent Systems 13:2, 123. [CrossRef]
316. J. Peres, R. Oliveira, L.S. Serafim, P. Lemos, M.A. Reis, S. Feyo de AzevedoHybrid
Modelling of a PHA Production Process Using Modular Neural Networks 18,
733-738. [CrossRef]
317. Michael L. Kalish, Stephan Lewandowsky, John K. Kruschke. 2004. Population
of Linear Experts: Knowledge Partitioning and Function Learning. Psychological
Review 111:4, 1072-1099. [CrossRef]
318. Lee-Xieng Yang, Stephan Lewandowsky. 2004. Knowledge Partitioning in
Categorization: Constraints on Exemplar Models. Journal of Experimental
Psychology: Learning, Memory, and Cognition 30:5, 1045-1064. [CrossRef]
319. Sheng-Uei Guan,, Jun Liu,, Yinan Qi,. 2004. An Incremental Approach to
Contribution-Based Feature Selection. Journal of Intelligent Systems 13:1, 15.
[CrossRef]
320. Sheng-Uei Guan,, Jun Liu,. 2004. Incremental Neural Network Training with an
Increasing Input Dimension. Journal of Intelligent Systems 13:1, 43. [CrossRef]
321. Kai Huang, Robert F. Murphy. 2004. From quantitative microscopy to automated
image understanding. Journal of Biomedical Optics 9:5, 893. [CrossRef]
322. Lewis Bott, Evan Heit. 2004. Nonmonotonic Extrapolation in Function Learning.
Journal of Experimental Psychology: Learning, Memory, and Cognition 30:1, 38-50.
[CrossRef]
323. Ori Rosen, Ayala Cohen. 2003. Analysis of growth curves via mixtures. Statistics
in Medicine 22:23, 3641-3654. [CrossRef]
324. L Chu Kiong. 2003. Nonlinear dynamic system identification and control via
constructivism inspired neural network. Applied Soft Computing 3:3, 237-257.
[CrossRef]
325. C.M. Bachmann, M.H. Bettenhausen, R.A. Fusina, T.F. Donato, A.L. Russ,
J.W. Burke, G.M. Lamela, W.J. Rhea, B.R. Truitt, J.H. Porter. 2003. A credit
assignment approach to fusing classifiers of multiseason hyperspectral imagery.
IEEE Transactions on Geoscience and Remote Sensing 41:11, 2488-2499. [CrossRef]
326. D.J. Miller, J. Browning. 2003. A mixture model and EM-based algorithm for class
discovery, robust classification, and outlier rejection in mixed labeled/unlabeled
data sets. IEEE Transactions on Pattern Analysis and Machine Intelligence 25:11,
1468-1483. [CrossRef]


--- Page 34 ---

327. L Chu Kiong. 2003. Extrapolation detection and novelty-based node insertion for
sequential growing multi-experts network. Applied Soft Computing 3:2, 159-175.
[CrossRef]
328. K Samejima. 2003. Inter-module credit assignment in modular reinforcement
learning. Neural Networks 16:7, 985-994. [CrossRef]
329. S Mounce. 2003. Sensor-fusion of hydraulic data for burst detection and location
in a treated water distribution system. Information Fusion 4:3, 217-229. [CrossRef]
330. Jun Tani, M. Ito. 2003. Self-organization of behavioral primitives as multiple
attractor dynamics: A robot experiment. IEEE Transactions on Systems, Man, and
Cybernetics - Part A: Systems and Humans 33:4, 481-488. [CrossRef]
331. R Brouwer. 2003. Training a feed-forward network with incomplete data due to
missing input variables. Applied Soft Computing 3:1, 23-36. [CrossRef]
332. Md.M. Islam, Xin Yao, K. Murase. 2003. A constructive algorithm for training
cooperative neural network ensembles. IEEE Transactions on Neural Networks
14:4, 820-834. [CrossRef]
333. H KIM. 2003. An efficient model order selection for PCA mixture model. Pattern
Recognition Letters 24:9-10, 1385-1393. [CrossRef]
334. Chee Peng Lim, R.F. Harrison. 2003. Online pattern classification with multiple
neural network systems: an experimental study. IEEE Transactions on Systems, Man
and Cybernetics, Part C (Applications and Reviews) 33:2, 235-247. [CrossRef]
335. RONAN COLLOBERT, YOSHUA BENGIO, SAMY BENGIO. 2003.
SCALING LARGE LEARNING PROBLEMS WITH HARD PARALLEL
MIXTURES. International Journal of Pattern Recognition and Artificial Intelligence
17:03, 349-365. [CrossRef]
336. H Kim. 2003. Extensions of LDA by PCA mixture model and class-wise features.
Pattern Recognition 36:5, 1095-1105. [CrossRef]
337. N. Garcia-Pedrajas, C. Hervas-Martinez, J. Munoz-Perez. 2003. COVNET: a
cooperative coevolutionary model for evolving artificial neural networks. IEEE
Transactions on Neural Networks 14:3, 575-596. [CrossRef]
338. Liu Yong, Zou Xiu-fen. 2003. Analysis of negative correlation learning. Wuhan
University Journal of Natural Sciences 8:1, 165-175. [CrossRef]
339. Liu Yong, Zou Xiu-fen. 2003. From designing a single neural network to designing
neural network ensembles. Wuhan University Journal of Natural Sciences 8:1,
155-164. [CrossRef]
340. D. Windridge, J. Kittler. 2003. A morphologically optimal strategy for classifier
combinaton: multiple expert fusion as a tomographic process. IEEE Transactions
on Pattern Analysis and Machine Intelligence 25:3, 343-353. [CrossRef]
341. Shimon Edelman, Nathan Intrator. 2003. Towards structural systematicity in
distributed, statically bound visual representations. Cognitive Science 27:1, 73-109.
[CrossRef]


--- Page 35 ---

342. J Tani. 2003. Learning to generate articulated behavior through the bottom-up
and the top-down interaction processes. Neural Networks 16:1, 11-23. [CrossRef]
343. S Cohen. 2002. Automatic model selection in a hybrid perceptron/radial network.
Information Fusion 3:4, 259-266. [CrossRef]
344. M.A.L. Thathachar, P.S. Sastry. 2002. Varieties of learning automata: an overview.
IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) 32:6,
711-722. [CrossRef]
345. M Su. 2002. A hybrid learning system for image deblurring. Pattern Recognition
35:12, 2881-2894. [CrossRef]
346. Andrew D. Back, Tianping Chen. 2002. Universal Approximation of Multiple
Nonlinear Operators by Neural Networks. Neural Computation 14:11, 2561-2566.
[Abstract] [PDF] [PDF Plus]
347. A. Kehagias, V. Petridis. 2002. Predictive modular neural networks for unsupervised
segmentation of switching time series: the data allocation problem. IEEE
Transactions on Neural Networks 13:6, 1432-1449. [CrossRef]
348. L XU. 2002. BYY harmony learning, structural RPCL, and topological self-
organizing on mixture models1. Neural Networks 15:8-9, 1125-1151. [CrossRef]
349. J. SVENSSON. 2002. AN MLP TRAINING ALGORITHM TAKING INTO
ACCOUNT KNOWN ERRORS ON INPUTS AND OUTPUTS. International
Journal of Neural Systems 12:05, 369-379. [CrossRef]
350. Michalis K. Titsias, Aristidis Likas. 2002. Mixture of Experts Classification Using
a Hierarchical Mixture Model. Neural Computation 14:9, 2221-2244. [Abstract]
[PDF] [PDF Plus]
351. R Brouwer. 2002. A feed-forward network for input that is both categorical and
quantitative. Neural Networks 15:7, 881-890. [CrossRef]
352. GIULIANO ARMANO, ANDREA MURRU, FABIO ROLI. 2002. STOCK
MARKET PREDICTION BY A MIXTURE OF GENETIC-NEURAL
EXPERTS. International Journal of Pattern Recognition and Artificial Intelligence
16:05, 501-526. [CrossRef]
353. Lizhong Wu, S.L. Oviatt, P.R. Cohen. 2002. From members to teams to
committee-a robust approach to gestural and multimodal recognition. IEEE
Transactions on Neural Networks 13:4, 972-982. [CrossRef]
354. H ElKadi. 2002. Energy-based fatigue life prediction of fiberglass/epoxy
composites using modular neural networks. Composite Structures 57:1-4, 85-89.
[CrossRef]
355. E. Mizutani, K. Nishio. 2002. Multi-illuminant color reproduction for electronic
cameras via CANFIS neuro-fuzzy modular network device characterization. IEEE
Transactions on Neural Networks 13:4, 1009-1022. [CrossRef]
356. Peter DayanReinforcement Learning . [CrossRef]


--- Page 36 ---

357. PITOYO HARTONO, SHUJI HASHIMOTO. 2002. EXTRACTING
THE PRINCIPAL BEHAVIOR OF A PROBABILISTIC SUPERVISOR
THROUGH NEURAL NETWORKS ENSEMBLE. International Journal of
Neural Systems 12:03n04, 291-301. [CrossRef]
358. Purushottam Papatla, Mariam (Fatemeh) Zahedi, Marijana Zekic-Susac. 2002.
Leveraging the Strengths of Choice Models and Neural Networks: A Multiproduct
Comparative Analysis. Decision Sciences 33:3, 433-461. [CrossRef]
359. A.M. Martinez. 2002. Recognizing imprecisely localized, partially occluded, and
expression variant faces from a single sample per class. IEEE Transactions on
Pattern Analysis and Machine Intelligence 24:6, 748-763. [CrossRef]
360. Ronan Collobert, Samy Bengio, Yoshua Bengio. 2002. A Parallel Mixture of SVMs
for Very Large Scale Problems. Neural Computation 14:5, 1105-1114. [Abstract]
[PDF] [PDF Plus]
361. Z Zhou. 2002. Ensembling neural networks: Many could be better than all.
Artificial Intelligence 137:1-2, 239-263. [CrossRef]
362. Sheng-Uei Guan, Shanchun Li. 2002. Parallel growing and training of neural
networks using output parallelism. IEEE Transactions on Neural Networks 13:3,
542-550. [CrossRef]
363. J Dong. 2002. Local learning framework for handwritten character recognition.
Engineering Applications of Artificial Intelligence 15:2, 151-159. [CrossRef]
364. L.I. Kuncheva. 2002. Switching between selection and fusion in combining
classifiers: an experiment. IEEE Transactions on Systems, Man and Cybernetics, Part
B (Cybernetics) 32:2, 146-156. [CrossRef]
365. E. Moreau, C. Mallet, S. Thiria, B. Mabboux, F. Badran, C. Klapisz. 2002.
Atmospheric Liquid Water Retrieval Using a Gated Experts Neural Network.
Journal of Atmospheric and Oceanic Technology 19:4, 457-467. [CrossRef]
366. C.C. Chibelushi, F. Deravi, J.S.D. Mason. 2002. A review of speech-based bimodal
recognition. IEEE Transactions on Multimedia 4:1, 23-37. [CrossRef]
367. Michael A. Erickson, John K. Kruschke. 2002. Rule-based extrapolation in
perceptual categorization. Psychonomic Bulletin & Review 9:1, 160-168. [CrossRef]
368. G. Mayraz, G.E. Hinton. 2002. Recognizing handwritten digits using hierarchical
products of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence
24:2, 189-197. [CrossRef]
369. Sheng-Uei Guan,, Jun Liu,. 2002. Incremental Ordered Neural Network Training.
Journal of Intelligent Systems 12:3, 137. [CrossRef]
370. David Mitchell, Robert Pavur. 2002. Using modular neural networks for business
decisions. Management Decision 40:1, 58-63. [CrossRef]
371. Sham Kakade, Peter Dayan. 2002. Acquisition and extinction in autoshaping.
Psychological Review 109:3, 533-544. [CrossRef]


--- Page 37 ---

372. Sheng-Uei Guan,, Peng Li,. 2002. Feature Selection for Modular Neural Network
Classifiers. Journal of Intelligent Systems 12:3, 173. [CrossRef]
373. Sun-Gi Hong, Sang-Keon Oh, Min-Soeng Kim, Ju-Jang Lee. 2002. Evolving
mixture of experts for nonlinear time series modelling and prediction. Electronics
Letters 38:1, 34. [CrossRef]
374. Stephan Lewandowsky, Michael Kalish, S. K. Ngang. 2002. Simplified learning
in complex situations: Knowledge partitioning in function learning. Journal of
Experimental Psychology: General 131:2, 163-193. [CrossRef]
375. Sheng-Uei Guan,, Peng Li,. 2002. A Hierarchical Incremental Learning Approach
to Task Decomposition. Journal of Intelligent Systems 12:3, 201. [CrossRef]
376. Giulio Cifarelli. 2001. Introduction. The European Journal of Finance 7:4, 286-288.
[CrossRef]
377. Mehdi Azzouzi, Ian T. Nabney. 2001. Dynamic local models for segmentation and
prediction of financial time series. The European Journal of Finance 7:4, 289-311.
[CrossRef]
378. Christian Broberger, Tomas Hökfelt. 2001. Hypothalamic and vagal neuropeptide
circuitries regulating food intake. Physiology & Behavior 74:4-5, 669-682.
[CrossRef]
379. R. Polikar, L. Upda, S.S. Upda, V. Honavar. 2001. Learn++: an incremental
learning algorithm for supervised neural networks. IEEE Transactions on Systems,
Man and Cybernetics, Part C (Applications and Reviews) 31:4, 497-508. [CrossRef]
380. Tao Hong, M.T.C. Fang. 2001. Detection and classification of partial discharge
using a feature decomposition-based modular neural network. IEEE Transactions
on Instrumentation and Measurement 50:5, 1349-1354. [CrossRef]
381. T. S. Hu, K. C. Lam, S. T. Ng. 2001. River flow time series prediction with a range-
dependent neural network. Hydrological Sciences Journal 46:5, 729-745. [CrossRef]
382. L See. 2001. Multi-model data fusion for hydrological forecasting. Computers &
Geosciences 27:8, 987-994. [CrossRef]
383. L Kuncheva. 2001. Using measures of similarity and inclusion for multiple classifier
fusion by decision templates. Fuzzy Sets and Systems 122:3, 401-407. [CrossRef]
384. Jochen Triesch, Christoph von der Malsburg. 2001. Democratic Integration: Self-
Organized Integration of Adaptive Cues. Neural Computation 13:9, 2049-2074.
[Abstract] [PDF] [PDF Plus]
385. Tze Leung Lai, Samuel Po-Shing Wong. 2001. Stochastic Neural Networks With
Applications to Nonlinear Time Series. Journal of the American Statistical Association
96:455, 968-981. [CrossRef]
386. A.A. Ilumoka. 2001. Efficient and accurate crosstalk prediction via neural net-based
topological decomposition of 3-D interconnect. IEEE Transactions on Advanced
Packaging 24:3, 268-276. [CrossRef]


--- Page 38 ---

387. David C. Becalick, Timothy J. Coats. 2001. Comparison of Artificial Intelligence
Techniques with UKTRISS for Estimating Probability of Survival after Trauma.
The Journal of Trauma: Injury, Infection and Critical Care 51:1, 123-133.
[CrossRef]
388. Jefferson T. Davis, Athanasios Episcopos, Sannaka Wettimuny. 2001. Predicting
direction shifts on Canadian-US exchange rates with artificial neural networks.
International Journal of Intelligent Systems in Accounting, Finance & Management
10:2, 83-96. [CrossRef]
389. Yuhong Yang. 2001. Adaptive Regression by Mixing. Journal of the American
Statistical Association 96:454, 574-588. [CrossRef]
390. J PERES, R OLIVEIRA, S FEYODEAZEVEDO. 2001. Knowledge based
modular networks for process modelling and control. Computers & Chemical
Engineering 25:4-6, 783-791. [CrossRef]
391. Edmondo Trentin. 2001. Networks with trainable amplitude of activation
functions. Neural Networks 14:4-5, 471-493. [CrossRef]
392. J Svensson, M von Hellermann, R W T König. 2001. Plasma Physics and Controlled
Fusion 43:4, 389-403. [CrossRef]
393. Hsin-Chia Fu, Yen-Po Lee, Cheng-Chin Chiang, Hsiao-Tien Pao. 2001. Divide-
and-conquer learning and modular perceptron networks. IEEE Transactions on
Neural Networks 12:2, 250-263. [CrossRef]
394. A Fiordaliso. 2001. Autostructuration of fuzzy systems by rules sensitivity analysis.
Fuzzy Sets and Systems 118:2, 281-296. [CrossRef]
395. YOUNÈS 
BENNANI, 
FABRÌCE 
BOSSAERT. 
2001. 
MODULAR
CONNECTIONIST MODELLING AND CLASSIFICATION APPROACHES
FOR LOCAL DIAGNOSIS IN TELECOMMUNICATION TRAFFIC
MANAGEMENT. International Journal of Computational Intelligence and
Applications 01:01, 53-70. [CrossRef]
396. H. Yin, N.M. Allinson. 2001. Self-organizing mixture networks for probability
density estimation. IEEE Transactions on Neural Networks 12:2, 405-411.
[CrossRef]
397. A Fiordaliso. 2001. A constrained Takagi–Sugeno fuzzy system that allows
for better interpretation and analysis. Fuzzy Sets and Systems 118:2, 307-318.
[CrossRef]
398. Lei Xu. 2001. Best Harmony, Unified RPCL and Automated Model Selection for
Unsupervised and Supervised Learning on Gaussian Mixtures, Three-Layer Nets
and ME-RBF-SVM Models. International Journal of Neural Systems 11:01, 43-69.
[CrossRef]
399. L Kuncheva. 2001. Decision templates for multiple classifier fusion: an
experimental comparison. Pattern Recognition 34:2, 299-314. [CrossRef]


--- Page 39 ---

400. B Zhang. 2001. A nonlinear neural network model of mixture of local
principal component analysis: application to handwritten digits recognition. Pattern
Recognition 34:2, 203-214. [CrossRef]
401. ZOUBIN GHAHRAMANI. 2001. AN INTRODUCTION TO HIDDEN
MARKOV MODELS AND BAYESIAN NETWORKS. International Journal of
Pattern Recognition and Artificial Intelligence 15:01, 9-42. [CrossRef]
402. R. Feraund, O.J. Bernier, J.-E. Viallet, M. Collobert. 2001. A fast and accurate
face detector based on neural networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence 23:1, 42-53. [CrossRef]
403. C.J. Harris, X. Hong. 2001. Neurofuzzy mixture of experts network parallel
learning and model construction algorithms. IEE Proceedings - Control Theory and
Applications 148:6, 456. [CrossRef]
404. S Sternberg. 2001. Separate modifiability, mental modules, and the use of pure and
composite measures to reveal them. Acta Psychologica 106:1-2, 147-246. [CrossRef]
405. X. Dai. 2001. CMA-based nonlinear blind equaliser modelled by a two-layer
feedforward neural network. IEE Proceedings - Communications 148:4, 243.
[CrossRef]
406. Y. Bengio, V.-P. Lauzon, R. Ducharme. 2001. Experiments on the application of
IOHMMs to model financial returns series. IEEE Transactions on Neural Networks
12:1, 113-123. [CrossRef]
407. Naonori Ueda. 2001. Transactions of the Japanese Society for Artificial Intelligence
16, 299-308. [CrossRef]
408. Qiang Gan, C.J. Harris. 2001. A hybrid learning scheme combining EM and
MASMOD algorithms for fuzzy local linearization modeling. IEEE Transactions
on Neural Networks 12:1, 43-53. [CrossRef]
409. Martin Kreutz, Maik Anschütz, Thorsten Grünendick, Andreas Rick, Stefan
Gehlen, Klaus Hoffmann. 2001. AUTOMATED DIAGNOSIS OF SKIN
CANCER USING DIGITAL IMAGE PROCESSING AND MIXTURE-
OF-EXPERTS. Biomedizinische Technik/Biomedical Engineering 46:s1, 376-377.
[CrossRef]
410. S LIEHR, K PAWELZIK. 2000. A trading strategy with variable investment from
minimizing risk to profit ratio. Physica A: Statistical Mechanics and its Applications
287:3-4, 524-538. [CrossRef]
411. Dirk Husmeier. 2000. The Bayesian Evidence Scheme for Regularizing Probability-
Density Estimating Neural Networks. Neural Computation 12:11, 2685-2717.
[Abstract] [PDF] [PDF Plus]
412. L.C. Jain, L.I. Kuncheva. 2000. Designing classifier fusion systems by genetic
algorithms. IEEE Transactions on Evolutionary Computation 4:4, 327-336.
[CrossRef]


--- Page 40 ---

413. T. Higuchi, Xin Yao, Yong Liu. 2000. Evolutionary ensembles with negative
correlation learning. IEEE Transactions on Evolutionary Computation 4:4, 380-387.
[CrossRef]
414. M.A. Carreira-Perpinan. 2000. Mode-finding for mixtures of Gaussian
distributions. IEEE Transactions on Pattern Analysis and Machine Intelligence 22:11,
1318-1323. [CrossRef]
415. D WEST. 2000. Neural network credit scoring models. Computers & Operations
Research 27:11-12, 1131-1152. [CrossRef]
416. E Adi-Japha. 2000. Regulation of division of labour between cognitive systems
controlling action. Cognition 76:1, 1-11. [CrossRef]
417. S. Gutta, J.R.J. Huang, P. Jonathon, H. Wechsler. 2000. Mixture of experts for
classification of gender, ethnic origin, and pose of human faces. IEEE Transactions
on Neural Networks 11:4, 948-960. [CrossRef]
418. Andreas S. Weigend, Shanming Shi. 2000. Predicting daily probability
distributions of S&P500 returns. Journal of Forecasting 19:4, 375-392. [CrossRef]
419. Wenxin Jiang. 2000. The VC Dimension for Mixtures of Binary Classifiers. Neural
Computation 12:6, 1293-1301. [Abstract] [PDF] [PDF Plus]
420. Ichiro Takeuchi, Takeshi Furuhashi. 2000. A description of dynamic behavior of
sensory/motor systems with fuzzy symbolic dynamic systems. Artificial Life and
Robotics 4:2, 84-88. [CrossRef]
421. Wenxin Jiang, M.A. Tanner. 2000. On the asymptotic normality of hierarchical
mixtures-of-experts for generalized linear models. IEEE Transactions on
Information Theory 46:3, 1005-1013. [CrossRef]
422. Fu-Lai Chung, Ji-Cheng Duan. 2000. On multistage fuzzy neural network
modeling. IEEE Transactions on Fuzzy Systems 8:2, 125-142. [CrossRef]
423. LARS 
ASKER, 
MATS 
DANIELSON, 
LOVE 
EKENBERG. 
2000.
COMMITTEES OF LEARNING AGENTS. International Journal of Uncertainty,
Fuzziness and Knowledge-Based Systems 08:02, 187-202. [CrossRef]
424. A. Khotanzad, H. Elragal, T.-L. Lu. 2000. Combination of artificial neural-
network forecasters for prediction of natural gas consumption. IEEE Transactions
on Neural Networks 11:2, 464-473. [CrossRef]
425. A. Karniel, G.F. Inbar. 2000. Human motor control: learning to control a time-
varying, nonlinear, many-to-one system. IEEE Transactions on Systems, Man and
Cybernetics, Part C (Applications and Reviews) 30:1, 1-11. [CrossRef]
426. Shiro Ikeda. 2000. Acceleration of the EM algorithm. Systems and Computers in
Japan 31:2, 10-18. [CrossRef]
427. Chapter 8 Design issues — Neural networks 1, 89-102. [CrossRef]
428. Qun Zhao, Jose C. Principe, Victor L. Brennan, Dongxin Xu, Zheng Wang.
2000. Synthetic aperture radar automatic target recognition with three strategies of
learning and representation. Optical Engineering 39:5, 1230. [CrossRef]


--- Page 41 ---

429. Azriel Rosenfeld, Harry Wechsler. 2000. Pattern recognition: Historical perspective
and future directions. International Journal of Imaging Systems and Technology 11:2,
101-116. [CrossRef]
430. Evan Heit, Lewis BottKnowledge selection in category learning 39, 163-199.
[CrossRef]
431. D West. 2000. Improving diagnostic accuracy using a hierarchical neural network
to model decision subtasks. International Journal of Medical Informatics 57:1, 41-55.
[CrossRef]
432. A.K. Jain, P.W. Duin, Jianchang Mao. 2000. Statistical pattern recognition: a
review. IEEE Transactions on Pattern Analysis and Machine Intelligence 22:1, 4-37.
[CrossRef]
433. G. Deng, H. Ye, L.W. Cahill. 2000. Adaptive combination of linear predictors for
lossless image compression. IEE Proceedings - Science, Measurement and Technology
147:6, 414. [CrossRef]
434. J. L. Castro, M. Delgado, C. J. Mantas. 2000. MORSE: A general model to
represent structured knowledge. International Journal of Intelligent Systems 15:1,
27-43. [CrossRef]
435. J. Peres, R. Oliveira, S. Feyo de AzevedoKnowledge based modular networks for
process modelling and control 8, 247-252. [CrossRef]
436. Paul J. WerbosNeurocontrollers . [CrossRef]
437. Yair BartalDivide-and-Conquer Methods . [CrossRef]
438. A.N. Srivastava, R. Su, A.S. Weigend. 1999. Data mining for features using
scale-sensitive gated experts. IEEE Transactions on Pattern Analysis and Machine
Intelligence 21:12, 1268-1279. [CrossRef]
439. Ferdinando A Mussa-Ivaldi. 1999. Modular features of motor control and learning.
Current Opinion in Neurobiology 9:6, 713-717. [CrossRef]
440. Mitsuo Kawato. 1999. Internal models for motor control and trajectory planning.
Current Opinion in Neurobiology 9:6, 718-727. [CrossRef]
441. Chin-Teng Lin, I-Fang Chung. 1999. A reinforcement neuro-fuzzy combiner for
multiobjective control. IEEE Transactions on Systems, Man and Cybernetics, Part
B (Cybernetics) 29:6, 726-744. [CrossRef]
442. KE CHEN, HUISHENG CHI. 1999. A MODULAR NEURAL NETWORK
ARCHITECTURE FOR PATTERN CLASSIFICATION BASED ON
DIFFERENT FEATURE SETS. International Journal of Neural Systems 09:06,
563-581. [CrossRef]
443. K Chen. 1999. Improved learning algorithms for mixture of experts in multiclass
classification. Neural Networks 12:9, 1229-1252. [CrossRef]
444. W Jiang. 1999. On the identifiability of mixtures-of-experts. Neural Networks 12:9,
1253-1258. [CrossRef]


--- Page 42 ---

445. Bao-Liang Lu, H. Kita, Y. Nishikawa. 1999. Inverting feedforward neural networks
using linear and nonlinear programming. IEEE Transactions on Neural Networks
10:6, 1271-1290. [CrossRef]
446. J Tani. 1999. Learning to perceive the world as articulated: an approach
for hierarchical learning in sensory-motor systems. Neural Networks 12:7-8,
1131-1141. [CrossRef]
447. M Dailey. 1999. Organization of face and object recognition in modular neural
network models. Neural Networks 12:7-8, 1053-1074. [CrossRef]
448. H Ando. 1999. Unsupervised visual learning of three-dimensional objects using a
modular network architecture. Neural Networks 12:7-8, 1037-1051. [CrossRef]
449. H Gross. 1999. Generative character of perception: a neural architecture for
sensorimotor anticipation. Neural Networks 12:7-8, 1101-1129. [CrossRef]
450. J Hansen. 1999. Combining predictors: comparison of five meta machine learning
methods. Information Sciences 119:1-2, 91-105. [CrossRef]
451. D.J. Miller, Lian Yan. 1999. Critic-driven ensemble classification. IEEE
Transactions on Signal Processing 47:10, 2833-2844. [CrossRef]
452. Bao-Liang Lu, M. Ito. 1999. Task decomposition and module combination based
on class relations: a modular neural network for pattern classification. IEEE
Transactions on Neural Networks 10:5, 1244-1256. [CrossRef]
453. B. Apolloni, I. Zoppis. 1999. Sub-symbolically managing pieces of symbolical
functions for sorting. IEEE Transactions on Neural Networks 10:5, 1099-1122.
[CrossRef]
454. A.D. Wilson, A.F. Bobick. 1999. Parametric hidden Markov models for gesture
recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 21:9,
884-900. [CrossRef]
455. Sun-Yuan Kung, J. Taur, Shang-Hung Lin. 1999. Synergistic modeling and
applications of hierarchical fuzzy neural networks. Proceedings of the IEEE 87:9,
1550-1574. [CrossRef]
456. I-Cheng Yeh. 1999. Modeling chaotic two-dimensional mapping with fuzzy-
neuron networks. Fuzzy Sets and Systems 105:3, 421-427. [CrossRef]
457. Wenxin Jiang, Martin A. Tanner. 1999. On the Approximation Rate
of Hierarchical Mixtures-of-Experts for Generalized Linear Models. Neural
Computation 11:5, 1183-1198. [Abstract] [PDF] [PDF Plus]
458. J Zhang. 1999. Inferential estimation of polymer quality using bootstrap aggregated
neural networks. Neural Networks 12:6, 927-938. [CrossRef]
459. Y Yeun. 1999. Function approximations by coupling neural networks and genetic
programming trees with oblique decision trees. Artificial Intelligence in Engineering
13:3, 223-239. [CrossRef]


--- Page 43 ---

460. S.A. Rizvi, L.-C. Wang, N.M. Nasrabadi. 1999. Rate-constrained modular
predictive residual vector quantization of digital images. IEEE Signal Processing
Letters 6:6, 135-137. [CrossRef]
461. R Sun. 1999. Multi-agent reinforcement learning: weighting and partitioning.
Neural Networks 12:4-5, 727-753. [CrossRef]
462. Ori Rosen, Martin Tanner. 1999. Mixtures of proportional hazards regression
models. Statistics in Medicine 18:9, 1119-1131. [CrossRef]
463. I.A. Taha, J. Ghosh. 1999. Symbolic interpretation of artificial neural networks.
IEEE Transactions on Knowledge and Data Engineering 11:3, 448-463. [CrossRef]
464. J Alexander. 1999. Template-based procedures for neural network interpretation.
Neural Networks 12:3, 479-498. [CrossRef]
465. 1999. Developing robust non-linear models through bootstrap aggregated neural
networks. Neurocomputing 25:1-3, 93-113. [CrossRef]
466. GASSER AUDA, MOHAMED KAMEL. 1999. MODULAR NEURAL
NETWORKS: A SURVEY. International Journal of Neural Systems 09:02,
129-151. [CrossRef]
467. A. Sarajedini, R. Hecht-Nielsen, P.M. Chau. 1999. Conditional probability density
function estimation with sigmoidal neural networks. IEEE Transactions on Neural
Networks 10:2, 231-238. [CrossRef]
468. Ran Avnimelech, Nathan Intrator. 1999. Boosting Regression Estimators. Neural
Computation 11:2, 499-520. [Abstract] [PDF] [PDF Plus]
469. Suzanna Becker. 1999. Implicit Learning in 3D Object Recognition: The
Importance of Temporal Context. Neural Computation 11:2, 347-374. [Abstract]
[PDF] [PDF Plus]
470. C. Di Natale, E. Proietti, R. Diamanti, A. D'Amico. 1999. Modeling of APCVD-
doped silicon dioxide deposition process by a modular neural network. IEEE
Transactions on Semiconductor Manufacturing 12:1, 109-115. [CrossRef]
471. Matthew N. Dailey, Garrison W. CottrellChapter 10 Prosopagnosia in modular
neural network models 121, 165-184. [CrossRef]
472. Robert A. Jacobs. 1999. Computational studies of the development of functionally
specialized neural modules. Trends in Cognitive Sciences 3:1, 31-38. [CrossRef]
473. Kenneth J. Kurtz, Dedre Gentner, Virginia GunnReasoning 145-200. [CrossRef]
474. V. Ramamurti, J. Ghosh. 1999. Structurally adaptive modular networks for
nonstationary environments. IEEE Transactions on Neural Networks 10:1, 152-160.
[CrossRef]
475. M A Wani, D T Pham. 1999. Efficient control chart pattern recognition through
synergistic and distributed artificial neural networks. Proceedings of the Institution of
Mechanical Engineers, Part B: Journal of Engineering Manufacture 213:2, 157-169.
[CrossRef]


--- Page 44 ---

476. P Schyns. 1999. Dr. Angry and Mr. Smile: when categorization flexibly modifies
the perception of faces in rapid visual presentations. Cognition 69:3, 243-265.
[CrossRef]
477. S Khan. 1998. Statistical and neural classifiers to detect traffic operational problems
on urban arterials. Transportation Research Part C: Emerging Technologies 6:5-6,
291-314. [CrossRef]
478. Tom Ziemke. 1998. Adaptive Behavior in Autonomous Agents. Presence:
Teleoperators and Virtual Environments 7:6, 564-587. [Abstract] [PDF] [PDF Plus]
479. Rainer Dietrich, Manfred Opper. 1998. Journal of Physics A: Mathematical and
General 31:46, 9131-9147. [CrossRef]
480. I-Cheng Yeh. 1998. Modeling Concrete Strength with Augment-Neuron
Networks. Journal of Materials in Civil Engineering 10:4, 263-268. [CrossRef]
481. R O'Reilly. 1998. Six principles for biologically based computational models of
cortical cognition. Trends in Cognitive Sciences 2:11, 455-462. [CrossRef]
482. C.L. Fancourt, J.C. Principe. 1998. Competitive principal component analysis
for locally stationary time series. IEEE Transactions on Signal Processing 46:11,
3068-3081. [CrossRef]
483. R. Sun, T. Peterson. 1998. Autonomous learning of sequential tasks: experiments
and analyses. IEEE Transactions on Neural Networks 9:6, 1217-1234. [CrossRef]
484. K. Rose. 1998. Deterministic annealing for clustering, compression, classification,
regression, and related optimization problems. Proceedings of the IEEE 86:11,
2210-2239. [CrossRef]
485. J. Yen, Liang Wang, C.W. Gillespie. 1998. Improving the interpretability of TSK
fuzzy models by combining global learning and local learning. IEEE Transactions
on Fuzzy Systems 6:4, 530-537. [CrossRef]
486. D Wolpert. 1998. Multiple paired forward and inverse models for motor control.
Neural Networks 11:7-8, 1317-1329. [CrossRef]
487. J Buessler. 1998. Visually guided movements: learning with modular neural maps
in robotics. Neural Networks 11:7-8, 1395-1415. [CrossRef]
488. S.K.T. Kriebel, W. Brauer, W. Eifler. 1998. Coastal upwelling prediction with a
mixture of neural networks. IEEE Transactions on Geoscience and Remote Sensing
36:5, 1508-1518. [CrossRef]
489. Chi-Hang Lam, F. Shin. 1998. Formation and dynamics of modules in a dual-
tasking multilayer feed-forward neural network. Physical Review E 58:3, 3673-3677.
[CrossRef]
490. C. Ornes, J. Sklansky. 1998. A visual neural classifier. IEEE Transactions on
Systems, Man and Cybernetics, Part B (Cybernetics) 28:4, 620-625. [CrossRef]
491. A Mirhosseini. 1998. Human Face Image Recognition: An Evidence Aggregation
Approach. Computer Vision and Image Understanding 71:2, 213-230. [CrossRef]


--- Page 45 ---

492. James A. Reggia, Sharon Goodall, Yuri Shkuro. 1998. Computational Studies
of Lateralization of Phoneme Sequence Generation. Neural Computation 10:5,
1277-1297. [Abstract] [PDF] [PDF Plus]
493. Sun-Yuan Kung, Jenq-Neng Hwang. 1998. Neural networks for intelligent
multimedia processing. Proceedings of the IEEE 86:6, 1244-1272. [CrossRef]
494. Y. Shimshoni, N. Intrator. 1998. Classification of seismic signals by integrating
ensembles of neural networks. IEEE Transactions on Signal Processing 46:5,
1194-1201. [CrossRef]
495. Ke Chen. 1998. A connectionist method for pattern classification with diverse
features. Pattern Recognition Letters 19:7, 545-558. [CrossRef]
496. A.J. Zeevi, R. Meir, V. Maiorov. 1998. Error bounds for functional approximation
and estimation using mixtures of experts. IEEE Transactions on Information
Theory 44:3, 1010-1025. [CrossRef]
497. P Szymanski. 1998. Hybrid interior point training of modular neural networks.
Neural Networks 11:2, 215-234. [CrossRef]
498. R. Rae, H.J. Ritter. 1998. Recognition of human head orientation based on artificial
neural networks. IEEE Transactions on Neural Networks 9:2, 257-265. [CrossRef]
499. M.M. Poulton, R.A. Birken. 1998. Estimating one-dimensional models from
frequency-domain electromagnetic data using modular neural networks. IEEE
Transactions on Geoscience and Remote Sensing 36:2, 547-555. [CrossRef]
500. David J. Miller, Hasan S. Uyar. 1998. Combined Learning and Use for a Mixture
Model Equivalent to the RBF Classifier. Neural Computation 10:2, 281-293.
[Abstract] [PDF] [PDF Plus]
501. E Bax. 1998. Validation of average error rate over classifiers. Pattern Recognition
Letters 19:2, 127-132. [CrossRef]
502. Kishan Mehrotra, Chilukuri K. MohanModular neural networks 3, 147-181.
[CrossRef]
503. Ian T Nabney, Christopher M BishopModeling Wind Direction from Satellite
Scatterometer Data 295-301. [CrossRef]
504. Itiel E. Dror, Christopher S. SchreinerChapter 4 Neural networks and perception
126, 77-85. [CrossRef]
505. 1998. Analysis of powder factors for tunnel blasting using neural networks.
Fragblast 2:4, 433-448. [CrossRef]
506. Monica Bianchini, Paolo Frasconi, Marco Gori, Marco MagginiOptimal learning
in artificial neural networks: A theoretical view 2, 1-51. [CrossRef]
507. Colin CampbellConstructive learning techniques for designing neural network
systems 3, 91-145. [CrossRef]
508. Yoram Singer. 1997. Adaptive Mixtures of Probabilistic Transducers. Neural
Computation 9:8, 1711-1733. [Abstract] [PDF] [PDF Plus]


--- Page 46 ---

509. Athanasios Kehagias, Vassilios Petridis. 1997. Time-Series Segmentation Using
Predictive Modular Neural Networks. Neural Computation 9:8, 1691-1709.
[Abstract] [PDF] [PDF Plus]
510. A.L. McIlraith, H.C. Card. 1997. Birdsong recognition using backpropagation and
multivariate statistics. IEEE Transactions on Signal Processing 45:11, 2740-2748.
[CrossRef]
511. A.V. Rao, D. Miller, K. Rose, A. Gersho. 1997. Mixture of experts regression
modeling by deterministic annealing. IEEE Transactions on Signal Processing 45:11,
2811-2820. [CrossRef]
512. N INTRATOR, S EDELMAN. 1997. Competitive learning in biological and
artificial neural computation. Trends in Cognitive Sciences 1:7, 268-272. [CrossRef]
513. C Williams. 1997. Instantiating Deformable Models with a Neural Net. Computer
Vision and Image Understanding 68:1, 120-126. [CrossRef]
514. J Newman. 1997. A Neural Global Workspace Model for Conscious Attention.
Neural Networks 10:7, 1195-1206. [CrossRef]
515. H Bersini. 1997. Now comes the time to defuzzify neuro-fuzzy models. Fuzzy Sets
and Systems 90:2, 161-169. [CrossRef]
516. Robert A. Jacobs. 1997. Nature, nurture, and the development of functional
specializations: A computational approach. Psychonomic Bulletin & Review 4:3,
299-309. [CrossRef]
517. Daniel M. Wolpert. 1997. Computational approaches to motor control. Trends in
Cognitive Sciences 1:6, 209-216. [CrossRef]
518. Yu Hen Hu, S. Palreddy, W.J. Tompkins. 1997. A patient-adaptable ECG beat
classifier using a mixture of experts approach. IEEE Transactions on Biomedical
Engineering 44:9, 891-900. [CrossRef]
519. V. Petridis, A. Kehagias. 1997. Predictive modular fuzzy systems for time-series
classification. IEEE Transactions on Fuzzy Systems 5:3, 381-397. [CrossRef]
520. Kukjin Kang, Jong-Hoon Oh, Chulan Kwon. 1997. Learning by a population of
perceptrons. Physical Review E 55:3, 3257-3261. [CrossRef]
521. R Jacobs. 1997. A Bayesian Approach to Model Selection in Hierarchical Mixtures-
of-Experts Architectures. Neural Networks 10:2, 231-241. [CrossRef]
522. Robert A. Jacobs. 1997. Bias/Variance Analyses of Mixtures-of-Experts
Architectures. Neural Computation 9:2, 369-383. [Abstract] [PDF] [PDF Plus]
523. J.A. Benediktsson, J.R. Sveinsson, O.K. Ersoy, P.H. Swain. 1997. Parallel
consensual neural networks. IEEE Transactions on Neural Networks 8:1, 54-64.
[CrossRef]
524. Ath. Kehagias, Vas. Petridis. 1997. Predictive Modular Neural Networks for Time
Series Classification. Neural Networks 10:1, 31-49. [CrossRef]


--- Page 47 ---

525. Sung-Bae Cho. 1997. Neural-network classifiers for recognizing totally
unconstrained handwritten numerals. IEEE Transactions on Neural Networks 8:1,
43-53. [CrossRef]
526. Shimon Edelman, Nathan IntratorLearning as Extraction of Low-Dimensional
Representations 36, 353-380. [CrossRef]
527. Anders Krogh, Peter Sollich. 1997. Statistical mechanics of ensemble learning.
Physical Review E 55:1, 811-825. [CrossRef]
528. Elizabeth Bates, Donna Thal, Doris Trauner, Judi Fenson, Dorothy Aram, Julie
Eisele, Ruth Nass. 1997. From first words to grammar in children with focal brain
injury. Developmental Neuropsychology 13:3, 275-343. [CrossRef]
529. Chuanyi Ji, Sheng Ma. 1997. Combinations of weak classifiers. IEEE Transactions
on Neural Networks 8:1, 32-42. [CrossRef]
530. Assaf J. Zeevi, Ronny Meir. 1997. Density Estimation Through Convex
Combinations of Densities: Approximation and Estimation Bounds. Neural
Networks 10:1, 99-109. [CrossRef]
531. V Desai. 1996. A comparison of neural networks and linear scoring models in the
credit union environment. European Journal of Operational Research 95:1, 24-37.
[CrossRef]
532. R.C. Miall, D.M. Wolpert. 1996. Forward Models for Physiological Motor
Control. Neural Networks 9:8, 1265-1279. [CrossRef]
533. Pierre Baldi, Yves Chauvin. 1996. Hybrid Modeling, HMM/NN Architectures,
and Protein Applications. Neural Computation 8:7, 1541-1565. [Abstract] [PDF]
[PDF Plus]
534. Y. Bengio, P. Frasconi. 1996. Input-output HMMs for sequence processing. IEEE
Transactions on Neural Networks 7:5, 1231-1249. [CrossRef]
535. Fengchun Peng, Robert A. Jacobs, Martin A. Tanner. 1996. Bayesian Inference
in Mixtures-of-Experts and Hierarchical Mixtures-of-Experts Models with an
Application to Speech Recognition. Journal of the American Statistical Association
91:435, 953-960. [CrossRef]
536. David A. Cohn. 1996. Neural Network Exploration Using Optimal Experiment
Design. Neural Networks 9:6, 1071-1083. [CrossRef]
537. Christopher M. Bishop, Ian T. Nabney. 1996. Modeling Conditional Probability
Distributions for Periodic Variables. Neural Computation 8:5, 1123-1133.
[Abstract] [PDF] [PDF Plus]
538. A. Khotanzad, J.J.-H. Liou. 1996. Recognition and pose estimation of unoccluded
three-dimensional objects from a two-dimensional perspective view by banks of
neural networks. IEEE Transactions on Neural Networks 7:4, 897-906. [CrossRef]
539. J.del.R. Millan. 1996. Rapid, safe, and incremental learning of navigation strategies.
IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) 26:3,
408-420. [CrossRef]


--- Page 48 ---

540. Peter M. Williams. 1996. Using Neural Networks to Model Conditional
Multivariate Densities. Neural Computation 8:4, 843-854. [Abstract] [PDF] [PDF
Plus]
541. D. Sarkar. 1996. Randomness in generalization ability: a source to improve it. IEEE
Transactions on Neural Networks 7:3, 676-685. [CrossRef]
542. E. Alpaydin, M.I. Jordan. 1996. Local linear perceptrons for classification. IEEE
Transactions on Neural Networks 7:3, 788-794. [CrossRef]
543. Klaus Pawelzik, Jens Kohlmorgen, Klaus-Robert Müller. 1996. Annealed
Competition of Experts for a Segmentation and Classification of Switching
Dynamics. Neural Computation 8:2, 340-356. [Abstract] [PDF] [PDF Plus]
544. Vassilios Petridis, Athanasios Kehagias. 1996. A Recurrent Network
Implementation of Time Series Classification. Neural Computation 8:2, 357-372.
[Abstract] [PDF] [PDF Plus]
545. David Heath, Simon Kasif, Steven SalzbergChapter 18 Committees of decision trees
113, 305-317. [CrossRef]
546. V. Petridis, A. Kehagias. 1996. Modular neural networks for MAP classification
of time series and the partition algorithm. IEEE Transactions on Neural Networks
7:1, 73-86. [CrossRef]
547. Masahiko Shizawa. 1996. Multivalued regularization network-a theory of
multilayer networks for learning many-to-h mappings. Electronics and
Communications in Japan (Part III: Fundamental Electronic Science) 79:9, 98-113.
[CrossRef]
548. Sung-Bae Cho. 1995. Fuzzy aggregation of modular neural networks with ordered
weighted averaging operators. International Journal of Approximate Reasoning 13:4,
359-375. [CrossRef]
549. C Malsburg. 1995. Binding in models of perception and brain function. Current
Opinion in Neurobiology 5:4, 520-526. [CrossRef]
550. Younès Bennani. 1995. A Modular and Hybrid Connectionist System for Speaker
Identification. Neural Computation 7:4, 791-798. [Abstract] [PDF] [PDF Plus]
551. W Duch. 1995. Feature space mapping as a universal adaptive system. Computer
Physics Communications 87:3, 341-371. [CrossRef]
552. Peter Dayan, Richard S. Zemel. 1995. Competition and Multiple Cause Models.
Neural Computation 7:3, 565-579. [Abstract] [PDF] [PDF Plus]
553. G. Deco, D. Obradovic. 1995. Decorrelated Hebbian Learning for Clustering and
Function Approximation. Neural Computation 7:2, 338-348. [Abstract] [PDF]
[PDF Plus]
554. M SPITZER. 1995. A neurocomputational approach to delusions. Comprehensive
Psychiatry 36:2, 83-105. [CrossRef]


--- Page 49 ---

555. Tor A. Johansen, Bjarne A. Foss. 1995. Identification of non-linear system
structure and parameters using regime decomposition. Automatica 31:2, 321-326.
[CrossRef]
556. N Borghese. 1995. Generation of Temporal Sequences Using Local Dynamic
Programming. Neural Networks 8:1, 39-54. [CrossRef]
557. S Amari. 1995. Information geometry of the EM and em algorithms for neural
networks. Neural Networks 8:9, 1379-1408. [CrossRef]
558. Tor A. Johansen, Bjarne A. Foss. 1995. Semi-empirical modeling of non-linear
dynamic systems through identification of operating regimes and local models.
Modeling, Identification and Control: A Norwegian Research Bulletin 16:4, 213-232.
[CrossRef]
559. M Jordan. 1995. Convergence results for the EM approach to mixtures of experts
architectures. Neural Networks 8:9, 1409-1431. [CrossRef]
560. Steven J. Nowlan, Terrence J. Sejnowski. 1994. Filter selection model for motion
segmentation and velocity integration. Journal of the Optical Society of America A
11:12, 3177. [CrossRef]
561. S Sathiya Keerthi, B Ravindran. 1994. A tutorial survey of reinforcement learning.
Sadhana 19:6, 851-889. [CrossRef]
562. R. S. Shadafan, M. Niranjan. 1994. A Dynamic Neural Network Architecture by
Sequential Partitioning of the Input Space. Neural Computation 6:6, 1202-1222.
[Abstract] [PDF] [PDF Plus]
563. Robert A. Jacobs, Stephen M. Kosslyn. 1994. Encoding Shape and Spatial
Relations: The Role of Receptive Field Size in Coordinating Complementary
Representations. Cognitive Science 18:3, 361-386. [CrossRef]
564. Alan L. Yuille, Paul Stolorz, Joachim Utans. 1994. Statistical Physics, Mixtures
of Distributions, and the EM Algorithm. Neural Computation 6:2, 334-340.
[Abstract] [PDF] [PDF Plus]
565. R BATTITI, A COLLA. 1994. Democracy in neural nets: Voting schemes for
classification. Neural Networks 7:4, 691-707. [CrossRef]
566. Garrison Cottrell, Kim Plunkett. 1994. Acquiring the Mapping from Meaning to
Sounds. Connection Science 6:4, 379-412. [CrossRef]
567. E.L. Hanzevack, T.W. Long, J.F. Menousek. 1994. NEURAL NETS SPEED
FLOW CALCULATION. Chemical Engineering Communications 129:1, 41-51.
[CrossRef]
568. ANNETTE KARMILOFF-SMITH, ANDY CLARK. 1993. What's Special
About the Development of the Human Mind/Brain?. Mind & Language 8:4,
569-581. [CrossRef]
569. WILLIAM BECHTEL. 1993. The Path Beyond First-Order Connectionism.
Mind & Language 8:4, 531-539. [CrossRef]


--- Page 50 ---

570. William Bechtel. 1993. Currents in connectionism. Minds and Machines 3:2,
125-153. [CrossRef]
571. J McClelland. 1993. Computational approaches to cognition: top-down
approaches. Current Opinion in Neurobiology 3:2, 209-216. [CrossRef]
572. Suzanna Becker, Geoffrey E. Hinton. 1993. Learning Mixture Models of Spatial
Coherence. Neural Computation 5:2, 267-277. [Abstract] [PDF] [PDF Plus]
573. Lei Xu. 1993. Least mean square error reconstruction principle for self-organizing
neural-nets. Neural Networks 6:5, 627-648. [CrossRef]
574. Robert A. Jacobs,, Michael I. Jordan. 1992. Computational Consequences of a
Bias toward Short Connections. Journal of Cognitive Neuroscience 4:4, 323-336.
[Abstract] [PDF] [PDF Plus]
575. Robert A. Jacobs, Michael I. Jordan, Andrew G. Barto. 1991. Task Decomposition
Through Competition in a Modular Connectionist Architecture: The What and
Where Vision Tasks. Cognitive Science 15:2, 219-250. [CrossRef]
576. Terence D. Sanger. 1991. A Tree-Structured Algorithm for Reducing
Computation in Networks with Separable Basis Functions. Neural Computation
3:1, 67-78. [Abstract] [PDF] [PDF Plus]
View publication stats
```

---

## Section 1.4: Algebraic_Point_Set_Surfaces

Source File: Algebraic_Point_Set_Surfaces.txt

### Paper Content:

```
# Algebraic_Point_Set_Surfaces.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Algebraic_Point_Set_Surfaces.pdf
# Output: ../layer2_completion/txt/Algebraic_Point_Set_Surfaces.txt


--- Page 1 ---

Algebraic Point Set Surfaces
Ga¨el Guennebaud
Markus Gross
ETH Zurich
Figure 1: Illustration of the central features of our algebraic MLS framework. From left to right: efﬁcient handling of very complex point
sets, fast mean curvature evaluation and shading, signiﬁcantly increased stability in regions of high curvature, sharp features with controlled
sharpness. Sample positions are partly highlighted.
Abstract
In this paper we present a new Point Set Surface (PSS) deﬁnition
based on moving least squares (MLS) ﬁtting of algebraic spheres.
Our surface representation can be expressed by either a projection
procedure or in implicit form. The central advantages of our ap-
proach compared to existing planar MLS include signiﬁcantly im-
proved stability of the projection under low sampling rates and in
the presence of high curvature. The method can approximate or
interpolate the input point set and naturally handles planar point
clouds. In addition, our approach provides a reliable estimate of the
mean curvature of the surface at no additional cost and allows for
the robust handling of sharp features and boundaries. It processes
a simple point set as input, but can also take signiﬁcant advantage
of surface normals to improve robustness, quality and performance.
We also present an novel normal estimation procedure which ex-
ploits the properties of the spherical ﬁt for both direction estima-
tion and orientation propagation. Very efﬁcient computational pro-
cedures enable us to compute the algebraic sphere ﬁtting with up to
40 million points per second on latest generation GPUs.
CR Categories:
I.3.5 [Computer Graphics]: Computational Ge-
ometry and Object Modeling—Curve and surface representations
Keywords:
point based graphics, surface representation, moving
least square surfaces, sharp features.
1
Introduction
A key ingredient of most methods in point based graphics is the un-
derlying meshless surface representation which computes a contin-
uous approximation or interpolation of the input point set. The by
far most important and successful class of such meshless represen-
tations are point set surfaces (PSS) [Alexa et al. 2003] combining
high ﬂexibility with ease of implementation. PSS generally deﬁne
a smooth surface using local moving least-squares (MLS) approxi-
mations of the data [Levin 2003]. The degree of the approximation
can easily be controlled, making the approach naturally well suited
to ﬁlter noisy input data. In addition, the semi-implicit nature of
the representation makes PSS an excellent compromise combining
advantages both of explicit representations, such as parametric sur-
faces, and of implicit surfaces [Ohtake et al. 2003].
Since its inception, signiﬁcant progress has been made to better
understand the properties and limitations of MLS [Amenta and
Kil 2004a,2004b] and to develop efﬁcient computational schemes
[Adamson and Alexa 2004]. A central limitation of the robustness
of PSS, however, comes from the plane ﬁt operation that is highly
unstable in regions of high curvature if the sampling rate drops be-
low a threshold. Such instabilities include erroneous ﬁts or the lim-
ited ability to perform tight approximations of the data. This be-
havior sets tight limits to the minimum admissible sampling rates
for PSS [Amenta and Kil 2004b; Dey et al. 2005].
In this paper we present a novel deﬁnition of moving least squares
surfaces called algebraic point set surfaces (APSS). The key idea
is to directly ﬁt a higher order algebraic surface [Pratt 1987] rather
than a plane. For computational efﬁciency all methods in this paper
focus on algebraic sphere ﬁtting, but the general concept could be
applied to higher order surfaces as well. The main advantage of
the sphere ﬁtting is its signiﬁcantly improved stability in situations
where planar MLS fails. For instance, tight data approximation is
accomplished, spheres perform much better in the correct handling
of sheet separation (ﬁgure 3) and exhibit a high degree of stability
both in cases of undersampling (ﬁgure 2) and for very large weight
functions. The speciﬁc properties of algebraic spheres make APSS
superior to simple geometric sphere ﬁtting. It allows us to elegantly
handle planar areas or regions around inﬂection points as limits in
which the algebraic sphere naturally degenerates to a plane.
Furthermore, the spherical ﬁtting enables us to design interpolatory
weighting schemes by using weight functions with singularities at
zero while overcoming the fairness issue of previous MLS surfaces.
The sphere radius naturally serves as a for-free and reliable esti-
mate of the mean curvature of the surface. This enables us, for
instance, to compute realtime accessibility shading on large input
objects (ﬁgure 1).
Central to our framework are the numerical procedures to efﬁciently
perform the sphere ﬁt. For point sets with normals we designed


--- Page 2 ---

(a)
(b)
(c)
Figure 2: The undersampled ear of the Stanford bunny (a) using
normal averaging plane ﬁt (SPSS) with h = 1.8 (b) and our new
APSS with h = 1.7 (c).
a greatly simpliﬁed and accelerated algorithm whose core part re-
duces to linear least squares. For point sets without normals, we
employ a slightly more expensive ﬁtting scheme to estimate the
surface normals of the input data. In particular, this method al-
lows us to improve the normal estimation by [Hoppe et al. 1992].
The tighter ﬁt of the sphere requires on average less projection it-
erations to achieve the same precision making the approach even
faster than the most simple plane ﬁt MLS. Our implementation on
the latest generation GPUs features a performance up to 45 millions
of points per second, sufﬁcient to compute a variety of operations
on large point sets in realtime.
Finally, we developed a simple and powerful extension of [Fleish-
man et al. 2005] to robustly handle sharp features, such as bound-
aries and creases, with a built-in sharpness control (ﬁgure 1).
2
Related Work
Point set surfaces were introduced to computer graphics by [Alexa
et al. 2003]. The initial deﬁnition is based on the stationary set
of Levin’s moving least squares (MLS) projection operator [Levin
2003]. The iterative projection involves a non-linear optimization
to ﬁnd the local reference plane, a bivariate polynomial ﬁt and it-
eration. By omitting the polynomial ﬁtting step, Amenta and Kil
[2004a] showed that the same surface can be deﬁned and computed
by weighted centroids and a smooth gradient ﬁeld. This leads to
a signiﬁcantly simpliﬁed implicit surface deﬁnition [Adamson and
Alexa 2004] and faster algorithms, especially in the presence of
normals [Alexa and Adamson 2004]. In particular, the projection
can be accomplished on a plane passing through the weighted av-
erage of the neighboring samples with a normal computed from the
weighted average of the adjacent normals. In the following we will
refer to this efﬁcient variant as SPSS for Simple PSS.
Boissonnat and Cazals [2000] and more speciﬁcally Shen et al.
[2004] proposed a similar, purely implicit MLS (IMLS) surface
(a)
(b)
(c)
Figure 3: Sheet separation with conventional PSS compared to our
new spherical ﬁt: (a) Standard PSS with covariance analysis (or-
ange) and normal averaging (green). (b) Our APSS without (or-
ange) and with (green) normal constraints. The best plane and best
algebraic sphere ﬁts for the blue point in the middle are drawn in
blue. For this example the normals were computed using our tech-
nique from section 5 which can safely propagate the orientation
between the two sheets (c).
Figure 4: A 2D oriented point set is approximated (left) and in-
terpolated (right) using various PSS variants: SPSS (blue), IMLS
(red), HPSS (magenta) and our APSS (green).
representation deﬁned by a local weighted average of tangential im-
plicit planes attached to each input sample. This implicit surface
deﬁnition was initially designed to reconstruct polygon soups, but
the point cloud case was recently analyzed by Kolluri [2005] and
made adaptive to the local feature size by Dey et al. [2005]. Note
that in this paper IMLS refers to the simple deﬁnition given in [Kol-
luri 2005]. If applied without polygon constraints and in the case
of sparse sampling, we found that the surface can grow or shrink
signiﬁcantly as a function of the convexity or concavity of the point
cloud. To alleviate this problem, Alexa and Adamson [2006] en-
forced convex interpolation of an oriented point set using singular
weight functions and Hermite centroid evaluations (HPSS).
Likewise relevant to our research is prior art on normal estimation.
Many point based algorithms, including some of the aforedescribed
PSS variants, require surface normals. A standard procedure pro-
posed by [Hoppe et al. 1992] is to estimate their directions using
a local plane ﬁt followed by a propagation of the orientations us-
ing a minimum spanning tree (MST) strategy and a transfer heuris-
tic based on normal angles. Extending this technique, Mitra and
Nguyen [2003] take into account the local curvature and the amount
of noise in order to estimate the optimal size of the neighborhood
for the plane ﬁt step. However, the inherent limitations of the plane
ﬁt step and the propagation heuristic require a very dense sampling
rate in regions of high curvature.
Intrinsically, a PSS can only deﬁne a smooth closed manifold sur-
face. Even though boundaries could be handled by thresholding an
off-center distance [Adamson and Alexa 2004], obtaining satisfac-
tory boundary curves with such an approach is usually difﬁcult or
even impossible. In order to detect and reconstruct sharp creases
and corners in a possibly noisy point cloud, Fleishman et al. [2005]
proposed a reﬁtting algorithm that locally classiﬁes the samples
into multiple pieces of surfaces according to discontinuities of the
derivative of the surface. While constituting an important progress,
the method requires very dense point clouds, it is rather expensive,
and it offers only limited ﬂexibility to the user. Also, potential insta-
bilities in the classiﬁcation can create discontinuous surface parts.
A second important approach is the Point-Sampled Cell Complexes
[Adamson and Alexa 2006b] which allows to explicitly represent
sharp features by decomposing the object into cells of different di-
mensions. While this approach allows to handle a wide variety of
cases (e.g., non-manifoldness), the decomposition and the balanc-
ing of the cells’ inﬂuence on the shape of the surface demands effort
by the user, making the method unsuitable for some applications.
As we will elaborate in the following sections, the algebraic sphere
ﬁtting overcomes many of the aforedescribed limitations by the sig-
niﬁcantly increased robustness of the ﬁt in the presence of low sam-
pling rates and high curvature.
3
Overview of the APSS framework
Given a set of points P = {pi ∈Rd}, we deﬁne a smooth surface
SP approximating P using a moving least squares spherical ﬁt to
the data. Our approach handles both simple point clouds and point


--- Page 3 ---

c
pi
p
i
pj
c
c
u(p )
i
u(x)
u(m)
m
x=q0
q1
(a)
(b)
(c)
(d)
Normal direction 
estimation (5.1)
using a sphere fitting 
without normal (4.2)
Normal orientation 
propagation (5.2)
using a sphere fitting 
without normal (4.2)
Projection operator (4.4) 
using normal constraint 
sphere fitting (4.3)
Implicit definition (4.4)
Figure 5: Overview of our APSS framework. (a) Evaluation of the normal direction at point pi. (b) Propagation of a consistent normal
orientation from pi to pj. (c) First iteration of the projection of the point x onto the APSS. (d) Illustration of the scalar ﬁeld deﬁned by APSS.
clouds with normals. The presence of surface normals leads to sim-
pliﬁed, more efﬁcient, and more robust ﬁtting algorithms (see sec-
tion 4.3). We recommend to estimate them from the input point set
using the procedures of section 5 and section 4.2 as a preprocessing
step.
Figure 5 presents the procedural ﬂow using a 2D example. Starting
from a simple point cloud, we ﬁrst evaluate the normal directions
by locally ﬁtting an algebraic sphere u at each point pi (ﬁgure 5a).
Next, a consistent normal orientation is robustly propagated from
the point pi to its neighbor pj by approximating the surface in-
between using an approximating sphere (ﬁgure 5b). Given the es-
timated normals, a simpliﬁed spherical ﬁtting technique can be ap-
plied to compute our ﬁnal APSS SP which can be deﬁned as the
set of stationary points of a projection operator projecting a point
onto a locally ﬁtted sphere (ﬁgure 5c). As an alternative our surface
SP can also be deﬁned as the zero set of a scalar ﬁeld representing
the distance between the evaluation point and a locally ﬁtted sphere
(ﬁgure 5d). The former deﬁnition is well suited for resampling,
while the latter one is convenient to raytrace the surface [Adam-
son and Alexa 2003; Wald and Seidel 2005] or to perform Boolean
operations [Pauly et al. 2003].
The key ingredients of our framework are the computational algo-
rithms to robustly and efﬁciently ﬁt a sphere to a set of points in a
moving least squares fashion. These methods will be presented for
the cases of a simple point cloud as well as for a point cloud with
normals in sections 4.2 and 4.3. Based on the procedures for alge-
braic sphere ﬁts, we will deﬁne and discuss our APSS deﬁnition in
section 4.4. Two signiﬁcant extensions include normal estimation
in section 5 and the handling of sharp features in section 6.
4
Sphere Fitting and APSS
In this section we will discuss the core mathematical deﬁnitions and
computational procedures of our APSS framework. We will elabo-
rate on the sphere ﬁtting problem and show how previous methods
can be adapted to deﬁne a MLS surface based on sphere ﬁts. As a
central computational algorithm, we will present a novel and very
efﬁcient sphere ﬁtting method taking into account surface normals.
4.1
General Issues
Weighting scheme. Throughout the paper we will utilize the fol-
lowing generic weight function
wi(x) = φ
∥pi −x∥
hi(x)

(1)
describing the weight of the point pi for the local evaluation po-
sition x. φ is a smooth, decreasing weight function and hi(x) de-
scribes the local feature size. hi(x) can be constant, depend on x as
in [Pauly et al. 2003], or only on pi, i.e., hi(x) = hi as in [Adamson
and Alexa 2006a]. As a proper choice of φ we suggest the follow-
ing compactly supported polynomial
φ(x) =

(1−x2)4
if x < 1
0
otherwise.
(2)
This function performs a smooth data approximation and avoids
square root evaluations for the distance computation. Interpolation
can be achieved using functions with a singularity at zero, see [Shen
et al. 2004; Adamson and Alexa 2006b].
Spherical ﬁtting. The problem of ﬁtting a sphere to a set of points
is not new and several methods have been proposed in the past.
It is important to distinguish geometric approaches from algebraic
ones. Geometric ﬁtting denotes algorithms minimizing the sum of
the squared Euclidean distances to the given points (e.g., see [Gan-
der et al. 1994]). Geometric sphere ﬁts have several drawbacks.
First, because of the non-linear nature of the problem, the solution
can only be found using an iterative, expensive approach. More im-
portantly, since these algorithms are either based on a center-radius
or parametric representation of the sphere, they become unstable
when the best ﬁt tends to a plane. This limits the practical utility of
such methods for our purposes.
An elegant alternative is to substitute the geometric distance by an
algebraic distance as in [Pratt 1987]. An algebraic sphere is thus
deﬁned as the 0-isosurface of the scalar ﬁeld su(x) = [1, xT , xT x]u,
where u = [u0, ..., ud+1]T ∈Rd+2 is the vector of scalar coefﬁcients
describing the sphere. For ud+1 ̸= 0 the corresponding center c and
radius r are easily computed as:
c = −
1
2ud+1
[u1, ..., ud]T , r =
q
cT c−u0/ud+1
(3)
with d being the dimension. In degenerate cases, u corresponds to
the coefﬁcients of a plane equation with u0 representing the plane’s
distance from the origin and [u1,..,ud]T being its normal.
4.2
Fitting Algebraic Spheres Without Normals
Let n be the number of points and let W(x) and D respectively be
the n×n diagonal weight matrix and the n×(d +2) design matrix
deﬁned as:
W(x) =


w0(x)
...
wn−1(x)

, D =


1
pT
0
pT
0 p0
...
...
...
1
pT
n−1
pT
n−1pn−1

.
(4)
Then the solution u(x) of our algebraic sphere ﬁt at a given point
x ∈Rd can be expressed as:
u(x) = argmin
u, u̸=0
W
1
2 (x)Du

2
.
(5)
In order to avoid the trivial solution u(x) = 0, u has to be con-
strained by a metric. The choice of this constraint signiﬁcantly in-
ﬂuences the solution of the above minimization problem. Ideally
we want the algebraic ﬁt to behave as closely as possible to a ge-
ometric ﬁt, but at the same time to reliably handle the planar case.
Among all constrained methods that have been proposed, we found
Pratt’s constraint [Pratt 1987] the most suitable for our purposes.
Pratt’s constraint ﬁxes the norm of the gradient at the surface of the
sphere to unit length 1. This is accomplished by ∥(u1, ..., ud)∥2 −
4u0ud+1 = 1 and ensures that the algebraically ﬁtted sphere is close


--- Page 4 ---

to the least squares Euclidean best ﬁt, i.e., the geometric ﬁt (see
also ﬁgure 6). By rewriting the above constraint in matrix form
uT Cu = 1, the solution u(x) of our minimization problem yields as
the eigenvector of the smallest positive eigenvalue of the following
generalized eigenproblem:
DT W(x)Du(x) = λCu(x), with C =


0
0
···
0
−2
0
1
0
.
.
.
...
..
.
0
1
0
−2
0
···
0
0

.
(6)
Note that this method is only used to estimate the missing normals
of input point sets. An efﬁcient algorithm for running the APSS in
the presence of normals will be derived in the following section.
4.3
Fitting Spheres to Points with Normals
While the problem of ﬁtting a sphere to points has been investigated
extensively, there exists to our knowledge no method to take spe-
ciﬁc advantage of normal constraints at the sample positions. We
derive a very efﬁcient algorithm by adding the following derivative
constraints ∇su(pi) = ni to our minimization problem (5). Note
that, by deﬁnition, it holds that ∥ni∥= 1, which constrains both di-
rection and magnitude of the normal vector. This is especially im-
portant to force the algebraic distance to be close to the Euclidean
distance for points close to the surface of the sphere. Furthermore,
the normal constraints lead to the following standard linear system
of equations that can be solved very efﬁciently:
W
1
2 (x)Du
=
W
1
2 (x)b
(7)
where
W(x) =


. . .
wi(x)
βwi(x)
. . .
βwi(x)
. . .


, D =


.
..
.
..
.
..
1
pT
i
pT
i pi
0
eT
0
2eT
0 pi
.
..
.
..
.
..
0
eT
d−1
2eT
d−1pi
.
..
.
..
.
..


, b =


.
.
.
0
eT
0 ni
.
..
eT
d−1ni
.
..


. (8)
Here, {ek} denote the unit basis vectors of our coordinate system.
The scalar β allows us to weight the normal constraints. We will
discuss the proper choice of this parameter subsequently. We solve
this equation using the pseudo-inverse method, i.e.:
u(x) = A−1(x)ˆb(x)
(9)
where both the (d+2)×(d+2) weighted covariance matrix A(x) =
DT W(x)D and the vector ˆb(x) = DT W(x)b can be computed di-
rectly and efﬁciently by weighted sums of the point coefﬁcients.
The issue of afﬁne invariance of this method deserves some further
discussion. While the method’s invariance under translations and
rotations is trivial, the mix of constraints representing algebraic dis-
tances and distances between unit vectors makes it slightly sensitive
to scale. A simple and practical solution is to choose a large value
for β, e.g. β = 106h(x)2 where h(x) = ∑i wi(x)hi(x)
∑i wi(x)
is a smooth func-
tion describing the local neighborhood size (1). This effectively
assigns very high importance to the derivative constraints, which,
prescribed at given positions and with a ﬁxed norm, are sufﬁcient
to ﬁt a spherical isosurface to the data. The positional constraints
still specify the actual isovalue. Not only does this choice leave the
ﬁtting invariant under scale, but it also makes it more stable, much
less prone to oscillations (ﬁgure 3b) and less sensitive to outliers.
This choice does not unbalance the relative importance of the posi-
tions and normals of the samples because the derivative constraints
depend on both the sample positions and normals.
Figure 6: The Euclidean distance (green cone) versus an algebraic
distance (orange paraboloid) to the 2D yellow circle. Pratt’s nor-
malization makes these two surfaces tangent to each other at the
given circle.
4.4
Algebraic Point Set Surfaces
Implicit surface deﬁnition. The previous sections provide all in-
gredients to deﬁne an MLS surface based on sphere ﬁts. Our APSS
SP, approximating or interpolating the point set P = {pi ∈Rd},
yields as the zero set of the implicit scalar ﬁeld f(x) representing
the algebraic distance between the evaluation point x and the ﬁtted
sphere u(x). This ﬁeld is illustrated in ﬁgure 5d:
f(x) = su(x)(x) =
h
1, xT , xT x
i
u(x) = 0 .
(10)
If needed the actual Euclidean distance to the ﬁtted algebraic sur-
face can be computed easily by its conversion to an explicit form.
Gradient. This implicit deﬁnition allows us to conveniently com-
pute the gradient of the scalar ﬁeld which is needed to obtain the
surface normal or to perform an orthogonal projection. We com-
pute the gradient ∇f(x) as follows:
∇f(x) = [1, xT , xT x]∇u(x) +


0
eT
0
2eT
0 x
...
...
...
0
eT
d−1
2eT
d−1x

u(x).
(11)
The value of the gradient ∇u(x) =

du(x)
dx0 , du(x)
dx1 , ...

depends on the ﬁt-
ting method. With the Pratt’s constraint, the derivatives can be com-
puted as the plane ﬁt with covariance analysis case (see [Alexa and
Adamson 2004]). With our normal constraint, the derivatives can
be directly computed from equation (9):
du(x)
dxk
=
A−1(x)

−dA(x)
dxk
u(x)+ d ˆb(x)
dxk

.
(12)
Curvature. The implicit deﬁnition can be used to compute higher
order differential surface operators, such as curvature.
In prac-
tice, however, the evaluation of the shape matrix involves expensive
computations. Our sphere ﬁt provides an elegant estimate of the
mean curvature readily available by the radius of the ﬁtted sphere
(3). This estimate of the mean curvature is in general very accurate,
except when two pieces of a surface are too close to each other.
In this case the samples of the second surface can lead to an over-
estimation of the curvature. Note that in such cases our normal
constraint ﬁtting method still reconstructs a correct surface without
oscillations (ﬁgure 3b). The sign of this inexpensive mean curva-
ture estimate is determined by the sign of un+1 and can be utilized
for a variety of operations, such as accessibility shading shown in
ﬁgure 1.
Projection procedure.
Since the presented APSS surface deﬁni-
tion is based on a standard MLS, all the projection operators for the
plane case can easily be adapted to our setting by simply replac-
ing the planar projection by a spherical projection. Following the
concept of almost orthogonal projections in [Alexa and Adamson
2004] we recommend the following procedure as a practical recipe
to implement APSS: Given a current point x, we iteratively deﬁne
a series of points qi, such that qi+1 is the orthogonal projection of
x onto the algebraic sphere deﬁned by u(qi). Starting with q0 = x,


--- Page 5 ---

the projection of x onto the surface is asymptotically q∞. The re-
cursion stops when the displacement is lower than a given precision
threshold. This procedure is depicted in ﬁgure 5c.
Including surface normals into the deﬁnition comes with additional
signiﬁcant advantages. First, it makes the scalar ﬁeld f(x) to be
consistently signed according to the inside/outside relative position
of x. Such consistent orientation is fundamental to perform boolean
operations or collision detections for instance. Furthermore, point-
normals provide a ﬁrst order approximation of the underlying sur-
face and thus allow for smaller neighborhoods at the same accuracy.
Tighter neighborhoods do not only increase stability, but also accel-
erate the processing. To this end, we recommend the normal esti-
mation procedure described in the following section to preprocess
a raw input point cloud.
5
Estimation of Surface Normals
Our method for normal estimation draws upon the same principle as
the one proposed by Hoppe et al. [1992]. The novelties include the
normal direction estimation and new heuristics for the propagation
of the normal orientation. We will focus on these two issues.
5.1
Normal Direction
To estimate the normal direction ni of a point pi of our input point
set, we essentially take the gradient direction at pi of our MLS sur-
face deﬁnition described in the previous section, i.e., ni = ∇f(pi).
Here, the computation of the algebraic sphere is based on the eigen-
vector method described in section 4.2. In practice, we can approx-
imate the accurate gradient direction of the scalar ﬁeld f (appendix)
with the gradient of the ﬁtted algebraic sphere u(pi), i.e.:
ni ≈∇su(pi)(pi) =


0
eT
0
2eT
0 pi
...
...
...
0
eT
d−1
2eT
d−1pi

u(pi).
(13)
This step is illustrated in ﬁgure 5a. Although the sphere normal
can differ from the actual surface normal, the above simpliﬁcation
is reasonable, because the approximation is usually very close to
the accurate surface normal for points evaluated close to an input
sample. In practice, we never experienced any problems with it.
During this step we also store for each point a conﬁdence value
µi that depends on the sanity of the neighborhood used to estimate
the normal direction. We take the normalized residual value, i.e.,
µi = ¯λ/∑d+1
k=0 |λk| where λk are the d + 2 eigenvalues and ¯λ is the
smallest positive one. Similar conﬁdence estimates were used be-
fore by several authors (e.g. [Pauly et al. 2004]) in combination
p
ph
i
pj
m
c
pi
pj
(a)
(b)
Figure 7: (a) Illustration of the BSP neighborhood of a point pi:
each neighbor deﬁnes a halfspace limiting the neighborhood. The
blue points correspond to a naive kNN with k = 6. (b) Illustration
of Ψi j. The angle between the gradient of the ﬁtted sphere (green
arrows) and the normal direction (shown in red here) serves as a
measure for the reliability of the normal propagation.
(a)
(b)
(c)
(f)
(d)
(e)
Figure 8: (a) Closeup view of the tip of the ear of ﬁgure 2 after
recomputing the normals using our method. A few normals are
incorrectly oriented and eventually break the APSS reconstruction
(b) when using a weight radius of h = 2. A smoothed manifold
reconstruction (c) can still be obtained when increasing the weight
radius signiﬁcantly (h = 3.5). Figures (d) and (e) show the normals
after 15 and 30 iterations of our normal optimization procedure
respectively. (f) APSS reconstruction of (e) with h = 2.
with covariance analysis to perform adaptive resampling. We will
use this conﬁdence value in the next section for our propagation
algorithm.
5.2
Propagation of Orientation
Our second contribution concerns the propagation of normal orien-
tation. Following [Hoppe et al. 1992], we use a minimum spanning
tree (MST) strategy which is initialized by setting the normal ori-
entation of a single point in contact with the point set’s bounding
box to point outside.
The major difference of our approach lies in the propagation
method and the weight function for the MST. To propagate the
orientation from a point pi of normal ni to a point pj of normal
nj we locally ﬁt an algebraic sphere at m = pi+pj
2
halfway be-
tween the two points.
Figures 3c and 5b serve for illustration.
The sphere gives us a reliable approximation of the surface con-
necting the two points. It hence allows us to reliably transfer a
given normal orientation even across sharp features or along two
close surface sheets. The normal nj of the point pj is ﬂipped if
∇su(m)(pi)T ni ·∇su(m)(pj)T nj < 0.
The purpose of the weight function of the MST is to favor edges
along which it is safe to transfer the normal orientation using the
previous procedure. The MST assigns an edge i-j if and only if
the two involved samples pi and pj are neighbors. Unlike Hoppe’s
k-nearest neighborhood deﬁnition (kNN), we utilize a somewhat
more elaborate BSP neighborhood deﬁnition. This deﬁnition re-
moves from a large kNN of the point pi every point that is behind
another neighbor, i.e., every point pj such that there exist a third
point ph satisfying (pi −ph)T (pj −ph) < 0. Figure 7a depicts an
example. This neighborhood deﬁnition allows us to select reliably
the closest neighbors in each direction, even in cases of strongly
non-uniform sampling. In addition it avoids that the propagation
procedure jumps over relevant samples.
To compute the weight of an edge i-j we consider the following
two terms: The ﬁrst one takes the sum of the conﬁdence values of
its adjacent vertices: µi j = µi +µj. The rationale of this heuristic is


--- Page 6 ---

to push the samples with the lowest conﬁdence values to the bottom
(i.e. the leaves) of the tree. The second term Ψij is computed by
ﬁrst ﬁtting a sphere at the center m of the edge and by quantifying
the difference between the gradient of the sphere and the estimated
normal direction. Ψi j is deﬁned as the dot product between the two
vectors, i.e., Ψij = 1−(|∇su(m)(pi)T ni|+|∇su(m)(pj)T nj|)/2. As
illustrated in ﬁgure 7b, this conﬁrms our intuition that the propa-
gation of normal directions is less obvious when the two vectors
deviate strongly. We ﬁnally weight each edge i-j by a weighted
sum of µi j and Ψij, an empirical choice being 8µij +Ψi j.
For some rare difﬁcult cases, we propose to use the actual normals
to further optimize the normals with low conﬁdence. This is ac-
complished by using our stable spherical ﬁtting method from sec-
tion 4.3. For the ﬁt, we weight each normal constraint according to
its respective conﬁdence value. This ensures that samples of higher
conﬁdence have increased importance for the ﬁt. To further opti-
mize accuracy, we process the samples starting with the ones having
the highest average conﬁdence of their neighborhood. Our normal
estimation procedure is depicted ﬁgure 8.
6
Sharp Features
In this section we present a new approach to handle sharp features
such as creases, corners, borders or peaks. We will explain it in the
context of APSS, but the discussed techniques can easily be used
with any PSS deﬁnition. Our approach combines the local CSG
rules proposed in [Fleishman et al. 2005] with a precomputed par-
tial classiﬁcation, and we present extensions for the robust handling
of borders and peaks. During the actual projection of a point x onto
the surface, we ﬁrst group the selected neighbors by tag values.
Samples without tags are assigned to all groups. Then, the APSS is
executed for each group and the actual point is projected onto each
algebraic sphere. Next, we detect for each pair of groups whether
the sharp crease they form originates from a boolean intersection or
from a union and apply the corresponding CSG rule. We refer to
[Fleishman et al. 2005] for the details of this last step, and focus on
the novel aspects of our approach.
6.1
Tagging a Point Cloud
In a ﬁrst preprocessing step or at run time, we compute a partial
classiﬁcation of the samples by assigning a tag value to each sample
in the adjacency of a sharp feature. Two adjacent samples must
have the same tag if they are on the same side of the discontinuity
and a different tag otherwise. This classiﬁcation can be achieved
in several ways from fully automatic to entirely manual processing.
Tags can easily be set automatically during a CSG operation [Pauly
et al. 2003; Adams and Dutr´e 2003]. Tags can also be conveniently
painted by using a brush tool. Figure 9 displays a simple tag stroke.
Such interactive painting provides full user control over the position
of sharp features and it can be applied very efﬁciently.
Figure 9: A sharp crease is obtained by painting two strokes on an
initially smooth point cloud.
a
pi
b i
x
i
n (x)
i
same tag
same tag
n j,0
n j,1
n i,0
n i,1
Figure 10: Left: A cubic B´ezier curve connects two samples to
propagate the tagging and to correctly align the respective normals.
Right: Deﬁnition of a peak sample pi of axis ai and angle bi. Its
normal ni(x) depends on the actual evaluation point x.
Automatic Tagging by Local Classiﬁcation
Another way to automatically tag the point cloud would be to use
the local classiﬁcation scheme of Fleishman et al. [2005]. While
being very efﬁcient, this method, however, cannot guarantee a con-
sistent global classiﬁcation. Therefore, we developed the follow-
ing grouping method that guarantees global consistency. The local
classiﬁcation is applied to each sample in a breadth ﬁrst order of
the Euclidean minimum spanning tree, each sample counting the
number of times it is assigned a given tag value. If only one group
is found (smooth part) then the algorithm continues with the next
sample. Otherwise, the tag value of each group is determined from
the tag value with the maximum count of all samples. In addition,
each local group must have different tag values. If required, new
tags are created. Next, the group tags are propagated to their sam-
ples. The ﬁnal tag value of a sample is the one of maximum count.
Tagging from “Sharp” Points
A common way to represent sharp features in point sampled ge-
ometry is to use “sharp” points, i.e., points with multiple normals
[Pauly et al. 2003; Wicke et al. 2004; Guennebaud et al. 2005].
Usually, two normals are assigned to a point on a crease and three
normals to a corner point. We handle such explicit representations
within our framework by the following two step tagging procedure:
The ﬁrst step focuses on tagging the normals of the sharp samples
only. For simplicity, we will ﬁrst only consider the case of edge
points, i.e., points having two normals. We start with any sharp
sample, assign a different tag value to each of its normals, and prop-
agate the tag values along the crease lines into each direction. Our
propagation method is mainly based on the construction of an inter-
polating cubic B´ezier curve between sharp samples. This enables
us to safely and naturally handle high curvature of the crease lines.
Such a curve can be constructed easily by taking the cross prod-
uct of the two normals of the samples as the curve’s end tangent
vectors. Figure 10 shows the idea.
We scale the tangent vectors such that their length is equal to the
third of the Euclidean distance between the two samples and orient
them such that the length of the resulting curve is minimal. The best
possible successor of a sharp point pi along a crease line is thus the
sharp point p j that leads to the shortest curve connecting it to pi.
In practice, we approximate the length of the B´ezier curve by taking
the length of its control polygon. We also propagate the tags of
the point’s normals through tangent orientation. If the normals of
the successor are already tagged, the propagation for this branch
stops, and the corresponding tag values are marked. Otherwise,
the procedure continues by searching the best successor of pj in
the outgoing direction until all sharp samples are tagged. Handling
corner points is accomplished considering them as three distinct
edge samples, and when a corner is reached, a new tag value is
assigned to the third normal.


--- Page 7 ---

(b)
(a)
Figure 11: (a) Illustration of boundaries handled using “clipping
samples” and an interpolatory weight function. (b) Illustration of
peaks handled without any treatment (left part) and with our special
“peak” samples (right part).
The second step involves a propagation of the tag values to the sam-
ples close to a sharp edge, i.e., where distance is deﬁned by the ac-
tual weighting scheme. Tagging only the samples close to a sharp
crease speciﬁcally enables us to deal with non-closed creases as in
ﬁgure 9. If pi is a smooth point and pj its closest sharp point having
a non-zero intersection of inﬂuence radii, the tag value of pi yields
as the tag value of the normal of pj deﬁning the plane closest to pi.
A result of this procedure is depicted in ﬁgure 1-right.
6.2
Extensions
Object boundaries can be treated using “clipping samples”. A set
of clipping points deﬁnes a surface that is only used to clip another
surface. In order to simplify both the representation and the user
control, we place such clipping samples at the positions of bound-
ary samples. Similar to the above notion of sharp points it is sufﬁ-
cient to add a clipping normal to each sample at the desired bound-
ary. A clipping normal is set orthogonal to the desired boundary
curve and should be tangent to the surface. Figure 11a illustrates
the concept. Obviously, clipping samples may also be tagged to
deﬁne boundary curves with discontinuities. Finally, the evaluation
of the surface or the projection onto the surface only requires a mi-
nor additional modiﬁcation. Clipping samples are always treated in
separate groups and the reconstructed clipping surface is only used
to clip the actual surface by a local intersection operation. Auto-
matic detection of boundaries in a point cloud can easily be done
by analyzing the neighborhood of each sample, such as in [Guen-
nebaud et al. 2005].
Handling “peak” discontinuities also deserves some special at-
tention. We propose to explicitly represent a peak by a point pi
equipped with an axis ai and an angle bi, i.e., by a cone, as illus-
trated in ﬁgure 10. During the local ﬁtting step, we take as the
normal ni(x) of such a peak sample the normal of the cone in the
direction of the evaluation point x. ni(x) thus yields as the unit
vector ai rotated by angle π
2 −bi and axis ai × (x −pi). We also
guarantee that the surface will be C0 at the peak by attaching an
interpolating weight function to peak samples. The results in ﬁgure
11b demonstrate that our method produces high quality peaks.
Figure 12: Illustration of the sharpness control, from left to right:
α = 0, α = 0.15, α = 0.5, α = 1.
The sharpness control of a crease feature by a user parameter can
easily be added to our concept. To this end we insert all samples
into all groups but assign a lower weight to a sample when inserted
into a group with a tag different from its own one. We deﬁne a
global or local parameter α ∈[0,1] which modulates the weight of
these samples. For α = 0 we obtain a fully sharp feature, for α = 1
the feature is completely smoothed since all groups are weighted
identically. Values in-between permit a continuous transition from
smooth to sharp (ﬁgure 12).
7
Results
7.1
Implementation and Performance
In addition to a software implementation, we accelerated the pro-
jection operator described in section 4.4 on a GPU. Our implemen-
tation relies on standard GPGPU techniques and computes all the
steps of the projection, including neighbor queries, the ﬁtting step
and the orthogonal projection, in a single shader. We use grids
of linked lists as spatial acceleration structures. Our implementa-
tion performs about 45 million projection iterations per second on
a NVidia GeForce 8800-GTS for APSS, compared to 60 million
for SPSS. As detailed in table 1, our APSS projection operator con-
verges about two times faster than SPSS making APSS overall about
1.5 times faster for the same precision. As we can see, a single it-
eration is usually sufﬁcient to obtain a reasonable precision, while
SPSS achieves a similar accuracy only after 2 to 3 iterations.
# iter.
1
2
3
4
5
6
APSS
2.01e-4
3.72e-5
1.9e-5
1.53e-5
1.38e-5
1.28e-5
SPSS
1.94e-3
4.62e-4
1.67e-4
7.53e-5
3.95e-5
2.32e-5
Table 1: Comparison of the convergence of APSS and SPSS. The
values indicate the relative average precision obtained after the
given number of iterations for a typical set of models.
The results of MLS surfaces depend signiﬁcantly on the weight
functions. Although we tried to ﬁnd best settings for each method,
other weight functions could lead to other, potentially better results.
In particular, we performed all our comparisons on uniform point
clouds using the weight function described in section 4.1 with a
constant weight radius hi(x) = h·r where h is an intuitive scale fac-
tor and r is the average point spacing.Moreover, our IMLS imple-
mentation corresponds to the “simple” deﬁnition given in [Kolluri
2005]:
f(x) = ∑wi(x)(x−pi)T ni
∑wi(x)
= 0.
(14)
In order to conveniently handle arbitrary point clouds we suggest to
use hi(x) = h·ri where ri is the local point spacing that is computed
as the distance to the farthest neighbor of the point pi using our
BSP neighborhood deﬁnition. An open source library of our APSS
framework is available at: http://graphics.ethz.ch/apss.
7.2
Analysis of Quality and Stability
Approximation and interpolation quality
Figure 4 compares the ability of our spherical MLS to perform tight
approximation and convex interpolation. For interpolation we used
φ(x) = log(x)4 for both APSS and IMLS and φ(x) = 1/x2 for the
two others. The values of h used for this ﬁgure are summarized in
the following table (interpol./approx.):
SPSS
HPSS
IMLS
APSS
3.35 / na
na / na
1.95 / 1.95
1.95 / 3.25


--- Page 8 ---

(a)
(b)
(d)
(c)
Figure 13: (a) Initial dense chameleon model from which 4k samples have been uniformly picked (depicted with red dots). This low sampled
model is reconstructed using APSS (h = 1.9) (b), SPSS (h = 1.8) (c) and IMLS (h = 2.6) (d).
Stability for Low Sampling Rates
One of the central advantages of our spherical MLS is its ability
to robustly handle very low sampling densities as illustrated in ﬁg-
ure 13. The ﬁgure compares the approximation quality of a uni-
formly sampled 4K chameleon model for APSS, SPSS and IMLS.
We can clearly see the superior quality of APSS. We computed the
texture colors from the original dense model. In order to quantita-
tively evaluate the robustness under low sampling and the approx-
imation quality, we iteratively created a sequence of subsampled
point sets P0,P1,... by ﬁrst decomposing the current point set Pi
into disjunctive point pairs using a minimum weight perfect match-
ing (see [Waschb¨usch et al. 2004] for a similar scheme). A subse-
quent projection of the pair centers onto the actual PSS Si yields a
reduced point set Pi+1. The graph in ﬁgure 14 shows the relative
average distance of the initial point set P0 onto various PSS Si as
a function of the number of samples. Note that the graph is given
on a logarithmic scale. The results demonstrate that the APSS is
about three times more precise than the planar MLS variants. Fur-
thermore, the planar versions break already at much higher sam-
pling rates (crosses in the diagram), while APSS allows for stable
ﬁt at very low sampling rate. The instability of Levin’s operator is
mainly due to the polynomial projection which requires a relatively
large and consistent neighborhood. The distribution of the error is
shown for an example in ﬁgure 15.
Stability for Changes of Weights
Figure 16 compares the stability of APSS versus SPSS as a function
of the size of the weight radii. We utilized a higher order genus
model with very ﬁne structures and a low sampling density. As can
be seen SPSS exhibits undesired smoothing already at small radii
and breaks quickly at larger sizes. In this example, APSS always
delivers the expected result, and the surface stays reasonably close
to original data even for large radii.
7.3
Unwanted Extra Zeros
Our spherical ﬁtting algorithm can exhibit an increased sensitivity
with respect to unwanted stationary points away from the surface.
We refer to [Adamson and Alexa 2006a] for a discussion of the pla-
nar ﬁt. With spheres, such extra zeros can specially occur close to
153k 76k
38k
19k
9.5k 4.7k
1.4k 1.2k
597
299
150
75
38
19
1E-6
1E-5
1E-4
1E-3
1E-2
1E-1
APSS
SPSS
IMLS
Levin's
     # pts
Average displacement
Figure 14: Logarithmically scaled graph of the average projection
error for the Chinese Dragon model.
the boundary of the surface deﬁnition domain D when for instance
a small set of points yields a small and stable sphere that lies en-
tirely inside D. D is the union of the balls centered at the sample
positions pi and having the radius of the weight function (h·ri). We
handle such spurious extra zeros by limiting the extent of D, i.e. by
both reducing the ball radii and removing regions in which fewer
than a given number (e.g., 4) of samples have non-zero weights.
We found that this solution is working well in practice, but admit
that in-depth theoretical analysis is needed as part of future work.
8
Conclusion and Future Work
We have demonstrated that the planar ﬁt utilized in conventional
PSS implementations can successfully be replaced by spherical ﬁts.
Our sphere ﬁt MLS has numerous signiﬁcant advantages and over-
comes some of the intrinsic limitations of planar MLS. In particular,
we obtain an increased stability for low sampling rates, a tighter ﬁt,
a better approximation quality, robustness, and for-free mean cur-
vature estimation. Fast numerical procedures and efﬁcient imple-
mentations allow for the processing of very complex, high-quality
models and make APSS as fast as simple MLS. Extensions of the
method handle discontinuities of various kinds. An apparent nat-
ural extension of the method would be to employ ellipsoids, espe-
cially for highly anisotropic objects, such as cylinders. Algebraic
ellipsoidal ﬁts, however, do not naturally degenerate to planes. In
contrast, our method handles such objects correctly, as can be seen
on the saddle conﬁguration of ﬁgure 1-right.
Future work comprises applications of the representation in inter-
active modeling. We believe that the method is well-suited for de-
formations of point sampled geometry where the surface normals
could be estimated directly from the gradient operator. Another im-
portant issue is the theoretical analysis of both unwanted extra zeros
and the sampling requirements for our setting. Finally, we want to
explore its extension to non-manifold geometry as well as realtime
ray-tracing of dynamic APSS.
(a)
(b)
(c)
0%
0.6%
Figure 15: Chinese Dragon model (150 K) approximated with
APSS (a) and color-coded magnitude of the displacement between
the input samples and a low resolution model (19 K) using APSS
(b) and SPSS (c).


--- Page 9 ---

Figure 16: A sparsely sampled point model (19 K) handled with
APSS (top row) and with SPSS (botton row) using different sizes of
the weight radii: 1.6, 4, 8.
Acknowledgments
This work was supported in part by the ERCIM “Alain Bensoussan”
Fellowship Programme. We would like to thank all reviewers for
their insightful comments and discussions of the method, as well
as Mario Botsch and Ronny Peikert for proof reading the paper.
The models of ﬁgures 15 and 16 are provided courtesy of INRIA
and SensAble by the AIM@SHAPE Shape Repository. The tree of
ﬁgure 1 is provided courtesy of Vincent Forest.
References
ADAMS, B., AND DUTR´E, P. 2003. Interactive boolean opera-
tions on surfel-bounded solids. ACM Transactions on Graphics
(SIGGRAPH 2003 Proceedings) 22, 3, 651–656.
ADAMSON, A., AND ALEXA, M. 2003. Approximating and inter-
secting surfaces from points. In Proceedings of the Eurographics
Symposium on Geometry Processing 2003, 230–239.
ADAMSON, A., AND ALEXA, M. 2004. Approximating bounded,
non-orientable surfaces from points. In Proceedings of Shape
Modeling International 2004, IEEE Computer Society.
ADAMSON, A., AND ALEXA, M.
2006. Anisotropic point set
surfaces. In Afrigraph ’06: Proceedings of the 4th international
conference on Computer graphics, virtual reality, visualisation
and interaction in Africa, ACM Press, 7–13.
ADAMSON, A., AND ALEXA, M. 2006. Point-sampled cell com-
plexes. ACM Transactions on Graphics (SIGGRAPH 2003 Pro-
ceedings) 25, 3, 671–680.
ALEXA, M., AND ADAMSON, A. 2004. On normals and projection
operators for surfaces deﬁned by point sets. In Proceedings of the
Eurographics Symposium on Point-Based Graphics, 149–156.
ALEXA, M., AND ADAMSON, A. 2006. Interpolatory point set
surfaces - convexity and hermite data. Submitted paper.
ALEXA, M., BEHR, J., COHEN-OR, D., FLEISHMAN, S., LEVIN,
D., AND SILVA, C. T. 2003. Computing and rendering point set
surfaces. IEEE Transactions on Computer Graphics and Visual-
ization 9, 1, 3–15.
AMENTA, N., AND KIL, Y. 2004. Deﬁning point-set surfaces.
ACM Transactions on Graphics (SIGGRAPH 2004 Proceedings)
23, 3, 264–270.
AMENTA, N., AND KIL, Y. 2004. The domain of a point set sur-
face. In Proceedings of the Eurographics Symposium on Point-
Based Graphics 2004, 139–147.
BOISSONNAT, J.-D., AND CAZALS, F. 2000. Smooth shape recon-
struction via natural neighbor interpolation of distance functions.
In Proceedings of the 16th Annual Symposium on Computational
Geometry, ACM Press, 223–232.
DEY, T. K., AND SUN, J. 2005. An adaptive MLS surface for re-
construction with guarantees. In Proceedings of the Eurograph-
ics Symposium on Geometry Processing 2005, 43–52.
DEY, T. K., GOSWAMI, S., AND SUN, J. 2005. Extremal sur-
face based projections converge and reconstruct with isotopy.
manuscript.
FLEISHMAN, S., COHEN-OR, D., AND SILVA, C. T. 2005. Robust
moving least-squares ﬁtting with sharp features. ACM Transac-
tions on Graphics (SIGGRAPH 2005 Proceedings) 24, 3, 544–
552.
GANDER, W., GOLUB, G. H., AND STREBEL, R. 1994. Least-
squares ﬁtting of circles and ellipses. BIT Numerical Mathemat-
ics 34, 4, 558–578.
GUENNEBAUD, G., BARTHE, L., AND PAULIN, M. 2005. Interpo-
latory reﬁnement for real-time processing of point-based geom-
etry. Computer Graphics Forum (Proceedings of Eurographics
2005) 24, 3, 657–666.
HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, J., AND
STUETZLE, W. 1992. Surface reconstruction from unorganized
points. In Proc. of ACM SIGGRAPH ’92, ACM Press, 71–78.
KAZHDAN, M., BOLITHO, M., AND HOPPE, H. 2006. Poisson
surface reconstruction. In Proceedings of the Eurographics Sym-
posium on Geometry Processing 2006, 43–52.
KOLLURI, R. 2005. Provably good moving least squares. In ACM-
SIAM Symposium on Discrete Algorithms, 1008–1018.
LEVIN, D. 2003. Mesh-independent surface interpolation. Geo-
metric Modeling for Scientiﬁc Visualization, 181–187.
MITRA, N. J., NGUYEN, A., AND GUIBAS, L. 2004. Estimating
surface normals in noisy point cloud data. International Journal
of Computational Geometry and Applications 14, 4–5, 261–276.
OHTAKE, Y., BELYAEV, A., ALEXA, M., TURK, G., AND SEI-
DEL, H.-P. 2003. Multi-level partition of unity implicits. ACM
Transactions on Graphics (SIGGRAPH 2003 Proceedings) 22,
3, 463–470.
PAULY, M., KEISER, R., KOBBELT, L. P., AND GROSS, M. 2003.
Shape modeling with point-sampled geometry. ACM Transac-
tions on Graphics (SIGGRAPH 2003 Proceedings) 22, 3.
PAULY, M., MITRA, N. J., AND GUIBAS, L. 2004. Uncertainty
and variability in point cloud surface data. In Proceedings of the
Eurographics Symposium on Point-Based Graphics, 77–84.
PRATT, V. 1987. Direct least-squares ﬁtting of algebraic surfaces.
In Proc. of ACM SIGGRAPH ’87, ACM Press, 145–152.
SHEN, C., O’BRIEN, J. F., AND SHEWCHUK, J. R. 2004. Inter-
polating and approximating implicit surfaces from polygon soup.
ACM Transactions on Graphics (SIGGRAPH 2004), 896–904.
WALD, I., AND SEIDEL, H.-P. 2005. Interactive ray tracing of
point based models. In Proceedings of the Eurographics Sympo-
sium on Point Based Graphics 2005.
WASCHB ¨USCH, M., GROSS, M., EBERHARD, F., LAMBORAY,
E., AND W ¨URMLIN, S. 2004. Progressive compression of point-
sampled models. In Proceedings of the Eurographics Symposium
on Point-Based Graphics 2004, 95–102.
WICKE, M., TESCHNER, M., AND GROSS, M. 2004. CSG tree
rendering of point-sampled objects. In Proceedings of Paciﬁc
Graphics 2004, 160–168.
```

---

## Section 1.5: A Survey of Surface Reconstruction from Point Clouds

Source File: A Survey of Surface Reconstruction from Point Clouds.txt

### Paper Content:

```
# A Survey of Surface Reconstruction from Point Clouds.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/A Survey of Surface Reconstruction from Point Clouds.pdf
# Output: ../layer2_completion/txt/A Survey of Surface Reconstruction from Point Clouds.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/299541828
A Survey of Surface Reconstruction from Point Clouds
Article  in  Computer Graphics Forum · March 2016
DOI: 10.1111/cgf.12802
CITATIONS
507
READS
6,638
8 authors, including:
Pierre Alliez
National Institute for Research in Computer Science and Control
167 PUBLICATIONS   13,573 CITATIONS   
SEE PROFILE
Gaël Guennebaud
National Institute for Research in Computer Science and Control
49 PUBLICATIONS   2,987 CITATIONS   
SEE PROFILE
Joshua Aaron Levine
University of Arizona
72 PUBLICATIONS   2,325 CITATIONS   
SEE PROFILE
Andrei Sharf
60 PUBLICATIONS   4,338 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Pierre Alliez on 26 February 2019.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

State of the Art in Surface Reconstruction from Point
Clouds
Matthew Berger, Andrea Tagliasacchi, Lee Seversky, Pierre Alliez, Joshua
Levine, Andrei Sharf, Claudio Silva
To cite this version:
Matthew Berger, Andrea Tagliasacchi, Lee Seversky, Pierre Alliez, Joshua Levine, et al.. State
of the Art in Surface Reconstruction from Point Clouds. Eurographics 2014 - State of the Art
Reports, Apr 2014, Strasbourg, France. 1 (1), pp.161-185, 2014, EUROGRAPHICS star report.
<http://diglib.eg.org/EG/DL/conf/EG2014/stars/161-185.pdf>.
<10.2312/egst.20141040>.
<hal-01017700>
HAL Id: hal-01017700
https://hal.inria.fr/hal-01017700
Submitted on 3 Jul 2014
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entiﬁc research documents, whether they are pub-
lished or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destin´ee au d´epˆot et `a la diﬀusion de documents
scientiﬁques de niveau recherche, publi´es ou non,
´emanant des ´etablissements d’enseignement et de
recherche fran¸cais ou ´etrangers, des laboratoires
publics ou priv´es.


--- Page 4 ---

Author version 2014/
STAR – State of The Art Report
State of the Art in Surface Reconstruction from Point Clouds
Matthew Berger1
Andrea Tagliasacchi2
Lee M. Seversky1
Pierre Alliez3
Joshua A. Levine4
Andrei Sharf5
Claudio T. Silva6
1Air Force Research Laboratory, Information Directorate
2École Polytechnique Fédérale de Lausanne (EPFL)
3Inria Sophia-Antipolis - Mediterranée
4Clemson University
5Ben-Gurion University
6New York University, School of Engineering
Abstract
The area of surface reconstruction has seen substantial progress in the past two decades. The traditional problem
addressed by surface reconstruction is to recover the digital representation of a physical shape that has been
scanned, where the scanned data contains a wide variety of defects. While much of the earlier work has been
focused on reconstructing a piece-wise smooth representation of the original shape, recent work has taken on more
specialized priors to address signiﬁcantly challenging data imperfections, where the reconstruction can take on
different representations – not necessarily the explicit geometry. This state-of-the-art report surveys the ﬁeld of
surface reconstruction, providing a categorization with respect to priors, data imperfections, and reconstruction
output. By considering a holistic view of surface reconstruction, this report provides a detailed characterization of
the ﬁeld, highlights similarities between diverse reconstruction techniques, and provides directions for future work
in surface reconstruction.
1. Introduction
The modeling, recognition, and analysis of the world around
us is a longstanding goal in the ﬁeld of Computer Graphics.
Central to these objectives is a means of obtaining a digital
representation of objects in the real world. Surface reconstruc-
tion is concerned with recovering such information, where
the basic problem is to capture a 3D point cloud that sam-
ples the real world, and reconstruct as much information as
possible concerning the scanned objects.
Surface reconstruction came to importance primarily as
a result of the ability to acquire 3D point clouds and hence
there are very close ties between how the data is acquired and
the method used to reconstruct it. Early on, these techniques
ranged from active methods such as optical laser-based range
scanners, structured light scanners, and LiDAR scanners,
as well as passive methods such as multi-view stereo. A
recent trend has seen the massive proliferation of point clouds
from commodity real-time scanners such as the Microsoft
Kinect. As the diversity, ease of use, and popularity of 3D
acquisition methods continues to increase, so does the need
for the development of new surface reconstruction techniques.
Acquisition methods tend to produce point clouds contain-
ing a variety of properties and imperfections that pose signif-
icant challenges for surface reconstruction methods. These
properties, in conjunction with the nature of the scanned
shape, effectively distinguish the class of reconstruction meth-
ods that exist today. This diverse set of techniques ranges
from methods that assume a well-sampled point cloud, gen-
eralize to arbitrary shapes, and produce a watertight surface
mesh, to methods that make very loose assumptions on the
quality of the point cloud, operate on speciﬁc classes of
shapes, and output a non-mesh based shape representation.
It is with this rich space of algorithms in mind that we sur-
vey the ﬁeld of surface reconstruction and provide a detailed
taxonomy of existing methods. This categorization is timely,
as we see the ﬁeld of surface reconstruction diverging from
its more traditional class of methods in an effort to handle
more challenging data imperfections.
Our survey presents surface reconstruction algorithms
from the perspective of priors: assumptions made by algo-
rithms in order to combat imperfections in the point cloud
and recover as much information about the shape as possible.
Without prior assumptions, the reconstruction problem is ill-
posed; an inﬁnite number of surfaces pass through or near the
data points. Assumptions are usually imposed on the point
cloud itself, such as sampling density, level of noise, and mis-
c
⃝Author version


--- Page 5 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
Figure 1: Surface reconstruction has grown in diversity
in recent years, with a wide variety of techniques taking
on specialized priors. ROSA [TZCO09], shown on the left,
uses volumetric smoothness to aid in reconstruction. Non-
local consolidation [ZSW∗10], shown in the middle, uses
global regularity in the form of structural repetition. Part
composition [SFCH12], shown on the right, uses data-driven
techniques to perform reconstruction.
alignment. But just as importantly they are also imposed on
the scanned shape, such as local surface smoothness, volumet-
ric smoothness, absence of boundaries, symmetries, shape
primitives, global regularity, and data-driven assumptions. In
some instances, requirements are made on knowledge of the
acquisition, such as scanner head position, as well as RGB
images of the object. In other cases, the user is involved in
prescribing high-level cues for reconstruction. All of these
factors permit the regularization of the otherwise ill-posed
problem of surface reconstruction, particularly when process-
ing a point cloud containing severe imperfections. Figure 1
depicts several different priors used to reconstruct surfaces
from challenging point clouds.
Historically, priors have evolved according to the types
of point clouds being processed. For instance, local surface
smoothness priors were developed primarily to handle small
objects acquired from desktop scanners. Mobile, real-time
scanners have enabled the dynamic acquisition of more gen-
eral scenes, rather than single objects, prompting more spe-
cialized structural and data-driven priors. Since priors tend
to be coupled with the type of acquisition, we argue that
this perspective of surface reconstruction is beneﬁcial for
understanding how to process future types of acquired point
clouds.
Organization. Our STAR is organized as follows. In Sec-
tion 2 we characterize the problem of surface reconstruc-
tion by examining common input and output characteristics,
namely:
• Point Cloud Artifacts: the imperfections of the point
cloud that the method is able to effectively handle.
• Input Requirements: the types of inputs associated with
a point cloud required by the algorithm.
• Shape Class: the class of shapes that the method is capable
of reconstructing.
• Reconstruction Output: the representation and level of
detail of the reconstruction output.
We use these factors as a way of examining surface recon-
struction methods by prior, starting with traditional surface
smoothness priors in Section 3, and delving into specialized
priors in Sections 4–9. In Table 1.1 we provide a summary
of surface reconstruction methods by prior, characterizing
their input and output, as well as their level of robustness to
various artifacts. We discuss methods for evaluating surface
reconstruction in Section 10, and conclude in Section 11 with
a discussion on future trends in surface reconstruction.
1.1. Survey Scope and Related Works
There are many facets to surface reconstruction. This survey
focuses on those relating to the reconstruction from point
clouds of static objects and scenes acquired through 3D scan-
ners, wherein the point cloud contains a considerable level of
imperfection. Furthermore, we concentrate on methods that
approximate the input point cloud.
Urban reconstruction. Our survey covers a wide variety of
reconstruction methods, with urban reconstruction from point
clouds among them. We note that [MWA∗13] surveys urban
reconstruction more broadly: 3D reconstruction from images,
image-based facade reconstruction, as well as reconstruction
from 3D point clouds. Although there exists some overlap
between the surveys, we cover these methods in a different
context, namely the priors that underly the reconstruction
methods and how they address challenges in point cloud
reconstruction.
Surface completion. Given a surface with boundaries, there
exists many methods for inpainting and surface completion
for handling missing data. Though one may use such ap-
proaches for reconstruction by ﬁrst reconstructing a surface
with boundary from a point cloud, this can be quite challeng-
ing given other imperfections in the data. These methods are
not covered in this survey and we refer the reader to the recent
survey of [ACK13] on surface completion.
Interpolatory reconstruction. An important ﬁeld of surface
reconstruction methods are those that interpolate a point
cloud without any additional information, such as normals
c
⃝Author version


--- Page 6 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
Method
Point Cloud Artifacts
Input Requirements
Shape Class
Reconstruction Output
nonuniform
sampling
noise
outliers
misalignment
missing data
unoriented
normals
oriented
normals
scanner
information
RGB image
Surface Smoothness
Tangent Planes [HDD∗92]
#
#
general
implicit ﬁeld
RBF [CBC∗01]
#
#
✓
general
implicit ﬁeld
MLS [ABCO∗03]
#
#
✓
general
point set
MPU [OBA∗03a]
#
#
#
✓
general
implicit ﬁeld
Poisson [KBH06]
#
 
#
#
#
✓
general
implicit ﬁeld
Graph Cut [HK06]
#
#
#
#
#
general
volumetric segmentation
Unoriented Indicator [ACSTD07]
#
 
#
#
#
✓
general
implicit ﬁeld
LOP [LCOLTE07]
 
 
#
#
general
point set
Visibility
VRIP [CL96]
#
 
#
✓
general
implicit ﬁeld
TVL1-VRIP [ZPB07]
#
 
#
#
#
✓
general
implicit ﬁeld
Signing the Unsigned [MDGD∗10]
#
 
 
#
✓
general
implicit ﬁeld
Cone Carving [SSZCO10]
#
#
 
✓
✓
general
implicit ﬁeld
Multi-Scale Scan Merge [FG11]
 
 
#
✓
general
implicit ﬁeld
Volumetric smoothness
ROSA [TZCO09]
#
#
 
✓
organic
skeleton curve
Arterial Snakes [LLZM10]
#
#
 
✓
man-made
skeleton curve
VASE [TOZ∗11]
#
#
 
✓
general
implicit ﬁeld
l1 Skeleton [HWCO∗13]
#
#
 
organic
skeleton curve
Geometric Primitives
Primitive Completion [SDK09]
#
#
#
 
✓
CAD
volumetric segmentation
Volume Primitives [XF12]
#
#
#
 
✓
indoor environment
interior volume
Point Restructuring [LA13]
#
#
#
#
#
✓
✓
general
volumetric segmentation
CCDT [vKvLV13]
#
#
#
#
✓
✓
urban environment
volumetric segmentation
Global Regularity
Symmetry [PMW∗08]
#
#
 
✓
architectural
point set
Nonlocal Consolidation [ZSW∗10]
 
#
#
 
✓
architectural
point set
2D-3D Facades [LZS∗11]
#
#
 
✓
✓
architectural
point set
Globﬁt [LWC∗11]
 
#
#
 
✓
man-made
primitive relations
Data-driven
Completion by Example [PMG∗05]
#
#
 
✓
general
point set
Semantic Modeling [SXZ∗12]
#
#
 
✓
✓
indoor scene objects
deformed model
Shape Variability [KMYG12]
#
#
 
✓
indoor scene objects
deformed model
Part Composition [SFCH12]
#
#
 
✓
✓
man-made
deformed model parts
Interactive
Topological Scribble [SLS∗07]
#
#
 
✓
general
implicit ﬁeld
Smartboxes [NSZ∗10]
 
#
#
 
✓
architectural
primitive shapes
O-Snap [ASF∗13]
#
#
#
 
✓
architectural
primitive shapes
Table 1: A categorization of surface reconstruction in terms of the type of priors used, the ability to handle point cloud artifacts,
input requirements, shape class, and the form of the reconstruction output. Here # indicates that the method is moderately
robust to a particular artifact and  indicates that the method is very robust. ✓indicates an input requirement and ✓indicates
optional input.
or scanner information. Delaunay-based methods are quite
common in this area. The basic idea behind these methods
is that the reconstructed triangulated surface is formed by a
subcomplex of the Delaunay triangulation. A comprehensive
survey of these methods is presented in [CG06], as well as
the monograph of [Dey07]. A very attractive aspect of such
methods is that they come with provable guarantees in the
geometric and sometimes topological quality of the recon-
struction if a sufﬁciently dense sampling of the input surface
is provided. These methods place rather strong requirements
on the point cloud and are impractical for scanned real-world
scenes containing signiﬁcant imperfections. Hence we do
not cover these methods, since we focus on reconstruction
techniques capable of dealing with challenging artifacts.
2. Characterizing Surface Reconstruction
Surface reconstruction methods typically have to handle var-
ious types of imperfections, make certain requirements on
input associated with the point cloud, contain restrictions
on the class of shapes that they can reconstruct, and may
produce reconstructions of different forms. Here we summa-
rize each of these properties in order to provide a detailed
characterization of surface reconstruction.
2.1. Point Cloud Artifacts
The properties of the input point cloud are an important fac-
tor in understanding the behavior of reconstruction methods.
Here we provide a characterization of point clouds according
to properties that have the most impact on reconstruction
algorithms: sampling density, noise, outliers, misalignment,
and missing data. See Figure 2 for a 2D illustration of these
artifacts.
Sampling density. The distribution of the points sampling
the surface is referred to as sampling density. 3D scans typi-
cally produce a nonuniform sampling on the surface, which
can be due to the distance from the shape to the scanner posi-
c
⃝Author version


--- Page 7 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
(a) Original shape
(b) Nonuniform sampling
(c) Noisy data
(d) Outliers
(e) Misaligned scans
(f) Missing data
Figure 2: Different forms of point cloud artifacts, shown
here in the case of a curve in 2D.
tion, the scanner orientation, as well as the shape’s geometric
features. See Figure 2(b) for an illustration of nonuniform
sampling on a curve. Many surface reconstruction algorithms
must be able to estimate a notion of sampling density at every
point, see e.g. [LCOL06, WSS09], and hence the level of
nonuniformity in the sampling can have a great impact on
estimation accuracy.
Noise. Points that are randomly distributed near the surface
are traditionally considered to be noise – see Figure 2(c). The
speciﬁc distribution is commonly a function of scanning arti-
facts such as sensor noise, depth quantization, and distance
or orientation of the surface in relation to the scanner. For
some popular scanners, noise is introduced along the line
of sight, and can be impacted by surface properties, includ-
ing scattering characteristics of materials. In the presence
of such noise, the typical goal of surface reconstruction al-
gorithms is to produce a surface that passes near the points
without overﬁtting to the noise. Robust algorithms that im-
pose smoothness on the output [KBH06], as well as methods
that employ robust statistics [OGG09], are common ways of
handling noise. We note that spatially varying noise poses a
signiﬁcant challenge [GCSA13], where for many scanners,
the noise level is correlated with the depth measurement –
see [KE12] for such an error study done on the Kinect.
Outliers. Points that are far from the true surface are clas-
siﬁed as outliers. Outliers are commonly due to structural
artifacts in the acquisition process. In some instances, out-
liers are randomly distributed in the volume, where their
density is smaller than the density of the points that sample
the surface. Outliers can also be more structured, however,
where high density clusters of points may exist far from the
surface, see Figure 2(d). This can occur in multi-view stereo
acquition, where view-dependent specularities can result in
false correspondences; see Figure 10. Unlike noise, outliers
are points that should not be used to infer the surface, ei-
ther explicitly through detection [LCOLTE07], or implicitly
through robust methods [MDGD∗10].
Misalignment. The imperfect registration of range scans
results in misalignment. Misalignment tends to occur for
a registration algorithm when the initial conﬁguration of
a set of range scans is far from the optimal alignment –
see [vKZHCO11] for a survey on registration techniques.
Misalignment is a signiﬁcant challenge for surface recon-
struction, as it introduces structured noise via scans that are
slightly offset from the surface; see Figure 5(a). For instance,
it may be inappropriate to simply ﬁnd the surface that passes
near the scans since this may result in sharp discontinuities
between different scans. Figure 2(e) illustrates such a case,
where there can exist discontinuities when the red and orange
samples stop overlapping.
Missing data. A motivating factor behind many reconstruc-
tion methods is dealing with missing data. Missing data is due
to such factors as limited sensor range, high light absorption,
and occlusions in the scanning process where large portions
of the shape are not sampled. Although some of these arti-
facts may be reduced as scanning technology advances with
higher precision, denser sampling, and lower noise levels,
occlusion remains a persistent problem due to the physical
constraints of the device. We note that missing data differs
from nonuniform sampling, as the sampling density is zero
in such regions – see Figure 2(f).
Many methods deal with missing data by assuming that the
scanned shape is watertight [CBC∗01,Kaz05,KBH06,HK06,
ACSTD07]. Within this setting, the goal of some methods is
to handle the aforementioned challenges where data exists,
and infer geometry in parts of the surface that have not been
sampled. Other methods are focused on handling missing
data by trying to infer topological structures in the original
surface at the possible expense of retaining geometric ﬁdelity,
for instance, ﬁnding a surface that is homeomorphic to the
original shape [SLS∗07].
If the level of missing data is signiﬁcant, for instance a
single scan, then trying to infer the entire shape can be too
ambiguous. Some methods focus on performing reconstruc-
tion only on the available information, effectively preserving
the boundaries from the scan [DGQ∗12]. Other approaches
make prior knowledge and assumptions on the missing region,
permitting the reconstruction of higher-level information.
This can range from inferring a skeleton [TZCO09], shape
primitives [SDK09], symmetry relationships [PMW∗08], and
canonical regularities [LWC∗11].
2.2. Point Cloud Input
Reconstruction methods have different types of input require-
ments associated with a point cloud. The bare minimum
requirement of all algorithms is a set of 3D points that sample
the surface. Working with the points alone, however, may
fail to sufﬁciently regularize the problem of reconstruction
c
⃝Author version


--- Page 8 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
for certain types of point clouds. Other types of input can be
extremely beneﬁcial in reconstruction from challenging point
clouds. We consider the following basic forms of inputs com-
monly associated with point clouds: surface normals, scanner
information, and RGB imagery.
2.2.1. Surface Normals
Surface normals are an extremely useful input for recon-
struction methods. For smooth surfaces the normal, uniquely
deﬁned at every point, is the direction perpendicular to the
point’s tangent space. The tangent space intuitively represents
a localized surface approximation at a given point. Surface
normals may be oriented, where each normal is consistently
pointing inside or outside of the surface, or may lack such a
direction. Normals that are oriented provide extremely useful
cues for reconstruction algorithms. However, we note that
if certain information associated with the point cloud is not
present, obtaining an orientation can be challenging.
Unoriented normals. Normals that do not possess direction
– the input normal at every point can be expected to be point-
ing either in the inside or the outside of the surface – are
considered to be unoriented normals. This information can
be used in a number of ways: determining planar regions
in a point cloud [SWK07], the projection of a point onto an
approximation of the surface [ABCO∗03], the construction of
an unsigned distance ﬁeld [AK04], or computing covariance
matrices [ACSTD07]. Unoriented normals are typically com-
puted directly from the point cloud. This is because additional
scanner-speciﬁc information can be used to provide a means
to infer normal orientation. A popular and simple method for
computing the normal at a given point p is to perform prin-
cipal component analysis (PCA) in a local neighborhood of
p, see e.g. [HDD∗92]. More speciﬁcally, if we denote a local
neighborhood of p by Np, then the basic way to apply PCA
is to compute the spectral decomposition of the covariance
matrix:
Cp = ∑
q∈Np
(p−q)(p−q)⊺
(1)
The eigenvector of Cp associated with the smallest eigenvalue
deﬁnes the unoriented normal – assuming eigenvalues have
a multiplicity of 1, the eigenvectors are unique up to a sign.
Note that if the smallest eigenvalue is 0, then the region de-
ﬁned by p and Np is planar, since the eigenvectors associated
with the 2 largest eigenvalues capture all of the variance in
the data.
PCA deﬁnes a least-squares estimation of a tangent plane
and there are many other methods for computing unori-
ented normals: using a weighted covariance matrix [PMG04],
higher-order approximations via osculating jets [CP05], or ro-
bust methods that employ l1 norm minimization [ASGCO10].
Common to all methods is the need to deﬁne a local neigh-
borhood of points, where the neighborhood should be small
enough to accurately represent a point’s tangent space. The
scale of the neighborhood should be proportional to the sam-
pling density at the point, where estimation of sampling den-
sity is itself a challenging problem, particularly when faced
with nonuniform sampling; see [LCOL06,WSS09]. Further-
more, noise and misalignment may necessitate larger neigh-
borhood sizes in order to combat such imperfections, yet
the size should not be so large as to no longer reﬂect the
point’s tangent space [GG07]. All of these difﬁculties often
result in imperfect normal estimation and surface reconstruc-
tion algorithms must be robust to inaccuracies in unoriented
normals.
Oriented normals. Normals that have consistent directions,
either pointing in the inside or the outside of the surface are
referred to as being oriented. Knowledge of the exterior and
interior of the surface has proven extremely useful in surface
reconstruction. It can be used to construct a signed distance
ﬁeld over the ambient space, where up to a sign, the ﬁeld takes
on positive values in the exterior and negative values in the
interior. The surface is then represented by the zero crossing
of the signed distance ﬁeld. Other methods generalize this to
implicit ﬁelds and indicator functions, but the basic idea of
trying to construct the exterior and interior remains the same,
see [CBC∗01,OBA∗03a,KBH06] to name a few.
There are numerous ways to compute oriented normals. If
the original 2D range scans are known, then the 2D lattice
structure provides a way of performing consistent orientation
since one always knows how to turn clockwise around a given
vertex. For instance, if we denote the point in a range scan at
pixel (x,y) as px,y, then one can take the normal at px,y simply
as the cross product between (px+1,y −px,y) and (px,y+1 −
px,y). If the point cloud is noisy, then this method can produce
rather noisy normals, since it does not use nearby points in
overlapping scans. If the view direction is known, then one
can ﬁrst estimate unoriented normals as previously discussed
to better handle noise and then use the view direction to
determine the orientation. This can be done by choosing the
orientation that results in the normal vector having the largest
angle with the view direction.
If scanner information is absent altogether, then one must
orient the points exclusively from the unoriented normals.
A very common method for achieving this is to start from a
single point containing an initial orientation and propagate
the orientation to nearby points whose unoriented normals
are facing a similar direction [HDD∗92]. While there exist
multiple extensions to this method [HLZ∗09, LW10], they
face difﬁculty in the presence of nonuniform sampling, noise,
and misalignment and as a result can leave some normals
unoriented or pointing in the wrong direction – see Figure 3.
The impact on surface reconstruction largely depends on the
distribution of incorrect orientations: if randomly distributed,
then methods may treat this as spurious noise, but if incorrect
orientations are clustered together over large regions, then
this form of structured noise can be difﬁcult to handle – see
Figure 3 for an illustration.
c
⃝Author version


--- Page 9 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
Figure 3: The impact of incorrect normal orientation. On the
left we show the result of normal orientation via [HDD∗92],
where red splats indicate incorrect orientation. The results
of running Poisson surface reconstruction [KBH06] on this
point cloud are shown in mid-left, where we indicate un-
wanted surface components due to the clustered normal
ﬂips. Similarly, on the right we show the orientation results
of [LW10], and the corresponding results of [KBH06].
2.2.2. Scanner Information
The scanner from which the point cloud was acquired can
provide useful information for surface reconstruction. As
discussed, the 2D lattice structure of a scan can be used to
determine normal orientation. It can also be used to estimate
the sampling density for a given scan and as a result it can be
used to detect certain forms of outliers in the scan – points
whose scan lattice neighbors are at a far greater distance than
the sampling density are likely outliers. However, caution
must be taken in distinguishing outliers from sharp features.
Scanner information may also be used to deﬁne the con-
ﬁdence of a point, which is useful in handling noise. For
instance, the approach of [CL96] demonstrates how to use
conﬁdence measures to suppress noise when integrating range
scans into a reconstructed surface. Certain scanners (e.g. Li-
DAR) can provide conﬁdence measures in the form of the
reﬂectivity measured at each point.
One can also derive conﬁdence through line of sight in-
formation. Line of sight is deﬁned as a collection of line
segments between each point in the point cloud and the scan-
ner head position from which that point was acquired. In
particular, we can use the grazing angle formed by line of
sight at each point to derive a notion of conﬁdence. The graz-
ing angle is the incident angle between the point’s surface
normal and its line of sight. In active scanning systems such
as optical laser-based scanners, the cause of inaccuracy stems
from the large area formed by the projection (i.e., the laser
stripe) onto the surface at a large grazing angle. Hence we can
assign a conﬁdence measure as being inversely proportional
to the grazing angle.
Note that line of sight also deﬁnes a region of space that
may be marked as lying outside of the shape. Combining line
of sight from multiple scans reﬁnes the bounding volume in
which the surface lies – this volume is known as the visual
hull. This information is particularly useful when handling in-
complete data – for instance, line of sight can be used to infer
that there exists a large concavity in the shape [TOZ∗11].
2.2.3. RGB Imagery
Different acquisition modalities that complement depth ac-
quisition can be of great assistance. RGB image acquisition is
a very common modality that accompanies numerous sensors,
such as the Microsoft Kinect. In the case of the Kinect, the
RGB camera is co-located with the IR camera, hence assum-
ing the two are calibrated, it is straightforward to identify
corresponding depth and RGB values at a pixel level. RGB
images are most useful for reconstruction when they are able
to complement depth information that was not measured by
the data. By fusing features present in the depth scan with
image-based features, one can then employ this for inferring
depth from images [LZS∗11]. Contours in an image can also
be used by considering their corresponding unsigned distance
ﬁeld, and how it relates to the point cloud’s unsigned distance
ﬁeld [SFCH12].
2.3. Shape Class
Surface reconstruction algorithms can be further distin-
guished by the class of shapes they support. Although earlier
reconstruction methods tended not to focus on speciﬁc types
of shapes [HDD∗92,CL96], by restricting to a shape class,
specialized assumptions can be made on what the recon-
structed surface should be. These shape speciﬁc assumptions
greatly help regularize the problem especially in the pres-
ence of point cloud artifacts. Quite often, a reconstruction
prior is in part driven by a shape class, so understanding the
characteristics of a shape class is an essential component to
gaining insight into surface reconstruction. Here we cover
the following predominant shape classes: CAD models, man-
made shapes, organic shapes, architectural models, urban
environments, and indoor environments.
CAD models. These models are typically composed of a
collection of simpler geometric primitives such as planes,
cylinders, and spheres. The detection of such instances in the
point cloud can be used for denoising and in the presence of
missing data the assembly of all detected primitives can be
used to infer missing regions by extending and intersecting
primitives [SDK09] – see Figure 12.
Man-made (synthetic) shapes. These shapes often contain
certain canonical geometric properties such as coplanar faces,
orthogonal faces, as well as faces that form equal angles that
often repeatedly appear and relate different parts of the shape.
This form of global regularity is often due to aesthetic con-
siderations and a variety of practical constraints, such as cost
considerations, functional requirements, and fabrication con-
straints. For man-made shapes, this derived form of regularity
can greatly help the problem [LWC∗11] – see Figure 14.
Organic shapes. These shapes tend to contain a more free-
form structure and are often composed of curvilinear ele-
ments. For instance, trees [LYO∗10] possess a strong skeletal
c
⃝Author version


--- Page 10 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
structure, where imposing certain 1D priors [TZCO09] on
the reconstruction can be used to handle signiﬁcant missing
data in the scanning process – see Figure 11.
Architectural models. A subset of man-made shapes, these
shapes contain similar global regularities in addition to many
other constraints such as upright orientation and various
functional constraints. By assuming that the shape is an ar-
chitectural model, the problem can be greatly regularized
by making assumptions on facade structures [ZSW∗10],
manhattan-world geometry [VAB12], and structural regu-
larity [PMW∗08] – see Figure 16.
Urban environments. Often composed of a limited number
of object types, urban environments are well-suited for data-
driven methods for reconstruction. For instance one can make
assumptions on the presence of ground, buildings, vegetation,
and other urban objects to aid in reconstruction [MWA∗13].
Indoor environments. The types of shapes within this envi-
ronment tend to be a mixture of man-made and organic. A dis-
tinguishing factor behind indoor environments is that similar
to urban environments, there exist a small number of object
types. For instance, in a typical ofﬁce setting there exists a
variety of chairs, desks, and tables. Furthermore, each type of
object can often be deﬁned through a low-dimensional shape
space, permitting data-driven methods [NXS12] and methods
that utilize a deformable model for each object [KMYG12] –
see Figure 15.
2.4. Reconstruction Output
It is desirable for a reconstruction algorithm to produce a
faithful and detailed representation of the scanned shape’s
surface. For challenging point clouds containing signiﬁcant
imperfections, it may be unrealistic to expect such highly de-
tailed information as the output. However for certain methods
it is still possible to obtain a less informative yet valuable
shape representation.
Methods targeting detailed reconstruction generally pro-
duce as output either a discrete surface or an implicit function.
The implicit function can either be in the form of a signed
distance ﬁeld [HDD∗92] or an indicator function [Kaz05].
Implicit functions are usually sampled on an underlying grid,
where the reconstructed surface is found via isocontouring for
an appropriate isovalue. For a regular grid, the well-known
Marching Cubes [LC87] is commonly used to extract the
surface [HDD∗92,CL96,CBC∗01,Kaz05]. Other methods
use grids such as octrees [KBH06,MPS08] or adaptive 3D
triangulations [ACSTD07, MDGD∗10] to adapt grid reso-
lution to the point sampling density. Contouring an octree
presents several difﬁculties in ensuring watertight, manifold
surface meshes – see [JLSW02, KKDH07, MS10] for sev-
eral approaches. Contouring a triangulation can be done via
marching tetrahedra, but if a mesh with a lower number of
triangles and well-conditioned (i.e. good aspect ratio) trian-
gles is desired, then Delaunay reﬁnement techniques may be
used [BO05].
Some techniques may not produce a surface represen-
tation, but rather a resampled point set that addresses im-
perfections present in the original point cloud [ABCO∗03,
FCOS05b, LCOLTE07, HLZ∗09]. For severe levels of im-
perfections, some reconstruction methods may be unable to
provide such detailed representations, but still recover an
informative representation such as a collection of shape prim-
itives [JKS08,SDK09,RKMP13], a curve skeleton [TZCO09,
CTO∗10,LLZM10,HWCO∗13], a deformed model that best
matches the point cloud [SXZ∗12, NXS12, KMYG12], or
segmented parts from multiple models [SFCH12].
3. Surface Smoothness Priors
Early surface reconstruction techniques were developed to
handle broad assumptions on the type of shape being recon-
structed. These methods were developed to handle sampling
and noise artifacts, while also supporting small amounts of
missing data. A commonality shared across all of these tech-
niques is the use of a surface smoothness prior to constrain
the output surface, while also ensuring that the reconstructed
surface remains close to the input data. Methods in this cate-
gory vary based on the smoothness constraints and how they
are prescribed in practice. More speciﬁcally, this category
of methods can roughly be divided into local smoothness,
global smoothness, and piecewise smoothness.
Methods that prescribe local smoothness ensure that the
output surface is smooth where the input point cloud ex-
ists [ABCO∗03,OBA∗03a]. In regions away from the point
cloud, however, these methods can behave poorly, failing
to reconstruct the correct global geometry and topology. In
contrast, global smoothness methods prescribe that the en-
tirety of the output surface is smooth [CBC∗01, KBH06].
Hence, these methods are better suited for handling miss-
ing data, as they are all targeted at producing a watertight
surface. These methods are further distinguished by often
solving an optimization problem that leads to the use of a
global solver of some kind – typically a linear system, an
eigenvalue problem, or a graph cut optimization. Piecewise
smooth methods are focused on explicitly recovering sharp
features or boundary components, while ensuring smooth-
ness away from these features, where smoothness may be
prescribed locally [DHOS07] or globally [ASGCO10].
Notation. We ﬁrst ﬁx the notation for this section and all
subsequent sections. We assume that we are given a point
cloud P which is a sampling of a shape S. Individual points
in P are indexed as pi ∈P for the i’th point. Many methods
also require normals associated with the point cloud, where
we deﬁne the normal ﬁeld N as a set of normal vectors such
that for each pi ∈P there is an accompanying normal ni ∈N.
The distinction between oriented and unoriented normals is
made explicit for each method.
c
⃝Author version


--- Page 11 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
(a)
(b)
(c)
Figure 4: When sampling density is insufﬁcient to resolve
local curvature (a), the plane ﬁtting operation employed by
moving least squares [ABCO∗03] becomes highly unstable
(b). APSS [GG07] addresses this problem by locally ﬁtting
spheres instead of planes. Employing spheres tackles the
aforementioned problem while remaining computationally
inexpensive.
3.1. Local Surface Smoothness Priors
The pioneering method of [HDD∗92] was hugely inﬂuential
on the class of methods that impose local smoothness priors.
This method approximates a signed distance ﬁeld f : R3 →R
by assigning, for each point in the ambient space x ∈R3, its
signed projection onto the tangent plane of its closest point
to P, denoted pi:
f(q) = (q−pi)·ni.
(2)
Note that the normal ﬁeld N must be oriented in order to
obtain an estimate of the signed distance ﬁeld. The surface is
then deﬁned by the zero level set of f. Although straightfor-
ward to implement, this approach suffers from several issues.
The method is very sensitive to the estimated normals – noisy
normals, or worse inverted normal orientations, can give rise
to very inaccurate signed distance estimates. Furthermore,
in the presence of nonuniform sampling, choosing the clos-
est tangent plane to deﬁne a signed projection can produce
a rather noisy output – see Figure 6, mid-left. Subsequent
methods based on local surface smoothness have focused on
addressing such issues.
Moving least squares (MLS). These methods approach re-
construction by approximating the surface as a spatially-
varying low-degree polynomial – see [CWL∗08] for a survey
on MLS methods. More speciﬁcally, in the traditional MLS
formulation [ABCO∗03], points are ﬁrst locally parameter-
ized by their projection on the local tangent space. Then, in
this parameterized space, a weighted ﬁtting estimates a low-
degree bivariate polynomial approximating the input cloud.
MLS then deﬁnes the evaluating point’s projection onto the
reconstructed surface as the closest point to the bivariate poly-
nomial [ABCO∗03]. We note that this projection process only
requires unoriented normals and can also be used to deﬁne
an unsigned distance function [AK04].
Such a projection process allows for resampling the point
cloud, producing a so called point set surface – the surface is
implicitly deﬁned as the ﬁxed point of the projection operator.
However, it is nontrivial to explicitly construct a continuous
representation, for instance an implicit function or a triangle
mesh, when the user wants to know the precise geometry and
topology of the MLS surface for geometry processing tasks
that require a continuous surface representation. Advancing
front methods [SFS05,SSFS06] produce a triangle mesh by
incrementally laying out triangles to compose fronts, where
vertex positions are determined through the MLS deﬁnition.
Care must be taken, however, when fronts merge and split.
If normals are oriented, then one can simplify the MLS def-
inition to arrive at an implicit function representation for the
surface. Namely, the method of [AA04] constructs an implicit
function at a point as the signed distance to a weighted aver-
age of nearby tangent planes. The method of [SOS04] uses a
weighted average of distances to nearby tangent planes. Both
methods assume that it is possible to construct a well-deﬁned
tangent plane at each evaluation point, which may not exist
for sparsely sampled data. In this case, a higher-order approx-
imation such as algebraic point set surfaces [GG07], which
uses an MLS deﬁnition with spheres for shape approximation,
can be more robust – see Figure 4.
A key property of MLS is the use of a weighting func-
tion, used to give larger inﬂuence to points near the evalu-
ating point in both estimating the tangent space as well as
constructing the polynomial. This can be used to combat
moderate levels of noise by allowing the weight function to
have a larger spatial inﬂuence. For nonuniform sampling, it is
necessary to deﬁne a weight function whose spatial support
varies as a function of the sampling density. This may be
done via a user-deﬁned scale proportional to an estimation
of the density [GG07], as well as methods whose support
is derived from a data-dependent error bound on the MLS
approximation [LCOL06]. However, MLS methods are in
general unable to provide a good surface approximation in
the presence of missing data, since it is necessary to use a
rather large spatial support size in such regions for which a
tangent plane (or sphere) may provide a poor ﬁt.
Multi-level partition of unity (MPU). For this set of tech-
niques, the reconstruction problem is approached as a hierar-
chical ﬁtting problem [OBA∗03a]. At a certain scale, a local
shape ﬁt is determined adequate if its error residual is suf-
ﬁciently small, otherwise the occupied space is reﬁned and
the overall procedure is repeated. Once all shape ﬁts have
been performed, an implicit function over the entire volume
is formed by smoothly blending nearby ﬁts. Note that the
deﬁnition of an implicit function requires the shape ﬁts to be
signed ﬁts, hence requiring oriented normals. Compared to
MLS methods, MPU is more robust to nonuniform sampling
since it does not require an estimate of the sampling den-
sity: a shape ﬁt is only accepted if it is below a certain error
residual. The level of smoothness and hence robustness to
noise can be adjusted by the error residual tolerance. Missing
c
⃝Author version


--- Page 12 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
Figure 5: The scan on the left contains outliers and scan
misalignment, particularly near its boundaries. The output
of [LCOLTE07], shown on the right, is able to robustly deal
with such challenging data through the use of multivariate
median data ﬁtting, and a point attraction term in the output.
data can be addressed by allowing for the extrapolation and
subsequent blending of spatially adjacent shape ﬁts. However,
such an extrapolation may produce erroneous surface sheets,
depending on the form of missing data. To resolve such poor
ﬁts, a diffusion operator can be deﬁned on the collection of
shape ﬁts, in order to perform smoothing directly on the MPU
representation [NOS09].
Parameterization-free projection. These methods project
the point cloud, without normals, onto the multivariate me-
dian, where a point balancing term ensures samples are as-
uniformly-as-possible distributed in the output [LCOLTE07].
This method is limited to outputting a resampled point cloud,
where the resampled points are restricted to where the input
data lies, retaining boundary components. However, by using
a multivariate median, parameterization-free projection does
not need to estimate a local tangent plane or perform shape
ﬁtting as in MLS and MPU methods, respectively. Hence for
strong noise, outliers, and even misalignment, this type of
approach is quite robust – see Figure 5. This approach was ex-
tended by [HLZ∗09] to handle highly nonuniform sampling
by incorporating an estimation of sampling density into the
balancing term.
3.2. Global Surface Smoothness Priors
Global smoothness priors typically involve higher order
smoothness, large-scale smoothness, or both. High order
smoothness relates to the variation of differential properties
of the surface: area, tangent plane, curvature, etc. Large-scale
herein relates to the spatial scale where smoothness is en-
forced.
Radial basis functions (RBFs). RBFs are a well-known
method for scattered data interpolation. Given a set of points
with prescribed function values, RBFs reproduce functions
containing a high degree of smoothness through a linear com-
bination of radially symmetric basis functions. For surface re-
construction, the method of [CBC∗01] constructs the surface
(a)
(b)
(c)
(d)
Figure 6: A point cloud sampling a sphere consisting of 150
points (a) is reconstructed by [HDD∗92] resulting in a C0
surface (b). A locally supported RBF [Wen95] reconstruct
a C1 surface, while the global triharmonic RBF (∆3 f = 0,
φ(x) = x3) outperforms the previous methods, although in-
curring a high computational cost.
by ﬁnding an implicit function deﬁned via RBFs whose zero
level set represents the surface. More speciﬁcally they use
globally-supported basis functions of the form φ(x) = ∥x∥2.
The implicit function f may then be expressed as:
f(x) = p(x)+
n
∑
i=1
λiφ(x−pi),
(3)
where p denotes a low-degree polynomial and the basis func-
tions are shifted by the evaluation point x.
The coefﬁcients λi are found by prescribing, as interpo-
lation constraints, a function value of 0 for pi ∈P. Off-
surface constraints are necessary to avoid the trivial solution
of f(x) = 0 for x ∈R3. Positively (resp. negative) valued
constraints are set for points displaced at pi along ni in the
positive (resp. negative) direction. The displaced points are
selected such that each one’s closest point in P is pi. The coef-
ﬁcients λi are found via a dense linear system in n, where by
exploiting the structure of φ, fast multipole methods are used
to reduce the complexity from O(n3) to O(nlogn) [CBC∗01].
An advantage to using globally-supported basis functions
for surface reconstruction is that the resulting implicit func-
tion is globally smooth; see Figure 6(d) for triharmonic RBFs,
compared to compactly-supported basis functions [Wen95]
shown in Figure 6(c). Hence RBFs can be effective in pro-
ducing a watertight surface in the presence of nonuniform
sampling and missing data. However, when the input con-
tains moderate noise, determining the proper placement of
off-surface points can become challenging.
Indicator function. These methods approach surface recon-
struction by estimating a soft labeling that discriminates the
interior from the exterior of a solid shape. This is accom-
plished by ﬁnding an implicit function χ that best represents
the indicator function, taking on the value of 0 in the interior
of the shape and 1 otherwise. The key observation in this
class of methods is that, assuming a point cloud with oriented
normals, χ can be found by ensuring its gradient is as-close-
as-possible to the normal ﬁeld N, in a least-squares sense,
via ∥∇χ−N∥2
2. If we apply the divergence operator to this
problem, then this amounts to solving the following Poisson
c
⃝Author version


--- Page 13 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
equation:
∇·∇χ = ∆χ = ∇·N.
(4)
Once solved, the surface is found via χ with a suitable iso-
value, typically the average or median value of χ evaluated at
all of the input points.
The approach of [Kaz05] solves this by transforming the
problem into the frequency domain, where the Fourier trans-
forms of ∆χ and ∇·N result in a simple algebraic form for
obtaining the Fourier representation of χ. By operating in the
frequency domain, however, it is necessary to use a regular
grid in order to apply the FFT, hence limiting spatial reso-
lution in the output. In order to scale to larger resolutions,
the method of [KBH06] directly solves for χ in the spatial
domain via a multi-grid approach, hierarchically solving for
χ in a coarse-to-ﬁne resolution manner.
Indicator function methods are an instance of gradient-
domain techniques, which are known to be robust meth-
ods for such applications as high dynamic range compres-
sion [FLW02] and image blending [PGB03]. For surface
reconstruction, such a gradient-domain formulation results in
robustness to nonuniform sampling, noise, outliers, and to a
certain extent missing data. The implicit function’s gradient
being well-constrained at the data points enforces smooth-
ness and a quality ﬁt to the data and since the gradient is
assigned zero away from the point cloud, χ is smooth and
well-behaved in such regions. Furthermore, for small scan
misalignment, normals tend to point in a consistent direc-
tion, which yields a well-deﬁned gradient ﬁt for the indicator
function. Several extensions have been made to this original
formulation, addressing limitations related to streaming re-
construction [MPS08], faithfulness to the input [KH13], and
sensitivity to normals [ACSTD07].
The work of [MPS08] solves the Poisson equation in a sim-
ilar manner to [Kaz05], but rather than using a Fourier basis,
it uses wavelets in such a way that computing the wavelet
transform of χ may be done in a local manner, where higher
resolution wavelet bases contain smaller spatial support. Com-
pact support is particularly advantageous in streaming surface
reconstruction, where the reconstruction is done on a subset
of the data at a time.
A known issue with the approach of [KBH06] is that ﬁt-
ting directly to the gradient of χ can result in over-smoothing
of the data [KH13, Fig. 4(a)]. To address this, the method
of [KH13] directly uses the point cloud as positional con-
straints into the optimization, resulting in a screened Poisson
problem. Similarly, the method of [CT11] incorporates po-
sitional, gradient, as well as Hessian constraints on the im-
plicit function, where the Hessian constraint can improve
surface extrapolation in regions of missing data [KH13,
Fig. 6(a)]. The main difference between the approaches is
that [KH13] solves the problem via a ﬁnite-element formula-
tion, whereas [CT11] use ﬁnite-differences, due to the com-
plexity in discretizing the Hessian term. In particular, the
Figure 7: From the point cloud on the left, we show a slice
of the implicit function produced in [ACSTD07] on the right,
where only unoriented normals are necessary in producing a
signed ﬁeld. An isotropic surface mesh of the zero isosurface
of the signed ﬁeld is also depicted.
screened Poisson formulation [KH13] is up to 2 orders of
magnitude faster than [CT11], see [KH13, Table 1].
All of the above approaches rely on oriented normals,
where although such methods can tolerate sparsely distributed
normal orientation ﬂips, large continuous clusters of improper
normal orientation can signiﬁcantly impact these methods.
To address this, the method of [ACSTD07] uses covariance
matrices to represent unsigned orientations, rather than using
normals. A covariance matrix is formed at a given point by
taking a union of neighboring Voronoi cells around the point.
The anisotropy of the covariance acts as a notion of normal
conﬁdence. The implicit function is found by maximizing its
gradient’s alignment with the principal component of the co-
variance matrix at each point, while ensuring smoothness and
a proper signed function by enforcing the function’s bihar-
monic energy to be small. This amounts to solving a sparse
symmetric generalized eigenproblem – Figure 7 depicts a
slice of the resulting eigenvector for the kitty point cloud.
Volumetric segmentation. These methods perform recon-
struction via a hard labeling of a volumetric discretization,
where the goal is to label cells as being either interior or
exterior to the surface. The method of [KSO04] constructs a
graph Laplacian from the Delaunay triangulation of P, where
each node represents a tetrahedron of the triangulation and
each edge measures the likelihood of the surface passing
through the adjacent tetrahedra. The Laplacian eigenvector
with smallest nonzero eigenvalue then smoothly segments
tetrahedra into interior and exterior, as this eigenvector simul-
taneously seeks a smooth labeling and a partitioning with low
edge weights. This approach has shown to be robust to noise
and outliers without the use of normals, thanks to the robust-
ness of spectral partitioning. Since it produces an explicit
c
⃝Author version


--- Page 14 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
volume segmentation, it also ensures a watertight surface.
However, in regions of missing data, the discretization from
the Delaunay triangulation may be too coarse, giving a poor
approximation to the surface [KSO04, Fig. 6].
Methods based on graph cuts are also used in ﬁnding a
volumetric segmentation. The method of [HK06] ﬁrst deﬁnes
a small crust on the exterior and interior through a dilation
operation on point-occupied voxels. A graph is imposed on
the crust where edges reﬂect the local surface conﬁdence –
a function of a smoothed unsigned distance, while nodes on
the boundaries of the interior and exterior crust are connected
to a source and sink node, respectively. To impose global
smoothness, non-terminal edges also include a small regu-
larization constant, which encourages minimal surface area.
This method does not use normals, as it only needs to com-
pute an unsigned distance in order to deﬁne edge weights in
the graph cut solve. This results in robustness to nonuniform
sampling, noise, and misalignment. Furthermore, the minimal
surface area regularization allows for the method to handle
missing data, where we note that such a regularization is ab-
sent in the spectral segmentation approach of [KSO04]. How-
ever, computing the crust such that the interior and exterior
are identiﬁable can be challenging in certain conﬁgurations.
3.3. Piecewise Surface Smoothness Priors
Moving from the smooth, closed case to the piecewise smooth
case (possibly with boundaries) is substantially harder as the
ill-posed nature of the problem applies to each sub-feature of
the inferred shape. The features of a piecewise smooth surface
range from boundary components, sharp creases, corners,
and more speciﬁc features such as tips, darts, and cusps.
In addition, the inferred surface may be either a stratiﬁed
manifold or a general surface with non-manifold features.
Another difﬁculty stems from the fact that a feature is a
notion that exists at speciﬁc scales, such that reconstruction
and feature approximation cannot be decoupled.
Semi-sharp features. One step toward piecewise smooth-
ness is a class of feature-preserving methods based on im-
plicit representations. Sharp features can be captured through
locally adapted anisotropic basis functions [DTS01]. Adam-
son and Alexa [AA06] rely on an anisotropic moving least
squares (MLS) approach, where the anisotropy is based on
principal curvatures derived from the points’ positions and
normals. Oztireli et al. [OGG09] extend the MLS through ker-
nel regression to allow for sharper features. However, none of
these techniques reproduce truly sharp features: the features
in a reconstruction contain varying degrees of smoothing.
Moreover, the presence of sharpness in the geometry of a
point set is detected only locally, which often leads to frag-
mented creases when defects are present [ASGCO10, Fig.
6].
Locally-sharp features. Another way to detect local sharp-
ness consists in performing a local clustering of estimated
normals [OBA∗03b]: the algorithm ﬁts as many quadrics
as the number of clusters of normals. Improved robustness
is achieved in [FCOS05b] by segmenting neighborhoods
through region growing. Lipman et al. [LCOL07] enriches
the MLS projection framework with sharp edges driven by
the local error of the MLS approximation. However, the
locality of the feature detection can generate fragmented
sharp edges, much like general feature detection approaches
[GWM01,PP09].
Globally-sharp features. To reduce crease fragmentation,
some approaches favor the extraction of long sharp features.
Pauly et al. [PKG03] use a multi-scale approach to detect
feature points and construct a minimum-spanning tree to infer
a feature graph. Daniels et al. [DHOS07] uses a robust projec-
tion operator onto sharp creases and grow a set of polylines
through projected points. Jenke et al. [JWS08] extract feature
lines by robustly ﬁtting local surface patches and computing
the intersection of close patches with dissimilar normals.
Sharp and robust. Only few approaches tackle the combined
issue of robustness to defect-laden point clouds and feature-
preserving reconstruction [FCOS05a,ASGCO10,HWG∗13].
The method of [FCOS05a] uses a least median of squares
regression scheme in its region growing approach to handle
outliers in the data. The approach of [ASGCO10] ﬁrst esti-
mates normals that preserve sharp features in the shape via
l1 sparse reconstruction, i.e. the vector of all neighboring
normal differences should be sparse, where large nonzero dif-
ferences reﬂect sharp features. The positions are then found
as offsets from the recovered sharp normals. The method
of [HWG∗13] allows for the preservation of sharp features
in a resampled point cloud by ﬁrst resampling away from
detected edges in order to reliably compute normals and then
upsampling in a manner that preserves sharp features, deter-
mined by the normals. This method employs a similar energy
to [LCOLTE07] away from sharp features, hence they are
able to handle similar types of defects in the point cloud.
4. Visibility Priors
Although methods based on surface smoothness priors can
support a large variety of inputs, such a rather general as-
sumption places restrictions on the extent to which they can
support substantial artifacts in the point cloud. To handle chal-
lenging point clouds, it is useful to consider speciﬁc priors
for reconstruction. In this section we consider visibility as a
prior and how it can help regularize certain reconstruction
problems.
Visibility has generally been used in three different ways.
The ﬁrst class of methods considers how to use the visibility
provided by the scanner that produced the point cloud – this
is used primarily to obtain the line of sight associated with
each sample; see Section 4.1. The second class of methods
uses line of sight that is not provided from the scanner, but
rather approximated from the exterior space; see Section 4.2.
c
⃝Author version


--- Page 15 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
The third class of methods uses visibility to approximate
parity – the number of times a ray intersects a surface – in
order to approximate the interior and exterior, as discussed
in Section 4.3. These types of methods tend to make little as-
sumptions on the shape class, and usually produce an implicit
function or a surface mesh in output.
4.1. Scanner Visibility
The most common method for using the visibility information
provided by a scanner is the merging of individual range
scans. This is the approach taken by [CL96], where a signed
distance function is incrementally built up over each range
scan. More speciﬁcally, each scan is ﬁrst triangulated via
its image lattice structure. Then for a given ray originating
at the scanner head position, the signed distance ﬁeld for
points along the ray are updated based on their distance to
the ray’s intersection point with the scan. Furthermore, as
discussed in Section 2.2.2, a conﬁdence can be assigned to
each point in the range scan via line of sight information,
where [CL96] associates low conﬁdence weights with high
grazing angles. This is particularly useful in combating noise
in the point cloud, since one can easily over smooth or under
smooth if no conﬁdence values are associated with points;
see [CL96, Fig. 4].
Furthermore, one can perform space carving through line
of sight information, via marking regions of space observed
by the scanner as empty. The approach of [CL96] uses this
information to extract geometry between regions marked
empty and regions that are unseen, where the assumption is
that unseen regions are the interior of the shape. This is very
useful in resolving ambiguous topology in regions of missing
data; see [TOZ∗11, Fig.9].
For other forms of missing data, the approach of [CL96]
will typically preserve the hole as it does not enforce any
type of smoothness prior. It is possible to incorporate a mini-
mal surface area regularization to encourage smoothness in
regions of missing data, while using line-of-sight as a data-
ﬁtting term. Existing approaches solve such a formulation via
level-set models [Whi98] and graph cut optimization [LB07].
The method of [LPK09] seeks an interior and exterior label-
ing of tetrahedra from a Delaunay triangulation of the point
cloud, similar to [KSO04], but formulates it as a graph cut
problem using line of sight information. At each tetrahedron,
the method accumulates evidence for belonging to the exte-
rior through line of sight of all range scans, hence assuming
outliers are randomly distributed, this method is robust to
such defects; see [LPK09, Fig. 13].
For scans that contain a high level of misalignment and
structured outliers, the method of [ZPB07] approaches range
scan merging by using the l1 norm for the data term, and the
minimization of the signed distance gradient magnitude as the
regularization term. This type of regularization, commonly
known as total variation denoising, allows the algorithm
(a)
(b)
(c)
Figure 8: The point cloud “hidden point removal” operator
from [KTB07] applied to an input (a) determines the subset
of visible points as viewed from a given viewpoint (b). Given
this labeling, a view-dependent on-the-ﬂy reconstruction (c)
can be obtained by retaining the topology of well shaped
triangles from the convex hull of the spherical inversion.
to be robust to structured outliers and scan misalignment;
see [ZPB07, Fig. 4].
The method of [FG11] considers the case when range
scans have widely varying scales – the range scans have very
different sampling densities. In such cases, merging multiple
scans of a coarse scale with a single scan at a ﬁne scale can
overly smooth out the ﬁne-grained detail. [FG11] extends
[CL96] by constructing a hierarchical signed distance ﬁeld.
This permits retaining the high resolution detail of ﬁne-scale
scans, while capturing the more general scene present in
coarse-scale scans.
Scanner visibility was recently used in [UB13] in the recon-
struction of thin objects, such as street signs. The method em-
ploys a point-based representation for reconstruction, where a
particle system is used to satisfy a data-ﬁtting term, an energy
that encourages the vector formed by neighboring points to
lie on one another’s tangent plane, and a term that penalizes
neighboring points if their normals are in different directions.
Each input point’s target output point is restricted to be along
its line of sight, which helps to constrain point movement in
the particle system and greatly simpliﬁes the optimization.
4.2. Exterior Visibility
It is possible to exploit visibility even in the absence of ex-
plicit information from the scanner. Given a chosen camera
position, point set visibility [KTB07] determines the portion
of the point cloud that is not self-occluded. First, a spherical
inversion of the point cloud with respect to the given query
point is computed. Then, visible points are simply identiﬁed
as those that lie on the convex hull of this set – see Figure 8.
While [MTSM10] extended this method to handle moder-
ate levels of noise, the input point cloud must respect strict
sampling assumptions to produce satisfactory results.
Occlusion culling. The method of [CCLN10] builds upon
these ideas and reconstructs a watertight surface by carving
c
⃝Author version


--- Page 16 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
Figure 9: The approach of [MDGD∗10] ﬁrst computes a
robust unsigned distance function (left), and constructs an
interior/exterior labeling (middle), and associated conﬁdence
(right) of the labeling. Note that low conﬁdence is associated
with regions of missing data, such as the bottom of the scan.
the space occluded by the point cloud when observed by a
sufﬁciently large and randomly sampled set of directions.
Similarly to [KTB07], the input cloud has to satisfy certain
stringent sampling conditions, limiting its effectiveness with
undersampling and missing data. Conditions on sampling are
relaxed in [CLCL11] where inconsistencies are detected by
observing that if one point’s Voronoi pole [AB99] lies in the
exterior, the other Voronoi pole should be in the interior. If
both are occluded or visible via [KTB07], this indicates an
inconsistency. Unfortunately, since the method uses Voronoi
poles, which cannot always be robustly estimated in the pres-
ence of missing data, its applicability remains limited.
Cone carving. The method of [SSZCO10] hypothesizes that
each point in the cloud must have been observed from the
scanner head. It computes high-likelihood visibility cones
originating at each sample and takes the boundary of the
union of all cones as an approximation to the surface. This
method can be used to infer the geometry in large regions
of missing data for challenging scenarios, i.e. two thin, spa-
tially close, and undersampled surface sheets – producing
topologically clean surfaces. The main disadvantage with the
approach is its lack of scalability, since it takes linear time to
construct a cone at each point, resulting in a total quadratic
running time.
4.3. Parity
An alternative way of using visibility is to deﬁne a measure
of parity. Assuming a closed surface, the parity for a given
ray (point and direction) is deﬁned as the number of times
the ray intersects the surface – if an odd number of times, this
indicates the point lies in the interior, otherwise the point is
in the exterior. This general idea can be extended to a point
cloud, giving rise to a notion of uncertainty in whether or
not a point belongs in the exterior or interior. The approach
of [MDGD∗10] constructs parity on the point cloud through
the use of a robust unsigned distance function. Namely, they
compute a crust, or an offset surface, around the point cloud
via the unsigned distance and evaluate parity at a given point
by randomly shooting rays in different directions, where the
Figure 10: The point cloud on the left was captured via
multi-view stereo – note the substantial structured outliers.
The center image is a zoomed-in portion of the point cloud,
centered on the object of interest. The method of [GCSA13]
(right) is highly robust to outliers, while still preserving the
details of the object, due to the use of parity and an unsigned
distance function which adapts to the noise.
crust is used to determine intersections with the surface. A
point will very likely be exterior or interior if the parity es-
timates are consistent over all directions. This uncertainty
estimate is used in constructing an implicit function, consist-
ing of a data-ﬁtting term and a smoothness term, such that
high smoothness constraints will be assigned to regions that
have high uncertainty (i.e. high disagreement in parity). Fig-
ure 9 shows the unsigned distance function for a challenging
point cloud, along with its sign estimate and conﬁdence in
sign. This approach is highly robust to noise and outliers
due to the use of a robust unsigned distance function, which
does not require the estimation of normals. It is also robust to
missing data, and since its regularization is spatially-varying
according to the uncertainty in parity, it will not over smooth
the data where it exists. However, since smoothness is en-
forced via a Laplacian regularization, this could still result
in poor behavior in regions of missing data, giving the in-
correct topology. The method of [SY12] addresses this by
performing space carving, guided by a parity estimate, to only
carve out space where there does not exist highly conﬁdent
interior regions. This can better retain topological features
such as tunnels, where smoothness priors may erroneously
over smooth and ﬁll these regions in.
The method of [GCSA13] extends [MDGD∗10] by using
a robust unsigned distance function that is adaptive to the
noise level in the point cloud. The method produces a sign
estimate over a random collection of line segments in the
volume. To determine the parity for each line segment, rather
than using a crust as in [MDGD∗10], they look at all local
minima in the unsigned distance along the segment, ﬂip the
function according to the local minima, and of all possible
ﬂipped minima choose the one that is smoothest. Figure 10
demonstrates the method’s robustness to strong, structured
outliers.
c
⃝Author version


--- Page 17 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
model image
input point cloud
reconstructed surface
user-edited reconstruction
Figure 11: The high-level geometric regularizer provided by “arterial snakes” [LLZM10] is effective in reconstructing the
delicate geometry of the scanned tubular model affected by noise, outliers, reduced sampling density and missing data. The
sparsity of the model does not only allow the correction of these data imperfections, but also enables intuitive data/user-driven
reconstruction.
5. Volumetric smoothness
In order to handle challenging forms of missing data, a com-
mon way to regularize surface reconstruction is to enforce
that the local shape thickness of a surface (i.e. a measure-
ment of its local volume) varies smoothly. For watertight
shapes, local thickness is measured by the radii of maximally
inscribed spheres of its medial axis transform. However, as
the medial axis is an alternative full representation of the
shape, determining the medial axis over which to perform
these measurements is an inherently difﬁcult problem – as
difﬁcult as the reconstruction problem itself.
Skeletal regularizers. The “rosa” method from [TZCO09]
addresses this issue by assuming that the medial axis of a
shape can be approximated by curves instead of surfaces, that
is, by a curve-skeleton. For organic geometry, a reconstruction
of the skeleton can be obtained even in the presence of miss-
ing data by exploiting the redundancies of local rotational
symmetry. Given a skeletal structure the geometry can be
reconstructed in regions of missing data by a three-step pro-
cess: ﬁrst, the distance from the cloud to the curve-skeleton is
cylindrically parameterized on the curve-skeleton; then, an in-
painting of the distance function is performed on this domain;
ﬁnally, the inpainted point cloud can be processed by one of
the algorithms in Section 3. It is important to note that a cylin-
drical parameterization prior constrains the class of shapes
for which a reconstruction is possible to one having a star-
shaped cross section. While the skeleton extraction method
in [TZCO09] suffered the limitation of requiring oriented nor-
mals, subsequent research showed how it is possible to extract
skeletons directly from unstructured point clouds [CTO∗10]
and even in the presence of outliers [HWCO∗13]. To over-
come other limitations, in [CTO∗10] a ﬁrst attempt at remov-
ing the necessity for an explicit cylindrical parameterization
was made, while also allowing user-input to locally modify
the regularizer.
Deformable models. Another way to take advantage of volu-
metric smoothness is to grow a deformable model from within
the volume. The approach of [SLS∗06] slowly grows mul-
tiple surface fronts, guided by a signed distance ﬁeld, from
within the interior volume out to the point cloud. The method
takes care to split and merge fronts in order to progressively
reconstruct the surface in growing levels of details. Noise and
missing data are handled by enforcing a smoothness term on
all fronts as they evolve. This conservative approach leads
to a better control and interpretation of the reconstructed
topology and can be extended to reconstruct a skeletal curve
directly from the point cloud [SLSK07].
To handle higher amounts of missing data, the approach
of [LLZM10] presents a deformable model for reconstructing
skeletal curves of man-made shapes composed of a collection
of tubular components such as metal rods and canes. The
arterial snakes model geometry is obtained by sweeping a
ﬁxed-topology cross section through the input point cloud
starting from regions of high data ﬁdelity. Their approach im-
plicitly encodes volumetric regularization by constraining the
solution to one exhibiting a smoothly varying cross-section,
while also allowing high-level geometric constraints like pla-
narity, contact tangency, and symmetry to further regularize
the reconstruction.
Organic skeletal geometry. Tubular components exhibiting
piecewise smooth radii variations are also suitable to model
organic and natural geometry like trees [RFL∗05, NFD07,
LYO∗10] or blood vessels [KQ04]. In tree reconstruction,
biological constraints are exploited to simplify the problem
and increase resiliency to data imperfections. Noise is intro-
duced by foliage, as this is typically too small to be ﬁnely
resolved by the scanning process and a substantial amount
of self-occlusion is caused by dense branching structures.
Therefore, the focus of the reconstruction is tree branches,
where their approach is to ﬁrst reconstruct the skeleton of the
captured tree. The skeleton is assumed to be a directed acyclic
graph rooted at the ground plane; limbs are typically piece-
wise smooth and their thickness almost everywhere smoothly
varying, where a pipe-model [RFL∗05] controlling thick-
ness variations can be used at branching locations. While
a pair of orthogonal images have been shown sufﬁcient to
c
⃝Author version


--- Page 18 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
hallucinate the 3D information with the help of some user
interaction [NFD07], recent research has attempted to model
the tree structure directly from 3D point clouds [LYO∗10].
Generalized volumetric smoothness. Several methods exist
for imposing volumetric smoothness on the medial axis of
the shape, supporting a much broader class of shapes. The
method of [BS12] approaches reconstruction by segmenting
the point cloud into volumetric regions and in each region tak-
ing the union of balls to obtain a coarse surface representation.
The union of these regions then serves as an initial surface for
the method of [SLS∗06], to recover ﬁne details. Key to their
segmentation is a distance measure deﬁned directly on the
point cloud that robustly measures the likelihood of a medial
ball being generated by any pair of points. This method is ro-
bust to noise and missing data, particularly when there exists
nearby surface sheets, but can fail in regions where parts of
the surface corresponding to medial sheets are missing.
Among methods employing volumetric regularization
[TOZ∗11] is the most general. The surface encoding the
solution is iteratively evolved by level-sets toward the data
considering both visibility and surface smoothness, similarly
to [Whi98], while simultaneously enforcing a volumetric
prior based on the medial axis of the evolving interface. This
technique can prevent the formation of unnecessary holes
in thin surfaces due to under-sampling, as the formation of
a topological feature would correspond to a quickly vanish-
ing medial radii. Furthermore, since the medial axis encodes
local reﬂectional symmetry, this allows for information to
be effectively propagated throughout the surface permitting
the reconstruction of challenging data like the geometry of
the highly concave areas in a vase – see [TOZ∗11, Fig.5].
While highly general, the instability of the medial axis to
surface-perturbations and the complexity of its computation
limit the applicability of the method.
6. Geometric Primitives
The detection of simpler geometric structures in a point cloud
has shown to be particularly beneﬁcial for surface recon-
struction. Knowledge of a surface that can be described as the
composition of geometric primitives can be extremely helpful
for denoising and ﬁlling in missing data. Not all shapes ad-
here to this prior, but typically CAD and architectural models
can be described in this manner.
Detecting primitives. The method of [SWK07] is an effec-
tive method for ﬁnding geometric primitives in shapes. It uses
RANSAC to robustly ﬁnd planes, spheres, cylinders, cones,
and torii, through an efﬁcient means of sampling points for
ﬁtting and evaluating scores, both based on locality sensitive
methods. Importantly, this method produces primitives that
partially match the point cloud – the collection of these shapes
can then be used for reconstruction. We note that although
this method can detect a small set of easily parameterizable
shapes, efﬁcient pose detection methods for arbitrary shapes
can also be used [DUNI10].
Figure 12: (left) CAD models are often obtained by con-
structive solid geometry as a composition of simple prim-
itives: planes, spheres, cones, etc. (middle) Randomized
search [SWK07] can be used to detect such primitives in the
point cloud data even in the presence of noise, outliers and
missing data. (right) The primitives can then be extrapolated
to obtain a watertight surface from incomplete data [SDK09].
Primitive consolidation. The work of [JKS08] takes a set
of detected plane primitives and performs reconstruction by
aligning and merging the boundaries of adjacent primitives.
More speciﬁcally, the boundaries of the plane primitives are
extracted and an optimal conﬁguration of boundaries is found
by imposing a data-ﬁtting term to the original boundary as
well as a term that favors the snapping of boundary points and
corner points of neighboring planes. Explicitly using corners
prevents the boundary from smoothing itself out. The advanc-
ing front method of [SFS05] is used to extract a surface mesh.
This method can reconstruct CAD and architectural models
alike, producing a surface mesh that retains the detected prim-
itive structures. However, the method requires that adjacent
primitive boundaries should be geometrically close to each
other, which may not be satisﬁed if primitive detection is
noisy, or if missing data is present. The method of [SDK09]
resolves this by explicitly extrapolating shape primitives (of
all kinds) and forming the resulting output as the intersection
of the extrapolated primitives. This extrapolation of primi-
tives is formulated as a graph cut problem, where in addition
to a standard minimal surface area term, a data ﬁtting term
is used that ensures the surface normal at a given point (the
edge in the graph) is aligned with all intersecting primitives
at that point. This does not constrain the primitives in a local
manner: primitives whose boundaries are far away can even-
tually meet up and intersect with this method, as illustrated
in Figure 12.
Augmenting primitive information. Although the method
of [SDK09] can robustly handle missing data, it can be sensi-
tive to noisy primitives which may fail to deﬁne a coherent
model when extrapolated. The work of [CLP10] instead uses
line of sight information to help penalize poorly extrapo-
lated primitives. Namely, this work takes the set of primitives
as well as an additional set of primitives formed near the
boundaries of the input primitives and constructs a cell com-
plex reﬂecting the extrapolation of the primitives. An energy,
similarly solved via graph cuts is then formed, where the
data-ﬁtting term uses line of sight information to penalize
c
⃝Author version


--- Page 19 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
facets in the complex lying in regions marked empty via space
carving. The method of [RKMP13] uses the point cloud, line
of sight, and edge features in corresponding RGB images to
infer constraints for ﬁnding the boundary of each input shape
primitive, consequently producing the reconstructed surface.
Volumetric primitives. In the case of indoor scene recon-
struction, an alternative to surface primitives is to employ
volumetric primitives to model the interior space. In [XF12]
the volume is modeled by ﬁtting simple cuboids to the empty
space deﬁned by the boundaries of the scan data. A set of 2D
constructive solid geometry (CSG) vertical slices are built by
incrementally adding and removing candidate rectangles that
best model the interior – a function of line of sight informa-
tion. A similar process is used to stack up these slices to build
a set of volumetric primitives, producing a 3D CSG model
that composes the interior. The method of [OLA13] starts
from a volume decomposition of the space, and labels volu-
metric cells as interior and exterior through a parity-driven
cost function solved via graph cuts, where parity is measured
through ray intersections of extracted horizontal and vertical
structures. Such volumetric methods guarantee a watertight
output, and are highly robust to missing data, since only a par-
tial sampling of the volume boundary is needed for a quality
ﬁt.
Hybrid methods. A limitation of primitive-based methods
is that they do not degrade gracefully if certain portions of
the shape are poorly explained by a primitive. The method
of [LA13] resolves this by introducing a hybrid approach
to reconstruction: shape primitives are used to resample the
point cloud and enforce structural constraints in the output,
such as sharp features between adjacent primitives and cor-
ners, while a visibility-driven prior is employed in regions
where a primitive ﬁt is not found. A similar approach was
proposed in [vKvLV13], where planar polygons of sufﬁcient
ﬁtting quality are extracted and a conforming, constrained
Delaunay triangulation is constructed on the polygons and
the remaining points so that the polygons are preserved in
the triangulation. A visibility-driven graph cut problem is
then solved, similarly to [LPK09], such that the extracted
polygon primitives are retained, while the rest of the points
rely on line of sight information for reconstruction. These
types of methods greatly generalize the class of shapes in
which primitive-based reconstruction may be performed.
7. Global Regularities
Certain classes of shapes, namely CAD, man-made, urban,
and architectural shapes, possess a certain level of global
regularity. Regularity in a shape can take many forms: a vase
described by a surface of revolution, a building composed of
facade elements, or a mechanical part consisting of recurring
orientation relationships between sub-parts. All of these are
examples of the following more general properties: symmetry,
repetition, and canonical relationships. Commonly associ-
ated with high-level shape analysis [MWZ∗13], these priors
Figure 13:
From the scan on the left, the method
of [PMW∗08] is able to discover local similarity transforma-
tions which relate individual elements. This can be used to
resample the point cloud, as well as extrapolate the scan into
missing regions, shown on the right.
have also shown to be of great use in handling severe defects
in a point cloud.
7.1. Symmetry
Symmetry is a well-studied problem in shape analy-
sis [MPWC13]. Symmetry detection is focused on ﬁnding
either global or local transformations on the shape that maps
the entire shape, or a subset of the shape, onto itself. Find-
ing such transformations can be extremely useful for surface
reconstruction in handling noise and missing data.
Global similarity. The method of [LA11] applies this to the
case of missing data for single-scan completion by ﬁnding
small surface patches of the scan that exhibit either bilateral,
rotational, or surface-of-revolution symmetry, and then apply-
ing the detected transformation to the rest of the scan to infer
the missing data. This can produce a complete model from
a single view and does not assume a speciﬁc type of shape
class. However, it assumes that a shape can be well described
by the application of a single simple transformation, which
does not always hold.
Local similarity. Rather than imposing global relationships,
in [PMW∗08] the authors focus on ﬁnding repeating elements
(small subsets of the point cloud) that can be mapped onto
one another by local similarity transformations. They show
that the repetition of elements in a point cloud manifests
as a lattice structure in a suitable transformation space. In
particular, partial matches become prominent in this transfor-
mation space, hence repeating elements of varying levels of
missing data can be robustly detected and used to reconstruct
incomplete regions, see Figure 13. The method of [LCDF10]
ﬁnds symmetries in incomplete point clouds by construct-
ing an afﬁnity matrix that measures how symmetric all pairs
of points are. The key insight made by [LCDF10] is that
this matrix should be block-diagonal for many types of sym-
metries – i.e. rotational, bilateral, intrinsic. By considering
powers of this matrix, the authors demonstrate how incom-
plete matches become more pronounced, allowing for a wide
range of detected symmetries in challenging point clouds
containing noise, outliers, and missing data. These simpler
forms of symmetry can be generalized to a notion of subspace
symmetries [BWM∗11], where a symmetry group is deﬁned
c
⃝Author version


--- Page 20 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
by a set of local transformations as well as a low-dimensional
shape space, in order to handle more general types of shapes.
7.2. Structural Repetition
In certain cases it is difﬁcult to ﬁnd repeating elements in a
point cloud through symmetry transformations. Instead, di-
rectly seeking repeating elements in a transformation-free
manner can provide us with more ﬂexibility in the reconstruc-
tion process.
The method of [ZSW∗10] utilizes this observation for re-
construction of building facades in noisy terrestrial LiDAR
scans, where occlusions from vegetation or other objects re-
sult in signiﬁcant missing data. For a given type of facade
element, each elements’ planes are detected via [SWK07]
and the individual elements are registered at a per-plane level.
Once registered, denoising is performed across all elements
via the individually registered planes and the consolidated
facade element is projected back onto each instance for recon-
struction. The mutual use of information across all elements
allows one to robustly remove noise and ﬁll in missing data.
A drawback to the approach of [ZSW∗10] is the strict
requirement of user interaction. This limitation was addressed
in the work of [SHFH11] by adaptively partitioning facades.
The approach of [WS12] takes the consolidated point cloud
of [ZSW∗10] and segments it into depth layers and uses a
grammar deﬁnition to individually segment each depth layer
into facades via the optimal sequence of grammar derivations.
Facades may be appropriately extruded at each depth layer
to obtain a polygonal representation of the building, at the
possible expense of detail in the geometry due to the lack of
expression in the shape grammar.
The method of [FS12] tackles occluded facade scans by
analyzing vertical scanlines consisting of columns of points
along major facade planes to detect periodic features. The
Discrete Fourier Transform is computed for each scanline
and used to extract the period of repetitive structures. Holes
are ﬁlled by extending the periodic features into occluded
regions.
Another means of detecting regularity in incomplete scans
is to ﬁnd regularity in associated RGB imagery, and prop-
agate this information back to the 3D scan to perform re-
construction. The approach of [LZS∗11] achieves this by
decomposing the RGB image into depth layers via the 3D
scan, and upon detecting symmetries with respect to each
layer via [WFP10], consolidates all element instances to ro-
bustly denoise and ﬁll in missing data across the instances.
7.3. Canonical Relationships
Another useful prior on global regularities is the canonical
intra-relationship between parts of a scene, or parts of a shape.
Such relationships can be parallel or coplanar parts, recurring
orthogonality between planes, concentric or co-axial parts,
input scan
ransac
globfit
Figure 14: From a set of misaligned scans shown on the
left, the primitives extracted via [SWK07] (middle) retain the
misalignment. Globﬁt [LWC∗11] (right) is able to correct
misalignment by enforcing consistent canonical relationships
across primitives.
and regularity in orientation. This often arises in CAD models
due to fabrication restrictions and budget considerations, as
well as urban environments due to functional constraints.
Manhattan constraints. Perhaps the simplest form of a
canonical relationship is the Manhattan-world (MW) as-
sumption: all planar primitives in a scene belong to one
of three mutually orthogonal planes. This can simplify
facade reconstruction, as in the aforementioned methods
of [ZSW∗10, SHFH11, WS12, LZS∗11]. In [VAB12], MW
is used for building reconstruction by ﬁrst classifying points
by shape type – wall, edge, convex corner, or concave corner
– and clustering points of a similar type. After constructing
MW-aligned bounding boxes on all clusters, volume regions
are found via parity, analogous to [MDGD∗10], where inte-
rior regions of consistent parity are considered to belong to
the building’s volume. As edges and corners are detected via
relationships between walls, this method is robust to missing
data, but may be sensitive to noise for adjacent wall conﬁgu-
rations.
Consolidating relationships. The method of [LWC∗11] re-
constructs CAD shapes consisting of a much richer variety of
canonical relationships compared to MW. Namely, starting
from an initial set of detected primitives [SWK07], parallel,
orthogonal, angle-equality, and distance-equality relation-
ships are individually detected and carefully selected so as
to not cause any relationship conﬂicts. By enforcing these
relationships, structured noise such as scan misalignment can
be effectively handled – see Figure 14.
Canonical building relationships. The work of [LWC∗11]
was extended to the case of reconstruction of buildings from
2.5D scans in [ZN12]. The basic observation in this approach
is that there exists three fundamental type of relationships in
buildings: roof-roof relationships that consist of orientation
and placement equalities, roof-roof boundary relationships
that consist of parallelism and orthogonality relationships,
and boundary-boundary relationships that consist of height
and position equality. Upon ﬁnding the relationships via clus-
tering (i.e., clustering similar angles, equality, etc..), they are
used to inform the primitive ﬁtting method so that the prim-
c
⃝Author version


--- Page 21 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
Figure 15: From the incomplete and cluttered scan on the left, the approach of [NXS12] ﬁrst oversegments the point cloud
(mid-left), then iteratively merges segments which agree on class labels (mid-right), and ﬁnally deforms a set of detected models
to best resemble their segmented objects (right).
itives simultaneously ﬁt to the data and to the relationships.
Analogous to [LWC∗11], this allows for robust reconstruction
from building scans containing strong structured noise.
8. Data-driven priors
The previously discussed priors may not always be appropri-
ate and in practice, certain shapes may simply fail to adhere
to these priors. A more ﬂexible method of specifying a prior
is through a data-driven means: using a collection of known
shapes to help perform reconstruction. This can allow for the
reconstruction of individual objects and more generally, the
reconstruction of scenes composed of a certain type of envi-
ronment and multiple types of objects. Fine-grained scene
reconstruction is extremely challenging due to the large miss-
ing data often associated with scans, such as those derived
from the Microsoft Kinect. Hence, if reconstructing the ge-
ometry is infeasible, these methods instead seek the most
similar object in a database, and if necessary, its deformation
in order to ﬁt the input data.
Data driven inpainting. An earlier method of recon-
structing individual objects from a database was proposed
in [PMG∗05], where an incomplete input point cloud is
matched against a database of complete shapes and the most
relevant shape is then rigidly deformed into the point cloud to
recover missing data. This allows for a watertight reconstruc-
tion, but for the algorithm to be effective, strong similarity
between the input data and the best-matching object is nec-
essary. This limitation was addressed by [GSH∗07] by using
local shape priors, where a collection of overlapping point
cloud patches are matched against a local shape prior database
and the retrieved priors are used to reconstruct the surface.
A major drawback of these approaches is the assumption
that the point cloud is sufﬁciently dense, so that matching
the cloud against a set of complete shapes is meaningful.
However, for the acquisition of scenes, in particular indoor
environments, the scanned objects may be too incomplete to
permit this due to occlusion, limited observation directions,
or the geometry is too ﬁne-grained with respect to scanner
resolution. Furthermore, for these methods to be applicable
to scene reconstruction, objects need to be individually seg-
mented.
Reconstruction
by
rigid
retrieval.
The
method
of [SXZ∗12] approaches this problem by ﬁrst seman-
tically segmenting the point cloud and then ﬁnding a
complete model to replace each segment. More speciﬁcally,
given a set of scans and RGB images, a conditional random
ﬁeld class labeling problem is formulated, balancing two
objectives: a data ﬁtting term based on a database of
class-labeled objects and a smoothness term favoring local
consistency in appearance and geometry. A training set
of point clouds is built, where for each object scans are
constructed via a virtual camera over a range of orientations
and distances from the object. This allows the model to be
robust with regard to missing data, as the input should map
to one of these poses. A random forest classiﬁcation is built
over this training set allowing for the closest complete object
to an incomplete scan to be retrieved. Although retrieving
a rigid template from a database can effectively handle
incomplete data, only rigid transformations with uniform
scaling are considered to best align each matched object.
Reconstruction by non-rigid retrieval. A natural extension
of reconstruction by retrieval is to consider non-rigid trans-
formations of the template geometry to the input data. This
is addressed in [NXS12] where upon ﬁnding a certain se-
mantic class for a segmented object in the point cloud, every
model is non-rigidly deformed via localized scale deforma-
tions. The best match is identiﬁed as the model with the
smallest registration residual. This method approaches clas-
siﬁcation differently by building a semantically-labeled seg-
mentation through incremental selection of oversegmented
surface patches. A patch is chosen if the resulting merged
object has high conﬁdence in its label. This is particularly ef-
fective in noisy, outlier-ridden highly cluttered environments
– see Figure 15. The authors of [KMYG12] extend these ideas
by noting that in indoor environments it is common to have
the same object in multiple poses. Their technique incorpo-
rates a deformation model directly into the segmentation and
classiﬁcation problem, rather than as a post-processing step.
A deformation model is learned over multiple incomplete
scans for a given object, allowing the object to be identiﬁed
by incomplete observations of its parts. Given an input scan, it
is ﬁrst over-segmented and then iteratively merged into parts,
where parts are matched against learned local deformation
modes of a model. Part relationships are then used to verify
the global quality of a match. Compared to [NXS12,SXZ∗12],
such deformation models allow one to reconstruct a broader
c
⃝Author version


--- Page 22 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
set of objects, and lessens the need for a potentially large
database for recognition.
Reconstruction by part composition. A disadvantage of the
above approaches is that the granularity of the reconstruction
is at the level of a whole model, that is, combining parts from
different models is not possible. The approach of [SFCH12]
overcomes this by combining individual parts from different
objects to best compose the input scan. Namely, starting
from a database of segmented models, 3D data is combined
with RGB imagery to ﬁnd candidate parts from the database
matching the input. In particular, the use of RGB data can
help ﬁnd parts that are completely missing in the 3D scan.
The best combination of candidates that closely match the
geometry, while consisting of a small intersection with each
other, composes the ﬁnal model.
Model-based SFM. The method of [BCLS13] performs re-
construction at the category level (i.e. car, fruit), by learning
a mean shape prior along with corresponding feature points
on the shape from a given set of object instances acquired
through structure from motion (SFM). Then for a given point
cloud and associated imagery, its points are matched to the
learned feature points and used to deform the mean shape to
the given point cloud. Such an approach allows for the recon-
struction of coarse details via the learned mean shape, while
preserving ﬁne details present in the point cloud. Such details
are likely to be lost by the previously described approaches.
9. User-Driven Methods
Incorporating the user in the process of surface reconstruction
has shown to be extremely beneﬁcial in dealing with chal-
lenging point clouds. The key distinguishing factors between
user-driven methods are the level of intuition and ease of
use an interface provides and the extent at which the user
interaction is coupled with the underlying reconstruction
technique. User-driven techniques were ﬁrst introduced as a
means for generating required information as inputs to the
reconstruction algorithm, where recent and more encompass-
ing approaches tightly integrate the user interaction with the
core reconstruction algorithm. Below we group user-driven
techniques by the type of information that is solicited from
the user.
Feature classiﬁcation cues. The methods of [FCOS05b]
and [GG07] showcase how user input can be used to pro-
vide valuable information for reconstruction. In both methods
user input is used to augment the point cloud with tags classi-
fying different surface attributes such as regions of surface
smoothness [FCOS05b] and sharp features [GG07]. A simple
brush tool that acts directly on the point cloud enables the
user to identify regions of interest. The tags are incorporated
with moving least squares [ABCO∗03] to better handle sharp
feature regions that are difﬁcult to detect and reconstruct auto-
matically. Another example of user interaction is the method
of [LZS∗11], which requires the user to correlate 2D photos
(a)
(b)
(c)
Figure 16: Given an input point cloud, simple planar prim-
itives identiﬁed by RANSAC (a) may result in coarse and
incomplete geometry (b). By exploiting the user’s high-level
knowledge while remaining faithful to the input data (b) a
constrained optimization allows to recover a high-quality
model (c).
and LiDAR scans by identifying corresponding 2D rectangle
regions in both data sources.
Topology cues. The method of [SLS∗07] demonstrates how
user information can be more tightly integrated to guide re-
construction in an interactive fashion. Speciﬁcally, the ap-
proach obtains watertight and topologically correct recon-
structions through the automatic detection of topologically
weak regions in a given reconstruction. These low-conﬁdence
regions are then presented to the user to be resolved, via scrib-
bles on a 2D tablet, which translate to interior and exterior
constraints, or potentially no constraints if the user deems
the region valid. The reconstruction is then updated, and the
process repeats through further user edits.
Structural repetition cues. The ability to incorporate user
input in reconstruction allows one to forego traditional wa-
tertight and topological requirements. User interactivity and
automatic reconstruction can be tightly coupled in an inte-
grated interactive environment and is especially useful for
the reconstruction of large-scale data. These techniques are
best suited when the sampled objects exhibit high repetition
and can be adequately represented in terms of a collection of
simple geometric primitives. In [NSZ∗10] the authors present
a technique to rapidly reconstruct architectural models, such
as those acquired from the scanning of large urban environ-
ments. The key idea of the approach is to enable the user to
deﬁne and manipulate simple geometric building blocks in
the form of axis-aligned rectangular cuboids named smart-
boxes. The user sequentially places the smartboxes into the
scene, where contextual regularities and symmetries between
boxes are automatically captured and used to expedite the
ﬁtting process. The ﬁnal placement of the user manipulated
primitives is determined through an interactive optimization
procedure that automatically adjusts the location, orientation,
and sizes of the box primitives by considering how the cuboid
ﬁts the data and its relationship in context with previously
placed boxes. The method also allows for the grouping and
c
⃝Author version


--- Page 23 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
manipulation of multiple boxes for rapidly copying and ﬁtting
recurring structures.
Primitive relationship cues. This class of methods ap-
proaches the reconstruction of models that can be assembled
by simple polygonal primitives. The role of the user is to
provide hints in how to improve the connectivity of the re-
constructed model. The approach of [CC08] is focused on
building reconstruction from incomplete scans. They ﬁrst
estimate planar polygons as well as their boundaries from
the scans. Acknowledging that this estimation will be im-
perfect under missing data, the authors then allow for the
user to identify boundary lines that bound an absent polygon,
as well as specify multiple polygons that intersect to yield
absent edges or corners. This information is used to infer a
watertight polygonal model of the building. To support more
complicated polygonal relationships the method of [ASF∗13]
reconstructs a closed polygonal model by snapping each poly-
gon into alignment with neighboring primitives by solving
a combined optimization problem involving both local and
global spatial constraints – see Figure 16. The interactive
component provides the user a set of modeling operations
comprised of simple edits restricted to an automatically de-
termined plane, hence ﬁxing the view for the user. A polygon
edit mode allows the user to reﬁne existing polygons by edit-
ing their boundaries and merging multiple disconnected poly-
gons. A polygon sketching mode allows the user to provide
new polygons for regions where automatic planar detection
failed due to insufﬁcient points or incorrect estimation due
to noise or outliers in the data. For both modes, the user has
to only provide coarse edits, as the automatic snapping opti-
mization is used to align polygon boundaries based on both
local and global relations between primitives.
10. Evaluation of Surface Reconstruction
Given the wide diversity in reconstruction methods, the man-
ner in which one reconstruction is evaluated compared to
another may differ. In this section we look at different evalua-
tion criteria used in the surface reconstruction literature.
Geometric Accuracy. Perhaps the most common method of
evaluation is to directly compare the geometry of the recon-
struction output to the ground truth surface from which the
scan was obtained. Hausdorff distance, mean distance, as well
as measuring error in normals are common geometric error
measures in this scenario. However, it is often challenging to
obtain the notion of a ground truth surface from a physical
shape. Hence, computational representations of shapes are
typically used as the ground truth [Kaz05, MPS08], where
synthetic scanning of the digital representation can be used in
place of an actual scanner [BLN∗13]. In some cases, a direct
comparison to ground truth data is insufﬁcient when targeting
reconstruction under an error tolerance or comprising several
levels of details (LODs). This suggests evaluating instead the
complexity-distortion tradeoff, or the capability to generate
LODs that are both controllable via intuitive parameters and
meaningful for the targeted applications. Such evaluation cri-
teria consist of the coherence of LODs across the scene, the
ability to incrementally reﬁne the geometry, and the level of
abstraction provided by the LODs, analogous to [MZL∗09].
Topological Accuracy. Another important evaluation criteria
is the recovery of higher-level information of the shape and in
particular, its topology. Certain methods are concerned with
reconstructing a shape with the correct genus [SLS∗07], while
other methods that focus on recovering a skeletal representa-
tion of the shape are more concerned with the topology of the
underlying skeletal structure – recovering important branches
and junctions in the skeleton. Such a topological structure
is of particular importance for structural shape editing ap-
plications [LLZM10] and nonrigid registration [ZST∗10].
However, we note that most skeleton-based methods are of-
ten concerned with qualitative evaluation, hence it can be
difﬁcult to compare different skeleton extraction methods.
Structure Recovery. Beyond geometry and topology it is
also sometimes desirable to recover the structure during re-
construction. Beyond the simple notion of scene decomposi-
tion, the term structure has a broad meaning, ranging from the
dimension of geometric entities (manifolds, stratiﬁed man-
ifolds, non-manifold conﬁgurations) to adjacency relation-
ships through canonical geometric relationships (parallelism,
co-planarity, orthogonality, concentricity, co-axiality) and
regularities (repetitions, symmetries). In addition, controlling
the structure encompasses recovery, preservation, and rein-
forcement. Structure is especially relevant when dealing with
large-scale scenes, not just individual objects, where scenes
are composed of a collection of objects which may have
structural interrelationships. Structure as well as global regu-
larities are also a means to improve robustness and resilience
to missing data and go beyond reconstruction to consolidation
and abstraction.
Shape Recognition. For data-driven methods that deal
with substantial missing data, recovering precise geome-
try is often impractical. Instead, recognizing whole shape
classes [SXZ∗12] or parts of shapes [SFCH12] from an in-
complete scan are common methods of evaluating reconstruc-
tion quality. Similar types of recognition methods can be used
in evaluating the detection of geometric primitives.
Ease of Use An important evaluation criterion is the ease of
use of a surface reconstruction algorithm. For methods that
are automatic, this can amount to how sensitive a method
is to its set of parameters across a large range of inputs.
For user-driven methods, this translates to the practicality
of the user interface. This can be evaluated through user
studies [ASF∗13] that measure the time it takes to reconstruct
a model. The studies perform these evaluations on both users
who are familiar with computer graphics and 3D modeling as
well as those that are novices.
Reproducibility An important consideration in evaluating
the quality of a reconstruction method is its level of repro-
ducibility. Perhaps the simplest means of determining repro-
c
⃝Author version


--- Page 24 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
ducibility is whether or not certain methods are made publicly
available or have been implemented by a third party, as this
can be an important indicator of implementation complexity
and algorithm robustness. Some surface reconstruction imple-
mentations are publicly available, which provides a sense of
methods that have a high level of reproducibility. For instance,
Poisson surface reconstruction [KBH06] is a widely used sur-
face reconstruction method as the code is highly stable and
reliable. The issue of reproducibility and provenance is well
studied in other areas, including visualization and scientiﬁc
workﬂows [SFC07,FKSS08], and it has been shown to be use-
ful for studying 3D model construction [DKP11]. Given the
increasing complexity of reconstruction algorithms, the issue
of reproducibility is likely to be of increasing importance.
11. Conclusions
The area of surface reconstruction has grown from methods
that handle limited defects in point clouds while producing
detailed reconstructions, to methods that handle substantial
artifacts and produce high-level surface representations. Our
survey provides insight into this wide array of methods, high-
lighting strengths and limitations that currently exist in the
ﬁeld. In doing so, our survey should also point the way to-
wards future work across all of the different priors – making
potential connections across input assumptions, point cloud
properties, and shape classes that have not been previously
considered.
Hints and solvers. In our survey of recent work we observed
how the surface reconstruction problem is tackled through
either increasingly sophisticated solvers or richer reconstruc-
tion hints that make the problem easier to solve. For example,
the availability of oriented normals requires only a linear
solve through the Poisson reconstruction approach [KH13],
while unoriented normals require solving for a generalized
eigenvalue problem [ACSTD07]. Other hints such as gen-
eralized parity requires two linear solves: a ﬁrst solve to
consolidate the local hints as sign guesses, and a second solve
to recover a signed implicit function [GCSA13].
Innovations in acquisition. As 3D acquisition methods con-
tinue to increase in variety and popularity, surface reconstruc-
tion will continue to be an important component in acquiring
real-world shapes. To provide some historical context, con-
sider the rise in accessibility of the optical laser-based trian-
gulation scanner: since such a device provides line of sight
information, this resulted in a whole category of visibility
priors (Section 4) such as VRIP [CL96]. A more recent ex-
ample is the Microsoft Kinect: the real-time capture of depth
and RGB imagery has resulted in a new slate of techniques
for reconstructing indoor scenes through data-driven means
(Section 8) such as [SXZ∗12,NXS12,KMYG12], since ob-
jects in indoor environments tend to satisfy such priors. As
novel acquisition sensors and modalities are developed, it
is likely that surface reconstruction will become even more
specialized in order to handle the nuances of the acquisition
type. In this setting our survey should prove useful in the
development of novel priors that need to handle such new
acquisition methods.
Acquisition ubiquity. Beyond the increasing variety of sen-
sors, we are also witnessing a rapid evolution of the acqui-
sition paradigms. The acquisition of our physical world can
now be complemented by exploiting the massive data sets
shared online, referred to as community data. We also predict
a future where geometric data are acquired through dissem-
inated sensors, yielding disseminated data. This evolution
translates into a paradox: despite expectations that technologi-
cal advances should improve quality, these data are hampered
with high variability and unprecedented amount and variety
of defects. In addition, we are observing a trend brought on by
the speed of technological progress: while many practitioners
use high-end acquisition systems, an increasing number of
them turn to consumer-level acquisition devices, willing to
replace an accurate albeit expensive acquisition by a series
of low-cost acquisitions – see recent work on 3D acquisi-
tion from mobile phones [TKM∗13]. These new acquisition
paradigms translate into a lower control over the acquisition
process, which must be compensated by an increased robust-
ness of the algorithms and structural or physical a priori
knowledge. Recent works in hair reconstruction [LLR13] and
foliage reconstruction [BNB13] demonstrate the challenges
brought on by acquisition in uncontrolled environments.
Big data and online algorithms. Last, the scale of acquired
data is also quickly growing: we no longer deal exclusively
with individual shapes, but with entire scenes, possibly at the
scale of entire cities with many objects deﬁned as structured
shapes. Recovering the structure of such large scale scenes is
a stimulating scientiﬁc challenge. We also envision a future
where the common on-disk paradigm must be replaced by
online algorithms that perform reconstruction during acqui-
sition. Recent works such as Kinect Fusion [NDI∗11] and
extensions [CBI13,NZIS13] demonstrate the practicality of
building such online systems. There are applications such
as aero-reconstruction for disaster management where tight
timing restrictions make an online reconstruction approach
indispensable. In particular, we foresee a need to extend the
surveyed priors into the online setting, in order to support
such challenging problems in surface reconstruction.
Acknowledgments. Pierre Alliez is supported by an ERC
Starting Grant “Robust Geometry Processing” (257474).
Claudio Silva was partially supported by the National Science
Foundation grant MRI-1229185, an IBM Faculty Award, and
a grant from the Gordon and Betty Moore and Alfred P. Sloan
Foundations. Joshua Levine is supported by the National Sci-
ence Foundation award IIS-1314757.
Author Bios
Matthew Berger is a researcher at the Air Force Research
Laboratory, Information Directorate. He received his PhD de-
gree in Computing in 2013 from the University of Utah. His
c
⃝Author version


--- Page 25 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
research interests include various geometry processing topics
such as surface reconstruction, meshing, and shape analy-
sis, as well as computer vision topics such as 3D geometry
acquisition, motion segmentation, and object recognition.
Andrea Tagliasacchi is a postdoctoral scholar in the Com-
puter Graphics and Geometry Laboratory at the Ecole Poly-
technique Federale de Lausanne (EPFL). He received his
MSc from Politecnico di Milano and a PhD from Simon
Fraser University (SFU) under the joint supervision of Prof.
Richard (Hao) Zhang and Prof. Daniel Cohen-Or. His re-
search interests include computer graphics, computer vision
and machine learning with a focus in geometry processing.
Lee M. Seversky is a research scientist at the Air Force Re-
search Laboratory, Information Directorate. He received his
PhD degree in Computer Science from Binghamton Univer-
sity where he investigated robust methods for processing and
analyzing incomplete point cloud data. He is currently a prin-
cipal researcher with the Air Force pursuing new problems
in the areas of large-scale geometry processing, computer
graphics, and computer vision.
Pierre Alliez is Senior Researcher and team leader at In-
ria Sophia-Antipolis - Mediterranée. He has authored many
scientiﬁc publications and several book chapters on topics
commonly referred to as geometry processing: mesh com-
pression, surface reconstruction, mesh generation, surface
remeshing and mesh parameterization. He is an associate edi-
tor of the Computational Geometry Algorithms Library, ACM
Transactions on Graphics, Graphical Models and Computer
Aided Geometric Design. He received in 2005 the EURO-
GRAPHICS young researcher award. He was awarded in
2011 a Starting Grant from the European Research Council
on Robust Digital Geometry Processing.
Joshua A. Levine is an assistant professor in the Visual Com-
puting division of the School of Computing at Clemson Uni-
versity. He received his PhD from The Ohio State Univer-
sity and completed a postdoc at the University of Utah’s
SCI Institute. His research interests include geometric mod-
eling, visualization, mesh generation, topological analysis,
surface reconstruction, vector ﬁelds, medical imaging, com-
puter graphics, and computational geometry.
Andrei Sharf is a faculty at the computer sciene department
at Ben-Gurion University, Israel. Previosly, he has been a
Visiting Associate Professor at the Shenzhen Institute of Ad-
vanced Technology(SIAT) Chinese Academy of Sciences and
a Postdoctoral researcher at the School of Computer Sci-
ence in UC-Davis U.S. His research interests are in computer
graphics, including surface reconstruction and interaction
techniques, geometry processing, urban modeling and motion
analysis. He received his PhD on “Surface Reconstruction
Techniques for Imperfect Raw Data” from Tel-Aviv Univer-
sity in 2007 under the supervision of Daniel Cohen-Or and
Ariel Shamir.
Claudio T. Silva is Professor in the School of Engineering,
and Head of Disciplines at the Center for Urban Science
and Progress, New York University. He is also afﬁliated
with Courant and the Center for Data Science. Claudio
has been active in geometry processing for almost 20
years, with a special emphasis on surface reconstruction
and point-set surfaces. He has served on more than 100
program committees, and he is currently on the editorial
board of the ACM Transactions on Spatial Algorithms and
Systems (TSAS), Computer Graphics Forum, Computing
in Science and Engineering, The Visual Computer, and
Graphical Models. He received four IBM Faculty Awards
and ten best paper awards. He is a Fellow of the IEEE.
References
[AA04]
ALEXA M., ADAMSON A.: On normals and projection
operators for surfaces deﬁned by point sets. In Proc. of the EG
conf. on Point-Based Graphics (2004). 8
[AA06]
ADAMSON A., ALEXA M.: Anisotropic point set surfaces.
In Proc. AFRIGRAPH (2006). 11
[AB99]
AMENTA N., BERN M.:
Surface reconstruction by
voronoi ﬁltering. Discrete & Computational Geometry (1999). 13
[ABCO∗03]
ALEXA M., BEHR J., COHEN-OR D., FLEISHMAN
S., LEVIN D., SILVA C.: Computing and rendering point set
surfaces. Trans. on Visualization and Computer Graphics (2003).
3, 5, 7, 8, 19
[ACK13]
ATTENE M., CAMPEN M., KOBBELT L.: Polygon mesh
repairing: An application perspective. ACM Computing Surveys
(CSUR) (2013). 2
[ACSTD07]
ALLIEZ P., COHEN-STEINER D., TONG Y., DES-
BRUN M.: Voronoi-based variational reconstruction of unoriented
point sets. In Computer Graphics Forum (Proc. of the Symposium
on Geometry Processing) (2007). 3, 4, 5, 7, 10, 21
[AK04]
AMENTA N., KIL Y. J.: Deﬁning point-set surfaces. ACM
Trans. on Graphics (2004). 5, 8
[ASF∗13]
ARIKAN M., SCHWÄRZLER M., FLÖRY S., WIMMER
M., MAIERHOFER S.: O-snap: Optimization-based snapping for
modeling architecture. ACM Trans. Graph. (Proc. SIGGRAPH)
(2013). 3, 20
[ASGCO10]
AVRON H., SHARF A., GREIF C., COHEN-OR D.:
ℓ1-sparse reconstruction of sharp point set surfaces. ACM Trans.
on Graphics (2010). 5, 7, 11
[BCLS13]
BAO S. Y., CHANDRAKER M., LIN Y., SAVARESE
S.: Dense object reconstruction with semantic priors. In CVPR
(2013). 19
[BLN∗13]
BERGER M., LEVINE J. A., NONATO L. G., TAUBIN
G., SILVA C. T.: A benchmark for surface reconstruction. ACM
Trans. on Graphics (2013). 20
[BNB13]
BRADLEY D., NOWROUZEZAHRAI D., BEARDSLEY
P.: Image-based reconstruction and synthesis of dense foliage.
ACM Trans. Graph. (Proc. SIGGRAPH) (2013). 21
[BO05]
BOISSONNAT J.-D., OUDOT S.: Provably good sampling
and meshing of surfaces. Graphical Models (2005). 7
[BS12]
BERGER M., SILVA C. T.: Medial kernels. In Computer
Graphics Forum (Proc. of Eurographics) (2012). 15
[BWM∗11]
BERNER A., WAND M., MITRA N. J., MEWES D.,
SEIDEL H.-P.: Shape analysis with subspace symmetries. Com-
puter Graphics Forum (2011). 16
c
⃝Author version


--- Page 26 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
[CBC∗01]
CARR J. C., BEATSON R. K., CHERRIE J. B.,
MITCHELL T. J., FRIGHT W. R., MCCALLUM B. C., EVANS
T. R.: Reconstruction and representation of 3d objects with radial
basis functions. In Proc. of ACM SIGGRAPH (2001). 3, 4, 5, 7, 9
[CBI13]
CHEN J., BAUTEMBACH D., IZADI S.: Scalable real-
time volumetric surface reconstruction. ACM Transactions on
Graphics (TOG) (2013). 21
[CC08]
CHEN J., CHEN B.: Architectural modeling from sparsely
scanned range data. International Journal of Computer Vision
(2008). 20
[CCLN10]
CHEN Y.-L., CHEN B.-Y., LAI S.-H., NISHITA T.:
Binary orientation trees for volume and surface reconstruction
from unoriented point clouds. Computer Graphics Forum (2010).
12
[CG06]
CAZALS F., GIESEN J.: Delaunay triangulation based
surface reconstruction. In Effective Computational Geometry for
Curves and Surfaces. Springer, 2006. 3
[CL96]
CURLESS B., LEVOY M.: A volumetric method for build-
ing complex models from range images. In Proc. of ACM SIG-
GRAPH (1996). 3, 6, 7, 12, 21
[CLCL11]
CHEN Y.-L., LEE T.-Y., CHEN B.-Y., LAI S.-H.:
Bipartite polar classiﬁcation for surface reconstruction. Computer
Graphics Forum (2011). 13
[CLP10]
CHAUVE A.-L., LABATUT P., PONS J.-P.:
Robust
piecewise-planar 3d reconstruction and completion from large-
scale unstructured point data. In CVPR (2010). 15
[CP05]
CAZALS F., POUGET M.: Estimating differential quanti-
ties using polynomial ﬁtting of osculating jets. Computer Aided
Geometric Design (2005). 5
[CT11]
CALAKLI F., TAUBIN G.: Ssd: Smooth signed distance
surface reconstruction. Computer Graphics Forum (2011). 10
[CTO∗10]
CAO J., TAGLIASACCHI A., OLSON M., ZHANG H.,
SU Z.: Point cloud skeletons via laplacian based contraction. In
Proc. of IEEE Shape Modeling International (2010). 7, 14
[CWL∗08]
CHENG Z.-Q., WANG Y.-Z., LI B., XU K., DANG G.,
JIN S.-Y.: A survey of methods for moving least squares surfaces.
In Proceedings of the Fifth Eurographics/IEEE VGTC conference
on Point-Based Graphics (2008), Eurographics Association. 8
[Dey07]
DEY T. K.: Curve and surface reconstruction: algorithms
with mathematical analysis. Cambridge University Press, 2007. 3
[DGQ∗12]
DEY T. K., GE X., QUE Q., SAFA I., WANG L.,
WANG Y.: Feature-preserving reconstruction of singular surfaces.
In Computer Graphics Forum (2012). 4
[DHOS07]
DANIELS J. I., HA L. K., OCHOTTA T., SILVA C. T.:
Robust smooth feature extraction from point clouds. In Proc. of
Shape Modeling and Applications (2007). 7, 11
[DKP11]
DENNING J. D., KERR W. B., PELLACINI F.: Meshﬂow:
interactive visualization of mesh construction sequences. In ACM
Trans. on Graphics (2011). 21
[DTS01]
DINH H. Q., TURK G., SLABAUGH G.: Reconstruct-
ing surfaces using anisotropic basis functions. In International
Journal of Computer Vision (2001). 11
[DUNI10]
DROST B., ULRICH M., NAVAB N., ILIC S.: Model
globally, match locally: Efﬁcient and robust 3d object recognition.
In CVPR (2010). 15
[FCOS05a]
FLEISHMAN S., COHEN-OR D., SILVA C.: Robust
moving least-squares ﬁtting with sharp features. In ACM Trans.
Graph. (Proc. SIGGRAPH) (2005). 11
[FCOS05b]
FLEISHMAN S., COHEN-OR D., SILVA C. T.: Robust
moving least-squares ﬁtting with sharp features. ACM Trans.
Graph. (Proc. SIGGRAPH) (2005). 7, 11, 19
[FG11]
FUHRMANN S., GOESELE M.: Fusion of depth maps with
multiple scales. In Proc. of ACM SIGGRAPH Asia (2011). 3, 12
[FKSS08]
FREIRE J., KOOP D., SANTOS E., SILVA C. T.: Prove-
nance for computational tasks: A survey. Comp. in Science &
Engineering (2008). 21
[FLW02]
FATTAL R., LISCHINSKI D., WERMAN M.: Gradient
domain high dynamic range compression. In ACM Trans. on
Graphics (2002). 10
[FS12]
FRIEDMAN S., STAMOS I.: Online facade reconstruction
from dominant frequencies in structured point clouds. In IEEE
CVPR Workshops (2012). 17
[GCSA13]
GIRAUDOT S., COHEN-STEINER D., ALLIEZ P.:
Noise-adaptive shape reconstruction from raw point sets. Com-
puter Graphics Forum (Proc. of the Symposium on Geometry
Processing) (2013). 4, 13, 21
[GG07]
GUENNEBAUD G., GROSS M.: Algebraic point set sur-
faces. ACM Trans. Graph. (Proc. SIGGRAPH) (2007). 5, 8,
19
[GSH∗07]
GAL R., SHAMIR A., HASSNER T., PAULY M.,
COHEN-OR D.: Surface reconstruction using local shape pri-
ors. In Computer Graphics Forum (Proc. of the Symposium on
Geometry Processing) (2007). 18
[GWM01]
GUMHOLD S., WANG X., MACLEOD R.: Feature ex-
traction from point clouds. In International Meshing Roundtable
(2001). 11
[HDD∗92]
HOPPE H., DEROSE T., DUCHAMP T., MCDONALD
J., STUETZLE W.:
Surface reconstruction from unorganized
points. In Computer Graphics (Proc. SIGGRAPH) (1992). 3, 5, 6,
7, 8, 9
[HK06]
HORNUNG A., KOBBELT L.: Robust reconstruction of
watertight 3d models from non-uniformly sampled point clouds
without normal information. In Computer Graphics Forum (Proc.
of the Symposium on Geometry Processing) (2006). 3, 4, 11
[HLZ∗09]
HUANG H., LI D., ZHANG H., ASCHER U., COHEN-
OR D.: Consolidation of unorganized point clouds for surface
reconstruction. ACM Trans. on Graphics (2009). 5, 7, 9
[HWCO∗13]
HUANG H., WU S., COHEN-OR D., GONG M.,
ZHANG H., LI G., CHEN B.: l1-medial skeleton of point cloud.
ACM Trans. Graph. (Proc. SIGGRAPH) (2013). 3, 7, 14
[HWG∗13]
HUANG H., WU S., GONG M., COHEN-OR D., AS-
CHER U., ZHANG H. R.: Edge-aware point set resampling. ACM
Trans. on Graphics (2013). 11
[JKS08]
JENKE P., KRÜCKEBERG B., STRASSER W.: Surface
reconstruction from ﬁtted shape primitives. In Proc. of Vision
modeling and Visualization (2008). 7, 15
[JLSW02]
JU T., LOSASSO F., SCHAEFER S., WARREN J.: Dual
contouring of hermite data. In ACM Trans. on Graphics (2002). 7
[JWS08]
JENKE P., WAND M., STRASSER W.: Patch-graph recon-
struction for piecewise smooth surfaces. Proc. of Vision modeling
and Visualization (2008). 11
[Kaz05]
KAZHDAN M.: Reconstruction of solid models from
oriented point sets. In Proc. of the EG/SIGGRAPH Symposium on
Geometry processing (2005). 4, 7, 10, 20
[KBH06]
KAZHDAN M., BOLITHO M., HOPPE H.: Poisson sur-
face reconstruction. In Proc. of the EG/SIGGRAPH Symposium
on Geometry processing (2006). 3, 4, 5, 6, 7, 10, 21
[KE12]
KHOSHELHAM K., ELBERINK S. O.: Accuracy and reso-
lution of kinect depth data for indoor mapping applications. Sen-
sors (2012). 4
c
⃝Author version


--- Page 27 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
[KH13]
KAZHDAN M., HOPPE H.: Screened poisson surface
reconstruction. ACM Trans. on Graphics (2013). 10, 21
[KKDH07]
KAZHDAN M., KLEIN A., DALAL K., HOPPE H.:
Unconstrained isosurface extraction on arbitrary octrees. In Sym-
posium on Geometry Processing (2007). 7
[KMYG12]
KIM Y. M., MITRA N. J., YAN D.-M., GUIBAS L.:
Acquiring 3d indoor environments with variability and repetition.
Proc. of ACM SIGGRAPH Asia (2012). 3, 7, 18, 21
[KQ04]
KIRBAS C., QUEK F.: A review of vessel extraction
techniques and algorithms. ACM Computing Surveys (2004). 14
[KSO04]
KOLLURI R., SHEWCHUK J. R., O’BRIEN J. F.: Spec-
tral surface reconstruction from noisy point clouds. In Proc. of
the EG/SIGGRAPH Symposium on Geometry processing (2004).
10, 11, 12
[KTB07]
KATZ S., TAL A., BASRI R.: Direct visibility of point
sets. ACM Trans. Graph. (Proc. SIGGRAPH) (2007). 12, 13
[LA11]
LAW A. J., ALIAGA D. G.: Single viewpoint model
completion of symmetric objects for digital inspection. Computer
Vision Image Understanding (2011). 16
[LA13]
LAFARGE F., ALLIEZ P.: Surface reconstruction through
point set structuring. Computer Graphics Forum (Proc. of Euro-
graphics) (2013). 3, 16
[LB07]
LEMPITSKY V., BOYKOV Y.: Global optimization for
shape ﬁtting. In CVPR (2007). 12
[LC87]
LORENSEN W. E., CLINE H. E.: Marching cubes: A high
resolution 3d surface construction algorithm. In Proc. of ACM
SIGGRAPH (1987). 7
[LCDF10]
LIPMAN
Y.,
CHEN
X.,
DAUBECHIES
I.,
FUNKHOUSER T.:
Symmetry factored embedding and
distance. ACM Trans. on Graphics (2010). 16
[LCOL06]
LIPMAN Y., COHEN-OR D., LEVIN D.: Error bounds
and optimal neighborhoods for mls approximation. In Computer
Graphics Forum (Proc. of the Symposium on Geometry Process-
ing) (2006). 4, 5, 8
[LCOL07]
LIPMAN Y., COHEN-OR D., LEVIN D.:
Data-
dependent MLS for faithful surface approximation. In Computer
Graphics Forum (Proc. of the Symposium on Geometry Process-
ing) (2007). 11
[LCOLTE07]
LIPMAN Y., COHEN-OR D., LEVIN D., TAL-EZER
H.: Parameterization-free projection for geometry reconstruction.
ACM Trans. Graph. (Proc. SIGGRAPH) (2007). 3, 4, 7, 9, 11
[LLR13]
LUO L., LI H., RUSINKIEWICZ S.: Structure-aware hair
capture. ACM Trans. Graph. (Proc. SIGGRAPH) (2013). 21
[LLZM10]
LI G., LIU L., ZHENG H., MITRA N. J.: Analysis,
reconstruction and manipulation using arterial snakes. Proc. of
ACM SIGGRAPH Asia (2010). 3, 7, 14, 20
[LPK09]
LABATUT P., PONS J.-P., KERIVEN R.: Robust and efﬁ-
cient surface reconstruction from range data. Computer Graphics
Forum (2009). 12, 16
[LW10]
LIU S., WANG C. C.: Orienting unorganized points for
surface reconstruction. Computers & Graphics (2010). 5, 6
[LWC∗11]
LI Y., WU X., CHRYSATHOU Y., SHARF A., COHEN-
OR D., MITRA N. J.: Globﬁt: consistently ﬁtting primitives
by discovering global relations. ACM Trans. Graph. (Proc. SIG-
GRAPH) (2011). 3, 4, 6, 17, 18
[LYO∗10]
LIVNY Y., YAN F., OLSON M., CHEN B., ZHANG H.,
EL-SANA J.: Automatic reconstruction of tree skeletal structures
from point clouds. Proc. of ACM SIGGRAPH Asia (2010). 6, 14,
15
[LZS∗11]
LI Y., ZHENG Q., SHARF A., COHEN-OR D., CHEN
B., MITRA N. J.: 2d-3d fusion for layer decomposition of urban
facades. In ICCV (2011). 3, 6, 17, 19
[MDGD∗10]
MULLEN P., DE GOES F., DESBRUN M., COHEN-
STEINER D., ALLIEZ P.: Signing the unsigned: Robust surface
reconstruction from raw pointsets. Computer Graphics Forum
(Proc. of the Symposium on Geometry Processing) (2010). 3, 4, 7,
13, 17
[MPS08]
MANSON J., PETROVA G., SCHAEFER S.: Streaming
surface reconstruction using wavelets. Computer Graphics Forum
(2008). 7, 10, 20
[MPWC13]
MITRA N. J., PAULY M., WAND M., CEYLAN D.:
Symmetry in 3d geometry: Extraction and applications.
In
Computer Graphics Forum (STAR Proceedings of Eurographics)
(2013). 16
[MS10]
MANSON J., SCHAEFER S.: Isosurfaces over simplicial
partitions of multiresolution grids. In Computer Graphics Forum
(2010). 7
[MTSM10]
MEHRA R., TRIPATHI P., SHEFFER A., MITRA N. J.:
Visibility of noisy point cloud data. Computers & Graphics (2010).
12
[MWA∗13]
MUSIALSKI P., WONKA P., ALIAGA D. G., WIM-
MER M., GOOL L., PURGATHOFER W.: A survey of urban
reconstruction. In Computer Graphics Forum (STAR Proceedings
of Eurographics) (2013). 2, 7
[MWZ∗13]
MITRA N. J., WAND M., ZHANG H., COHEN-OR D.,
BOKELOH M.: Structure-aware shape processing. In Computer
Graphics Forum (STAR Proceedings of Eurographics) (2013). 16
[MZL∗09]
MEHRA R., ZHOU Q., LONG J., SHEFFER A.,
GOOCH A., MITRA N. J.: Abstraction of man-made shapes.
ACM Transactions on Graphics (TOG) (2009). 20
[NDI∗11]
NEWCOMBE R. A., DAVISON A. J., IZADI S., KOHLI
P., HILLIGES O., SHOTTON J., MOLYNEAUX D., HODGES S.,
KIM D., FITZGIBBON A.: Kinectfusion: Real-time dense surface
mapping and tracking. In IEEE international symposium on Mixed
and augmented reality (ISMAR) (2011), IEEE. 21
[NFD07]
NEUBERT B., FRANKEN T., DEUSSEN O.: Approxi-
mate image-based tree-modeling using particle ﬂows. ACM Trans.
Graph. (Proc. SIGGRAPH) (2007). 14, 15
[NOS09]
NAGAI Y., OHTAKE Y., SUZUKI H.: Smoothing of
partition of unity implicit surfaces for noise robust surface recon-
struction. Computer Graphics Forum (Proc. of the Symposium on
Geometry Processing) (2009). 9
[NSZ∗10]
NAN L., SHARF A., ZHANG H., COHEN-OR D.,
CHEN B.: Smartboxes for interactive urban reconstruction. ACM
Trans. Graph. (Proc. SIGGRAPH) (2010). 3, 19
[NXS12]
NAN L., XIE K., SHARF A.:
A search-classify ap-
proach for cluttered indoor scene understanding. Proc. of ACM
SIGGRAPH Asia (2012). 7, 18, 21
[NZIS13]
NIESSNER M., ZOLLHÖFER M., IZADI S., STAM-
MINGER M.: Real-time 3d reconstruction at scale using voxel
hashing. ACM Transactions on Graphics (TOG) (2013). 21
[OBA∗03a]
OHTAKE Y., BELYAEV A., ALEXA M., TURK G.,
SEIDEL H.: Multi-level partition of unity implicits. ACM Trans.
Graph. (Proc. SIGGRAPH) (2003). 3, 5, 7, 8
[OBA∗03b]
OHTAKE Y., BELYAEV A., ALEXA M., TURK G.,
SEIDEL H.-P.: Multi-level partition of unity implicits. In ACM
Trans. Graph. (Proc. SIGGRAPH) (2003). 11
[OGG09]
OZTIRELI C., GUENNEBAUD G., GROSS M.: Feature
preserving point set surfaces based on non-linear kernel regression.
In Computer Graphics Forum (2009). 4, 11
c
⃝Author version


--- Page 28 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
[OLA13]
OESAU S., LAFARGE F., ALLIEZ P.:
Indoor scene
reconstruction using primitive-driven space partitioning and graph-
cut. In Proc. of EG workshop on Urban Data Modeling and Vis.
(Girona, Spain, 2013). 16
[PGB03]
PÉREZ P., GANGNET M., BLAKE A.: Poisson image
editing. ACM Trans. Graph. (Proc. SIGGRAPH) (2003). 10
[PKG03]
PAULY M., KEISER R., GROSS M.: Multi-scale feature
extraction on point-sampled surfaces. Computer Graphics Forum
(2003). 11
[PMG04]
PAULY M., MITRA N. J., GUIBAS L.: Uncertainty and
variability in point cloud surface data. In Proc. of the EG conf. on
Point-Based Graphics (2004). 5
[PMG∗05]
PAULY M., MITRA N. J., GIESEN J., GROSS M. H.,
GUIBAS L. J.: Example-based 3d scan completion. In Proc. of
the EG/SIGGRAPH Symposium on Geometry processing (2005).
3, 18
[PMW∗08]
PAULY M., MITRA N. J., WALLNER J., POTTMANN
H., GUIBAS L. J.: Discovering structural regularity in 3d ge-
ometry. ACM Trans. Graph. (Proc. SIGGRAPH) (2008). 3, 4, 7,
16
[PP09]
PANG X.-F., PANG M.-Y.: An algorithm for extracting ge-
ometric features from point cloud. Int. Conf. on Inf. Management,
Innovation Management and Ind. Eng. (2009). 11
[RFL∗05]
RUNIONS A., FUHRER M., LANE B., FEDERL P.,
ROLLAND-LAGAN A.-G., PRUSINKIEWICZ P.: Modeling and
visualization of leaf venation patterns. ACM Trans. Graph. (Proc.
SIGGRAPH) 24, 3 (2005), 702–711. 14
[RKMP13]
REISNER-KOLLMANN I., MAIERHOFER S., PUR-
GATHOFER W.: Reconstructing shape boundaries with multi-
modal constraints. Computers & Graphics (2013). 7, 16
[SDK09]
SCHNABEL R., DEGENER P., KLEIN R.: Completion
and reconstruction with primitive shapes. Computer Graphics
Forum (Proc. of Eurographics) (2009). 3, 4, 6, 7, 15
[SFC07]
SILVA C. T., FREIRE J., CALLAHAN S. P.: Provenance
for visualizations: Reproducibility and beyond. Comp. in Science
& Engineering (2007). 21
[SFCH12]
SHEN C.-H., FU H., CHEN K., HU S.-M.: Structure
recovery by part assembly. Proc. of ACM SIGGRAPH Asia (2012).
2, 3, 6, 7, 19, 20
[SFS05]
SCHEIDEGGER C. E., FLEISHMAN S., SILVA C. T.:
Triangulating point set surfaces with bounded error. In Proc. of
the EG/SIGGRAPH Symposium on Geometry processing (2005).
8, 15
[SHFH11]
SHEN C.-H., HUANG S.-S., FU H., HU S.-M.: Adap-
tive partitioning of urban facades. Proc. of ACM SIGGRAPH Asia
(2011). 17
[SLS∗06]
SHARF A., LEWINER T., SHAMIR A., KOBBELT L.,
COHEN-OR D.: Competing fronts for coarse–to–ﬁne surface re-
construction. Computer Graphics Forum (Proc. of Eurographics)
(2006). 14, 15
[SLS∗07]
SHARF A., LEWINER T., SHKLARSKI G., TOLEDO S.,
COHEN-OR D.: Interactive topology-aware surface reconstruc-
tion. ACM Trans. Graph. (Proc. SIGGRAPH) (2007). 3, 4, 19,
20
[SLSK07]
SHARF A., LEWINER T., SHAMIR A., KOBBELT L.:
On-the-ﬂy curve-skeleton computation for 3d shapes. In Computer
Graphics Forum (Proc. of Eurographics) (2007). 14
[SOS04]
SHEN C., O’BRIEN J. F., SHEWCHUK J. R.: Interpolat-
ing and approximating implicit surfaces from polygon soup. ACM
Trans. on Graphics (2004). 8
[SSFS06]
SCHREINER J., SCHEIDEGGER C. E., FLEISHMAN S.,
SILVA C. T.: Direct (re)meshing for efﬁcient surface processing.
Computer Graphics Forum (Proc. of Eurographics) (2006). 8
[SSZCO10]
SHALOM S., SHAMIR A., ZHANG H., COHEN-OR
D.: Cone carving for surface reconstruction. Proc. of ACM
SIGGRAPH Asia (2010). 3, 13
[SWK07]
SCHNABEL R., WAHL R., KLEIN R.: Efﬁcient ransac
for point-cloud shape detection.
Computer Graphics Forum
(2007). 5, 15, 17
[SXZ∗12]
SHAO T., XU W., ZHOU K., WANG J., LI D., GUO B.:
An interactive approach to semantic modeling of indoor scenes
with an rgbd camera. Proc. of ACM SIGGRAPH Asia (2012). 3,
7, 18, 20, 21
[SY12]
SEVERSKY L. M., YIN L.: A global parity measure for
incomplete point cloud data. Computer Graphics Forum (2012).
13
[TKM∗13]
TANSKANEN P., KOLEV K., MEIER L., CAMPOSECO
F., SAURER O., POLLEFEYS M.: Live metric 3d reconstruction
on mobile phones. In ICCV (2013). 21
[TOZ∗11]
TAGLIASACCHI
A.,
OLSON
M.,
ZHANG
H.,
HAMARNEH G., COHEN-OR D.: Vase: Volume-aware surface
evolution for surface reconstruction from incomplete point clouds.
Computer Graphics Forum (Proc. of the Symposium on Geometry
Processing) (2011). 3, 6, 12, 15
[TZCO09]
TAGLIASACCHI A., ZHANG H., COHEN-OR D.:
Curve skeleton extraction from incomplete point cloud. ACM
Trans. Graph. (Proc. SIGGRAPH) (2009). 2, 3, 4, 7, 14
[UB13]
UMMENHOFER B., BROX T.: Point-based 3d reconstruc-
tion of thin objects. In ICCV (2013). 12
[VAB12]
VANEGAS C. A., ALIAGA D. G., BENES B.: Automatic
extraction of manhattan-world building masses from 3d laser
range scans. Trans. on Visualization and Computer Graphics
(2012). 7, 17
[vKvLV13]
VAN KREVELD M., VAN LANKVELD T., VELTKAMP
R. C.: Watertight scenes from urban lidar and planar surfaces.
Computer Graphics Forum (2013). 3, 16
[vKZHCO11]
VAN KAICK O., ZHANG H., HAMARNEH G.,
COHEN-OR D.: A survey on shape correspondence. Computer
Graphics Forum (STAR Proceedings of Eurographics) (2011). 4
[Wen95]
WENDLAND H.: Piecewise polynomial, positive deﬁ-
nite and compactly supported radial functions of minimal degree.
Advances in computational Mathematics (1995). 9
[WFP10]
WU C., FRAHM J.-M., POLLEFEYS M.: Detecting large
repetitive structures with salient boundaries. In ECCV (2010). 17
[Whi98]
WHITAKER R. T.: A level-set approach to 3D reconstruc-
tion from range data. International Journal of Computer Vision
29, 3 (1998), 203–231. 12, 15
[WS12]
WAN G., SHARF A.: Grammar-based 3d facade segmen-
tation and reconstruction. Computers & Graphics (2012). 17
[WSS09]
WANG H., SCHEIDEGGER C. E., SILVA C. T.: Band-
width selection and reconstruction quality in point-based surfaces.
Trans. on Visualization and Computer Graphics (2009). 4, 5
[XF12]
XIAO J., FURUKAWA Y.: Reconstructing the worlds mu-
seums. In ECCV (2012). 3, 16
[ZN12]
ZHOU Q.-Y., NEUMANN U.: 2.5 d building modeling by
discovering global regularities. In CVPR (2012). 17
[ZPB07]
ZACH C., POCK T., BISCHOF H.: A globally optimal
algorithm for robust tv-l1 range image integration. In ICCV (2007).
3, 12
c
⃝Author version


--- Page 29 ---

Berger et al. / State of the Art in Surface Reconstruction from Point Clouds
[ZST∗10]
ZHENG Q., SHARF A., TAGLIASACCHI A., CHEN B.,
ZHANG H., SHEFFER A., COHEN-OR D.: Consensus skeleton
for non-rigid space-time registration. Computer Graphics Forum
(Proc. of Eurographics) (2010). 20
[ZSW∗10]
ZHENG Q., SHARF A., WAN G., LI Y., MITRA N. J.,
COHEN-OR D., CHEN B.: Non-local scan consolidation for 3d
urban scenes. ACM Trans. Graph. (Proc. SIGGRAPH) (2010). 2,
3, 7, 17
c
⃝Author version
View publication stats
```

---

## Section 1.6: A Survey of Vectorization Methods in Topological Data Analysis

Source File: A Survey of Vectorization Methods in Topological Data Analysis.txt

### Paper Content:

```
# A Survey of Vectorization Methods in Topological Data Analysis.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/A Survey of Vectorization Methods in Topological Data Analysis.pdf
# Output: ../layer2_completion/txt/A Survey of Vectorization Methods in Topological Data Analysis.txt


--- Page 1 ---

A Survey of Vectorization Methods
in Topological Data Analysis
Dashti Ali, Aras Asaad, Maria Jose Jimenez, Vidit Nanda, Eduardo Paluzo-Hidalgo,
and Manuel Soriano-Trigueros
Abstract. Attempts to incorporate topological information in supervised learning tasks have
resulted in the creation of several techniques for vectorizing persistent homology barcodes.
In this paper, we study thirteen such methods. Besides describing an organizational frame-
work for these methods, we comprehensively benchmark them against three well-known
classiﬁcation tasks. Surprisingly, we discover that the best-performing method is a simple
vectorization, which consists only of a few elementary summary statistics. Finally, we pro-
vide a convenient web application which has been designed to facilitate exploration and
experimentation with various vectorization methods.
Introduction
Propelled by deep theoretical foundations and a host of computational breakthroughs,
topological data analysis emerged roughly three decades ago as a promising method for
extracting insights from unstructured data [32, 14, 42, 44]. The principal instrument of
the enterprise is persistent homology; this consists of three basic steps, each relying on a
different branch of mathematics.
(1) Metric geometry: construct an increasing family {Xt} of cell complexes around the
input dataset X, where the indexing t is a scale parameter in R≥0.
(2) Algebraic topology: compute the d-th homology vector spaces Hd(Xt) for scales t in
R≥0 and dimensions d in Z≥0.
(3) Representation theory: decompose each family of vector spaces {Hd(Xt) | t ≥0} into
irreducible summands, thus producing a barcode.
The resulting barcodes are ﬁnite multisets of real intervals [p, q] ⊂R, which admit concrete
geometric interpretations in low dimensions — see Figure 1. The ultimate goal is to infer
the coarse geometry of X across various scales by examining the longer intervals in its
barcodes. Crucially, once the method for constructing {Xϵ} from X has been ﬁxed, the
entire persistent homology pipeline is unsupervised: one requires neither labelled data nor
hyperparameter tuning to produce barcodes from X.
At the other end of the data analysis spectrum lies supervised machine learning using
contemporary neural networks, which are replete with billions of tunable parameters and
Corresponding Author: nanda@maths.ox.ac.uk.
Authors listed in alphabetical order.
1
arXiv:2212.09703v1  [math.AT]  19 Dec 2022


--- Page 2 ---

2
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
Figure 1. An increasing family of cell complexes built around a point cloud dataset; the asso-
ciated barcode in dimensions 0 (blue) and 1 (red) catalogues the connected components and
cycles respectively.
gargantuan training datasets [3]. The practical aspects of deep neural networks appear to
be light years ahead of the underlying theory. It nevertheless remains the case that machine
learning has driven astonishing progress in the systematic automation of several important
classiﬁcation tasks. One direct consequence of these success stories is the irresistible urge
to combine topological methods with machine learning. The most common avenue for
doing so is to turn barcodes into vectors (lying in a convenient Euclidean space) which
then become input for suitably-trained neural networks.
The good news, at least from an engineering perspective, is that barcodes are inherently
combinatorial objects, and as such, they are remarkably easy to vectorize. Several dozen
vectorization methods have been proposed across the last decade, and new ones continue
to appear with alarming frequency and increasing complexity — the reader will encounter
thirteen of them here. The bad news, on the other hand, comes in the form of three serious
challenges which must be confronted by those who build or use such vectorizations:
(1) Given the large number of options, even established practitioners are not aware of
all the vectorization techniques; similarly, knowledge of which vectorizations are
suitable for which types of data is difﬁcult – if not impossible – to glean from the
published literature.
(2) There is a natural metric between barcodes called the bottleneck distance; when it
is endowed with this metric, the space of barcodes becomes inﬁnite-dimensional
and highly nonlinear. As such, it does not admit any faithful embeddings into
ﬁnite-dimensional vector spaces.
(3) Even the stable vectorizations, which preserve distances by mapping barcodes into
inﬁnite-dimensional vector spaces, may suffer from a lack of discriminative power


--- Page 3 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
3
in practice: by design, they are poor at distinguishing between datasets whose
coarse structures are similar and whose differences reside in ﬁner scales.
In This Paper. Here we seek to comprehensively describe, catalogue and benchmark
vectorization methods for persistent homology barcodes.
The ﬁrst contribution of this
paper is the following taxonomy of the known methods, which we hope will serve as a
convenient organizational framework for beginners and experts alike —
(1) Statistical vectorizations: these summaries consist of basic statistical quantities;
(2) Algebraic vectorizations: these are generated from polynomials;
(3) Curve vectorizations: these come from maps R →H, where H is a vector space;
(4) Functional vectorizations: these are maps of the form X →H for X ̸= R;
(5) Ensemble vectorizations: these are generated from collections of training barcodes.
There are unavoidable overlaps between these ﬁve categories. When such an overlap occurs,
we have placed the given vectorization technique in the earliest relevant category among
those in the list above; thus, an algebraic vectorization given by polynomial functions of
basic statistical quantities will be placed in category (1) rather than category (2). The reader
might claim, quite reasonably, that category (3) should be subsumed into category (4).
However, the sheer number of curve-based vectorizations compelled us to set them apart.
The second contribution of this paper is a comprehensive benchmarking of thirteen vec-
torization techniques across these ﬁve categories on three well-known image classiﬁcation
datasets. These datasets were selected to simultaneously (a) provide an increasing level
of difﬁculty for topological methods, and (b) to be instantly recognizable for the broader
machine learning community. These are: the Outex texture database [43], the SHREC14
shape retrieval dataset [47], and the Fashion-MNIST database [59]. Surprisingly, the best-
performing vectorization in all three cases is a rather na¨ıve one obtained by collecting basic
statistical quantities associated to (the multiset of) intervals in a given barcode.
Our third contribution is a companion web application which computes and visualizes
all thirteen vectorization techniques which have been investigated in this paper. In addi-
tion to running online1, this web app can also be downloaded2 and run locally on more
challenging datasets.
Not In This Paper. Vectorization methods form but a small part of the ever expanding
interface between topological data analysis and machine learning. As such, there are several
related techniques which are not benchmarked here. The precise inclusion criteria for our
study in this paper are as follows.
(1) We restrict our attention to those methods which produce genuine vectors from
barcodes. In particular, kernel methods [50, 17] are beyond the scope of this paper.
(2) We only consider those vectorizations that are either straightforward for us to im-
plement, or have an easily accessible and trusted implementation. For instance,
path signature based vectorizations [21, 33] are excluded.
(3) We do not compare machine learning architectures designed for the explicit pur-
pose of inferring (persistent) homology [16, 37, 40].
1https://persistent-homology.streamlit.app
2https://github.com/dashtiali/vectorisation-app


--- Page 4 ---

4
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
(4) We do not touch upon various attempts to design or study neural networks using
tools from topological data analysis [41, 15].
(5) Finally, even among methods which satisfy the ﬁrst four criteria, we have discarded
techniques which regularly obtained a classiﬁcation accuracy below ﬁfty percent.
Similar Efforts. The authors of [49] have summarised – but not compared – several
vectorization and kernel methods for barcodes. Another summary (sans comparison) may
be found in [53], with emphasis on metric aspects of the chosen vectorizations. The work
of [23] describes a common overarching framework for what we have called curve vec-
torizations here. More recently, [7] and [24] have described and compared ﬁve and four
vectorization methods respectively.
Outline. Notation and preliminaries involving barcodes are established in Section 1.
In Sections 2 and 3 we introduce the thirteen vectorizations (suitably organised into our
taxonomy) and the three datasets. Section 4 contains the results of our experiments whose
ﬁner details have been relegated to Appendices A and B. We provide a description of the
web app in Section 5 and some brief concluding remarks in Section 6.
1. Persistence Barcodes from Data
At its core, persistent homology studies sequences of ﬁnite-dimensional vector spaces
V = {Vi | 0 ≤i ≤n} and linear maps a = {ai : Vi−1 →Vi | 1 ≤i ≤n}:
V0
a1
/ V1
a2
/ · · ·
an
/ Vn.
Such sequences (V, a) are called persistence modules.
Among the simplest examples are
interval modules — for each pair of integers p ≤q with [p, q] ⊂[0, n], the corresponding
interval module (I[p,q], c[p,q]) has
dim I[p,q]
i
=
(
1
if p ≤i ≤q
0
otherwise;
similarly, the map c[p,q]
i
is the identity whenever p + 1 ≤i ≤q and zero otherwise.
1.1. Structure and Stability. Every persistence module decomposes into a direct sum
of interval modules. In particular, we have the following structure theorem [61, 18].
Theorem 1.1. For every persistence module (V, a), there exists a unique set Bar(V, a) of subin-
tervals of [0, n] along with a unique function Bar(V, a) →Z>0 denoted [p, q] 7→µp,q for which we
have an isomorphism
(V, a) ≃
M
[p,q]∈Bar(V,a)

I[p,q], c[p,q]µp,q .
Thus, the algebraic object (V, a) may be fully recovered (up to isomorphism) from purely
combinatorial data consisting of the set of intervals Bar(V, a) and the multiplicity function
µ. Alternately, one may view Bar(V, a) itself as a multiset with µp,q copies of each interval
[p, q]. This multiset is called the barcode of (V, a). It is often useful in applications to let the
vector spaces Vi be indexed by real numbers rather than integers. With this modiﬁcation in
place, Bar(V) becomes a collection of real intervals [p, q] ⊂R.


--- Page 5 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
5
The most important property of persistence modules, beyond the structure theorem, is
their stability [18]. There is a natural metric on the set of persistence modules called the
interleaving distance and a metric on the set of barcodes called the bottleneck distance
Theorem 1.2. The assignment (V, a) 7→Bar(V, a) is an isometry from the space of persistence
modules (with interleaving distance) to the space of barcodes (with bottleneck distance).
The advantage of this theorem is that barcodes remain robust to (certain types of) pertur-
bations of the original dataset, thus conferring upon the topological data analysis pipeline a
degree of noise-tolerance. The signiﬁcant difﬁculty from a statistical perspective, however,
is that the metric space of persistence barcodes with bottleneck distance is nonlinear —
even averages can not be deﬁned for arbitrary collections of barcodes [56, 26, 11].
1.2. Barcodes from Data. Persistence modules arise naturally from a wide class of
datasets.
The ﬁrst step in topological data analysis involves imposing the structure of
a ﬁltered cell complex – either simplicial [4, Chapter 8] or cubical [38] – from the data
[32, 14, 42]. The two most prominent examples of ﬁltered cell complex structures arising
from data are as follows.
(1) Given a ﬁnite point cloud X ⊂Rn, one constructs a family of increasing simpli-
cial complexes {Sϵ | ϵ ≥0} deﬁned as follows. A collection {x0, . . . , xk} forms a
k-simplex in Sϵ if and only if the (Euclidean) distance between xi and xj is no
larger than ϵ for all i, j in {0, . . . , k}. Since there are only ﬁnitely many ϵ values
at which new simplices are introduced, the ﬁltration is indexed by a subset of the
natural numbers. The collection Sϵ is called the Vietoris-Rips ﬁltration of X. These
ﬁltrations can be deﬁned for any metric space in a similar fashion.
(2) Consider a grayscale image I, given in terms of m × n pixels with intensity values
in the set {0, 1, . . . , 255}.
This naturally forms a two-dimensional cubical com-
plex, which can be endowed with the upper-star ﬁltration by intensity values. In
particular, each elementary cube of dimension < 2 appears at the smallest inten-
sity encountered among the 2-dimensional cubes in its immediate neighbourhood.
Higher-dimensional cubical ﬁltrations may be similarly generated from higher-
dimensional pixel grids.
Once the given dataset has been suitably modeled by a ﬁltered cell complex, persistence
modules are obtained by computing homology groups with coefﬁecients in a ﬁeld. The
reader who is interested in the deﬁnition and computation of homology is urged to either
consult standard algebraic topology references such as [35, Ch 2] or see the more recent
[44, 30, 42].
A substantial difﬁculty in topological data analysis is that although persistent homology
barcodes can be readily associated with a large class of datasets, the space of all such
barcodes is notoriously unpleasant to encounter from a statistical perspective. Fortunately,
barcodes are combinatorial objects which can be mapped to Hilbert spaces in a plethora
of reasonable ways. Indeed, across the last decade, such vectorization methods have been
proposed by various authors, and our main purpose in this work is to benchmark many of
these methods against standard classiﬁcation tasks.


--- Page 6 ---

6
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
2. Vectorization Methods for Barcodes
Throughout this section, we assume knowledge of the barcode B := Bar(V, a) of an R-
indexed persistence module along with its multiplicity function µ : B →Z>0. We note that
for each interval [p, q] in B the numbers p and q are called its birth and death respectively,
and the length q −p is called its lifespan.
2.1. Statistical Vectorizations. The ﬁrst and simplest category of vectorizations consid-
ered in this paper are generated from basic statistical quantities associated to the given
barcode. Variants of the following vectorization have been deﬁned and used on several
occasions — see for instance [5, sec 2.3] , [23, Sec 6.2.1] and [49, Sec 4.1.1].
Definition 2.1. The persistence statistics vector of µ : B →Z>0 consists of:
(1) the mean, the standard deviation, the median, the interquartile range, the full
range, the 10th, 25th, 75th and 90th percentiles of the births p, the deaths q, the
midpoints p+q
2
and the lifespans q −p for all intervals [p, q] in B counted with
multiplicity;
(2) the total number of bars (again counted with multiplicity), and
(3) the entropy of µ, deﬁned as the real number
Eµ := −∑
[p,q]∈B
µp,q ·
q −p
Lµ

· log
q −p
Lµ

,
where Lµ is the weighted sum
Lµ := ∑
[p,q]∈B
µp,q · (q −p).
(1)
The entropy from Deﬁnition 2.1(3) was introduced in [22, 52]. Our second statistical
vectorization is from [6], where entropy has been upgraded to a real-valued piecewise
constant function rather than a single number.
Definition 2.2. The entropy summary function of µ : B →Z>0 is the map Sµ : R →R
given by
Sµ(t) = −∑
[p,q]∈B
1p≤t<q · µp,q ·
q −p
Lµ

· log
q −p
Lµ

.
Here 1• is the indicator function — it equals 1 when the conditional • is true and it equals
0 otherwise. The number Lµ appearing in the expression above is deﬁned in (1).
The entropy summary function has also been called the life entropy curve, e.g., in [23].
2.2. Algebraic Vectorizations. The vectorizations in this category are generated using
polynomial maps constructed from the barcode µ : B →Z>0.
The ﬁrst example considered here is from [2]. It becomes convenient, for the purpose
of deﬁning it, to arbitrarily order the intervals in B as {[pi, qi] | 1 ≤i ≤n} with the under-
standing that each [p, q] occurs µp,q times in this ordered list.


--- Page 7 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
7
Definition 2.3. The ring of algebraic functions on µ : B →Z>0 consists of all those
R-polynomials f in variables {x1, y1, . . . , xn, yn} for which the following property holds:
there exist polynomials {gi | 1 ≤i ≤n} satisfying
∂f
∂xi
+ ∂f
∂yi
= (xi −yi) · gi.
(Here ∂f /∂xi indicates the partial derivative of f with respect to xi, and so forth).
The desired vectorization is obtained by selecting ﬁnitely many algebraic functions from
this ring and evaluating them at xi = pi and yi = qi for all i. The feature maps generated
by making such choices are sometimes called Adcock-Carlsson coordinates — see for instance
[46]. Letting qmax be the maximum death-value encountered among the intervals in B, four
of the most widely-used algebraic functions are:
f1 = ∑
i
pi(qi −pi)
f2 = ∑
i
(qmax −qi) (qi −pi)
f3 = ∑
i
p2
i (qi −pi)4
f4 = ∑
i
(qmax −qi)2 (qi −pi)4
Small changes in the barcode (in terms of bottleneck distance) are liable to create large
ﬂuctuations in the associated algebraic functions. The methods of tropical geometry were
used in [39] to address the bottleneck instability of algebraic functions. In this setting,
the standard polynomial operations (+, ×) are systematically replaced by (max, +). To
deﬁne the resulting vectorization, we once again use an ordering {[pi, qi] | 1 ≤i ≤n} of
the intervals in B.
Definition 2.4. A tropical coordinate function for µ : B →Z>0 is a function F of
variables {x1, y1, . . . , xn, yn} which is both tropical and symmetric as described below.
(1) Tropical: there is an expression for F which uses only the operations max, min, +
and −on the variables {xi} and {yi}.
(2) Symmetric: any permutation of {1, . . . , n}, when applied to both {xi} and {yi},
leaves F unchanged.
Let λi be the lifespan qi −pi of the i-th interval in B. To generate feature maps from the
tropical coordinate functions described above, one simply evaluates them at xi = λi and yi
equal to either max(rλi, pi) or min(rλi, pi) for a positive integer parameter r. Examples of
such tropical coordinate features include:
F1 = max
i
λi
F2 = max
i<j (λi + λj)
F3 = max
i<j<k(λi + λj + λk)
F4 =
max
i<j<k<l(λi + λj + λk + λl)
F5 = ∑
i
λi
F6 = ∑
i
min(rλi, pi),
along with the somewhat more complicated
F7 = ∑
j

max
i
 min(rλi, pi) + λi
 −(min(rλj, pj) + λj)

.
These seven tropical coordinates were used in [39] for performing classiﬁcation on the
MNIST database, with r = 28.


--- Page 8 ---

8
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
The third and ﬁnal algebraic vectorization considered here is generated by extracting
complex polynomials from barcodes [31, 27]. In what follows, the symbol i should be inter-
preted as √−1 (and not as an index for the intervals in B). Consider the three continuous
maps R, S, T : R2 →C deﬁned as follows:
R(x, y) = x + iy
S(x, y) =
( y−x
α
√
2 · (x + iy)
if (x, y) ̸= (0, 0)
0
otherwise
T(x, y) = y −x
2
·
h
(cos α −sin α) + i(cos α + sin α)
i
,
where α is the norm
p
x2 + y2.
Definition 2.5. Given a barcode µ : B →Z>0, let X : R2 →C be any one of the three
functions R, S, T deﬁned above. The complex polynomial vectorization of µ of type X is
the sequence of coefﬁcients of the complex polynomial in one variable z given by
CX(z) := ∏
[p,q]∈B
[z −X(p, q)]µp,q .
In practice, it is customary to either take only the ﬁrst few highest degree coefﬁcients of
CX(z) or to multiply it by a suitable power of z. This is done to guarantee that the feature
vectors assigned to a collection of barcodes all have the same dimension.
Other Algebraic Vectorizations: In the subsequent section, we describe how to extract
vectorizations by using barcode data to build curves which take values in a vector space.
Once such a curve has been extracted, one can compute its path signature via iterated in-
tegrals [20]. The path signature resides in the tensor algebra of the target vector space;
elements of the tensor algebra are equivalent to coefﬁcients of non-commuting polynomi-
als, and hence constitute algebraic vectorizations of barcodes — see [21, 33] for examples
of this approach.
2.3. Curve Vectorizations. There are several interesting ways of turning barcodes into
one or more curves, which for our purposes here mean (piecewise) continuous maps from
R to a convenient vector space. Feature vectors can then be constructed by sampling the
given curve at ﬁnite subsets of R. Perhaps the simplest and most widely used curve-based
vectorization is the following.
Definition 2.6. The Betti curve of µ : B →Z>0 is the curve βµ : R →R given by
βµ(t) = ∑
[p,q]∈B
1p≤t<q · µp,q.
Here 1• is the indicator function as described in Deﬁnition 2.2, so this function counts the
number of intervals (with multiplicity) in B which contain t. Very similar in spirit (and
formula) to the Betti curve is the following vectorization from [23].
Definition 2.7. The lifespan curve of µ : B →Z>0 is the map Lµ : R →R given by
Lµ(t) = ∑
[p,q]∈B
1p≤t<q · µp,q · (q −p).


--- Page 9 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
9
It is not difﬁcult to create very different-looking Betti and lifespan curves from two
barcodes which have arbitrarily small bottleneck distance — we can always add lots of very
small intervals to a given barcode without changing its bottleneck distance by a signiﬁcant
amount. One way to rectify the bottleneck instability of Betti and lifespan curves is to test
the containment not only of t in each interval [p, q] ∈B, but rather of the largest subinterval
of the form [t −s, t + s]. This modiﬁcation leads to one of the oldest and best-known stable
curve vectorizations [10, 12], as deﬁned below.
Definition 2.8. The persistence landscape of the barcode µ : B →Z>0 is a sequence
of curves

Λµ
i : R →[−∞, ∞] | i ∈Z>0
	
given by
Λµ
i (t) := sup


s ≥0


∑
[p,q]∈B
1[t−s,t+s]⊂[p,q] · µp,q

≥i


.
By convention, the supremum over the empty set is zero. Moreover, since our barcode B
is assumed to be ﬁnite, the landscape functions Λµ
i become identically zero for sufﬁciently
large i. An alternate approach to deﬁning persistence landscapes comes from the function
∆: B × R →R, given by
∆([p, q], t) := max (min(t −p, q −t), 0) .
(2)
For each i ∈Z>0, the curve Λµ
i from Deﬁnition 2.8 equals the i-th largest number in the
multiset that contains µp,q copies of ∆([p, q], t) for each interval [p, q] in B. The fourth and
ﬁnal curve vectorization that we consider here was introduced in [19], and it is also deﬁned
in terms of the functions ∆from (2).
Definition 2.9. Let w : B →R>0 be any function, which we will denote [p, q] 7→wp,q.
The w-weighted persistence silhouette of µ : B →Z>0 is the map φw
µ : R →R deﬁned as
the weighted average
φw
µ (t) := ∑wp,q · µp,q · ∆([p, q], t)
∑wp,q · µp,q
.
Here both sums on the right are indexed over all [p, q] ∈B, and ∆is deﬁned in (2).
Reasonable choices of weight functions are provided by setting wp,q = (q −p)α for a
real-valued scale parameter α ≥0. For small α, the shorter intervals dominate the value
of the silhouette curve, whereas for large α it is the longer intervals which play a more
substantial role — see [19, Sec 4] for details.
Other Curve Vectorizations: See the envelope embedding from [21], the accumulated per-
sistence function in [9], and the persistent Betti function of [57]. In [29], the persistent Betti
function is decomposed along the Haar basis to produce a vectorization. More recently,
[23] provides a general framework for constructing several different curve vectorizations.
2.4. Functional Vectorizations. Here we catalogue those barcode vectorizations which
are given by maps from spaces other than R. The ﬁrst, and perhaps most prominent mem-
ber of this category is the following vectorization from [1]. Its deﬁnition below makes use
of two auxiliary components besides the given barcode µ : B →Z>0. The ﬁrst is a contin-
uous, piecewise-differentiable function f : R2 →R≥0 satisfying f (x, 0) = 0 for all x ∈R.
And the second is a collection of smooth probability distributions Ψ :=

ψp,q | [p, q] ∈B
	
where ψp,q has mean (p, q −p).


--- Page 10 ---

10
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
Definition 2.10. The persistence surface of µ : B →Z>0 with respect to f and Ψ (as
described above) is the function R2 →R given by
ρµ
f,Ψ(x, y) = ∑
[p,q]∈B
µp,q · f (p, q −p) · ψp,q(x, y).
The persistence image Iµ
f,Φ of µ with respect to ( f, Φ) assigns a real number to every subset
Z ⊂R2; this number is given by integrating the persistence surface over Z:
Iµ
f,Ψ(Z) =
ZZ
Z ρµ
f,Ψ(x, y) dx dy.
In order to obtain a vector from the persistence image, one lets Z range over grid pix-
els in a rectangular subset of R2 and renormalizes the resulting array of numbers, thus
producing a grayscale image. Standard choices of f and Ψ =

ψp,q
	
are:
f (x, y) =





0
t ≤0
t/λmax
0 < t < λmax
1
t > λmax
ψp,q(x, y) =
1
2πσ2 · exp

−(x −p)2 + (y −(q −p))2
2σ2

.
Here λmax is the largest lifespan max[p,q]∈B(q −p) encountered among the intervals in B,
and σ is a user-deﬁned parameter which forms the common standard deviation of every
ψp,q in sight.
The second and ﬁnal functional vectorization which we will examine was introduced
in the paper [46]. Set W :=
(x, y) ∈R2 | 0 ≤x < y
	
, and note that points (x, y) ∈W
parameterize intervals [x, y] ⊂R with strictly positive length that could possibly lie in a
given barcode. Let Cc(W) be the set of all continuous functions f : W →R with compact
support3. The given barcode µ : B →Z>0 induces a function Vµ : Cc(W) →R via
Vµ( f ) = ∑
[p,q]∈B
µp,q · f (p, q −p).
(3)
A subset T of Cc(W) is called a template system if for any distinct pair µ1 : B1 →Z>0 and
µ2 : B2 →Z>0 of barcodes, there exists at least one f ∈T so that Vµ1( f ) ̸= Vµ2( f ).
Definition 2.11. Fix an integer n > 0 and let Subn(T) be the collection of all size n
subsets of a template system T as described above. The template function vectorization of
µ : B →Z>0 with respect to T is the map τ : Subn(T) →Rn deﬁned as follows. Given
f = { f1, . . . , fn} in Subn(T), the associated vector in Rn is
τµ( f ) :=
 Vµ( f1), . . . , Vµ( fn)

,
where Vµ( fi) is as deﬁned in (3).
3In other words, Cc(W) contains those continuous real-valued functions on W which evaluate to 0 outside
the intersection of a sufﬁciently large rectangle with W in R2.


--- Page 11 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
11
Two convenient choices of T, called tent functions and interpolating polynomials, have
been highlighted in [46]. Tent functions are indexed by points (u, v) ∈R2 and require an
additional parameter δ > 0; they have the form
gδ
u,v(x, y) = max

1 −1
δ · max(|x −u|, |y −v|), 0

(4)
By construction, each such function is supported on the square of side length 2δ around
the point (u, v) in the birth-lifespan plane. The normal pipeline for selecting ﬁnitely many
template functions requires covering a sufﬁciently large bounded subset of W with a square
grid and then selecting the appropriate tent functions supported on grid cells. We direct
interested readers to [46, Sections 6 and 7] for details on interpolating polynomials and for
suggestions on how one might select suitable n and f ∈Subn(T) for a given classiﬁcation
task.
Other Functional Vectorizations: See the generalised persistence landscape in [8] and the
crocker stacks of [58].
2.5. Ensemble Vectorizations. Our last category contains two methods which require
access to a sufﬁciently large collection of training barcodes µi : Bi →Z>0 in order to
generate a vectorization. The ﬁrst of these methods, introduced in [48], is a modiﬁcation of
the template system vectorization from Deﬁnition 2.11. We recall that W ⊂R2 is deﬁned
as {(x, y) | 0 ≤x < y} and that every barcode B is identiﬁed with a subset P(B) ⊂W via
the map that sends intervals [p, q] of positive length to points (p, q).
Definition 2.12. The adaptive template system induced by a collection of barcodes
{µi : Bi →Z>0} is obtained via the following two steps. Letting P ⊂W be the union
S
i P(Bi), one
(1) identiﬁes ﬁnitely many ellipses Ej ⊂W which tightly contain P, and then
(2) constructs suitable functions gj supported on Ej, as described in (5) below.
The desired vectorization of a new barcode µ : B →Z>0 is now obtained by using
these gj, rather than tent functions, as template functions in Deﬁnition 2.11. Three different
methods for ﬁnding the Ej can be found in [48, Sec 3]. Let v∗denote the transpose of a given
vector v in R2. Now each ellipse E with centre x = (x1, x2)∗corresponds to a symmetric
2 × 2 matrix A satisfying
E =
n
z ∈R2 | (z −x)∗A(z −x) = 1
o
.
Setting h(z) := (z −x)∗A(z −x), the adaptive template function g supported on E is
g(z) =
(
1 −h(z)
h(z) < 1
0
otherwise.
(5)
The second instance of an ensemble vectorization framework which we benchmark in
this paper is from [51]. Let µi : Bi →Z>0 be a collection of training barcodes as before, and
ﬁx a dimension parameter b ∈Z>0. Much like the adaptive template systems of Deﬁnition
2.12, the automatic topology-oriented learning (ATOL) vectorization is a two-step process for
mapping each Bi to a vector space, which in this instance is always Rb.


--- Page 12 ---

12
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
Definition 2.13. The ATOL contrast functions corresponding to the collection of bar-
codes {µi : Bi →Z>0} and parameter b ∈Z>0 are obtained as follows:
(1) Treating the point clouds
Pi :=
n
(p, q) ∈R2 | [p, q] ∈Bi and q > p
o
as discrete measures on R2, one estimates their average measure E.
(2) Let z := (z1, z2, . . . , zb) be a point sample in R2 drawn (in independent, identically
distributed function) along E. Deﬁne the real numbers σi(z) for 1 ≤i ≤b by
σi(z) := 1
2 max
j̸=i ∥zj −zi∥2,
where ∥• ∥2 denotes the usual Euclidean norm on R2.
The contrast functions

Ωi : R2 →R | 1 ≤i ≤b
	
are now given by
Ωi(x) = exp

−∥x −zi∥
σi(z)

.
The reader is directed to [51, Algorithm 1] for further details. Once the contrast func-
tions have been produced in the manner described above, the corresponding ATOL vector-
ization of a given barcode µ : B →Z>0 equals
 Ωµ
1, . . . , Ωµ
b

, where
Ωµ
i := ∑
[p,q]∈B
µp,q · Ωi(p, q).
Other Ensemble Vectorizations: The persistence codebooks approach from [60] proposes
three different types of barcode vectorizations; these are based on bag-of-word embeddings,
VLAD (vector of locally aggregated descriptors), and Fisher Vectors respectively.
3. Datasets
The vectorization methods described in the preceding section have been benchmarked
against three standard datasets; these are described below and arranged in increasing order
of difﬁculty for topological methods. All three of them have been used in the past for
comparing vectorizations (or kernels) for persistence barcodes [46, 48, 50, 17, 34, 21].
3.1. Outex. Outex is a database of images developed for the assessment of texture clas-
siﬁcation algorithms [43] — see Fig. 2, right-bottom, for some samples of textures from the
68 categories. Each texture class contains 20 images of size 128 × 128 pixels, which results
in 1, 360 images in total. We designed a reduced version of the experiment by randomly
selecting 10 of the total 68 classes in the dataset, which we refer to as Outex10 below. The
full classiﬁcation is referred to as Outex68. In both cases, a train/test split of 70/30 has
been applied.
We treat each image as a cubical complex; the ﬁltration is induced by considering the
pixel intensity on the 2-dimensional cells, which is inherited by other cells via the lower-
star and upper-star ﬁltrations. Persistent homology barcodes are computed in dimensions
0 and 1 using the GUDHI library [28]. No pre-processing has been applied to the images.


--- Page 13 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
13
Figure 2. Samples from datasets used in our experiments
3.2. SHREC14. The Shape Retrieval of a non-rigid 3D Human Models dataset, usually
abbreviated SHREC14 [47], is designed to test shape classiﬁcation and retrieval algorithms.
It contains real and synthetic human shapes and poses stored as 3D meshes (which are
already simplicial complexes). We use the synthetic part of the dataset; this constitutes
a classiﬁcation task with 15 classes (5 men, 5 women and 5 children), each one with 20
different poses — see the upper-right corner of Fig. 2.
We apply the Heat Kernel Signature (HKS) to obtain ﬁltrations [54, 50]. For a ﬁxed real
parameter t > 0, this ﬁltration assigns to each mesh point x the value
HKSt(x) =
∞
∑
i=0
e−λit · φi(x)2
(6)
Here λi and φi are eigenvalues and corresponding eigenfunctions of (a discrete approxi-
mation to) the Laplace-Beltrami operator of the given mesh. Every simplex of dimension
> 0 is assigned the largest value of HKSt encountered among its vertices. We used the
pre-computed barcodes (for such ﬁltrations across a range of t-values) which have been
provided in the repository4 accompanying [7]. Of the 300 samples, 70% were used for
training and the other 30% for testing.
3.3. FMNIST. The Fashion-MNIST database contains 28 × 28 grayscale images (7, 000
images per class, with 10 classes) — see the left side of Fig. 2 for some sample images. We
split this dataset into 60, 000 training and 10, 000 testing images.
The ﬁltration used for generating barcodes is as follows: we performed padding, me-
dian ﬁlter, and shallow thresholding before computing canny edges [13]. Then each pixel is
given a ﬁltration value equalling its distance from the edge-pixels. Finally, all other cells
inherit ﬁltration values from the top pixels via the lower star ﬁltration rule.
4https://github.com/barnesd8/machine_learning_for_persistence


--- Page 14 ---

14
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
4. Results
Here we report the classiﬁcation accuracy of the thirteen vectorization methods from
Section 2 on each of the three datasets from Section 3. Implementation details and pa-
rameter choices are provided in Appendix A. The source code is available at the following
GitHub repository: https://github.com/Cimagroup/vectorization-maps.
4.1. Outex. Table 1 displays the classiﬁcation accuracy for the smaller (and easier) ex-
periment on 10 classes. As one might expect, all techniques perform rather well, with
Persistence Statistics and Algebraic Functions sharing the best performance with 99.2%
accuracy each, followed closely by Persistent Silhouettes with 98.3% each.
Results from the full experiment with 68 classes are contained in Table 2; as one might
expect, the performance of every single vectorization degrades in the passage from Ou-
tex10 to Outex68. Here Persistence Statistics is the clear winner by a signiﬁcant margin,
earning 93.4% accuracy. Tropical Coordinates ranks second with 88.7%. Setting aside the
outstanding performance of Persistence Statistics, it appears clear from these results that
the algebraic vectorizations perform far better on Outex68 than the vectorizations from the
other categories.
Vectorization Method
Accuracy
Parameters
Estimator
Persistence Statistics
0.992
SVM, rbf kernel, C1, γ1
Entropy Summary
0.975
100
SVM, rbf kernel, C1, γ1
Algebraic Functions
0.992
SVM, linear kernel, C3
Tropical Coordinates
0.975
250
SVM, linear kernel, C4
Complex Polynomial
0.950
5, R
SVM, rbf kernel, C1, γ1
Betti Curve
0.908
200
SVM, rbf kernel, C1, γ1
Lifespan Curve
0.975
100
SVM, rbf kernel, C1, γ1
Persistence Landscape
0.975
50, 20
SVM, rbf kernel, C2, γ2
Persistence Silhouette
0.983
100, 0
SVM, rbf kernel, C1, γ1
Persistence Image
0.938
1, 25
RF,
n=500
Template Function
0.958
35, 20
SVM, rbf kernel, C1, γ1
Adaptive Template System
0.975
GMM, 40
SVM, rbf kernel, C1, γ1
ATOL
0.967
32
SVM, linear kernel, C4
Table 1. Outex10 results.
The relevant parameter values are C1 = 936.5391, γ1 = 0.0187,
C2 = 914.9620, γ2 = 0.0061, C3 = 86.0442, and C4 = 998.1848.
We note that the authors of [23] have also used Outex to compare the performance of
various curve vectorizations, with Persistence Statistics being used as a baseline. They also
obtained their best results with Persistence Statistics.
4.2. SHREC14. We used 10 different t-values t1 < t2 < · · · < t10, as in [50, 46, 48],
for generating ﬁltrations via the heat kernel from (6). At t10 we found several sparse or
empty barcodes, which led us to discard that classiﬁcation problem. Table 3 collects the
best performance for each method across the ﬁrst 9 values of t; it also contains values of
the optimal parameters (see Appendix A) and the optimal values of t.


--- Page 15 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
15
Vectorization Method
Accuracy
Parameters
Estimator
Persistence Statistics
0.934
SVM, rbf kernel, C1, γ1
Entropy Summary
0.859
100
SVM, poly kernel, C2, γ2, deg=2
Algebraic Functions
0.875
SVM, linear kernel, C4
Tropical Coordinates
0.887
50
SVM, linear kernel, C5
Complex Polynomial
0.846
10, R
SVM, linear kernel, C4
Betti Curve
0.804
200
SVM, rbf kernel, C1, γ1
Lifespan Curve
0.842
100
SVM, rbf kernel, C1, γ1
Persistence Landscape
0.822
50, 20
SVM, rbf kernel, C3, γ3
Persistence Silhouette
0.844
100, 1
SVM, linear kernel, C4
Persistence Image
0.762
1, 150
RF, n=500
Template Function
0.831
35, 20
RF, n=200
Adaptive Template Sys.
0.819
GMM, 50
SVM, linear kernel, C6
ATOL
0.854
16
SVM, linear kernel, C7
Table 2. Outex68 results. The optimal parameter values are C1 = 936.5391, γ1 = 0.0187, C2 =
957.5357, γ2 = 0.0120, C3 = 914.9620, γ3 = 0.0061, C4 = 998.1848, C5 = 884.1255, C6 = 143.1201
and C7 = 494.0596.
Vectorization Method
Accuracy
Parameters
Estimator
Persistence Statistics
0.947
t5
RF, n=100
Entropy Summary
0.723
t6, 200
RF, n=300
Algebraic Functions
0.909
t5
RF, n=500
Tropical Coordinates
0.844
t6, 50
SVM, linear kernel, C5
Complex Polynomial
0.889
t6, 20, S
SVM, linear kernel, C6
Betti Curve
0.728
t5, 200
RF, n=100
Lifespan Curve
0.878
t7, 200
SVM, linear kernel, C7
Persistence Landscape
0.889
t6, 50, 10
SVM, rbf kernel, C1, γ1
Persistence Silhouette
0.867
t6, 200, 2
SVM, rbf kernel, C2, γ2
Persistence Image
0.916
t5, 1, 10
RF, n=100
Template Function
0.944
t5, 14, 0.7
SVM, rbf kernel, C3, γ3
Adaptive Template Sys.
0.889
t5, GMM, 15
SVM, linear kernel, C8
ATOL
0.933
t8, 16
SVM, rbf kernel, C4, γ4
Table 3. Best performance of each method on SHREC14. The parameters are C1 = 835.6257,
γ1 = 0.0002, C2 = 212.6281, γ2 = 0.0031, C3 = 879.1425, γ3 = 0.0010, C4 = 936.5391, γ4 =
0.0187, C5 = 141.3869, C6 = 625.0300, C7 = 998.1848, C8 = 274.500.
Persistence Statistics yielded the best classiﬁcation accuracy of 94.7%, followed closely
by Template Functions at 94.4%. One remarkable feature of these results is that the dataset
does not appear to favour any one category of vectorizations over the other — it is possible
to achieve over 88% accuracy by using a suitable statistical, algebraic, curve, functional or
ensemble vectorization. In fact, only the curve-based vectorizations failed to achieve over


--- Page 16 ---

16
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
90% accuracy on this dataset. The variation of classiﬁcation accuracy with the heat kernel
parameter t is discussed in Appendix B.
4.3. FMNIST. The results of our experiments on FMNIST are recorded in Table 4. We
note that these experiments only used information contained in the 0-dimensional barcodes
and that the SVM classiﬁer was not used. The classiﬁcation accuracy of all the methods is
much lower than the corresponding ﬁgures for the two preceding datasets. Once more, the
Persistence Statistics vectorization takes the top spot with 74.9% and Template Functions
are slightly behind at 74.7%
Vectorization Method
Accuracy
Parameters
Persistence Statistics
0.749
Entropy Summary
0.696
30
Algebraic Functions
0.710
Tropical Coordinates
0.696
10
Complex Polynomial
0.661
10, R
Betti Curve
0.618
50
Lifespan Curve
0.692
30
Persistence Landscape
0.694
30, 5
Persistence Silhouette
0.670
30, 0
Persistence Image
0.698
1, 12
Template Functions
0.747
10, 2
Adaptive Template System
0.602
GMM, 5
ATOL
0.730
16
Table 4. FMNIST results. All the scores have been achieved for Random Forest classiﬁer with
100 trees.
One rather surprising aspect of these results is the fact that Adaptive Template Systems
performed far worse than ordinary Template Functions despite having recourse to 60, 000
training barcodes. We do not have a clear explanation for this phenomenon, particularly in
light of a fairly competitive performance from ATOL (which was also exposed to the same
training data).
5. Web Application
In order to illustrate and visualize the vectorization methods described here, we have
built an interactive web application that runs on any modern browser; it is available at
https://persistent-homology.streamlit.app/
The app has been built in Python using the Streamlit library together and makes use of
several existing Python libraries. The sidebar contains options for selecting different types
of input data and displays several options for data visualization. One sample image/point-
cloud from each of the three datasets used in this paper has been pre-loaded, but the user
is free to upload their own data. Speciﬁcations, formatting guidelines, and downloading


--- Page 17 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
17
instructions are available in our GitHub repository:
https://github.com/dashtiali/vectorisation-app
Figure 3. A screenshot of the web app
All of the barcode vectorization methods considered in this paper can be computed and
visualized in different formats (tables, bar graphs, scatter plots), depending on the type of
vectorization being invoked. Barcodes are computed by default in dimensions 0 and 1, and
depicted as in Figure 4.
Figure 4. Intervals in barcodes of dimensions 0 and 1 as displayed by the web app.
The Persistence Statistics vectorization is purely numerical, so we show its values in a
table, as in Figure 5.


--- Page 18 ---

18
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
Figure 5. The Persistence Statistics vectorization as shown in the web app.
Algebraic vectorizations are illustrated as bar graphs. In Figure 6, for instance, one
ﬁnds bars whose heights correspond to values attained by the 7 chosen tropical coordinate
polynomials on the input barcodes.
Figure 6. A visualization of the Tropical Coordinates vectorization from the web app.
Curve vectorizations, such as persistence landscapes, are depicted via piecewise-linear
graphs (see Figure 7). Sliders have been provided to set the resolution parameter.


--- Page 19 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
19
Figure 7. Persistence landscapes in the web app
Persistence images are displayed as heat maps — see Figure 8.
Figure 8. Persistence images as shown in the web app
Template Functions, their adaptive version, and ATOL are all displayed as bar graphs
with heights of bars indicating the values of the selected functions. Figure 9, for instance,
depicts Template Functions.


--- Page 20 ---

20
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
Figure 9. The web app visualization of template functions
It is our hope that users will beneﬁt from the ability to generate these visualizations
without having to write any code of their own. In order to facilitate downstream analysis,
the web app also provides the ability to download the vectors generated by each vectoriza-
tion method.
6. Concluding Remarks
At the time of writing, it remains difﬁcult to accurately pinpoint those attributes which
might make a given vectorization method a good choice for a particular classiﬁcation prob-
lem. There are no powerful theorems or immutable doctrines available to guide scientists
who wish to incorporate topological information into machine learning pipelines. In the
absence of such theoretical foundations, the best that one can expect are principled heuris-
tics supported by reproducible empirical evidence. This paper is an outcome of our efforts
to provide such evidence. En route, we have organized thirteen available vectorization
methods into ﬁve categories in Section 2 and provided a web application which will allow
others to conduct their own experiments involving these methods.
One possible conclusion that may be drawn from the results of Section 4 is that we
can dispense with sophisticated vectorization techniques and only use (some variant of)
Persistence Statistics. We do not necessarily suggest such a course of action. While it is
certainly true that Persistence Statistics earned top honors in all of our experiments and
is much faster to compute than the alternatives, there are other factors to consider. In
particular, no comparative study such as ours can be truly exhaustive. There is always the
chance that making different choices – for instance, using another dataset for classiﬁcation,
or adding some new polynomials to one of the algebraic vectorizations – could dramatically
update our priors about which methods perform best.


--- Page 21 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
21
Acknowledgments
M.J. Jimenez, E. Paluzo-Hidalgo and M. Soriano-Trigueros are funded by the Spanish grants Ministerio
de Ciencia e Innovacion - Agencia Estatal de Investigacion/10.13039/501100011033, PID2019-107339GB-I00
and Agencia Andaluza del Conocimiento, PAIDI-2020 P20-01145. M.J. Jimenez is also funded by a grant of
Convocatoria de la Universidad de Sevilla para la recualiﬁcacion del sistema universitario espa˜nol, 2021-23,
funded by the European Union, NextGenerationEU.
V. Nanda is supported by EPSRC grant EP/R018472/1 and by US AFOSR grant FA9550-22-1-0462.
We are grateful to the team of GUDHI and TEASPOON developers, for their work and their support.
We are also grateful to Streamlit for providing extra resources to deploy the web app online on Streamlit
community cloud.
References
[1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, S. Chepushtanova, E. Hanson,
F. Motta, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology.
J. Mach. Learn. Res., 18(1):218–252, 2017.
[2] A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes.
Homology, Homotopy Appl., 18:381–402, 2016.
[3] C. C. Aggarwal. Neural Networks and Deep Learning. Springer, 2018.
[4] M. A. Armstrong. Basic Topology. Springer, 1983.
[5] A. Asaad, D. Ali, T. Majeed, and R. Rashid. Persistent homology for breast tumor classiﬁcation using
mammogram scans. Mathematics, 10(21), 2022.
[6] N. Atienza, R. Gonzalez-Diaz, and M. Soriano-Trigueros. On the stability of persistent entropy and new
summary functions for topological data analysis. Pattern Recognition, 107:107509, 2020.
[7] D. Barnes, L. Polanco, and J. A. Perea. A comparative study of machine learning methods for persistence
diagrams. Frontiers in Artiﬁcial Intelligence, 4, 2021.
[8] E. Berry, Y.-C. Chen, J. Cisewski-Kehe, and B. T. Fasy. Functional summaries of persistence diagrams.
Journal of Applied and Computational Topology, 4(2):211–262, 2020.
[9] C. A. Biscio and J. Møller. The accumulated persistence function, a new useful functional summary statis-
tic for topological data analysis, with a view to brain artery trees and spatial point process applications.
Journal of Computational and Graphical Statistics, 28(3):671–681, 2019.
[10] P. Bubenik. Statistical topological data analysis using persistence landscapes. J. Mach. Learn. Res., 16(1):77–
102, Jan. 2015.
[11] P. Bubenik, V. de Silva, and V. Nanda. Higher interpolation and extension for persistence modules. SIAM
Journal on Applied Algebra and Geometry, 1(1):272–284, 2017.
[12] P. Bubenik and P. Dłotko. A persistence landscapes toolbox for topological statistics. Journal of Symbolic
Computation, 78:91–114, 2017.
[13] J. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine
Intelligence, PAMI-8(6):679–698, 1986.
[14] G. Carlsson. Topology and data. Bull. Amer. Math. Soc. (N.S.), 46(2):255–308, 2009.
[15] G. Carlsson and R. B. Gabrielsson. Topological approaches to deep learning. In Topological data analysis,
pages 119–146. Springer, 2020.
[16] M. Carri`ere, F. Chazal, Y. Ike, T. Lacombe, M. Royer, and Y. Umeda. Perslay: A neural network layer
for persistence diagrams and new graph topological signatures. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 2786–2796. PMLR, 2020.
[17] M. Carriere, M. Cuturi, and S. Oudot. Sliced wasserstein kernel for persistence diagrams. In International
conference on machine learning, pages 664–673. PMLR, 2017.
[18] F. Chazal, V. de Silva, M. Glisse, and S. Oudot. The Structure and Stability of Persistence Modules. Springer,
2016.
[19] F. Chazal, B. T. Fasy, F. Lecci, A. Rinaldo, and L. Wasserman. Stochastic convergence of persistence
landscapes and silhouettes. In Proceedings of the Thirtieth Annual Symposium on Computational Geometry,
SOCG’14, page 474–483, New York, NY, USA, 2014. Association for Computing Machinery.


--- Page 22 ---

22
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
[20] I. Chevyrev and A. Kormilitzin. A primer on the signature method in machine learning. arXiv:1603.03788
[stat.ML], 2016.
[21] I. Chevyrev, V. Nanda, and H. Oberhauser. Persistence paths and signature features in topological data
analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(1):192–202, 2020.
[22] H. Chintakunta, T. Gentimis, R. Gonzalez-Diaz, M. Jimenez, and H. Krim. An entropy-based persistence
barcode. Pattern Recognit., 48(2):391–401, Feb. 2015.
[23] Y. Chung and A. Lawson. Persistence curves: A canonical framework for summarizing persistence dia-
grams. Adv. Comput. Math., 48(1):6, 2022.
[24] F. Conti, D. Moroni, and M. A. Pascali. A topological machine learning pipeline for classiﬁcation. Math-
ematics, 10(17):3086, 2022.
[25] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.
[26] V. de Silva and V. Nanda. Geometry in the space of persistence modules. In Proceedings of the 29th Annual
Symposuim on Computational Geometry, ACM, pages 397–404, 2013.
[27] B. Di Fabio and M. Ferri. Comparing persistence diagrams through complex vectors. In V. Murino and
E. Puppo, editors, Image Analysis and Processing — ICIAP 2015, pages 294–305, Cham, 2015. Springer
International Publishing.
[28] P. Dlotko. Cubical complex. In GUDHI User and Reference Manual. GUDHI Editorial Board, 3.6.0 edition,
2022.
[29] Z. Dong, C. Hu, C. Zhou, and H. Lin. Vectorization of persistence barcode with applications in pattern
classiﬁcation of porous structures. Computers & Graphics, 90:182–192, 2020.
[30] H. Edelsbrunner and J. Harer. Computational Topology - an Introduction. American Mathematical Society,
2010.
[31] M. Ferri and C. Landi. Representing size functions by complex polynomials. Proc. Math. Met. in Pattern
Recognition, 9:16–19, 1999.
[32] R. Ghrist. Barcodes: the persistent topology of data. Bull. Amer. Math. Soc. (N.S.), 45(1):61–75, 2008.
[33] C. Giusti and D. Lee. Signatures, lipschitz-free spaces, and paths of persistence diagrams. arXiv preprint
arXiv:2108.02727, 2021.
[34] W. Guo, K. Manohar, S. L. Brunton, and A. G. Banerjee. Sparse-tda: Sparse realization of topological data
analysis for multi-way classiﬁcation. IEEE Transactions on Knowledge and Data Engineering, 30(7):1403–
1408, 2018.
[35] A. Hatcher. Algebraic topology. Cambridge University Press, 2002.
[36] T. K. Ho. Random decision forests. In Proceedings of 3rd international conference on document analysis and
recognition, volume 1, pages 278–282. IEEE, 1995.
[37] C. D. Hofer, R. Kwitt, and M. Niethammer. Learning representations of persistence barcodes. J. Mach.
Learn. Res., 20(126):1–45, 2019.
[38] T. Kaczynski, K. M. Mischaikow, M. Mrozek, and K. Mischaikow. Computational homology. Applied math-
ematical sciences (Springer-Verlag New York Inc.); v. 157. Springer, New York, 2004.
[39] S. Kaliˇsnik. Tropical coordinates on the space of persistence barcodes. Foundations of Computational Math-
ematics, 19(1):101–129, 2019.
[40] A. Keros, V. Nanda, and K. Subr. Dist2Cycle: a simplicial neural network for homology localization. In
Proceedings of the Thirty-Sixth AAAI Conference on Artiﬁcial Intelligence, 2022.
[41] M. Moor, M. Horn, B. Rieck, and K. Borgwardt. Topological autoencoders. In International conference on
machine learning, pages 7045–7054. PMLR, 2020.
[42] V. Nanda and R. Sazdanovic. Simplicial models and topological inference in biological systems. In Dis-
crete and Topological Models in Molecular Biology, pages 109–141. Springer, 2014.
[43] T. Ojala, T. Maenpaa, M. Pietikainen, J. Viertola, J. Kyllonen, and S. Huovinen. Outex-new framework for
empirical evaluation of texture analysis algorithms. In 2002 International Conference on Pattern Recognition,
volume 1, pages 701–706. IEEE, 2002.
[44] S. Y. Oudot. Persistence theory: from quiver representations to data analysis, volume 209. American Mathe-
matical Soc., 2017.
[45] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.


--- Page 23 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
23
[46] J. A. Perea, E. Munch, and F. A. Khasawneh. Approximating continuous functions on persistence dia-
grams using template functions. Foundations of Computational Mathematics, 2022.
[47] D. Pickup, X. Sun, P. L. Rosin, R. R. Martin, Z. Cheng, Z. Lian, M. Aono, A. Ben Hamza, A. Bronstein,
M. Bronstein, S. Bu, U. Castellani, S. Cheng, V. Garro, A. Giachetti, A. Godil, J. Han, H. Johan, L. Lai,
B. Li, C. Li, H. Li, R. Litman, X. Liu, Z. Liu, Y. Lu, A. Tatsuma, and J. Ye. SHREC’14 track: Shape retrieval
of non-rigid 3d human models. In Proceedings of the 7th Eurographics workshop on 3D Object Retrieval, EG
3DOR’14. Eurographics Association, 2014.
[48] L. Polanco and J. A. Perea. Adaptive template systems: Data-driven feature selection for learning with
persistence diagrams. In 2019 18th IEEE International Conference On Machine Learning And Applications
(ICMLA), pages 1115–1121. IEEE, 2019.
[49] C. S. Pun, K. Xia, and S. X. Lee. Persistent-homology-based machine learning and its applications–a
survey. arXiv preprint arXiv:1811.00252, 2018.
[50] J. Reininghaus, S. Huber, U. Bauer, and R. Kwitt. A stable multi-scale kernel for topological machine
learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4741–4748,
2015.
[51] M. Royer, F. Chazal, C. Levrard, Y. Umeda, and Y. Ike. Atol: Measure vectorization for automatic
topologically-oriented learning. In A. Banerjee and K. Fukumizu, editors, Proceedings of The 24th In-
ternational Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning
Research, pages 1000–1008. PMLR, 4 2021.
[52] M. Rucco, F. Castiglione, E. Merelli, and M. Pettini. Characterisation of the idiotypic immune network
through persistent entropy. In Proceedings of ECCS 2014, pages 117–128. Springer International Publish-
ing, 2016.
[53] A. Som, K. N. Ramamurthy, and P. Turaga. Geometric metrics for topological representations. In Hand-
book of Variational Methods for Nonlinear Geometric Data, pages 415–441. Springer, 2020.
[54] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and provably informative multi-scale signature based
on heat diffusion. In Computer graphics forum, volume 28, pages 1383–1392. Wiley Online Library, 2009.
[55] The GUDHI Project. GUDHI User and Reference Manual. GUDHI Editorial Board, 3.6.0 edition, 2022.
[56] K. Turner, Y. Mileyko, S. Mukherjee, and J. Harer. Fr´echet means for distributions of persistence dia-
grams. Discrete & Computational Geometry, 52:44–70, 2014.
[57] K. Xia. Persistent similarity for biomolecular structure comparison. Communications in Information and
Systems, 18(4):269–298, 2018.
[58] L. Xian, H. Adams, C. M. Topaz, and L. Ziegelmeier. Capturing dynamics of time-varying data via
topology. Foundations of Data Science, 4(1), 2022.
[59] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
[60] B. Zieli´nski, M. Lipi´nski, M. Juda, M. Zeppelzauer, and P. Dłotko. Persistence codebooks for topological
data analysis. Artiﬁcial Intelligence Review, 54(3):1969–2009, 2021.
[61] A. Zomorodian and G. Carlsson. Computing persistent homology. Discrete & Computational Geometry,
33(2):249–274, 2005.


--- Page 24 ---

24
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
Appendix A. Implementation and Parameter Details
We have made use of several existing software packages, such as GUDHI [55], Teaspoon5
or Scikit-learn [45], as well as our own implementations in some cases. Salient information
regarding each method has been provided in the list below. Full details can be found in the
GitHub repository accompanying this paper6.
A.1. Persistence Statistics. The persistence statistics vectorization from Deﬁnition 2.1
requires no additional parameters. We have implemented this method ourselves.
A.2. Entropy Summary Function. We have used the GUDHI implementation of the
entropy summary function from Deﬁnition 2.2.
There is a single resolution parameter
which selects the grid points on which the entropy summary function is sampled.
A.3. Algebraic Functions. The algebraic functions of Deﬁnition 2.3 are implemented
in the Teaspoon package. For reasons which remain unclear to us, this implementation
includes a ﬁfth tropical polynomial f5 = maxi{(qi −pi)} beyond the four ordinary polyno-
mials f1, . . . , f4 which were described after Deﬁnition 2.3. We do not expect that removing
this function will improve the results described below.
A.4. Tropical Coordinates. We have implemented the tropical polynomials F1, . . . , F7
described after Deﬁnition 2.4. The parameter r has been optimized over the set {10, 50, 250,
500, 800} for Outex and SHREC14, and over {10, 50, 250} for FMNIST.
A.5. Complex Polynomials. We have used the GUDHI implementation of complex
polynomials, which have been described in Deﬁnition 2.5. We generated the polynomials
with respect to all three of the transformations R, S, T : R2 →C. The number of coefﬁcients
used was chosen from {5, 10, 20} for Outex and SHREC14 and {3, 5, 10} for FMNIST.
A.6. Betti Curve. The Betti curve vectorization from Deﬁnition 2.6 has been imple-
mented in GUDHI, and it only requires a resolution parameter. This parameter was chosen
from {50, 100, 200} for Outex and SHREC14 and {15, 30, 50} for FMNIST.
A.7. Lifespan Curve. We implemented the lifespan curve ourselves, with a resolution
parameter optimised across the set {50, 100, 200} for Outex and SHREC14 and across the
set {15, 30, 50} for FMNIST.
A.8. Persistence Landscapes. We have used the GUDHI implementation of persistence
landscapes (see Deﬁnition 2.8). The are two parameters to consider: the resolution (to iden-
tify the grid points where each landscape is sampled) and the total number of landscapes
used. The resolution was optimized over {50, 100, 200} for Outex and SHREC14, and over
{15, 30, 50} for FMNIST; the number of landscapes ranged over {2, 5, 10, 20} for Outex and
SHREC14 and over {1, 2, 3, 5} for FMNIST.
5https://lizliz.github.io/teaspoon/index.html
6https://github.com/Cimagroup/vectorization-maps


--- Page 25 ---

VECTORIZATION METHODS IN TOPOLOGICAL DATA ANALYSIS
25
A.9. Persistence Silhouette. We have used the GUDHI implementation of persistence
silhouettes (see Deﬁnition 2.9). The resolution parameter was optimized over {50, 100, 200}
for Outex and SHREC14 and over {15, 30, 50} for FMNIST; the weight w ranged over
{0, 1, 2, 5, 10, 20} for Outex and SHREC14 and {0, 1, 2, 5} for FMNIST.
A.10. Persistence Images. Persistence images (from Deﬁnition 2.10) have been imple-
mented in GUDHI. The resolution parameter r, which results in images of size r × r, ranged
over {25, 75, 150} for Outex, over {10, 20, 40} for SHREC14, and over {3, 6, 12, 20} for FM-
NIST. Bandwidth values of the Gaussian kernel (σ in Deﬁnition 2.10) were chosen from
{0.05, 1} for Outex and from {0.05, 0.5, 1} for both, SHREC14 and FMNIST.
A.11. Template Functions. We have used code from the repository7 provided with the
paper [46] for computing template functions (see Deﬁnition 2.11). We use tent functions
as described in (2), which require two parameters: a grid resolution δ and a padding
parameter π (for enlarging the area covered by the square grid). We optimized over
• δ in {35, 50, 65} and π in {20, 25, 30} for Outex;
• δ in {3, 4, . . . , 14, 15} and π in {0.5, 0.6, . . . , 1.1, 1.2} for SHREC14;
• δ in {2, 3, 5, 10} and π in {0.5, 1, 2} for FMNIST.
A.12. Adaptive Template Systems. The implementation of adaptive template systems
(Deﬁnition 2.12) has also been sourced from the same repository as template functions.
We have used the Gaussian mixture model for generating ellipsoidal domains, and require
only one parameter: the number of clusters. This has been optimized over
• {10, 20, 30, 40, 50} for Outex,
• {5, 10, 15, 20, 25, 30, 35, 40, 45} for SHREC14, and
• {3, 4, 5, 10, 15} for FMNIST.
A.13. ATOL. The ATOL vectorization from Deﬁnition 2.13 has been implemented in
GUDHI, and it also requires the number of functions b as a parameter. We have optimized
this over {2, 4, 8, 16, 32, 64} for Outex and over {2, 4, 8, 16} for both SHREC14 and FMNIST.
A.14. Dimensions, Classiﬁers and Hyperparameters. In the case of Outex, we have
concatenated vectors arising from barcodes of dimensions 0 and 1; for SHREC14, the vec-
tors computed from only dimension 1 barcodes performed better, so the results are only
reported for them. Finally, only dimension 0 barcodes were taken to build vectors for FM-
NIST. We considered both Support Vector Machine (SVM) [25] and Random Forest (RF)
[36] classiﬁers. Due to convergence issues, only RF has been performed for FMNIST.
For each parameter of each vectorization method, we accomplished a hyperparameter
optimization process based on random search (when optimizing SVM and RF jointly) or
grid search (for optimizing RF), with 5-fold cross-validation on the training data, to ﬁnd the
best (hyper)parameters for both the machine learning models and the vectorization meth-
ods; then, we assigned to each method the parameters with the best average score among
all the 5-fold cross-validation scheme; ﬁnally the vectorization methods were evaluated on
the test dataset 100 times, to report the average accuracy.
7https://github.com/lucho8908/adaptive_template_systems


--- Page 26 ---

26
ALI, ASAAD, JIMENEZ, NANDA, PALUZO-HIDALGO, AND SORIANO-TRIGUEROS
Appendix B. Heat Kernel Parameter Dependence
As mentioned in Section 3, the ﬁltration for SHREC14 is generated using the Heat
Kernel Signature (6) which depends on a single parameter t. In Table 5 we depict the best
classiﬁcation accuracy of each vectorization method across all 9 values of t which were used
in our experiments.
Method
t1
t2
t3
t4
t5
t6
t7
t8
t9
Pers Stat
0.729
0.785
0.662
0.704
0.947
0.910
0.915
0.915
0.908
Ent Sum
0.378
0.333
0.522
0.536
0.656
0.723
0.633
0.656
0.530
Alg Fun
0.467
0.456
0.556
0.567
0.909
0.878
0.863
0.833
0.711
Trop Coord
0.505
0.556
0.522
0.612
0.822
0.844
0.833
0.767
0.800
Com Poly
0.322
0.456
0.400
0.467
0.856
0.889
0.844
0.850
0.790
Bet Cur
0.511
0.467
0.628
0.660
0.728
0.633
0.611
0.644
0.536
Lif Cur
0.456
0.411
0.593
0.639
0.789
0.833
0.878
0.833
0.798
Pers Land
0.700
0.511
0.789
0.778
0.878
0.889
0.857
0.833
0.789
Pers Sil
0.400
0.378
0.556
0.589
0.811
0.867
0.856
0.856
0.656
Pers Img
0.644
0.691
0.795
0.856
0.916
0.794
0.871
0.811
0.718
Temp Func
0.778
0.735
0.933
0.789
0.944
0.919
0.908
0.932
0.922
Ad Temp Sys
0.802
0.872
0.833
0.727
0.889
0.856
0.889
0.844
0.633
ATOL
0.828
0.786
0.911
0.833
0.906
0.867
0.900
0.933
0.867
Table 5. Best Results for SHREC14 for various vectorization methods across nine t-values.
We note that for small values of t, the ensemble vectorizations perform best, whereas for
intermediate and larger values both ensemble and functional vectorizations achieve good
performance. The algebraic and curve based vectorizations perform quite poorly for low
t-values, but tend to become more competitive between t5 and t8.
```

---

## Section 1.7: Bootstrapping the O(N) Vector Models

Source File: Bootstrapping the O(N) Vector Models.txt

### Paper Content:

```
# Bootstrapping the O(N) Vector Models.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Bootstrapping the O(N) Vector Models.pdf
# Output: ../layer2_completion/txt/Bootstrapping the O(N) Vector Models.txt


--- Page 1 ---

arXiv:1307.6856v2  [hep-th]  15 Oct 2015
Bootstrapping the O(N) Vector Models
Filip Kosa, David Polanda, David Simmons-Duﬃnb
a Department of Physics, Yale University, New Haven, CT 06520
b School of Natural Sciences, Institute for Advanced Study, Princeton, New Jersey 08540
Abstract
We study the conformal bootstrap for 3D CFTs with O(N) global symmetry. We
obtain rigorous upper bounds on the scaling dimensions of the ﬁrst O(N) singlet and
symmetric tensor operators appearing in the φi × φj OPE, where φi is a fundamental
of O(N). Comparing these bounds to previous determinations of critical exponents
in the O(N) vector models, we ﬁnd strong numerical evidence that the O(N) vector
models saturate the bootstrap constraints at all values of N. We also compute general
lower bounds on the central charge, giving numerical predictions for the values realized
in the O(N) vector models. We compare our predictions to previous computations in
the 1/N expansion, ﬁnding precise agreement at large values of N.


--- Page 2 ---

Contents
1
Introduction
3
2
Conformal Bootstrap with O(N) Global Symmetry
4
2.1
Statement of Crossing Symmetry
. . . . . . . . . . . . . . . . . . . . . . . .
4
2.2
Bounds from Convex Optimization
. . . . . . . . . . . . . . . . . . . . . . .
5
2.3
Formulation as a Semideﬁnite Program . . . . . . . . . . . . . . . . . . . . .
6
3
Rational Representations for Conformal Blocks
7
3.1
Why Rational Approximations Exist
. . . . . . . . . . . . . . . . . . . . . .
7
3.2
Poles in ∆and Recursion Relations for g∆,ℓ
. . . . . . . . . . . . . . . . . .
8
4
Results and Comparison to O(N) Vector Models
10
4.1
Bounds on O(N) Singlets
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
4.2
Bounds on O(N) Symmetric Tensors
. . . . . . . . . . . . . . . . . . . . . .
13
4.3
Bounds on the Central Charge . . . . . . . . . . . . . . . . . . . . . . . . . .
13
5
Discussion
16
A Improving Rational Approximations
19
B Implementation in Mathematica and SDPA-GMP
19
2


--- Page 3 ---

1
Introduction
Conformal ﬁeld theories (CFTs) oﬀer delightful examples of quantum ﬁeld theories that are
strongly coupled, yet contain enough symmetry and structure that they may turn out to
be tractable if the right techniques are found. Until recently, the idea of exploiting this
structure in order to ﬁnd complete non-perturbative solutions to theories was only carried
out successfully in 2D, most notably in the seminal work of [1]. However, over the last
several years great progress has been made at developing the conformal bootstrap [2, 3]
approach to CFTs in D > 2, where a large number of nontrivial bounds have been found
which follow very generally from the constraints of crossing symmetry and unitarity [4–16].
The results obtained so far have been particularly striking in 3D, where it was found in [13]
that the CFT described by the critical 3D Ising model occupies a special place in the space
allowed by crossing symmetry and unitarity. Moreover it appears possible that a robust
numerical solution to this theory can be obtained using bootstrap techniques [17].
In this paper we will extend the work of [13] to study 3D CFTs with an O(N) global
symmetry using the conformal bootstrap. We will focus on theories containing a scalar ﬁeld
φi(x) in the vector representation of O(N). The most notable theories falling into this class
are the critical O(N) vector models [18, 19], which describe second-order phase transitions
in a variety of real-world systems at small values of N [20], and are also solvable in a 1/N
expansion at large values of N (see [21] for a review). Moreover, the O(N)-singlet sector
of this theory is thought to be holographically described by a higher-spin gauge theory in
AdS4 [22]. Previously, bootstrap ideas have been applied to the O(N) vector models in the
1/N expansion, for example in work by Lang and R¨uhl [23–28], Petkou [29, 30], and more
recently Maldacena and Zhiboedov [31, 32]. Our approach allows bootstrap constraints to
be studied (albeit numerically) at any value of N.
Our primary goal, following previous numerical studies of the bootstrap [4–16], will
be to place general upper bounds on the scaling dimensions of the ﬁrst nontrivial scalar
operators (both O(N) singlets S and O(N) symmetric tensors Tij) in the φi × φj operator
product expansion (OPE). We will also place general lower bounds on the central charge
c, deﬁned as the coeﬃcient appearing in the two-point function of the stress-energy tensor.
We will then compare these bounds to the best previous results based on Monte Carlo
simulations, analytical estimates, and the 1/N expansion. In all cases where results are
known (including small values of N), we ﬁnd that the O(N) vector models saturate our
bounds, and moreover sit near special locations in the space allowed by crossing symmetry.
Inputting the previously measured values of ∆φ, this allows us to give sharp predictions for
∆S, ∆T, and c for diﬀerent values of N.
In order to eﬃciently implement the bootstrap in the presence of a global symmetry,
we will use techniques based on semideﬁnite programming, developed for 4D CFTs in [11].
Here we will show how to adapt this technique for CFTs in an arbitrary number of space-
time dimensions.
This requires approximating conformal blocks as rational functions of
the exchanged operator dimension ∆, and we will show that such a rational approximation
follows directly from a recursion relation expressing a general conformal block as a sum
over poles ∼1/(∆−∆∗) occurring at special (non-unitary) values of the dimension ∆∗
3


--- Page 4 ---

where the conformal multiplet contains a null state. This conformal block representation
generalizes an idea of Zamolodchikov, ﬁrst applied to Virasoro blocks in 2D [33, 34], to
arbitrary space-time dimensions.
This paper is organized as follows.
In section 2 we review the formulation of the
conformal bootstrap for CFTs containing an O(N) global symmetry, as well as convex
optimization techniques for placing bounds on operator dimensions and OPE coeﬃcients.
In section 3 we show how to ﬁnd rational representations for conformal blocks in arbitrary
space-time dimensions, presenting a new recursion relation for conformal blocks as a sum
over poles in ∆. In section 4 we present our bounds and a comparison with the O(N) vector
models. We conclude in section 5.
2
Conformal Bootstrap with O(N) Global Symmetry
2.1
Statement of Crossing Symmetry
Let us brieﬂy review the formulation of the conformal bootstrap for 3D CFTs with an O(N)
global symmetry. Further details can be found in [9–11]. We focus on theories containing
a scalar primary operator φi of dimension ∆φ, transforming as a fundamental under O(N).
The operator product of φi with itself takes the schematic form
φi × φj ∼
X
S+
δijO +
X
T +
O(ij) +
X
A−
O[ij],
(2.1)
where S+ denotes O(N) singlets of even spin, T + denotes O(N) symmetric tensors of even
spin, and A−denotes O(N) anti-symmetric tensors of odd spin.
By pairing up φ’s and performing the OPE, a four-point function can be decomposed
into conformal blocks as
x
2∆φ
12 x
2∆φ
34 ⟨φi(x1)φj(x2)φk(x3)φl(x4)⟩
=
X
S+
λ2
O(δijδkl)g∆,ℓ(u, v)
+
X
T +
λ2
O

δilδjk + δikδjl −2
N δijδkl

g∆,ℓ(u, v)
+
X
A−
λ2
O(δilδjk −δikδjl)g∆,ℓ(u, v),
(2.2)
where each sum runs over primary operators O of dimension ∆and spin ℓappearing in
φi × φj. Here, xij ≡xi −xj, λO is the OPE coeﬃcient of O, and the conformal blocks
g∆,ℓ(u, v) are functions of conformal cross-ratios
u = zz = x2
12x2
34
x2
13x2
24
,
v = (1 −z)(1 −z) = x2
14x2
23
x2
13x2
24
.
(2.3)
The four-point function itself should be independent of how we perform the OPE.
Swapping (1, i) ↔(3, k), we ﬁnd two diﬀerent conformal block expansions of a single four-
point function which must agree with each other. Writing out this condition and isolating
4


--- Page 5 ---

the coeﬃcient of each tensor structure that appears, we obtain three equations which can
be grouped into a vector “sum rule”
X
S+
λ2
OVS,∆,ℓ+
X
T +
λ2
OVT,∆,ℓ+
X
A−
λ2
OVA,∆,ℓ
=
0,
(2.4)
where
VS,∆,ℓ=


0
F −
∆,ℓ
F +
∆,ℓ

,
VT,∆,ℓ=


F −
∆,ℓ
(1 −2
N )F −
∆,ℓ
−(1 + 2
N )F +
∆,ℓ

,
VA,∆,ℓ=


−F −
∆,ℓ
F −
∆,ℓ
−F +
∆,ℓ

,
(2.5)
F ±
∆,ℓ(u, v) ≡v∆φg∆,ℓ(u, v) ± u∆φg∆,ℓ(v, u).
(2.6)
The coeﬃcients λ2
O appearing in (2.4) are unknown a-priori, with the exception of the unit
operator which has λO = 1 if φi is canonically normalized. However, we do know that
the OPE coeﬃcients λO must be real in unitary theories, which means that λ2
O is positive.
Further, in D-dimensional unitary theories the operator dimensions must satisfy the lower
bounds [35–39]
∆≥
(
D−2
2
if
ℓ= 0,
ℓ+ D −2
if
ℓ> 0,
(2.7)
where saturation occurs for free scalars (ℓ= 0) or conserved currents (ℓ> 0).
2.2
Bounds from Convex Optimization
From here, we follow the general strategy of [4] for putting bounds on CFT data. Let us
begin by isolating the unit operator in (2.4),
0
=
Vunit +
X
λ2
OVO.
(2.8)
The procedure is as follows:
1. Make an assumption about the CFT spectrum, for instance that all singlet scalars
have dimension above some ∆∗.
2. Try to ﬁnd a linear functional α such that
α(Vunit)
>
0,
α(VO)
≥
0 for all O satisfying the assumption in (1).
(2.9)
3. If such a functional exists, the assumption (1) is ruled out, since applying α to Eq. (2.8)
gives a contradiction. If not, we cannot conclude anything about our assumption.
Step 2 requires us to solve a convex optimization problem: we must search over the
vector space F of linear functionals, subject to linear constraints of the form α(V ) ≥0.
5


--- Page 6 ---

Each linear constraint restricts us to a half-space of F, and together these half-spaces carve
out a convex subset C ⊂F. We would like to determine whether this subset is non-empty.
In the case at hand, we will take our functional α to be linear combinations of derivatives
with respect to the cross-ratios z, z around the crossing symmetric point z = z = 1/2,
α


f1
f2
f3


=
3
X
i=1
X
0≤m+n<2k
ai
m,n∂m
z ∂n
z fi(z, z)

z=z=1/2 .
(2.10)
The parameter k controls the dimension of the space of linear functionals that we search
over. Note that truncating this search space leads to valid, though possibly suboptimal,
bounds. As we increase k, our bounds get better and better, converging to an optimal
bound as k →∞.
2.3
Formulation as a Semideﬁnite Program
A key diﬃculty in our convex optimization problem is that we have an inﬁnite number
of constraints on α — one for each O which could appear in the OPE. We must impose
α(FR,∆,ℓ) ≥0 for all representations R, dimensions ∆, and spins ℓobeying our assumptions
about the spectrum. An eﬃcient way to deal with this inﬁnity is to reformulate our problem
as a semideﬁnite program, which can include constraints of the form:
α(Pi(x)) for all x ≥0, where Pi(x) are polynomials in x.
(2.11)
Systems of these inequalities can be solved eﬃciently using interior point methods.
To write our constraints α(V ) ≥0 in this form, it suﬃces to ﬁnd an approximation
∂m
z ∂n
z g∆,ℓ(z, z)|z=z=1/2
≈
χℓ(∆)P (m,n)
ℓ
(∆),
(2.12)
where χℓ(∆) are positive functions, and P (m,n)
ℓ
(∆) are polynomials. (Crucially, χℓ(∆) is
independent of m, n.) Indeed, assuming Eq. (2.12), and combining Eqs. (2.5) and (2.10),
we see that
α(VR,∆,ℓ) ≥0
if and only if
X
m,n,i
ai
m,nP (m,n)
R,ℓ,i (∆) ≥0,
(2.13)
for polynomials P (m,n)
R,ℓ,i (∆). The dimensions ∆satisfy bounds ∆≥∆min,ℓ, so writing ∆=
∆min,ℓ+ x yields a set of inequalities in the form (2.11).
In [11], special analytic expressions for conformal blocks in even dimensions [40, 41]
were used to derive approximations of the form (2.12), which proved suﬃcient for apply-
ing semideﬁnite programming to even dimensional CFTs. These approximations worked
surprisingly well, but it was unclear how to generalize the techniques to CFTs in odd (or
fractional) space-time dimension.
In the next section, we will show that the existence of approximations (2.12) in any
space-time dimension follows naturally from representation theory of the conformal group.
6


--- Page 7 ---

x1 = −ρ
x2 = ρ
x3 = 1
x4 = −1
Figure 1: Conﬁguration of points for radial quantization in the ρ-coordinate [42].
This is suﬃcient for formulating our optimization problem as a semideﬁnite program, which
can then be solved using one of the many freely available semideﬁnite program solvers. We
give details of our implementation using the solver SDPA-GMP in Appendix B.
3
Rational Representations for Conformal Blocks
3.1
Why Rational Approximations Exist
To compute CFT bounds using semideﬁnite programming, we need precise, systematic
approximations for conformal blocks g∆,ℓin terms of positive functions times polynomials in
∆, or equivalently positive functions times rational functions of ∆with positive denominator.
The existence of such approximations follows from conformal representation theory. Recall
that the conformal block g∆,ℓis a sum over states in radial quantization
g∆,ℓ(u, v)
x
2∆φ
12 x
2∆φ
34
=
X
α=O, P O, P P O, ...
⟨0|φ(x1)φ(x2)|α⟩⟨α|φ(x3)φ(x4)|0⟩
⟨α|α⟩
,
(3.1)
where O is a conformal primary of dimension ∆and spin ℓ, and α runs over O and all of
its descendants. It will be convenient for our discussion to use the radial coordinates of
[42], where the points xi are arranged as in Fig. 1, and the coordinate ρ is given in terms of
cross-ratios by
ρ =
z
(1 + √1 −z)2,
ρ =
z
(1 + √1 −z)2.
(3.2)
The states P µ1 · · · P µn|O⟩have eigenvalue ∆+n under dilatation and can be decomposed
into traceless symmetric tensor representations of the rotation group. It follows that the
sum over states (3.1) can be written
g∆,ℓ(r, η)
=
∞
X
n=0
X
j
Bn,jr∆+n
j!
(2ν)j
Cν
j (η),
(3.3)
7


--- Page 8 ---

where r = |ρ|, η = cos θ = (ρ + ρ)/2|ρ|, ν = D/2 −1, and the Cν
j (η) are Gegenbauer poly-
nomials. The coeﬃcients Bn,j can be computed straightforwardly by solving the conformal
Casimir equation
Dg∆,ℓ= C∆,ℓg∆,ℓ,
C∆,ℓ= ∆(∆−D) + ℓ(ℓ+ D −2),
(3.4)
term by term around r = 0. Here, D is a second-order diﬀerential operator in the cross-
ratios u, v, representing the action of the quadratic Casimir of the conformal group on a
four-point function,
Dg(u, v)
=
1
2(L(1)
AB + L(2)
AB)(L(1)AB + L(2)AB)g(u, v).
(3.5)
Further details can be found in [42].
Solving the Casimir equation to second order in r, we ﬁnd [42]
B2,ℓ−2 =
ℓ(ℓ−1)(∆−ℓ−2ν)
2(ℓ+ ν −1)(ℓ+ ν)(∆−ℓ−2ν + 1),
B2,ℓ= ν ∆ν(ν −1) + (∆−1)ℓ(ℓ+ 2ν)
(∆−ν)(ℓ+ ν + 1)(ℓ+ ν −1),
B2ℓ+2 =
(∆+ ℓ)(ℓ+ 2ν)(ℓ+ 2ν + 1)
2(∆+ ℓ+ 1)(ℓ+ ν)(ℓ+ ν + 1).
(3.6)
Let us make some comments about these coeﬃcients. First, Bn,j is a rational function
of ∆.
This is a simple consequence of the Casimir equation (3.4), but it follows more
directly from the expression for g∆,ℓas a sum over states. Each term in the numerator and
denominator of (3.1) can be computed from the action of the conformal algebra on |O⟩.
This action is polynomial in ∆, so Bn,j is rational in ∆.
Secondly, the denominators of Bn,j are positive, as long as ∆obeys the unitarity
bound (2.7). This is because poles in Bn,j occur when a state |α⟩becomes null. However,
the absence of null or negative-norm states in a conformal multiplet is precisely what deﬁnes
the unitarity bound.
These facts are exactly what we need. We can obtain good approximations to conformal
blocks by truncating the expansion (3.3) at some high order r∆+N. Each term in the resulting
expression will be a rational function of ∆, times an overall factor of r∆. The error is na¨ıvely
of order rN, which is quite small at the crossing-symmetric point r = 3−2
√
2 ≈0.17. Indeed,
this na¨ıve estimate is correct, since it can be shown that the coeﬃcients Bn,j are uniformly
bounded as a function of ∆for all n, j.
The cost of keeping more terms in the series expansion (3.3) is that the degree of the
resulting rational approximation grows. As we’ll see in the next subsection, this growth is
slow and under control. Further, additional tricks can improve the rational approximation
signiﬁcantly without increasing the degree. We discuss these in Appendix A.
3.2
Poles in ∆and Recursion Relations for g∆,ℓ
We can be more precise about the structure of our rational approximations by exploiting
an idea of Zamolodchikov, originally applied to Virasoro blocks in 2 dimensions [33, 34].
8


--- Page 9 ---

Recall that poles in g∆,ℓas a function of ∆occur precisely at special values ∆= ∆∗
where some state |α⟩= P n|O⟩in the conformal multiplet of |O⟩becomes null.
When
this happens, all the descendants of |α⟩become null as well, and together they form a
nontrivial sub-representation of the (now reducible) multiplet of |O⟩. Since the pole in ∆
gets contributions from all states in this sub-representation, its residue is proportional to a
conformal block for a primary with the same dimension and spin as |α⟩:
g∆,ℓ
∼
cα
∆−∆∗
g∆α,ℓα
as
∆→∆∗,
(3.7)
where ∆α = ∆∗+ n, and cα is a coeﬃcient which is independent of conformal cross-ratios.
Poles in ∆determine g∆,ℓup to a function which is analytic on the entire complex plane.
Thus, we can write
g∆,ℓ(r, η)
=
g(∞)
ℓ
(∆, r, η) +
X
i
ci
∆−∆i
g∆i+ni,ℓi(r, η),
(3.8)
where g(∞)
ℓ
(∆, r, η) is an entire function of ∆. The block g∆,ℓhas an essential singularity of
the form r∆as ∆→∞. Stripping this oﬀ, we have
h∆,ℓ(r, η)
≡
r−∆g∆,ℓ(r, η),
(3.9)
h∆,ℓ(r, η)
=
h(∞)
ℓ
(r, η) +
X
i
cirni
∆−∆i
h∆i+ni,ℓi(r, η).
(3.10)
Note that the entire function h(∞)
ℓ
(r, η) = lim∆→∞h∆,ℓ(r, η) is now independent of ∆, since
it has no singularities as ∆→∞.
With the recursion relation (3.10), we can be more explicit about the form of our positive-
times-polynomial approximation (2.12). Truncating (3.10) to a ﬁnite number of poles ∆i,
it’s clear that
∂m
z ∂n
z g∆,ℓ(z, z)|z=z=1/2 ≈χℓ(∆)P (m,n)
ℓ
(∆),
where
χℓ(∆) ≡
r∆
Q
i(∆−∆i),
(3.11)
and r = 3 −2
√
2 is evaluated at the crossing-symmetric point.
Let us now determine the data entering (3.10). The function h(∞)
ℓ
can be computed
easily by solving the conformal Casimir equation to leading order in ∆,
h(∞)
ℓ
(r, η)
=
ℓ!
(2ν)ℓ
Cν
ℓ(η)
(1 −r2)νp
(1 + r2)2 −4r2η2.
(3.12)
In principle, the pole positions ∆i and coeﬃcients ci are determined by the conformal
algebra. It would be interesting to compute them directly. In practice, by inspecting the
solution to the conformal Casimir equation, we ﬁnd that there are three series of poles,
described in Table 1, whose coeﬃcients are as follows:
9


--- Page 10 ---

ni
∆i
ℓi
ci
2k
1 −ℓ−2k
ℓ+ 2k
c1(k)
k = 1, 2, . . .
2k
1 + ν −k
ℓ
c2(k)
k = 1, 2, . . .
2k
1 + ℓ+ 2ν −2k
ℓ−2k
c3(k)
k = 1, 2, . . . , ⌊ℓ/2⌋
Table 1: The positions of poles of g∆,ℓin ∆and their associated data.
There are three
types of poles, corresponding to the three rows in the table. The ﬁrst two types exist for all
positive integer k, while the third type exists for positive integer k ≤⌊ℓ/2⌋. The coeﬃcients
c1(k), c2(k), c3(k) are given in Eqs. (3.13).
c1(k)
=
−k(2k)!2
24k−1k!4
(ℓ+ 2ν)2k
(ℓ+ ν)2k
,
c2(k)
=
−k(ν + ℓ−k)(ν)k(1 −ν)k
 ν+ℓ+1−k
2
2
k
k!2(ν + ℓ+ k)
  ν+ℓ−k
2
2
k
,
c3(k)
=
−k(2k)!2
24k−1k!4
(1 + ℓ−2k)2k
(1 + ν + ℓ−2k)2k
.
(3.13)
The recursion relation Eq. (3.10), with poles listed in Table 1, reveals another fact that
proves useful for our implementation: when we truncate the series expansion for h∆,ℓat
order rN, the degree of the resulting rational approximation grows like N.
Not only is
each coeﬃcient Bn,j a rational function, but the number of new factors which enter the
denominator as we increase N →N + 2 is either 2 or 3 (depending on whether N ≤ℓ).
In practice, this means that we can compute the expansion (3.3) to extremely high order
without incurring too much of a performance hit from dealing with large degree polynomials.
Once we know the residues ci(k), our recursion relation (3.10) provides an extremely
eﬃcient way to compute conformal blocks, either analytically in a series expansion, or
numerically (for any number of derivatives around any point (r, η)).
It would be very
interesting to generalize these ideas to other four-point functions — for example, external
scalars φi with diﬀerent dimensions ∆i, or external operators with spin. Although much
progress has been made computing conformal blocks for these types of operators using series
manipulations [40, 41, 43, 13, 42], derivative relations [44, 45], and conformal integrals [46],
we believe the residues ci are the most convenient and directly useful data for numerical
bootstrap applications.
4
Results and Comparison to O(N) Vector Models
In this section we will present our results from the bootstrap.
We focus our attention
on computing bounds on the lowest singlet dimension ∆S, the lowest symmetric tensor
dimension ∆T, and the central charge c, as a function of the external scalar dimension ∆φ.
Let us begin by summarizing what is known about these quantities in the O(N) vector
10


--- Page 11 ---

N
∆φ = 1/2 + η/2
∆S = 3 −1/ν
∆T = 3 −φc/ν = 1 + ηc
1
0.51813(5) [47]
1.41275(25) [47]
–
0.51819(7) [48]
1.4130(4) [48]
–
2
0.51905(10) [49]
1.51124(22) [49]
1.237(4) [50]
3
0.51875(25) [51]
1.5939(10) [51]
1.211(3) [50]
4
0.51825(50) [52]
1.6649(35) [52]
1.189(2) [50]
5
0.5155(15) [53]
1.691(7) [53]
1.170(2) [50]
6
0.5145(15) [53]
1.733(8) [53]
–
Table 2: Previous determinations of operator dimensions in the O(N) vector models.
models. In these theories, results are typically phrased in terms of critical exponents, rather
than scaling dimensions. Concretely, ∆φ is related to the critical exponent η via ∆φ =
1/2 + η/2. The dimension of the O(N) singlet operator S is related to the critical exponent
ν via ∆S = 3 −1/ν. Finally, the dimension of the O(N) symmetric tensor operator T is
related to the crossover exponents φc and ηc, through the relations ∆T = 3 −φc/ν = 1 + ηc.
In Table 2 we show the most accurate determinations of these dimensions that we have
found in the literature for N = 1, 2, 3, 4, 5, 6 (with N = 1 being the 3D Ising model).
These quantities have also been computed in the large N limit. ∆φ is known to order
1/N3 while ∆S has only been computed to order 1/N2 (see [21] and references therein). The
crossover exponent connected to ∆T was also computed to order 1/N2 in [54]. The results
are:
∆φ
=
1
2 +
4
3π2
1
N −256
27π4
1
N2
+32 (−3188 + 3π2(−61 + 108 log(2)) −3402ζ(3))
243π6
1
N3 + O
 1
N4

∆S
=
2 −32
3π2
1
N + 32(16 −27π2)
27π4
1
N2 + O
 1
N3

∆T
=
1 + 32
3π2
1
N −512
27π4
1
N2 + O
 1
N3

.
(4.1)
Finally, let us mention that the leading correction to the central charge c was computed
in [29, 30] to be
c
Ncfree
= 1 −40
9π2
1
N + O
 1
N2

,
(4.2)
where cfree = D/(D −1) is the central charge of a free scalar ﬁeld.
4.1
Bounds on O(N) Singlets
Now we determine a general bound on the O(N) singlet operator dimension ∆S following the
procedure described in section 2.2. We assume there is a gap in the CFT spectrum so that all
11


--- Page 12 ---

∆φ
∆S
Ising
O(10)
O(20)
O(2)
O(3)
O(4)
O(5)
O(6)
O(N) Singlet Bounds
0.505
0.515
0.51
0.525
0.52
0.535
0.53
0.5
1
1.2
1.4
1.6
1.8
2
2.2
Figure 2: Upper bounds on the dimension of the lowest dimension singlet S in the φ × φ
OPE, where φ transforms as a vector under an O(N) global symmetry group.
Here, we
show N = 1, 2, 3, 4, 5, 6, 10, 20. The blue error bars represent the best available analytical and
Monte Carlo determinations of the operator dimensions (∆φ, ∆S) in the O(N) vector models
for N = 1, 2, 3, 4, 5, 6 (with N = 1 being the 3D Ising Model). The black crosses show the
predictions in Eq. (4.1) from the large-N expansion for N = 10, 20, ..., 100. In this expansion,
∆φ has been determined to three-loop order, while ∆S is at two-loop order. The dashed line
interpolates the large-N prediction for N ∈(4, ∞).
singlet scalar operators have dimension greater than ∆S, all symmetric tensor scalars have
dimension greater than 1, and the dimensions of all the other operators are constrained
only by the unitarity conditions. Note that due to the assumption on symmetric tensor
scalars this is not the most general bound. However, we found that this mild assumption
improves numerical stability while not signiﬁcantly aﬀecting the bound on ∆S – moreover
the assumption is certainly satisﬁed for O(N) vector models, as can be seen from previous
determinations of the operator dimensions (see Table 2).
The boundaries for the allowed values of ∆S as a function of ∆φ are shown in Fig. 2.
These bounds are determined by a bisection search in ∆S to within 10−3. The parameter
k of section 2.3, controlling the number of derivatives in the functional α, is set to k = 10
everywhere. For a given N, only the values of ∆S below the corresponding solid line are
allowed.
In Fig. 2 we see that the bounds on ∆S grow monotonically from ∆S = 1 at ∆φ = 0.5, the
point corresponding to the non-interacting theory. At a certain value of ∆φ, each boundary
line exhibits a change in the slope. This type of behavior was already discussed for the Ising
12


--- Page 13 ---

model bound in Ref. [13], where it was found that the change in slope occurs at the values
of ∆φ and ∆S corresponding to Ising model point. The changes in slopes are somewhat less
sharp for the O(N) theories; however, they occur in the vicinity of the O(N) vector model
points, shown as points with error bars in Fig. 2. These points always seem to lie very close
to the boundary; in fact it is possible to rigorously reduce the previous error bars using the
bound we obtained.
For large values of N, we can also compare the bound with the results for ∆φ and ∆S
obtained using the 1/N expansion, Eq. 4.1. The 1/N expansion results (shown as black
crosses in Fig. 2) are consistent with our bound at large N. Moreover, the change in the
slope of the boundary line is sharper for large N and it occurs very close to the O(N) vector
model points.
4.2
Bounds on O(N) Symmetric Tensors
Bounds on the dimension of the ﬁrst symmetric tensor scalar operator ∆T are obtained
following a similar procedure as for the singlet bounds. We assume that all the symmetric
tensor scalars have dimension greater than ∆T , all singlet scalars have dimension greater
than 1, and all other operator dimensions satisfy unitarity conditions. The allowed values
of ∆T as a function of ∆φ are shown in Fig. 3 — only the values of ∆T below the solid line
are allowed. Just like in the case of the singlet operators, the boundary lines start at the
free ﬁeld point, ∆φ = 0.5, ∆T = 1 and grow monotonically. The change of the slope is more
gradual than for the singlet bound. The O(N) vector model points from Table 2 appear to
be consistent with the bound to within error bars for N = 2, 3, 4, with some mild tension
between the N = 5 bound and the quoted results from [53] and [50].
At large N the symmetric tensor dimension ∆T approaches 1 in the O(N) model. This
is reﬂected in the fact that our bounds become more and more constraining as N →∞.
Moreover, we ﬁnd that the order 1/N2 computation of ∆T in Eq. 4.1 becomes very close
to our bound (and occurs near changes of slope) at large but ﬁnite values of N, shown in
black crosses at N = 10, 20, ... in Fig. 3.
4.3
Bounds on the Central Charge
The central charge c is deﬁned as a coeﬃcient in the two-point correlation function of the
canonically normalized stress tensor:
⟨Tµν(x1)Tρσ(x2)⟩= c
S2
D
1
x2D
12
1
2 [Iµρ(x12)Iνσ(x12) + Iµσ(x12)Iνρ(x12)] −1
Dηµνηρσ

,
(4.3)
where SD = 2πD/2/Γ(D/2).
In our notation, the central charge is related to the OPE
coeﬃcient of the stress tensor λS,3,2 by
λ2
S,3,2 =
∆2
φ
c/cfree
,
(4.4)
13


--- Page 14 ---

∆φ
∆T
O(10)
O(20)
O(2)
O(3)
O(4)
O(5)
O(6)
O(N) Symmetric Tensor Bounds
0.505
0.515
0.51
0.525
0.52
0.535
0.53
0.5
1.05
1.1
1.15
1
1.2
1.25
1.3
Figure 3: Upper bounds on the dimension of the lowest dimension symmetric tensor T in
the φ × φ OPE, where φ transforms as a vector under an O(N) global symmetry group, for
N = 2, 3, 4, 5, 6, 10, 20. We additionally assume that the lowest dimension singlet S has ∆S ≥
1. The blue error bars represent the best available analytical and Monte Carlo determinations
of the operator dimensions (∆φ, ∆T ) in the O(N) vector models for N = 2, 3, 4, 5.
Note
in particular that previous predictions for the O(5) model are essentially ruled out by our
bounds. The black crosses show the predictions in Eq. (4.1) from the large-N expansion for
N = 10, 20, ..., 100. The dashed line interpolates the large-N prediction for N ∈(4, ∞).
where cfree = D/(D −1) is the central charge of a free scalar ﬁeld.
We can ﬁnd an
upper bound on this OPE coeﬃcient as follows. Rewrite the sum rule (2.8), separating
the contribution of the stress tensor:
λ2
S,3,2VS,3,2 = −Vunit −
X
O̸=Tµν
λ2
OVO.
(4.5)
Applying a functional α such that α(VO) ≥0 for all operators in the spectrum other than
unit operator and normalized so that α(VS,3,2) = 1, Eq. (4.5) then yields the inequality
λ2
S,3,2 ≤−α(Vunit).
(4.6)
Finding the functional α that minimizes −α(Vunit) then gives the strongest upper bound on
the OPE coeﬃcient. By Eq. (4.4) this implies a lower bound on the central charge.
The most general bound would be obtained by making no assumptions about the op-
erator spectrum, except that they obey unitarity conditions.
However, we can obtain
a somewhat stronger bound by making additional assumptions about the spectrum.
In
14


--- Page 15 ---

∆φ
c/Ncfree
Ising
O(10)
O(20)
O(2)
O(3)
O(4)
O(5)
O(6)
O(N) Central Charge Bounds
0.505
0.515
0.51
0.525
0.52
0.535
0.53
0.5
1.025
1
0.875
0.925
0.95
0.975
0.9
Figure 4: Lower bounds on the central charge for theories containing a scalar φ transforming
as a vector under O(N). We additionally assume that ∆S, ∆T ≥1. The black crosses show
the predictions in Eq. (4.1) from the large-N expansion for N = 10, 20, ..., 100. The dashed
line shows the asymptotic behavior of the central charge as a function of ∆φ as N →∞.
particular, we can assume there are gaps in the spectrum of singlet and symmetric tensor
operators, as long as they are consistent with the results of previous subsections. Here we
will assume mild gaps, ∆S ≥1 and ∆T ≥1. This assumption on the operator dimension
spectrum is not too stringent; for example, we know from previous determinations that
O(N) vector models satisfy these conditions, see Table 2.
The central charge bound as a function of the scalar dimension ∆φ is shown in Fig. 4.
The central charge approximately scales linearly with N (exactly in the non-interacting
theory), so we have plotted c scaled to Ncfree.
At low values of ∆φ, all of the bounds
approach the same asymptote. The slope of the asymptote is −10/3, which is the same
curve that one obtains in the N →∞limit from Eqs. (4.1) and (4.2); i.e. the O(N) vector
model points will lie on that line for large values of N.
To obtain stronger bounds on the central charge we can introduce larger gaps in the
operator spectrum. In the plots of Fig. 5 we assumed that the gap in the singlet scalar
spectrum saturates the bound obtained in subsection 4.1, while the gap in the symmetric
tensor scalar spectrum is kept at ∆T ≥1. At low values of ∆φ, the bounds again approach
the same asymptote and in general don’t diﬀer too much from the bounds in the Fig. 4.
However, here the bounds exhibit a change in the slope at certain value of ∆φ. At larger
values of ∆φ the bounds are much stronger than the ones in Fig. 4. For large N values the
change in the slope occurs at the O(N) vector model points. At smaller N the change in the
15


--- Page 16 ---

c/Ncfree
∆φ
Ising
O(10)
O(20)
O(2)
O(3)
O(4)
O(5)
O(6)
O(N) Central Charge Bounds With Singlet Gap
0.505
0.515
0.51
0.525
0.52
0.535
0.53
0.5
1.025
1
0.875
0.925
0.95
0.975
0.9
Figure 5: Lower bounds on the central charge for theories containing a scalar φ transforming
as a vector under O(N). In this ﬁgure we assume that the lowest dimension singlet scalar
operators saturate the bounds we found in subsection 4.1, while the symmetric tensor scalar
operators are assumed to have dimensions ∆T ≥1. The black crosses show the predictions
in Eq. (4.1) from the large-N expansion for N = 10, 20, ..., 100. The dashed line shows the
asymptotic behavior of the central charge as a function of ∆φ as N →∞.
slope is more gradual, but still occurs at ∆φ close to the known values in the O(N) models.
Just like the operator dimensions in the previous subsections, the central charge values
obtained using 1/N expansion in the case of large N lie very close to the boundary line.
Encouraged by these facts, we can conjecture that the central charge values will lie on the
boundary even for low values of N. Using the values of ∆φ determined by other methods, we
can then make a prediction for the values of c. These are shown, along with our bootstrap
predictions for ∆S and ∆T, in Table 3. In calculating these values of c we used the bounds
of Fig. 5, since these are our strongest bounds for the O(N) vector models.
5
Discussion
Let us take a moment to reﬂect on the results of the previous section.
First, we have
discovered the remarkable fact that operator dimensions in the critical O(N) vector models
take on values which saturate general constraints from crossing symmetry and unitarity.
This gives us an organizing principle by which we can understand why these theories
are special – gaps in the spectrum of operator dimensions are maximized in a way that
16


--- Page 17 ---

N
∆φ
∆S
∆T
c/Ncfree
1
0.51813(5)
1.4119+0.0005
−0.0015
–
0.946600+0.000022
−0.000015
2
0.51905(10)
1.5118+0.0012
−0.0022
1.23613+0.00058
−0.00158
0.94365+0.00013
−0.00010
3
0.51875(25)
1.5942+0.0037
−0.0047
1.2089+0.0013
−0.0023
0.94418+0.00043
−0.00036
4
0.51825(50)
1.6674+0.0077
−0.0087
1.1864+0.0024
−0.0034
0.94581+0.00071
−0.00039
5
0.5155(15)
1.682+0.047
−0.048
1.1568+0.009
−0.010
0.9520+0.0040
−0.0030
6
0.5145(15)
1.725+0.052
−0.053
1.1401+0.0085
−0.0095
0.9547+0.0041
−0.0027
10
0.51160
1.8690+0.000
−0.001
1.1003+0.000
−0.001
0.96394
20
0.50639
1.9408+0.000
−0.001
1.0687+0.000
−0.001
0.97936
Table 3: The values of the scalar and symmetric tensor operator dimensions and the values
of the central charge saturating the obtained bound for the O(N) vector model values of ∆φ.
For N = 1, 2, 3, 4, 5, 6, the value of ∆φ is taken from Table 2; for N = 10, 20 the value of ∆φ
is the 3-loop large-N result. The errors reﬂect the uncertainty in the value of ∆φ. In the
determinations of ∆S,T we have also included a contribution to the error due to our bisection
precision of 0.001. This uncertainty is only in one direction, since the upper bound is rigorous.
is consistent with unitarity. It will be interesting to verify that this trend continues for
operators of higher dimension. A promising approach to extracting more of the spectrum in
these theories is to consider the locations of the zeros of α(VO) along the boundary [7, 15, 17].
We hope to develop this approach in the O(N) models in future work.
As far as we are aware, we have presented the ﬁrst predictions for the central charge in the
O(N) models at small values of N. In doing so we have veriﬁed that there is approximately
linear growth with N and that c < Ncfree for each value of N. It will be interesting if these
predictions can be veriﬁed in lattice simulations of the O(N) models – this will require a
robust lattice construction of the stress-energy tensor, which is a worthwhile task in its own
right. One can also easily extend these methods to determine the ﬂavor central charges,
appearing in ⟨JµJν⟩∝τ, where Jµ is the O(N) current.
In this work we only considered the constraints from crossing symmetry of ⟨φiφjφkφl⟩. It
is very interesting to extend this analysis to include constraints from other correlators, such
as ⟨φiφjφ2φ2⟩, ⟨φ2φ2φ2φ2⟩, ⟨φiφjJµJν⟩, or ⟨φiφjT µνT ρσ⟩. Such extensions are e.g. necessary
in order to learn about the Z2 odd operators in the spectrum of these theories. Studying
these correlators may also help to give a sharper criterion that can be used to determine
the value of ∆φ in the O(N) models, going beyond the fact that it appears to take a value
near (somewhat smooth) changes in slope of the general bounds.
The recursion representation for the conformal blocks presented in section 3 is a powerful
and eﬃcient method of computing conformal blocks in any number of dimensions. This
representation can for example be utilized in studies of CFTs that interpolate between
2 < D < 4, in D = 5, 7 where conformal blocks are similarly complicated, or perhaps in
17


--- Page 18 ---

constructing an argument (extending [55]) that nontrivial CFTs in large D do not exist.
The representation as a sum over poles in ∆may also be useful for making general analytic
arguments (going beyond the large spin arguments of [56, 57]) in the context of the conformal
bootstrap. The sum over poles may also be particularly interesting from the perspective of
Mellin amplitudes [58–66].
Finally, we’d like to emphasize that the O(N) models at N = 2, 3 have numerous
beautiful realizations in experimental condensed matter systems.
E.g., the O(2) model
describes the superﬂuid transition in 4He and the bicritical point in uniaxial magnets
such as GdAlO3, while the O(3) model describes the Curie transition in simple isotropic
magnets such as Ni, Fe, and EuO. Many more examples can be found in [20]. Thus, the
conformal bootstrap in 3D allows one to realize the physicists’ dream – it makes quantitative
predictions in strongly-interacting systems that can be experimentally tested!
Acknowledgements
We are grateful to Rich Brower, Sheer El-Showk, George Fleming, Liam Fitzpatrick, Fred
Hucht, Jared Kaplan, Miguel Paulos, Jo˜ao Penedones, Slava Rychkov, Leonardo Rastelli,
Balt van Rees, Alessandro Vichi, and Sasha Zhiboedov for discussions. We would also like
to thank the other organizers and participants in the Back to the Bootstrap 3 conference
at CERN. The work of DSD is supported by DOE grant number DE-SC0009988. DSD
would like to thank SLAC for hospitality while this work was completed. DP would like
to thank the Galileo Galilei Institute for Theoretical Physics and the INFN for hospitality
and partial support during the completion of this work. The computations in this paper
were run on the Bulldog computing clusters supported by the facilities and staﬀof the Yale
University Faculty of Arts and Sciences High Performance Computing Center, as well as
the Aurora computing cluster supported by the School of Natural Sciences Computing Staﬀ
at the Institute for Advanced Study.
18


--- Page 19 ---

A
Improving Rational Approximations
When truncated to a ﬁnite number of poles ∆i, the recursion relation
h∆,ℓ(r, η)
=
h(∞)
ℓ
(r, η) +
X
i
cirni
∆−∆i
h∆i+ni,ℓi(r, η)
(A.1)
gives a rational approximation for h∆,ℓ= r−∆g∆,ℓas a function of ∆. The precision of
this approximation increases as we include more and more poles ∆i. However, the degree
increases as well, and this can be problematic for computation. Larger degree polynomials
slow down semideﬁnite program solvers.
A useful compromise is to keep n poles ∆1, . . . , ∆n with the largest residues, and use
these as a basis to approximate other poles ∆j with smaller residues. That is, for poles with
small residues, we write
1
∆−∆j
≈
n
X
i=1
ai
∆−∆i
.
(A.2)
where the coeﬃcients ai are chosen to make the approximation as good as possible. In
this way, we can approximately include the contribution of the pole at ∆= ∆j without
increasing the degree of our rational function. Note that the ∆i lie below the unitarity
bound ∆unitarity, so ∆itself never approaches a pole when we compute CFT bounds.
How should we choose the coeﬃcients ai? We need Eq. (A.2) to hold to high accuracy
across all ∆≥∆unitarity (away from the singularities on both sides). A method that works
well in practice is to ensure that Eq. (A.2) and its ﬁrst n/2 derivatives hold exactly at
∆= ∆unitarity and at ∆= ∞. These conditions give n linear equations which determine the
ai.
In practice, including poles with residues less than 10−2 yields rational approximations
to conformal blocks which are correct to within 10−9. Including poles with residues less
than 10−10 yields approximations correct to within 10−22.
B
Implementation in Mathematica and SDPA-GMP
A brief summary of our implementation is as follows. All steps but the last are performed
in Mathematica.
1. We compute a rational approximation for derivatives of conformal blocks at the
crossing symmetric point ∂m
r ∂n
η g∆,ℓ≈r∆P (m,n)
ℓ
(∆)/Qℓ(∆), where r = 3 −2
√
2. This
can be done using either the Gegenbauer expansion and Casimir equation described
in [42], or more eﬃciently using our recursion relation (3.10). The recursion relation
can be implemented numerically in the space of vectors of (r, η) derivatives, where
multiplication by rk is a matrix on this space. We compute approximations up to
order 60 in the r-expansion.
19


--- Page 20 ---

2. We approximate “small” poles as described in Appendix A, resulting in a new rational
approximation with smaller degree ∂m
r ∂n
η g∆,ℓ≈r∆p(m,n)
ℓ
(∆)/qℓ(∆). To decide which
poles to keep and which poles to approximate, we choose a threshold value θ and
compute
θi
≡
max
m,n

Res∆→∆i ∂m
r ∂n
η g∆,ℓ
∂m
r ∂nη g∆,ℓ|∆=unitarity bound
 .
(B.1)
Poles with θi ≤θ are approximated in terms of other poles. In practice, we found
that θ = 10−2 gives a good tradeoﬀbetween accuracy and speed. We have checked
that our results remain essentially unchanged as θ is varied between 10−2 and 10−12.
After reducing the degree of our rational approximation, the factors r∆/qℓ(∆) can be
discarded. Henceforth, we will use “∼” to indicate approximate equality up to an
overall positive function of ∆.
3. To compute a bound, we need derivatives ∂m
z ∂n
z VR,∆,ℓ, where the vectors VR,∆,ℓare
deﬁned in (2.5). These are linearly related to the vectors of derivatives of g∆,ℓ,
∂m
z ∂n
z VR,∆,ℓ,i
∼
(MR,∆φ)mn
i
rsP (r,s)
ℓ
(∆)
(B.2)
where i = 1, 2, 3 runs over the components of VR,∆,ℓand MR,∆φ is a matrix depending
only on the representation/channel R and the external operator dimension ∆φ.
4. In section 2.3, we deﬁned a semideﬁnite program as an aﬃne optimization problem
which can include constraints of the form
α(Pi(x)) for all x ≥0, where Pi(x) are polynomials in x.
(B.3)
More precisely, semideﬁnite programs can include matrix inequalities of the form
X is positive semideﬁnite,
(B.4)
where X is a matrix of variables (which might be subject to additional linear con-
straints).
The transformation of a set of inequalities from the form (B.3) to the
form (B.4) is standard in the optimization literature [67] and is described in detail in
[11]. We transform our polynomial inequalities
ai
mn(MR,∆φ)mn
i
rsP (r,s)
ℓ
(∆min,ℓ+ x) ≥0
for x ≥0
(B.5)
into matrix inequalities in this fashion.
5. Once written in terms of matrix inequalities, our semideﬁnite program can be solved
using a variety of freely available tools. For this work, we use the solver SDPA-GMP
with the parameters listed in Table 4. Our plots are computed in parallel by assigning
each point to an individual cluster node. Our cluster management software is written
in Cloud Haskell.
20


--- Page 21 ---

parameter
value
maxIteration
1000
epsilonStar
10−20 (10−10)
lambdaStar
1020
omegaStar
1020
lowerBound
−1040
upperBound
1040
betaStar
0.1
betaBar
0.3
gammaStar
0.7
epsilonDash
10−20 (10−10)
precision
200 (300)
Table 4: SDPA-GMP parameters used in the calculation of the operator dimension bounds. In
parentheses are the values of the parameters used in the central charge bounds.
References
[1] A. A. Belavin, A. M. Polyakov, and A. B. Zamolodchikov, “Inﬁnite conformal
symmetry in two-dimensional quantum ﬁeld theory,”
Nucl. Phys. B241 (1984) 333–380.
[2] S. Ferrara, A. F. Grillo, and R. Gatto, “Tensor representations of conformal algebra
and conformally covariant operator product expansion,”
Annals Phys. 76 (1973) 161–188.
[3] A. M. Polyakov, “Nonhamiltonian approach to conformal quantum ﬁeld theory,” Zh.
Eksp. Teor. Fiz. 66 (1974) 23–42.
[4] R. Rattazzi, V. S. Rychkov, E. Tonni, and A. Vichi, “Bounding scalar operator
dimensions in 4D CFT,” JHEP 12 (2008) 031, arXiv:0807.0004 [hep-th].
[5] V. S. Rychkov and A. Vichi, “Universal Constraints on Conformal Operator
Dimensions,” Phys. Rev. D80 (2009) 045006, arXiv:0905.2211 [hep-th].
[6] F. Caracciolo and V. S. Rychkov, “Rigorous Limits on the Interaction Strength in
Quantum Field Theory,” Phys. Rev. D81 (2010) 085037,
arXiv:0912.2726 [hep-th].
[7] D. Poland and D. Simmons-Duﬃn, “Bounds on 4D Conformal and Superconformal
Field Theories,” JHEP 1105 (2011) 017, arXiv:1009.2087 [hep-th].
21


--- Page 22 ---

[8] R. Rattazzi, S. Rychkov, and A. Vichi, “Central Charge Bounds in 4D Conformal
Field Theory,” Phys. Rev. D83 (2011) 046011, arXiv:1009.2725 [hep-th].
[9] R. Rattazzi, S. Rychkov, and A. Vichi, “Bounds in 4D Conformal Field Theories with
Global Symmetry,” J. Phys. A44 (2011) 035402, arXiv:1009.5985 [hep-th].
[10] A. Vichi, “Improved bounds for CFT’s with global symmetries,”
JHEP 1201 (2012) 162, arXiv:1106.4037 [hep-th].
[11] D. Poland, D. Simmons-Duﬃn, and A. Vichi, “Carving Out the Space of 4D CFTs,”
JHEP 1205 (2012) 110, arXiv:1109.5176 [hep-th].
[12] S. Rychkov, “Conformal Bootstrap in Three Dimensions?,”
arXiv:1111.2115 [hep-th].
[13] S. El-Showk, M. F. Paulos, D. Poland, S. Rychkov, D. Simmons-Duﬃn, and A. Vichi,
“Solving the 3D Ising Model with the Conformal Bootstrap,”
Phys.Rev. D86 (2012) 025022, arXiv:1203.6064 [hep-th].
[14] P. Liendo, L. Rastelli, and B. C. van Rees, “The Bootstrap Program for Boundary
CFTd,” JHEP 07 (2013) 113, arXiv:1210.4258 [hep-th].
[15] S. El-Showk and M. F. Paulos, “Bootstrapping Conformal Field Theories with the
Extremal Functional Method,” Phys. Rev. Lett. 111 no. 24, (2013) 241601,
arXiv:1211.2810 [hep-th].
[16] C. Beem, L. Rastelli, and B. C. van Rees, “The N = 4 Superconformal Bootstrap,”
Phys. Rev. Lett. 111 (2013) 071601, arXiv:1304.1803 [hep-th].
[17] S. El-Showk, M. F. Paulos, D. Poland, S. Rychkov, D. Simmons-Duﬃn, and A. Vichi,
“Solving the 3d Ising Model with the Conformal Bootstrap II. c-Minimization and
Precise Critical Exponents,” J. Stat. Phys. 157 (2014) 869,
arXiv:1403.4545 [hep-th].
[18] E. Brezin, D. Wallace, and K. Wilson, “FEYNMAN-GRAPH EXPANSION FOR
THE EQUATION OF STATE NEAR THE CRITICAL POINT,”
Phys.Rev. B7 (1973) 232–239.
[19] K. Wilson and J. B. Kogut, “The Renormalization group and the epsilon expansion,”
Phys.Rept. 12 (1974) 75–200.
[20] A. Pelissetto and E. Vicari, “Critical phenomena and renormalization-group theory,”
Phys. Rept. 368 (2002) 549–727, arXiv:cond-mat/0012164.
[21] M. Moshe and J. Zinn-Justin, “Quantum ﬁeld theory in the large N limit: A Review,”
Phys.Rept. 385 (2003) 69–228, arXiv:hep-th/0306133 [hep-th].
[22] I. Klebanov and A. Polyakov, “AdS dual of the critical O(N) vector model,”
Phys.Lett. B550 (2002) 213–219, arXiv:hep-th/0210114 [hep-th].
22


--- Page 23 ---

[23] K. Lang and W. Ruhl, “Field algebra for critical O(N) vector nonlinear sigma models
at 2 < d < 4,” Z.Phys. C50 (1991) 285–292.
[24] K. Lang and W. Ruhl, “Anomalous dimensions of tensor ﬁelds of arbitrary rank for
critical nonlinear O(N) sigma models at 2 < d < 4 to ﬁrst order in 1/N,”
Z.Phys. C51 (1991) 127–132.
[25] K. Lang and W. Ruhl, “The Critical O(N) Sigma Model at Dimension 2 < D < 4
and Order 1/N2: Operator Product Expansions and Renormalization,”
Nucl. Phys. B377 (1992) 371–404.
[26] K. Lang and W. Ruhl, “The Scalar ancestor of the energy momentum ﬁeld in critical
sigma models at 2 < d < 4,” Phys.Lett. B275 (1992) 93–98.
[27] K. Lang and W. Ruhl, “The Critical O(N) sigma model at dimensions 2 < d < 4 :
Fusion coeﬃcients and anomalous dimensions,” Nucl.Phys. B400 (1993) 597–623.
[28] K. Lang and W. Ruhl, “Critical nonlinear O(N) sigma models at 2 < d < 4: The
Degeneracy of quasiprimary ﬁelds and it resolution,” Z.Phys. C61 (1994) 495–510.
[29] A. Petkou, “Conserved currents, consistency relations and operator product
expansions in the conformally invariant O(N) vector model,”
Annals Phys. 249 (1996) 180–221, arXiv:hep-th/9410093 [hep-th].
[30] A. C. Petkou, “C(T) and C(J) up to next-to-leading order in 1/N in the conformally
invariant O(N) vector model for 2 < d < 4 ,” Phys.Lett. B359 (1995) 101–107,
arXiv:hep-th/9506116 [hep-th].
[31] J. Maldacena and A. Zhiboedov, “Constraining Conformal Field Theories with A
Higher Spin Symmetry,” J. Phys. A46 (2013) 214011, arXiv:1112.1016 [hep-th].
[32] J. Maldacena and A. Zhiboedov, “Constraining conformal ﬁeld theories with a
slightly broken higher spin symmetry,” Class. Quant. Grav. 30 (2013) 104003,
arXiv:1204.3882 [hep-th].
[33] A. Zamolodchikov, “CONFORMAL SYMMETRY IN TWO-DIMENSIONS: AN
EXPLICIT RECURRENCE FORMULA FOR THE CONFORMAL PARTIAL
WAVE AMPLITUDE,” Commun.Math.Phys. 96 (1984) 419–422.
[34] A. Zamolodchikov, “Conformal symmetry in two-dimensional space: Recursion
representation of conformal block,”
Theoretical and Mathematical Physics 73 no. 1, (1987) 1088–1093.
[35] S. Ferrara, R. Gatto, and A. F. Grillo, “Positivity Restrictions on Anomalous
Dimensions,” Phys. Rev. D9 (1974) 3564.
[36] G. Mack, “All Unitary Ray Representations of the Conformal Group SU(2,2) with
Positive Energy,” Commun.Math.Phys. 55 (1977) 1.
23


--- Page 24 ---

[37] R. R. Metsaev, “Massless Mixed Symmetry Bosonic Free Fields in D- Dimensional
Anti-de Sitter Space-Time,” Phys. Lett. B354 (1995) 78–84.
[38] S. Minwalla, “Restrictions Imposed by Superconformal Invariance on Quantum Field
Theories,” Adv. Theor. Math. Phys. 2 (1998) 781–846, arXiv:hep-th/9712074.
[39] B. Grinstein, K. A. Intriligator, and I. Z. Rothstein, “Comments on Unparticles,”
Phys. Lett. B662 (2008) 367–374, arXiv:0801.1140 [hep-ph].
[40] F. Dolan and H. Osborn, “Conformal four point functions and the operator product
expansion,” Nucl.Phys. B599 (2001) 459–496, arXiv:hep-th/0011040 [hep-th].
[41] F. Dolan and H. Osborn, “Conformal partial waves and the operator product
expansion,” Nucl.Phys. B678 (2004) 491–507, arXiv:hep-th/0309180 [hep-th].
[42] M. Hogervorst and S. Rychkov, “Radial Coordinates for Conformal Blocks,”
Phys. Rev. D87 (2013) 106004, arXiv:1303.1111 [hep-th].
[43] F. Dolan and H. Osborn, “Conformal Partial Waves: Further Mathematical Results,”
arXiv:1108.6194v2 [hep-th].
[44] M. S. Costa, J. Penedones, D. Poland, and S. Rychkov, “Spinning Conformal
Correlators,” JHEP 1111 (2011) 071, arXiv:1107.3554 [hep-th].
[45] M. S. Costa, J. Penedones, D. Poland, and S. Rychkov, “Spinning Conformal Blocks,”
JHEP 1111 (2011) 154, arXiv:1109.6321 [hep-th].
[46] D. Simmons-Duﬃn, “Projectors, Shadows, and Conformal Blocks,”
JHEP 04 (2014) 146, arXiv:1204.3894 [hep-th].
[47] M. Hasenbusch, “Finite size scaling study of lattice models in the three-dimensional
Ising universality class,” Phys.Rev. B82 (2010) 174433,
arXiv:1004.4486 [cond-mat].
[48] M. Campostrini, A. Pelissetto, P. Rossi, and E. Vicari, “25th order high temperature
expansion results for three-dimensional Ising like systems on the simple cubic lattice,”
Phys.Rev. E65 (2002) 066127, arXiv:cond-mat/0201180 [cond-mat].
[49] M. Campostrini, M. Hasenbusch, A. Pelissetto, and E. Vicari, “The Critical
exponents of the superﬂuid transition in He-4,” Phys.Rev. B74 (2006) 144506,
arXiv:cond-mat/0605083 [cond-mat].
[50] P. Calabrese and P. Parruccini, “Harmonic crossover exponents in O(n) models with
the pseudo-epsilon expansion approach,” Phys.Rev. B71 (2005) 064416,
arXiv:cond-mat/0411027 [cond-mat].
[51] M. Campostrini, M. Hasenbusch, A. Pelissetto, P. Rossi, and E. Vicari, “Critical
exponents and equation of state of the three-dimensional Heisenberg universality
class,” Phys.Rev. B65 (2002) 144520, arXiv:cond-mat/0110336 [cond-mat].
24


--- Page 25 ---

[52] M. Hasenbusch, “Eliminating leading corrections to scaling in the three-dimensional
O(N) symmetric phi**4 model: N=3 and N=4,” J.Phys. A34 (2001) 8221–8236,
arXiv:cond-mat/0010463 [cond-mat].
[53] A. Butti and F. Parisen Toldin, “The Critical equation of state of the
three-dimensional O(N) universality class: N > 4,” Nucl.Phys. B704 (2005) 527–551,
arXiv:hep-lat/0406023 [hep-lat].
[54] J. Gracey, “Crossover exponent in O(N) phi**4 theory at O(1 / N**2),”
Phys.Rev. E66 (2002) 027102, arXiv:cond-mat/0206098 [cond-mat].
[55] A. L. Fitzpatrick, J. Kaplan, and D. Poland, “Conformal Blocks in the Large D
Limit,” JHEP 08 (2013) 107, arXiv:1305.0004 [hep-th].
[56] A. L. Fitzpatrick, J. Kaplan, D. Poland, and D. Simmons-Duﬃn, “The Analytic
Bootstrap and AdS Superhorizon Locality,” JHEP 12 (2013) 004,
arXiv:1212.3616 [hep-th].
[57] Z. Komargodski and A. Zhiboedov, “Convexity and Liberation at Large Spin,”
JHEP 11 (2013) 140, arXiv:1212.4103 [hep-th].
[58] G. Mack, “D-independent representation of Conformal Field Theories in D
dimensions via transformation to auxiliary Dual Resonance Models. Scalar
amplitudes,” arXiv:0907.2407 [hep-th].
[59] G. Mack, “D-dimensional Conformal Field Theories with anomalous dimensions as
Dual Resonance Models,” Bulg.J.Phys. 36 (2009) 214–226,
arXiv:0909.1024 [hep-th].
[60] J. Penedones, “Writing CFT correlation functions as AdS scattering amplitudes,”
JHEP 1103 (2011) 025, arXiv:1011.1485 [hep-th].
[61] A. L. Fitzpatrick, J. Kaplan, J. Penedones, S. Raju, and B. C. van Rees, “A Natural
Language for AdS/CFT Correlators,” JHEP 1111 (2011) 095,
arXiv:1107.1499 [hep-th].
[62] M. F. Paulos, “Towards Feynman rules for Mellin amplitudes,”
JHEP 1110 (2011) 074, arXiv:1107.1504 [hep-th].
[63] A. L. Fitzpatrick and J. Kaplan, “Analyticity and the Holographic S-Matrix,”
JHEP 1210 (2012) 127, arXiv:1111.6972 [hep-th].
[64] A. L. Fitzpatrick and J. Kaplan, “Unitarity and the Holographic S-Matrix,”
JHEP 1210 (2012) 032, arXiv:1112.4845 [hep-th].
[65] M. F. Paulos, M. Spradlin, and A. Volovich, “Mellin Amplitudes for Dual Conformal
Integrals,” JHEP 1208 (2012) 072, arXiv:1203.6362 [hep-th].
[66] A. L. Fitzpatrick and J. Kaplan, “AdS Field Theory from Conformal Field Theory,”
JHEP 1302 (2013) 054, arXiv:1208.0337 [hep-th].
25


--- Page 26 ---

[67] L. Vandenberghe and S. Boyd, “Semideﬁnite Programming,” SIAM Rev. 38 (1996) 49.
26
```

---

## Section 1.8: DELVING INTO LLMS’ VISUAL UNDERSTANDING ABILITY

Source File: DELVING INTO LLMS’ VISUAL UNDERSTANDING ABILITY.txt

### Paper Content:

```
# DELVING INTO LLMS’ VISUAL UNDERSTANDING ABILITY.pdf
# Converted: 2025-07-19 12:45:28
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/DELVING INTO LLMS’ VISUAL UNDERSTANDING ABILITY.pdf
# Output: ../layer2_completion/txt/DELVING INTO LLMS’ VISUAL UNDERSTANDING ABILITY.txt


--- Page 1 ---

Under review as a conference paper at ICLR 2024
DELVING INTO LLMS’ VISUAL UNDERSTANDING ABILITY
USING SVG TO BRIDGE IMAGE AND TEXT
Anonymous authors
Paper under double-blind review
ABSTRACT
Large language models (LLMs) have made signiﬁcant advancements in natural language
understanding. However, through that enormous semantic representation that the LLM has
learnt, is it somehow possible for it to understand images as well? This work investigates
this question. To enable the LLM to process images, we convert them into a representation
given by Scalable Vector Graphics (SVG). To study what the LLM can do with this XML-
based textual description of images, we test the LLM on three broad computer vision tasks:
visual reasoning, image classiﬁcation under distribution shift, and generating new images
using visual prompting. Even though we do not naturally associate LLMs with any visual
understanding capabilities, our results indicate that the LLM can indeed do a pretty decent
job in many of these tasks, potentially opening new avenues for research into LLMs ability
to understand images.
<?xml version="1.0" encoding="iso-8859-1"?>
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg"
xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
viewBox=“0 0 60 60” style=“enable-background:new 0 0 54 54;”
xml:space=“preserve”> </svg>
<rect id="Rectangle" data-name="Rectangle" width="58" height="58"
rx="2" transform="translate(1 1)" fill="white" stroke="skyblue" stroke-
miterlimit="10" stroke-width="1"/>
<ellipse style="fill:#88C057;" cx="30" cy="40" rx="27" ry="11.5"/>
<ellipse style="fill:#659C35;" cx="22" cy="40.833" rx="18" ry="7.667"/>
<ellipse style="fill:#38454F;" cx="24.071" cy="39.222" rx="6.071"
ry="4.722"/>
<polygon style="fill:#E64C3C;" points="21,3.5 40,8.5 21,14.5 "/>
<circle style="fill:#FFFFFF;" cx="43" cy="42.5" r="3"/>
<line style="fill:none;stroke:#ECF0F1;stroke-width:2;stroke-
linecap:round;stroke-miterlimit:10;" x1="22" y1="38.5" x2="22"
y2="3.5"/>
File header
Rectangle  
Polygon 
Line 
Image Border
flag
Circle 
flagpole
golf
grass
Set of ellipses 
(a)
(b)
Figure 1: (a) An SVG representation illustrating a golf course. Each geometric shape represents a distinct
object. (b) LLMs are able to understand and generate shapes, color, and relationships between different
elements in an interactive manner.
1
INTRODUCTION
Large-scale data and enormous compute: the effect of these two ingredients has been on display in recent
years in the signiﬁcantly increased capability of machine learning systems. Models operating on the two
most popular forms of data - image and text - have particularly felt that effect the most. From the side
dealing with textual data, we have seen the emergence of large language models (LLMs) such as Chat-
1


--- Page 2 ---

Under review as a conference paper at ICLR 2024
GPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b). Similarly, the vision side has also shown impressive
accomplishments (Dosovitskiy et al., 2020; Liu et al., 2022; Dehghani et al., 2023).
However, when we compare these two realms, the abilities of LLMs do stand out in a distinct way because
of their remarkable abilities in reasoning, in-context learning, and open-ended tasks (Bubeck et al., 2023).
These analytical capabilities are something that the vision models, despite their signiﬁcant advances, have not
yet mirrored to the same depth (Dehghani et al., 2023; Alayrac et al., 2022).
This distinction can be attributed to the inherent nature of their respective data: LLMs thrive on the diverse and
sequential structure of textual data, which is conducive to understanding intricate relationships and producing
contextually relevant responses. In contrast, the continuous and varied nature of visual data complicates the
discernment of nuanced relationships, potentially hindering the depth of analysis LVMs can achieve (Bar
et al., 2022; Diwan et al., 2022; Lake et al., 2017). Moreover, there is an ongoing debate on whether LLMs,
trained on internet-scale data, can learn world models that could lead to AGI capabilities, or that they are
fundamentally limited due to their lack of grounding on physical interaction and visual perception in the real
world (LeCun, 2022).
The disparity between LLMs and LVMs, and the debate on the necessity of physical interaction and perceptual
grounding, intrigue us to a question: Can LLMs, which have never seen visual data, understand and reason
about images? Answering this question will bring us closer to understanding the capabilities of LLMs beyond
the textual domain, whether they possess world models, and what their fundamental limitations may be. As
such, our study takes a small but important step toward this goal.
In order to enable an off-the-shelf pre-trained LLM to “read” images, we use the Scalable Vector Graphics
(SVG) (Ferraiolo et al., 2000) representation to convert images into readable text. Unlike traditional pixel-
based images, SVGs are described in XML, offering a text-based portrayal of mid-level shapes, curves, lines,
and colors, as shown in Figure 1. The textual nature of SVG provides a data modality that LLMs excel at,
acting as a bridge to apply their analytical strengths to the visual domain. While the Sparks of AGI paper
(Bubeck et al., 2023) showed some initial qualitative results on the image understanding capabilities of LLMs
using a similar idea, we provide a deeper, comprehensive study that includes both qualitative and quantitative
analyses on a variety of visual understanding and reasoning tasks.
Speciﬁcally, we evaluate whether an LLM can perform both discriminative and generative visual understanding
tasks. For discriminative tasks, we study their visual reasoning capability as well as few-shot in-context
learning performance for image classiﬁcation tasks, and measure their robustness to distribution shifts in
visual data. Surprisingly, despite never having seen dense visual data, LLMs perform much better than
chance and are often robust to distribution shifts. For generative tasks, we study LLMs’ image generation and
editing capabalities based on interactive, chat-based feedback. We ﬁnd that LLMs can identify and execute
transformations related to color, shape, style, and content within the SVG image representation, to generate
credible outcomes.
2
RELATED WORK
2.1
LEVERAGE LLMS FOR VISUAL TASKS
Upon observing the powerful reasoning capabilities of LLM, researchers began to harness its potential for
visual tasks. Presently, there are three primary approaches to utilizing LLM for these purposes: 1. The ﬁrst
approach involves using LLM to produce textual guidelines. Vision models then rely on these instructions to
execute a range of visual tasks. Examples include Visual ChatGPT (Wu et al., 2023), visual programming as
seen in Gupta & Kembhavi (2023), and ViperGPT (Sur´ıs et al., 2023). 2. The second approach, as illustrated
by LLaVa (Liu et al., 2023) and MiniGPT4 (Zhu et al., 2023), incorporates the pretrained vision encoder
model, along with a trainable linear projector. This allows for feeding visual features directly to LLMs,
demonstrating remarkable reasoning abilities. VisionLLM (Wang et al., 2023) presents bounding boxes
2


--- Page 3 ---

Under review as a conference paper at ICLR 2024
and segmentation masks in text format (as polygons), enabling LLMs to address more intricate perception
challenges. 3. The third approach seeks to represent images directly in a text-based format, bypassing the use
of visual encoders. The goal here is to allow LLMs to interpret these text-based representations. For instance,
LIFT (Dinh et al., 2022) represents images using their raw pixel values in textual form and then ﬁne-tunes the
language model on them for visual tasks. Another study (Bubeck et al., 2023) explores image generation by
expressing the image in text formats, like TiKZ or SVG.
Key difference: Our research aligns with the third approach. Unlike LIFT (Dinh et al., 2022), we represent
images using SVG, a format that inherently encodes more structural information than raw pixel values. This
could enable LLMs to better grasp intricate relationships and yield contextually relevant responses. Distinct
from the methods presented in Bubeck et al. (2023), we conduct a comprehensive study of how LLMs process
images via textual representations, including both discriminative and generative tasks.
2.2
SCALABLE VECTOR GRAPHICS
Vector graphics describe images as collections of parameterized shape primitives such as polygons, circles,
and rectangles, rather than a regular raster grid of pixel values (Peng & Zhang, 2004). This representation is
extensively supported by web browsers and can be rendered without any special software or plugins (Badros
et al., 2001). Primitives are usually characterized by a set of coordinates delineating their contour and the
associated color. This leads to a compact and inﬁnitely scalable representation where the appearance can
be easily modiﬁed by adjusting stroke or color parameters. Consequently, vector graphics are the preferred
choice among graphic artists and designers, as images maintain their sharpness regardless of the zoom
level. Encapsulated PostScript (EPS) and Scalable Vector Graphics (SVG) are two notable vector-based
formats (Ferraiolo et al., 2000).
SVG format stores images as XML-based text ﬁles that deﬁne geometrical objects and their properties (Fer-
raiolo et al., 2000), shown in Figure 1. This enables easy editing, manipulation, and embedding, which makes
SVG particularly versatile for web applications and graphic design tasks (Badros et al., 2001). EPS is another
vector format for high-quality graphics that can be resized without losing quality (Gruber et al., 2008). In this
paper, we employ large language models (LLMs) to understand images in the SVG format, achieving robust
shape-color debiasing along with enhanced visual understanding and generation.
2.3
LARGE LANGUAGE MODELS
Large Language Models (LLMs) have attracted much attention in recent years due to their remarkable
performance across numerous natural language processing tasks. GPT-3 (Brown et al., 2020a), developed
by OpenAI, is a prime example of this category, boasting an immense scale of 175 billion parameters and
human-like text generation capabilities. In a similar vein, BERT (Devlin et al., 2019) (Bidirectional Encoder
Representations from Transformers), introduced by Google, takes advantage of the transformer architecture
and has substantially enhanced the state-of-the-art across various tasks by learning deep bidirectional repre-
sentations. ChatGPT (OpenAI, 2023a), another noteworthy model, is a GPT variant speciﬁcally designed
for human-like conversational abilities. The most recent iteration, GPT-4 (OpenAI, 2023b), succeeds GPT-
3 (Brown et al., 2020b) and carries on the LLM advancements in terms of scale and performance. These
models lay the groundwork for our research, enabling us to investigate their potential in more complex tasks
such as image processing and understanding. Our work effectively illustrates the applicability of LLMs
to SVG-based image understanding and generation, paving the way for novel applications and research
directions in the visual domain.
3
TASKS AND EXPERIMENTAL RESULTS
The domain of computer vision contains a variety of problems, and often times, models need to have different
kinds of abilities to solve them. In this section, we wish to investigate if LLMs can indeed have those required
3


--- Page 4 ---

Under review as a conference paper at ICLR 2024
(a) In-distribution test set
(c) Out-of-distribution test set on shape
(d) Out-of-distribution test set on color
Unary: 
Q: What is the shape of the green object? 
A: Rectangle
Binary:
Q: How many objects have the same shape as the red object?
A: 4
Ternary:
Q: Is there an object lies on the line connecting the blue 
and gray objects?
A: Yes
(b) Sample questions from in-distribution test set
Figure 2: Illustration of In-Distribution and Out-of-Distribution Test Sets: (a) Images from the in-distribution
test set, showcasing random sampling of object color, shape, and location. (b) Accompanying each image
are questions assessing unary, binary, and ternary reasoning capabilities. (c) Expansion of shape variety to
include ellipses alongside rectangles and circles. (d) Introduction of additional colors, including magenta,
black, and purple, to the sampling palette.
abilities. However, for them to solve a computer vision problem, there needs to be a way for them to see an
image. SVG can be that bridge, where an image is converted into a structured XML code (please see Fig. 1
for an example). And like any other code, LLMs can potentially read the code and perform some task. To
test what is possible using this form of image representation, we consider three broad categories of computer
vision tasks.
Sec. 3.1 ﬁrst studies the problem of visual reasoning, where the model is asked certain kinds of questions
about the contents of an input image (e.g., How many objects have the same shape as the red object?). Next,
in Sec 3.2, we study how LLMs fare in image classiﬁcation tasks; especially under non-trivial settings like
distribution shifts and few-shot learning. After studying their abilities in discriminative tasks, we then test
the generative modeling capabilities of LLMs through the task of visual prompting, which asks the model to
generate a new image following the pattern depicted by three images - A : B, C : ?.
Unless otherwise mentioned, all the experiments will use GPT-4 OpenAI (2023b) as the LLM model. The
common theme across all these different experiments will be that the LLM will be processing all the images
converted in the form of SVG.
3.1
VISUAL REASONING
With all the successes in the traditional perception tasks like image classiﬁcation/segmentation Liu et al.
(2022); Kirillov et al. (2023), visual reasoning still remains a pivotal challenge for many modern computer
vision systems Zhang et al. (2019); Barrett et al. (2018); Santoro et al. (2017). It typically refers to the ability
of a model to answer questions about different constituents of an image. To better understand what entails
that task, we will discuss the dataset we will be using for this experiment.
Dataset:
We use Sort-of-Clevr dataset that was introduced by Santoro et al. Santoro et al. (2017). As
shown in Figure 2, each image in the dataset is composed of 6 objects, each with a unique color, having
a randomly chosen shape between rectangle and circle. For each such image, the dataset contains many
{question, answer} pairs. There are three categories of questions crafted to test three levels of reasoning
capabilities: unary, binary, and ternary relationships. An example question testing binary reasoning ability is -
What is the shape of the object closest to the gray object? Please refer to Fig. 2 for more examples of the
questions. Each question is, ultimately, a classiﬁcation problem. The methods are evaluated based on their
top-1 accuracy on questions from the test set of the dataset. Please refer to appendix for the details about the
test sets used for evaluation.
4


--- Page 5 ---

Under review as a conference paper at ICLR 2024
Methods:
Broadly speaking, we show the results on two kinds of methods - (i) LLMs which are not trained
for this task, and are only prompted during inference, and (ii) methods which are trained for this task on the
training set of Sort-of-Clevr dataset. Among the ﬁrst category, we analyze (a) GPT4-brief (OpenAI, 2023b),
(b) GPT-CoT (Chain of thought) and (c) LLaVa (Liu et al., 2023). Among the second category, we study
two methods: (a) CNN+MLP and (b) Relation Networks (Santoro et al., 2017). The reason we choose to
evaluate on this category of trained models is to have an idea of what the upper bound could be; i.e., how
difﬁcult the task really is. To evaluate any of the LLMs during inference, we transform the original images
into geometric primitives into their SVG format. Then we query the LLM using the following prompt: “Give
the following SVG image <svg>...</svg>, what is the shape of the red object?” The difference between
GPT4-brief and GPT4-CoT is the way we ask the ﬁnal question: in GPT4-brief, as the name suggests, our
ﬁnal question asks the model to provide the answer brieﬂy, whereas in GPT4-CoT, we explicitly ask the
model to break down its reasoning before arriving at the answer (please see the appendix for the exact prompt
used to elicit this behavior). By querying a LLM, we obtained answers which were then summarized post
human evaluation to determine the ﬁnal accuracy. Due to costs associated with probing GPT4 models, our
evaluation was restricted to 120 examples.
Table 1: Visual reasoning performance of Sort-of-Clevr dataset under in-distribution test set and out-of-
distribution test set with shape and color distribution shift.
Distribution shift
i.i.d.
o.o.d. Shape
o.o.d. Color
Question type
GPT4-brief
GPT4-CoT
CNN
Rel. Net.
GPT4-CoT
CNN
Rel. Net.
GPT4-CoT
CNN
Rel. Net.
Image format
SVG
SVG
PNG
PNG
SVG
PNG
PNG
SVG
PNG
PNG
Unary
0.50
0.90
0.65
0.89
0.95
0.58
0.82
0.95
0.56
0.83
Binary
0.90
0.36
0.57
0.66
0.95
0.36
0.44
0.95
0.57
0.66
Ternary
0.10
0.52
0.47
0.54
0.63
0.52
0.56
0.71
0.47
0.54
Average
0.50
0.89
0.65
0.75
0.84
0.49
0.61
0.87
0.53
0.67
Table 1 shows the top-1 accuracy of different methods (left: zero-shot inference of LLMs, right: methods
trained for the task). Note that the results of LLaVa are shown in the appendix. When looking at the two
LLM models which process images in the SVG format, GPT4-brief and GPT-CoT, we can ﬁrst see that their
performance is much higher than chance (many questions in the test set have 5-6 correct answers, thereby
reducing the chance performance accuracy; please see the appendix). Furthermore, the performance of
GPT-CoT even surpasses the performance of a model explicitly trained for this task. If we take a step back
and think once more about the nature of SVG representation (Fig. 1), the best case scenario might be when
images from the Sort-of-Clevr dataset have the locations of certain shapes embedded in their XML code.
But even if such a nicely structured code is often available to the LLM, to properly be able to reason about
questions like the ones described in Fig. 2, the LLM needs to precisely perform many mathematical relational
operations - all without ever being told how to do it. From that perspective, the results depict that LLMs
might be possessing much complex models already.
Distribution shift: Furthermore, to study an even more difﬁcult version of the problem, we test the perfor-
mance of models in distribution shifts. Speciﬁcally, we evaluate under both shape and color distribution shifts.
As for the color, we replace 3 colors from the original 6 colors with the new colors. For shape, we randomly
enlarge the options to further include the ellipse and triangle, as shown in Figure 2 (c) (d). As a result, each
object can sample the shape uniformly from the 4 choices. Importantly, we make sure that all visual reasoning
questions can be answered using the original one-hot choices for vision models like CNN-MLP and relation
networks.
As shown in Table 1, the LLM model (GPT4-CoT) using the SVG format to process images does not suffer
much by any of the newly added complications in the test images (e.g., more shapes added under the shape
distribution shift), maintaining its ability to perform the reasoning tasks. Here is what this means in simple
5


--- Page 6 ---

Under review as a conference paper at ICLR 2024
Table 2: Image classiﬁcation results with vision and language model. We utilized the Mini-MNIST dataset,
which comprises 100 images, to evaluate GPT4’s ability to understand SVG through both zero-shot and
one-shot in-context learning. To evaluate the model’s robustness against distribution shift, vision model
ConvNeXt and language model Vicuna are ﬁnetuned on the MNIST training set, and evaluated on the MNIST
test set, CMNIST-A, and CMNIST-B respectively. CMNIST denotes the Colored-MNIST dataset. ICL
denotes in-context learning.
Method
ConvNeXt(ﬁne-tuning)
Vicuna(ﬁne-tuning)
GPT4(Zero-Shot)
GPT4(One-Shot ICL)
Image Format
PNG
SVG
SVG
SVG
MNIST
99.5%
99.1%
20%
24%
CMNIST-(A)
79.5%
95.7%
16%
19%
CMNIST-(B)
32.6%
92.9%
13%
20%
words, as understood through an example: if in the original image (before distribution shift), there was a
red circle immediately to the left of a blue rectangle, even after introducing other shapes (e.g., triangles),
the LLM can still detect the red circle to the left of that rectangle. This is not something which is trivial,
because the models which were explicitly trained on the Sort-of-Clevr dataset do suffer a non-trivial loss in
performance; both in color and shape distribution shift. Overall, these results indicate that the internal model
used by the LLM is surprisingly effective at tasks that we wouldn’t have naturally thought of it being good at.
3.2
OUT-OF-DISTRIBUTION GENERALIZATION
To DNNs, innocuous transformations can completely change predictions. This has been reported in var-
ious cases such as shifting the image by a few pixels (Azulay & Weiss, 2018), adding a bit of random
noise (Hendrycks & Dietterich, 2019) or changing the background, color, or texture (He et al., 2021; Arjovsky
et al., 2019; Geirhos et al., 2018) while keeping the shape intact. In this section, we aim to investigate if
representing images as SVG could mitigate these issues. Speciﬁcally, we study if models learn to rely on the
color or the background rather than the actual task (recognizing the shapes).
Datasets: We have constructed two variants of the Colored-MNIST dataset to assess model robustness against
color and background variations. The ﬁrst version, termed Colored-MNIST-A, assigns a color of either red or
green to the foreground, with each color being selected randomly at an equal likelihood of 50%. In the more
challenging second version, dubbed Colored-MNIST-B, both the background and foreground are selected
from a color palette that includes black, white, red, blue, and green. The background and foreground colors
are always distinct, yielding 20 unique color combinations. Visualization of these Colored-MNIST datasets
can be viewed in Figure 3. Furthermore, we utilize the curve tracing algorithm to convert MNIST images into
the SVG format. More details can be found in the supplementary materials.
Task and experimental setting: In the ﬁrst setup, we ﬁne-tune the ImageNet pre-trained vision model
ConvNeXt (Liu et al., 2022) using PNG images and the pre-trained language model Vicuna using SVG-
converted images on MNIST. Subsequent testing is carried out on both Colored-MNIST variants (A) and (B).
This setup seeks to examine whether the model can prioritize shape over other features for its predictions.
In the second setup, our objective is to explore the potential of harnessing the potent in-context capabilities
of Large Language Models (LLMs) to enhance image classiﬁcation using SVG. To this end, we employ
GPT-4 (OpenAI, 2023b) to conduct both zero-shot and in-context learning on MNIST variants. More detail
on ConvNeXt and Vicuna ﬁne-tuning, prompting for in context learning can be found in the supplementary
materials.
Results and discussion: In Table 2, ﬁne-tuning Vicunna with SVG representations has shown promising
results on the CMNIST-A and CMNIST-B benchmarks, achieving accuracies of 95.7% and 92.9% respectively.
6


--- Page 7 ---

Under review as a conference paper at ICLR 2024
Figure 3: Illustration of the Out-of-Distribution generalization tasks. We train models on the standard
Vanilla MNIST dataset and evaluate them on the more challenging OoD datasets: Colored-MNIST-(A) and
Colored-MNIST-(B). The goal is to determine whether models inadvertently prioritize color or background
over the primary task of shape recognition.
Figure 4: Synthetic data study results. The generation results of our method are annotated with a red square.
This suggests some level of robustness against color and background perturbations. On the other hand,
ConvNeXt seems more susceptible to these perturbations, with a noticeable decline in performance on both
benchmarks compared to i.i.d results. We hypothesize that SVG might offer a representation more biased
towards shape, given its explicit textual encoding of object shapes, allowing for disentanglement of shape
from color information. Further, as illustrated in Table 2, there’s a notable 4% accuracy boost when using a
single in-context sample, as compared to a zero-shot classiﬁcation approach. This demonstrates the capability
of LLM to grasp visual concepts contextually.
3.3
VISUAL PROMPTING
The last two sections discussed the emergent abilities of LLMs in discriminative tasks. In this section, we
turn our attention towards the generative side, to see if LLMs can understand and generate logically coherent
images as well. In particular, we consider the task of visual prompting, where, given a series of images, the
goal is to understand the transformation and ﬁll in the remaining spot with an appropriate image.
Dataset
We follow Bar et al. (2022) to create a set of three simple synthetic tasks of ﬁlling in the remaining
spot (Fig. 4), and three of their combinations, and evaluate each model on 100 examples per task.
Tasks and Evaluation.
Every pair in our example set includes an SVG showcasing a colored shape along
with a corresponding SVG with speciﬁc transformations. The transformations consist of color, size, or a
combination of these aspects. We delve into a more detailed description of each task in the appendix. For
evaluation purposes, we adopt the method from Bar et al. (2022), for measuring and reporting the color-aware
mean Intersection over Union (mIOU).
7


--- Page 8 ---

Under review as a conference paper at ICLR 2024
Table 3: Synthetic data study results. We report the color-aware mIOU on the six tasks (Bar et al., 2022). It
demonstrates that GPT4 is able to understand and reason shape, color, and size transformation using SVG
representation.
Method
Color
Shape
Size
Color Shape
Color Size
Shape Size
VQGAN (Esser et al., 2021)
7.0
19.1
16.2
7.4
2.2
18.4
BEiT (Bao et al., 2022)
40.9
31.4
7.1
33.1
21.2
13.0
MAE (He et al., 2022)
70.2
44.0
34.7
19.3
19.0
46.0
MAE-VQGAN (Bar et al., 2022)
40.4
46.5
42.0
20.4
18.3
40.3
SVG with GPT4
100.0
92.6
100.0
92.6
100.0
86.5
Prompt.
Given two example pairs and a query SVG, we structure the text prompt in the same fashion for
all tasks. The prompt is designed to ﬁgure out the common transformation in the two examples ﬁrst and then
transform that query into the corresponding key SVG code. We include the prompt details in the appendix.
Qualitative and quantitative results.
The results are presented in Table 3. See Figure 4 for our generated
results. We believe that GPT4 can clearly understand simple shape, color, and size transformations by
analyzing the SVG code without any pixel-level information.
3.3.1
STYLE AND CONTENT EXTRAPOLATION
In this section, we assess if LLMs can extrapolate SVG codes with more challenging transformations, such as
content and style.
Style generation: We present LLMs with sample SVG letters. The ﬁrst task is to ﬁgure out the style in the
given examples. Then, given a new test query, the second task is to transform this given query so that it
adheres to the same stylistic conventions as the example letters. The qualitative results can be found in the
appendix.
Content generation: LLMs are shown two examples of SVG code pairs. Each pair consists of a query and
key pair (both are numbers), where the query describes an SVG code of a number, and the key describes
the SVG code of another number with an introduced mathematical operation. The operation can consist of
add, subtract, multiply, and divide. The mathematical operation should be held in both example pairs. The
ﬁrst task is to ﬁgure out the mathematical operation in the two examples. Then, given a new test query SVG
number, the second task is to identify what number it is and follow the mathematical operation discovered to
generate the corresponding test key number. We include qualitative results in Figure 5. The prompt details
can be found in the appendix.
4
LIMITATION
While our research demonstrates the potential of using Scalable Vector Graphics (SVG) with large language
models (LLMs) to tackle visual tasks without a parameterized visual encoder, the major limitation of SVG
representation is the loss of ﬁne details: Though our method of converting raster images into SVG format and
leveraging XML-based textual descriptions allows for efﬁcient processing of crisp graphics and designs, it is
not as effective in handling photographic content. As a result, ﬁne-grained details, such as image textures,
may be lost during conversion. Conversely, when the SVG code incorporates an excessive level of detail, its
sequence length can become prohibitively long, which can pose challenges for the training and inference of
current Transformer-based LLMs. Developing hybrid representations that can retain the advantages of both
discrete and continuous data, while preserving ﬁner details, is an area for future exploration. For example, in
8


--- Page 9 ---

Under review as a conference paper at ICLR 2024
Figure 5: Understanding SVG content through the lens of GPT-4: GPT-4 demonstrates its ability to generate
accurate content by analyzing the correlation between provided example number pairs, and subsequently
applying this relationship to ascertain the corresponding test key number. Remarkably, in scenarios where the
relationship exhibits ambiguity, GPT-4 cna identify multiple possible interpretations.
LLMs, the processing unit is the token, which can correspond to one or several words. However, in SVG,
we would prefer to have a speciﬁc embedding module for each geometric primitive in SVG, such as circles,
polygons, and so on.
Additionally, our empirical tests highlighted certain areas where LLMs fell short, particularly in handling
low-level image manipulation tasks. For instance, when prompted to manipulate SVG images in tasks like
enlarging dimensions, shrinking dimensions, or rotations, LLMs like GPT-4 displayed inadequate proﬁciency.
Such operations, which mandate considerable updates to the SVG code, currently lie outside the proﬁciency
range of these models.
In summary, while LLMs do present limitations, it offers promising initial results for the integration of LLMs
and SVG for visual tasks. Addressing these limitations could lead to more powerful image representation
algorithms and pave the way for more versatile and comprehensive artiﬁcial intelligence systems.
5
CONCLUSION
This paper explored the possibility of enabling large language models (LLMs) to “see” and process images
through the Scalable Vector Graphics (SVG) format. By converting raster images into SVG representations
and leveraging XML-based textual descriptions, we showed that LLMs have some ability understand and
manipulate images.
We studied LLMs’ capabilities across various visual reasoning, recognition, and generative tasks, revealing
the underlying shape-color disentanglement nature of SVG. Through these experiments, we showed that SVG
representation shows better performance compared to the closed-set trained model, and could continue reﬁne
the outcome with chat-based feedback.
This research can open the door to new opportunities in the realm of computer vision by integrating the
powerful capabilities of LLMs with SVG format. We believe that our work provides an initial exploratory step
for future research in the integration of LLMs and SVG for the development of advanced image representation
formats and more complex vision tasks. As we continue to explore the potential of large language models on
visual input, this approach could inspire further progress in the understanding of visual data with multi-modal
fusion approaches.
9


--- Page 10 ---

Under review as a conference paper at ICLR 2024
REFERENCES
Adobe Inc. Adobe illustrator. https://adobe.com/products/illustrator, 2019.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for
few-shot learning. NeurIPS, 35:23716–23736, 2022.
Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv
preprint arXiv:1907.02893, 2019.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small image
transformations? arXiv preprint arXiv:1805.12177, 2018.
Greg J Badros, Jojada J Tirtowidjojo, Kim Marriott, Bernd Meyer, Will Portnoy, and Alan Borning. A
constraint extension to scalable vector graphics. In WWW, pp. 489–498, 2001.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In
International Conference on Learning Representations, 2022. URL https://openreview.net/
forum?id=p-BhZSz59o4.
Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image
inpainting. NeurIPS, 35:25005–25017, 2022.
David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning
in neural networks. In International conference on machine learning, pp. 511–520. PMLR, 2018.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. In NeurIPS, 2020a.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
NeurIPS, 33:1877–1901, 2020b.
S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter
Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artiﬁcial general intelligence: Early
experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22
billion parameters. arXiv preprint arXiv:2302.05442, 2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In NAACL-HLT, 2019.
Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris
Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced ﬁne-tuning for non-language machine
learning tasks. Advances in Neural Information Processing Systems, 35:11763–11784, 2022.
Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. Why is winoground hard?
investigating failures in visuolinguistic compositionality. arXiv preprint arXiv:2211.00768, 2022.
10


--- Page 11 ---

Under review as a conference paper at ICLR 2024
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873–12883,
2021.
Jon Ferraiolo, Fujisawa Jun, and Dean Jackson. Scalable vector graphics (SVG) 1.0 speciﬁcation. iuniverse
Bloomington, 2000.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland
Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and
robustness. arXiv preprint arXiv:1811.12231, 2018.
Andreas R Gruber, Ronny Lorenz, Stephan H Bernhart, Richard Neub¨ock, and Ivo L Hofacker. The vienna
rna websuite. Nucleic acids research, 36(suppl 2):W70–W74, 2008.
Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without
training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
14953–14962, 2023.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked autoencoders
are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 16000–16009, 2022.
Yue He, Zheyan Shen, and Peng Cui. Towards non-iid image classiﬁcation: A dataset and baselines. Pattern
Recognition, 110:107383, 2021.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. arXiv preprint arXiv:1903.12261, 2019.
Inkscape Project. Inkscape. https://inkscape.org, 2020.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
Segment anything.
arXiv preprint
arXiv:2304.02643, 2023.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that
learn and think like people. Behavioral and brain sciences, 40:e253, 2017.
Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62,
2022.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available:
http://yann.lecun.com/exdb/mnist, 2, 2010.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv:2304.08485,
2023.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A
convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 11976–11986, 2022.
OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023a.
11


--- Page 12 ---

Under review as a conference paper at ICLR 2024
OpenAI. Gpt-4 technical report. 2023b.
Zhong-Ren Peng and Chuanrong Zhang. The roles of geography markup language (gml), scalable vector
graphics (svg), and web feature service (wfs) speciﬁcations in the development of internet geographic
information systems (gis). Journal of Geographical Systems, 6:95–116, 2004.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Timothy Lillicrap. A simple neural network module for relational reasoning. Advances in neural
information processing systems, 30, 2017.
D´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning.
arXiv preprint arXiv:2303.08128, 2023.
Vicuna. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://vicuna.
lmsys.org/, 2023.
VTracer. Vtracer. https://www.visioncortex.org/vtracer-docs, 2020.
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie
Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric
tasks. arXiv preprint arXiv:2305.11175, 2023.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:
Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and
analogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 5317–5327, 2019.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.
APPENDIX
A
EXPERIMENT DETAILS
A.1
DATASET
Human Designed SVG Dataset We collect a dataset from the public collection of SVG images.1 Speciﬁcally,
we collect the digits and icons to demonstrate image recognition and generation capabilities. Examples are
shown in Figure 6 (a) and (b).
Convert Raster Images to SVG 1) Directly convert using curve tracing. Given the rich set of natural
images in raster format, we utilize the curve tracing algorithm to convert RGB images into the SVG format.2
Speciﬁcally, we convert MNIST (LeCun et al., 2010) to SVG format using this approach, shown in Figure 6
(c).
1https://www.svgrepo.com/,
https://www.kaggle.com/datasets/victorcondino/
svgicons
2https://github.com/visioncortex/vtracer
12


--- Page 13 ---

Under review as a conference paper at ICLR 2024
(a)
(b)
(c)
(d)
Figure 6: Visualization of our datasets. (a) and (b) are human-designed SVG vectors and icons. (c) and (d)
are converted from raster images. Speciﬁcally, (c) is generated using curve tracing from MNIST (LeCun
et al., 2010), while (d) is generated using SAM (Kirillov et al., 2023) and curve tracing sequentially.
A.2
RASTER IMAGES TO SVG CONVERSION
One of the most fundamental pieces of information for visual perception is object shape. Our method can be
conceptualized as selectively diminishing details from an image, prioritizing the extraction of less signiﬁcant
shapes. This guided process of reduction offers a quantitative way to manage the amount of visual data present
within an image. Within this framework, we perceive segmentation as an example of extreme simpliﬁcation,
whereas vectorization serves as a more moderate form of the same. Here we introduce how we use such two
approaches to convert the raster images to SVG.
Image Vectorization.
The vector tracing algorithm operates in a sequential three-step process. Initially,
pixels are transformed into a deﬁned path. Subsequently, this path is condensed into a simpliﬁed polygonal
representation. Lastly, the polygon is reﬁned and approximated using a curve-ﬁtting (tracing) technique,
which enhances its smoothness.
There are several online tools to convert the raster images (jpg and png) into vector graphics (SVG), such as
Adobe Illustrator (Adobe Inc., 2019), Inkscape (Inkscape Project, 2020), and VTracer (VTracer, 2020). We
experiment with all of them and found that VTracer leads to the best balance between SVG complexity (code
length) and rich semantic representation.
In MNIST (LeCun et al., 2010), we use the default hyperparameters during conversion. Speciﬁcally, we
(i) ﬁrst binarize the MNIST pixel value from the continuous range [0, 255] to the binary set {0, 255} using
the threshold 127.5, (ii) set the foreground to black, and the background to white, and (iii) apply the vector
tracing algorithm VTracer.
Segmentation Prior.
As mentioned earlier, segmentation can provide a strong prior for object shape. We
want a generalist model that can segment any image, i.e., not trained and thus biased towards a certain dataset.
The Segment Anything (SA) (Kirillov et al., 2023) project introduces such an image segmentation model,
the Segment Anything Model (SAM), and a large-scale dataset, SA-1B, with the aim of achieving powerful
generalization and zero-shot transfer across diverse segmentation tasks, demonstrating competitive results
often surpassing prior fully supervised methods. We use the default hyper-parameters of SAM to segment
the whole image into a set of masks without class labels, where the color of each mask is represented by
the average value of the pixels within the mask. Speciﬁcally, we sample 32 query points per side (1024
points overall) to generate the image mask. Then we select the top 20 masks with the highest area as the ﬁnal
representation for an image.
13


--- Page 14 ---

Under review as a conference paper at ICLR 2024
We then use VTracer to transform the mask into SVG format. Note that, to reduce the ﬁnal SVG, we adjust
several settings: we set the number of signiﬁcant bits to use in an RGB channel to 0; we set the minimum
angle displacement degree to splice a spline to 90; we set the color difference between gradient layers to be
35; we consider a corner to have a minimum momentary angle of 0 degrees; we discard patches smaller than
16 pixels in size; and we perform iterative subdivide smoothing until all segments are shorter than 10 pixels.
A.3
FINE-TUNING DATASET FOR VICUNA
We use the same JSON format in Vicuna (Vicuna, 2023) to construct the ﬁne-tuning dataset. We use all the
training samples in MNIST, translating to 60,000 SVG images. For each sample, we construct one round of
conversation: (i) From human: ‘‘Which digit does the following SVG reflect?
<SVG
code here>’’, and (ii) From GPT: ‘‘<label>’’. Here <label> denotes the digit label, which
ranges from 0 to 9. Then we use this dataset to ﬁne-tune Vicuna using the default hyper-parameters 3 for 3
epochs.
A.4
PROMPT ENGINEERING
In this section, we provide the details of prompt engineering for each task. The prompt is designed to ﬁgure
out the common transformation in the SVG example pairs ﬁrst (each example pair consists of a query and
a key) and then transform the query into the corresponding key SVG by following the discovered common
transformation.
In-context Image Classiﬁcation.
In this task, in-context examples are aimed to provide more context
information using several image-label pairs, thus facilitating the ﬁnal classiﬁcation.
The speciﬁc
prompt utilized for this purpose using 3 in-context examples is detailed below:
‘‘Instruction:
please predict the digit number for each of the following SVG images.
Please think step by step, and closely look at the key identifying
image characteristics.
Please just tell me the image class, no other
information is needed.
Q: What digit does this SVG image represent?
<SVG code here> A: This SVG image represents digit <label> Q: What
digit does this SVG image represent?
<SVG code here> A: This SVG image
represents digit <label> Q: What digit does this SVG image represent?
<SVG code here> A: This SVG image represents digit <label> Q: What
digit does this SVG image represent?
<SVG code here> A: This SVG image
represents digit .
Synthetic Data Study:
In this task, the objective is to conduct an analytical evaluation of the provided
two example pairs, examining changes that occur in aspects such as color, shape, and size. The insight
gathered from this analysis will then be used to adapt the given query into its corresponding key. The speciﬁc
prompt utilized for this purpose is detailed below:
‘‘Please perform the following task
carefully.
In this task, you will be shown two examples of Scalable
Vector Graphics (SVG) code pairs.
Each pair consists of a query and
key pair, where the query describes the SVG code of the original image,
and the key describes the SVG code of the transformed image.
Each
will be named ‘‘Example Query #" and ‘‘Example Key #" respectively.
Your first task is to figure out the common transformation in the two
examples.
The transformation can consist of color, shape, size, or
any combination thereof.
Then, given a new test query SVG code (named
3https://github.com/lm-sys/FastChat
14


--- Page 15 ---

Under review as a conference paper at ICLR 2024
\Test Query"), your second task is to transform that query into the
corresponding key SVG code (named \Test Key"), following the common
transformation that you discovered in the two example pairs.
Here
are the two example query and key pairs:
Example Query 1:
<SVG code
here>; Example Key 1:<SVG code here>; Example Query 2:<SVG code here>;
Example Key 2:<SVG code here>; Here are the test query and key pair:
Test Query:<SVG code here>; Test Key:’’
Content Extrapolation:
In this task, LLMs are presented with SVG code pairs, each containing
a query-key set that depicts numbers.
The key introduces a consistent mathematical operation (ad-
dition, subtraction, multiplication, or division) to the query number.
The tasks are to identify this
operation in the examples and apply it to new test queries to generate corresponding test keys.
To
facilitate a more comprehensive understanding of SVG number codes for the LLM, we initially present
the SVG codes for numbers 0 through 9 to the LLM prior to posing speciﬁc queries.
The speciﬁc
prompt utilized for this purpose is detailed below:
‘‘Please perform the following task
carefully.
In this task, you will be shown two examples of Scalable
Vector Graphics (SVG) code pairs.
Each pair consists of a query and
key pair, where the query describes an SVG code of an integer number,
and the key describes the SVG code of another integer number with an
introduced mathematical operation.
Each will be named \Example Query
#" and \Example Key #" respectively.
In addition to the example pairs,
you will be shown a new test query SVG code (named \Test Query").
Your
first task is to identify which number each example query, example key,
and test query represents.
Your second task is to figure out all the
possible mathematical operations that are held for all given example
pairs.
The operation could be add, subtract, multiply, and divide (the
subtract or multiply factor could be a fraction).
Then, according to
the numbers of example pairs and test query you identified, your third
task is to predict the corresponding test key number (named \Test Key"),
following all the mathematical operations that you discovered in the
given example pairs.
Finally, you need to generate the corresponding
SVG code of the test key number.
Here are the two example query and
key pairs:
Example Query 1:
<SVG code here>; Example Key 1:<SVG code
here>; Example Query 2:<SVG code here>; Example Key 2:<SVG code here>;
Here are the test query and key pair:
Test Query:
<SVG code here>;
Test Key:
(Note:
think about four operations one by one, and the
operation should be consistent for all given example pairs)’’
B
VISUAL REASONING RESULTS OF LLAVA
Here we evaluate the recent multi-modal large language model, LLaVa (Liu et al., 2023). As the result in
Table 4 shows, LLaVa struggles at this fundamental reasoning task. Besides,we observe that LLaVa frequently
defaults to ’yes’ for yes/no queries and often resorts to random guessing for counting tasks. This behavior
underscores the limitations of current large multimodal models in structured and sophisticated reasoning.
C
QUALATATIVE CHAT RESULTS
Image Recognition and Manipulation.
In this section, we provide examples for chat-based im-
age recognition and manipulation using GPT4 (OpenAI, 2023b).
The qualitative results are shown
15


--- Page 16 ---

Under review as a conference paper at ICLR 2024
Table 4: Catogori-wise accuracy on the Sort-of-Clevr dataset.
Question type
GPT4-brief
GPT-CoT
LLaVa
CNN+MLP
Relation Networks
Unary
0.50
0.90
0.60
0.65
0.89
Binary
0.90
0.95
0.60
0.75
0.80
Ternary
0.10
0.88
0.10
0.55
0.55
Average
0.50
0.89
0.43
0.65
0.75
Generate SVG letter “D” 
with same style as “B” and 
“C”?
Keep font size consistent 
with “B” and “C”
What’s the object?
Segment the 
airplane
Airplane.
What’s the food?
Change lettuce to 
tomato
Sure! Here is the 
SVG drawing:
Great! Please 
remove cheese
Sure! Here is the 
SVG drawing:
The SVG drawing 
represents a 
hamburger.
(a)
(d)
(b)
What digit does 
this SVG represent?
No, it is 8. What 
about this? 
The SVG drawing 
represents the digit 2.
Apologies for the 
error. Current SVG 
represents digit 6.
No, it is 0. What 
about this?
1.
Great! What about 
this?
5.
Great! Thank you!
You're welcome! If 
you have any more 
questions or need 
further assistance, 
feel free to ask!
(c)
Figure 7: In-context learning and image generation capabilities of SVG with LLMs. (a) With human feedback,
LLM gradually performs better on digit classiﬁcation. (b) LLM powers SVG with the capability of image
recognition and referring segmentation. (c) With human feedback, the content generation performance
becomes better. (d) LLM can recognize and manipulate speciﬁc parts of the hamburger, such as removing or
replacing them.
16


--- Page 17 ---

Under review as a conference paper at ICLR 2024
in Figure 7 and 8: (a) SVG representation empowers robust in-context digit recognition capability
given different background and foreground colors, (b) GPT4 can recognize and depict the details of
a dog with the prompt: "a stylized bear or a similar mammal with a round face
and ears." Furthermore, GPT-4 can identify the location of the dog’s left eye and remove it. (c) GPT4 is
capable of recognizing a natural image from the CIFAR-10 dataset.
What object does this SVG 
represent? 
What kind of animal is it?
It's difficult to 
determine the exact 
animal based on the 
SVG, but it resembles 
a stylized bear or a 
similar mammal with a 
round face and ears.
This SVG represents a 
cute, symmetric face 
with ears, eyes, and a 
mouth.
This SVG 
represents a bird.
(a)
(b)
What digit does this 
SVG image represent?
No, this is digit 3. What 
about this? 
This SVG image 
represents the digit 0.
This SVG image 
represents the digit 6.
Great! What about 
this?
This SVG image 
represents the digit 1.
(c)
Remove the left eye
What animal does this 
SVG represent? 
Great, thank you!
You're welcome! I'm 
glad I could help. If you 
have any more 
questions, feel free to 
ask.
Figure 8: More qualitative results of chat-based image recognition and manipulation. (a) In-context digit
recognition in Colored-MNIST-(B). (b) GPT can explain and manipulate the dog SVG image. (c) GPT4 can
also recognize the bird from a CIFAR-10 example.
Referring Segmentation
The objective of the task is to label pixels in an image or video that correspond to
an object instance referred by a linguistic expression. SVG representation has two advantages. First, language
instruction is naturally embedded within the prompt, thus a separate design of the image segmentation model
is not needed. Second, a large corpus of text and programming languages including XML are seen during
pretraining, beneﬁting the vision-language understanding ability.
SVG is typically composed of several colored polygons, where each of them can correspond to a part of
the object. Therefore, we can use the referring segmentation instructions to guide the LLM in ﬁnding the
corresponding SVG code. Shown in Figure 7 (b) and (d), LLM can localize the object decently well. In (b),
the majority of the airplane is selected as foreground, while in (d), not only is the lettuce recognized, but also
the two pieces of cheese are localized and subsequently removed.
Style Extrapolation:
LLMs are provided with ﬁve example pairs and are tasked with deciphering
the stylistic attributes inherent in these examples.
Following this, a new test query is presented
to the LLMs.
Their objective is to modify this query into the corresponding key, ensuring that it
aligns with the same stylistic principles showcased in the example pairs.
The qualitative results
17


--- Page 18 ---

Under review as a conference paper at ICLR 2024
Figure 9: More qualitative results of style extrapolation. The generation results of our method are annotated
with a red square.
18


--- Page 19 ---

Under review as a conference paper at ICLR 2024
are shown in ﬁgure 9.
The speciﬁc prompt utilized for this purpose is detailed below:‘‘Please
perform the following task carefully.
In this task, you will be shown
five examples of Scalable Vector Graphics (SVG) code pairs.
Each pair
consists of a query and key pair (both are English letter), where
the query describes the SVG code of the original image, and the key
describes the SVG code of the transformed image.
Each will be named
\Example Query #" and \Example Key #" respectively.
Your first task
is to figure out the common transformation in the five examples.
The
transformation can consist of color, shape, size, style, font, and
background changes, or any combination thereof.
Even though you cannot
see the images, and only their SVG codes, you need to discover the
transformations that are happening at the image level and not just at
the code level.
Be detailed, and try to discover every change, and
the most important change is that the paths in the SVG code between
each query and key is different due to the common transformation but
the shapes of the letters that query and key are representing remain
the same.
Then, given a new test query SVG code (named \Test Query"),
your second task is to transform that query into the corresponding
key SVG code (named \Test Key"), following the common transformation
that you discovered in the five example pairs.
To help you better
understand the transformation, I will also inform you of what letter
each query and key represent.
You need to find the shape of each query
and key by analyzing their path.
Here are the five example query and
key pairs:
Example Query 1 (letter B):; Example Key 1 (letter B):<SVG
code here>; Example Query 2 (letter R):<SVG code here>; Example Key 2
(letter R):<SVG code here>; Example Query 3 (letter Z):<SVG code here>;
Example Key 3 (letter Z):<SVG code here>; Example Query 4 (letter
E):<SVG code here>; Example Key 4 (letter E):<SVG code here>; Example
Query 5 (letter N):<SVG code here>; Example Key 5 (letter N):<SVG code
here>; Here is the test query and key pair:
Test Query (letter #):;
Test Key:
’’
19
```

---

## Section 1.9: EC-Net.An Edge-Aware Point Set Consolidation Network_conference

Source File: EC-Net.An Edge-Aware Point Set Consolidation Network_conference.txt

### Paper Content:

```
# EC-Net.An Edge-Aware Point Set Consolidation Network_conference.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/EC-Net.An Edge-Aware Point Set Consolidation Network_conference.pdf
# Output: ../layer2_completion/txt/EC-Net.An Edge-Aware Point Set Consolidation Network_conference.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/328108835
EC-Net: An Edge-Aware Point Set Consolidation Network: 15th European
Conference, Munich, Germany, September 8–14, 2018, Proceedings, Part VII
Chapter  in  Lecture Notes in Computer Science · September 2018
DOI: 10.1007/978-3-030-01234-2_24
CITATIONS
126
READS
735
5 authors, including:
Lequan Yu
The University of Hong Kong
254 PUBLICATIONS   17,905 CITATIONS   
SEE PROFILE
Xianzhi Li
Chinese University of Hong Kong
30 PUBLICATIONS   2,739 CITATIONS   
SEE PROFILE
Chi-Wing Fu
Nanyang Technological University
315 PUBLICATIONS   18,937 CITATIONS   
SEE PROFILE
Pheng-Ann Heng
Chinese University of Hong Kong
806 PUBLICATIONS   40,662 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Lequan Yu on 18 February 2019.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

EC-Net: an Edge-aware Point set
Consolidation Network
Lequan Yu1,3 ⋆[0000−0002−9315−6527],
Xianzhi Li1 ⋆,
Chi-Wing Fu1,3,
Daniel Cohen-Or2,
Pheng-Ann Heng1,3
1The Chinese University of Hong Kong
2Tel Aviv University
3Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology,
Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China
{lqyu,xzli,cwfu,pheng}@cse.cuhk.edu.hk
dcor@mail.tau.ac.il
Abstract. Point clouds obtained from 3D scans are typically sparse,
irregular, and noisy, and required to be consolidated. In this paper, we
present the ﬁrst deep learning based edge-aware technique to facilitate
the consolidation of point clouds. We design our network to process
points grouped in local patches, and train it to learn and help consolidate
points, deliberately for edges. To achieve this, we formulate a regression
component to simultaneously recover 3D point coordinates and point-
to-edge distances from upsampled features, and an edge-aware joint
loss function to directly minimize distances from output points to 3D
meshes and to edges. Compared with previous neural network based
works, our consolidation is edge-aware. During the synthesis, our network
can attend to the detected sharp edges and enable more accurate 3D
reconstructions. Also, we trained our network on virtual scanned point
clouds, demonstrated the performance of our method on both synthetic
and real point clouds, presented various surface reconstruction results,
and showed how our method outperforms the state-of-the-arts.
Keywords: point cloud, learning, neural network, edge-aware
1
Introduction
Point cloud consolidation is a process of “massaging” a point set into a surface [1],
for enhancing the surface reconstruction quality. In the past two decades, a wide
range of techniques have been developed to address this problem, including
denoising, completion, resampling, and many more. However, these techniques
are mostly based on priors, such as piecewise smoothness. Priors are typically
over-simpliﬁed models of the actual geometry behavior, thus the prior-based
techniques tend to work well for speciﬁc class of models rather than being general.
To implicitly model and characterize the geometry behavior, one common way is
to take a data-driven approach and model the complex behavior using explicit
⋆indicates equal contributions.


--- Page 3 ---

2
L. Yu et al.
Fig. 1: Given a point cloud (a) with noisy samples in inhomogeneous distribution,
our method consolidates it and reconstructs a plausible surface (b). Compared
with PU-Net (c), our method is edge-aware and can preserve sharp features.
examples. Data-driven surface reconstruction techniques [2–5] are based on match-
ing local portions (often denoted as patches) to a set of examples. Particularly,
the emergence of neural networks and their startling performance provide a new
means for 3D reconstruction from point sets by data-driven learning [6–8]. One
of the main limitations of these neural network based methods is that they are
oblivious to sharp features on 3D objects, where undersampling problems are
typically more severe, making it challenging for an accurate object reconstruction.
In this paper, we present the ﬁrst edge-aware consolidation network, namely
EC-Net, for point cloud consolidation. The network is designed and trained, such
that the output points admit to the surface characteristic of the 3D objects in the
training set. More importantly, our method is edge-aware, in the sense that the
network learns the geometry of edges from the training set, and during the test
time, it identiﬁes edge points and generates more points along the edges (and
over the surface) to facilitate a 3D reconstruction that preserves sharp features.
Generally speaking, scanned point sets are irregular and non-uniform, and thus,
do not lend themselves to be learned by common convolutional neural networks
(CNN). Inspired by PointNet [9], we directly process 3D points by converting their
coordinates into deep features and producing more points by feature expansion [7].
Then, for eﬃcient learning of the edges, we design our network to process points
grouped as local patches in the point cloud. To do so, we develop a patch
extraction scheme that solely works on points, so that we can extract patches of
points for use consistently in both training and testing phases.
In addition, to train the network to be edge-aware, we associate edge and mesh
triangle information with the training patches, and train the network to learn
features from the patches by regressing point-to-edge distances and then the
point coordinates. More importantly, we design a novel edge-ware joint loss
function that can be eﬃciently computed for directly comparison between the
output points and ground truth 3D meshes. Our loss function encourages the


--- Page 4 ---

EC-Net
3
output points to be located close to the underlying surface and to the edges,
as well as distributed more evenly on surface. Then in the inference phase, the
network can generate and ﬁnd output points close to the edges. Since it is diﬃcult
to annotate edges directly in real scanned point clouds, we train our network
on synthesized virtual scanned point clouds, and show the performance of our
method on both real and virtual scanned point clouds. By using our trained
network, we show through various experiments that we can improve not only the
point cloud consolidation results (see Figures 1(b) & (c)), but also the surface
reconstruction quality, compared to various state-of-the-art methods. All the
code is available at the project webpage1.
Related works. Consolidating scanned data and imperfect point clouds has
been an active research area since the early 90’s [10–12]. We brieﬂy review some
traditional geometric works and then discuss some recent related works that
employ neural networks. For a more comprehensive survey, please refer to [13].
Point cloud consolidation. Early works in this area assumed smooth surface [1,
14, 15]. In [14], the parameterization-free local projection operator (LOP) was
devised to enhance the point set quality. However, these methods are oblivious
to sharp edges and corners. To consolidate a point set in an edge-aware manner,
some methods detected/sensed the sharp edges and arranged points deliberatively
along edges to preserve their sharpness [16–19]. Huang et al. [20] developed the
edge-aware resampling (EAR) algorithm; it computes reliable normals away from
edges and then progressively upsamples points towards the surface singularities.
Despite its promising results, EAR depends on the accuracy of the given/estimated
normal. Preiner et al. [21] developed CLOP, a continuous version of the LOP,
for fast surface construction using the Gaussian mixture model to describe the
point cloud density. To sum up, these geometric approaches either assume strong
priors or rely on extra geometric attributes for upsampling point sets.
Neural networks for mesh and point cloud processing. Motivated by the promising
results that deep learning methods have achieved for image and video problems,
there has been increasing eﬀort to leverage neural networks for geometry and
3D shape problems. To do so, early works extracted low-level geometric features
as inputs to CNNs [22, 23]. Other works converted the input triangular meshes
or point clouds to regular voxel grids [24–29] for CNN to process. However,
pre-extracting low-level features may bring bias, while a volume representation
demands a high computational cost and is constrained by its resolution.
Recently, point clouds have drawn more attention, and there are some works
to utilize neural networks to directly process point clouds. Qi et al. [9] ﬁrstly
developed the PointNet, a network that takes a set of unordered points in 3D as
inputs and learns features for object classiﬁcation and segmentation. Later, they
proposed the PointNet++ to enhance the network with a hierarchical feature
learning technique [30]. Subsequently, many other networks have been proposed
for high-level analysis problems with point clouds [31–40]. However, they all focus
1 https://yulequan.github.io/ec-net/index.html


--- Page 5 ---

4
L. Yu et al.
Fig. 2: The pipeline of EC-Net. For each point in an input patch, we ﬁrst encode
its local geometry into a feature vector f (size: N × D) using PointNet++, and
expand f into f ′ (size: rN × D2) using a feature expansion mechanism. Then,
we regress the residual point coordinates and also the point-to-edge distances (d)
from the expanded features, and form the output point coordinates by adding the
original point coordinates to the residual. Finally, the network identiﬁes points
on edges and yields output points. The network was trained with an edge-aware
joint loss function that has four terms; see the yellow boxes on the right.
on analyzing global or mid-level attributes of point clouds. In some other aspects,
Guerrero et al. [6] proposed a network to estimate the local shape properties in
point clouds, including normal and curvature. 3D reconstruction from 2D images
has also been widely studied [8, 41, 42]. Our work is most related to PU-Net [7],
which presented a network to upsample a point set. However, our method is
edge-aware, and we extract local patches and train the network to learn edges in
patches with a novel edge-aware joint loss function.
2
Method
In this section, we ﬁrst present the training data preparation (Sec. 2.1) and
the EC-Net architecture (Sec. 2.2). Then, we present the edge-aware joint loss
function (Sec. 2.3) and the implementation details (Sec. 2.4). Figure 2 shows the
pipeline of EC-Net; see the supplemental material for the detailed architecture.
2.1
Training data preparation
We train our network using point clouds synthesized from 3D objects, so that
we can have ground truth surface and edge information. To start, we collect 3D
meshes from ShapeNet [43] and other online repositories, including simple 3D
shapes, mechanical parts, and everyday objects such as chairs. Since we train the
network with patches as inputs, we prepare a large amount of patches on the 3D
meshes and do not require many meshes. Moreover, we manually sketch polylines
on each 3D mesh to annotate sharp edges on the meshes; see Figure 3(a).


--- Page 6 ---

EC-Net
5
Fig. 3: Example annotated edges (in red) on some of our collected 3D meshes
(a). Example point clouds produced from our virtual 3D scans (b). The point
density varies and the zoom-in windows also reveal the synthetic noise.
Fig. 4: Procedure to extract patches (a local group of points) from a point cloud;
note that (a) and (b) are available only in the training (but not inference) phase.
Virtual scanning. To obtain point clouds from the 3D mesh objects, we use
the following virtual scanning procedure. First, we normalize the mesh to ﬁt in
[−1, +1]3, and evenly arrange a circle of 30 virtual cameras (50◦ﬁeld of view)
horizontally around (and to look at) the object. We then put the cameras two
units from the object center and randomly perturb the camera positions slightly
upward, downward or sideway. After that, we produce a point cloud for each
camera by rendering a depth image of the object, adding quantization noise
(see Sec. 3) to the depth values and pixel locations, and backprojecting each
foreground pixel to obtain a 3D point sample. Then, we can compose the 3D point
clouds from diﬀerent cameras to obtain a virtual scanned data. Such sampling
procedure mimics a real scanner with surface regions closer to the virtual camera
receiving more point samples; see Figure 3(b) for two example results.
Patch extraction. From a point cloud (see Figure 4(c)), we aim to extract local
groups of points (patches), such that the points in a patch are geodesically close
to one another over the underlying surface. This is very important, since using
Euclidean distances to select points could lead to points on opposite sides of a
thin surface, e.g., see the thin plates in the chair shown in Figure 3(b). Compared


--- Page 7 ---

6
L. Yu et al.
with [7], our patch extraction procedure directly operates on point clouds, not
meshes, so we need a consistent extraction procedure for both network training
and inference, where ground truth meshes are not available during the inference.
To this end, we ﬁrst construct a weighted graph by considering each point as a
node and creating an edge from each point to its k-nearest neighboring (k-nn)
points, where k=10; see Figure 4(d). The edge weight is set as the Euclidean
distance between the two points. Then, we randomly select m=100 points as the
patch centroids; from each selected point, we use the Dijkstra algorithm to ﬁnd
the 2048 nearest points in terms of shortest path distances in the graph. Hence, we
can ﬁnd points that are approximately within a geodesic radius from the centroid.
Further, we randomly select ˆN=1024 points out of the 2048 points to introduce
randomness into the point distribution, and normalize the 3D coordinates of the
points to have zero mean inside a unit ball. For patches used for training, we also
ﬁnd the associated mesh triangles and annotated edge segments near the patches
as the ground truth information for training the network; see Figure 4(e).
2.2
Edge-aware Point set Consolidation Network
In this subsection, we present the major components of EC-Net; see Figure 2.
Feature embedding and expansion. This component ﬁrst maps the neigh-
boring information (raw 3D coordinates of nearby points) around each point into
a feature vector using PointNet++ [30] to account for the fact that the input
points are irregular and unordered. The output is a D-dimensional multi-scale
feature vector for each input point, where D is 256 in our experiments. In this
step, we make the following adaptation for our problem. By design, PointNet++
processes a full point cloud of an object, while EC-Net processes local patches.
Since patches have open boundary, points near the boundary have neighbors
mostly on one of its side only, so we found that the extracted features of these
points are less accurate. Hence, out of the ˆN feature vectors, we retain the N=
ˆ
N
2
feature vectors (denoted as f) corresponding to points closer to the patch cen-
troid. Next, the component synthesizes points by expanding features directly in
feature space using the feature expansion module in [7], since points and features
should be interchangeable. After this module, feature f (dimension: N × D) are
expanded to be f ′ (dimension: rN × D2), where r is the upsampling rate and
D2 is the new feature dimension, which is set as 128; see again Figure 2.
Edge distance regression. This component regresses a point-to-edge distance
for each expanded feature (or point, equivalently) later for edge points identiﬁca-
tion. The regressed distance is an estimated shortest distance from the output
point to the nearest annotated edge segment among all annotated edge segments
associated with the patch. To do this, we extract a distance feature fdist from the
expanded feature f ′ via a fully connected layer, and then regress the point-to-edge
distance d from fdist via another fully connected layer. We do this in two steps,
so that we can feed fdist also to the coordinate regression component.


--- Page 8 ---

EC-Net
7
Coordinate regression. This component reconstructs the 3D coordinates of
the output points; see Figure 2. First, we concatenate the expanded feature
f ′ with the distance feature fdist (from previous component) to form another
feature, since fdist contains certain point-to-edge distance information. Then, we
regress the point coordinates from the concatenated feature by applying two fully
connected layers. Note that we only regress the residual 3D coordinates, and the
network output 3D coordinates of the output points by adding the original 3D
coordinates of the input points to the regressed residual 3D coordinates.
Edge points identiﬁcation. Denoting di as the regressed point-to-edge dis-
tance of output point xi, we next ﬁnd a subset of output points, namely edge points
(denoted as S∆d with threshold ∆d) that are near the edges: S∆d = {xi}di<∆d.
Note that this component is performed in both training and inference phases.
2.3
Edge-aware joint loss function
The loss function should encourage the output points to be (i) located close to
the underlying object surface, (ii) edge-aware (located close to the annotated
edges), and (iii) more evenly distributed on the object surface. To this end, we
guide the network’s behavior by designing an edge-aware joint loss function with
the following four loss terms (see also the rightmost part of Figure 2):
Surface loss encourages the output points to be located close to the underlying
surface. When extracting each training patch from the input point clouds, we ﬁnd
triangles and edge segments associated with the patch; see Figure 4. Hence, we can
deﬁne surface loss using the minimum shortest distance from each output point xi
to all the mesh triangles T associated with the patch: dT (xi, T) = mint∈T dt(xi, t),
where dt(xi, t) is the shortest distance from xi to triangle t ∈T. It is worth noting
that to compute dt in 3D, we need to consider seven cases, since the point on t
that is the closest to xi may be located at triangle vertices, along triangle edges,
or within the triangle face. Experimentally, we found that the algorithm [44] for
calculating dt can be implemented using TensorFlow to automatically calculate
the gradients when training the network. With dT computed for all the output
points, we can sum them up to compute the surface loss:
Lsurf = 1
˜N
X
1≤i≤˜
N
d2
T (xi, T) ,
(1)
where ˜N = rN is the number of output points in each patch.
Edge loss encourages the output points to be edge-aware, i.e., located close to
the edges. Denoting E as the set of annotated edge segments associated with a
patch, we deﬁne edge loss using the minimum shortest distance from each edge
point to all the edge segments in the patch: dE(xi, E) = mine∈E de(xi, e), where
de(xi, e) is the shortest distance from edge point xi to any point on edge segment
e ∈E. Again, we implement the algorithm in [45] to calculate de for diﬀerent


--- Page 9 ---

8
L. Yu et al.
shortest distance cases using TensorFlow to automatically calculate the gradients.
Then, we sum up dE for all the edge points and obtain the edge loss:
Ledge =
P
xi∈S∆d d2
E(xi, E)
|S∆d|
,
where S∆d is the edge point set .
(2)
Repulsion loss encourages the output points to be more evenly distributed over
the underlying surface. Given a set of output points xi, i = 1... ˜N, it is deﬁned as
Lrepl =
1
˜N · K
X
1≤i≤˜
N
X
i′∈K(i)
η( || xi′ −xi || ) ,
(3)
where K(i) is the set of indices for the K-nearest neighborhood of xi (we set
K=4), || · || is the L2-norm, and η(r) = max(0, h2 −r2) is a function to penalize
xi if it is too close to some other nearby points, where h is empirically set as
0.03 (which is the mean separation distance between points estimated from the
number of points and bounding box diagonal length according to [20]). It is worth
noting that we only want to penalize xi when it is too close to some neighborhood
points, so we only consider a few nearest neighboring points around xi; moreover,
we remove the repulsion eﬀect when the point-to-point distance is above h.
Edge distance regression loss aims to guide the network to regress the point-
to-edge distances d for the rN output points; see Figure 2. Considering that it
is diﬃcult for the network to regress d, since not all output points are actually
close to the annotated edges. Hence, we design a truncated regression loss:
Lregr =
1
˜N
X
1≤i≤˜
N

Tb(dE(xi, E)) −Tb(di)
2 ,
(4)
where Tb(x) = max(0, min(x, b)) is a piecewise linear function with parameter b.
Empirically, we found the network training not sensitive to b, and set it as 0.5.
End-to-end training. When training the network, we minimize the combined
edge-aware joint loss function below with balancing weights α and β:
L = Lsurf + Lrepl + αLedge + βLregr .
(5)
In our implementation, we set α and β as 0.1 and 0.01, respectively.
2.4
Implementation Details
Network training. Before the training, each input patch is normalized to ﬁt
in [−1, 1]3. Then, we augment each patch on-the-ﬂy in the network via a series
of operators: a random rotation, a random translation in all dimensions by -0.2
to 0.2, a random scaling by 0.8 to 1.2, adding Gaussian noise to the patch points
with σ set as 0.5% of the patch bounding box size, and randomly shuﬄing the


--- Page 10 ---

EC-Net
9
ordering of points in the patch. We implement our method using TensorFlow, and
train the network for 200 epochs using the Adam [46] optimizer with a minibatch
size of 12 and a learning rate of 0.001. In our experiments, the default upsampling
rate r is set as 4. For threshold ∆d, we empirically set it as 0.15, since it is not
sensitive to slight variation in our experiments. Overall, it took around 5 hours
to train the network on an NVidia TITAN Xp GPU.
Network inference. We apply a trained network to process point clouds also
in a patch-wise manner. To do so, we ﬁrst ﬁnd a subset of points in a test point
cloud, and take them as centroids to extract patches of points using the procedure
in Sec. 2.1. For the patches to distribute more evenly over the point cloud (say
with Npt points), we use farthest point sampling to randomly ﬁnd M = β Npt
N
points in the test point cloud with parameter β, which is empirically set as three.
Hence, each point in the point cloud should appear roughly in β diﬀerent patches
on average. After extracting the patches, we feed them into the network and
apply the network to produce 3D coordinates and point-to-edge distances, as
well as to identify edge points (see Sec. 2.3). Unlike the training phase, we set a
smaller ∆d, which is 0.05. We use a larger ∆d in the training because training is
an optimization process, where we want the network to consider more points to
learn to identify the points near the edges.
Surface reconstruction. First, we build a k-nn graph for the output points
from network. Then, we ﬁlter edge points by ﬁtting line segments using RANSAC,
and ﬁlter surface points (not near edges points) by ﬁnding small groups of nearby
points in the k-nn graph in an edge-stopping manner and ﬁtting planes using
PCA. Edge stopping means we stop the breath-ﬁrst growth at edge points; this
avoids including irrelevant points beyond the edges. These steps are iterated
several times. Lastly, we ﬁll the tiny gap between edge and surface points by
including some original points in the gap, and by applying dart throwing to
add new points. To further reconstruct the surface, we follow the procedure in
EAR [20] to downsample the point set and compute normals, use ball pivoting [47]
or screened Poisson surface reconstruction [48] to reconstruct the surface, and
use a bilateral normal ﬁltering [49] to clean the resulting mesh.
3
Experiments
Dataset overview. Since most models in [7] are manifolds without sharp edges,
we collected 24 CAD models and 12 everyday objects as our training data set,
and manually annotate sharp edges on them; see supplemental material. Then,
we randomly crop 2,400 patches from the models (see Figure 2) to train our
network; see the procedure in Sec. 2.1. To perform the experiments presented in
this section, we do not reuse the models in the training data set but download
additional 3D models from ShapeNet [43]. For each testing model, we also use
the procedure in Sec. 2.1 to generate the virtual scanned point clouds as input.


--- Page 11 ---

10
L. Yu et al.
Fig. 5: Our method produces consolidated point clouds (b) that lead to higher
quality surface reconstruction results (c). Predicted edge points are shown in red.
Surface reconstruction results. We demonstrate the quality of our method
by applying it to consolidate point sets and reconstruct surfaces. Figure 5(a) shows
the input point clouds generated from the testing models shown in Figure 5(f),
while Figure 5(b) & (c) show our results (see supplemental material for additional
reconstruction results). To reconstruct the surfaces, we follow the procedure
in Sec. 2.4 and employ the ball pivoting algorithm (BPA) [47] to reconstruct
the surfaces. To show the eﬀect of our network, we apply the same procedure
with BPA to reconstruct surfaces directly from (i) the input point clouds (see
Figure 5(d)), and from (ii) the input point clouds with PCA plane ﬁtting for
denoising (see Figure 5(e)), without using our network to process the point clouds.
Comparing the resulting reconstructed surfaces with the ground truth meshes
shown in Figure 5(f), we can see that our method achieves the best visual quality,
particularly on preserving the edges. In addition, we compute the mean Hausdorﬀ
distance between the reconstructed surfaces and their corresponding ground
truth surfaces; see the mean errors shown in Figure 5(c), (d) & (e); our method
consistently has the lowest mean errors among all the testing models. These
quantitative results, together with the visual comparison, give evidence that our
method produces consolidated point sets and improves the surface reconstruction
quality. Note that without the knowledge of edges learnt and recognized by the
network, using PCA alone to denoise point clouds is not edge-aware; the sharp
edges would be wiped away, if we directly apply PCA to the raw point cloud,


--- Page 12 ---

EC-Net
11
Fig. 6: Comparing surface reconstruction results produced by using PU-Net (b),
EAR (c), and our method (d) to consolidate points with the original meshes (e).
Fig. 7: Comparing reconstruction results on real scans produced by direct recon-
struction (b) and with consolidation: CLOP, GPF, and our method (c-e).
leading to the inferior surface reconstructions shown in Figure 5(e). To sum up,
our consolidation facilitates high-quality reconstruction not only on sharp objects
but also on usual smooth objects. It is worth to note that with our consolidation,
the overall reconstruction quality also improves over the state-of-the-art surface
reconstruction methods on the benchmark models in [50]; due to page limit,
please see the supplemental material for more details.
Comparing with other methods. In the experiment, we compare our method
with state-of-the-art methods, EAR [20], CLOP [21], GPF [51], and PU-Net [7],
by applying them to process and consolidate point clouds before the screened
Poisson surface reconstruction [48]. As for PU-Net, we train a new model using
our training objects and code released by the author. For better comparison,
we also apply patch-based manner in testing phase, as this can achieve better
results. Figures 6(b), (c) & (d) present the comparison with EAR and PU-Net.
We can see from the results that the sharp edges in the original mesh are mostly
smoothed out if we take the point clouds from PU-Net for surface construction.
EAR better preserves the sharp features on the reconstructed surfaces, but in


--- Page 13 ---

12
L. Yu et al.
Table 1: Quantitative comparison: our method, PU-Net, and EAR.
Model
Mean (10−3)
RMS (10−3)
Our PU-Net EAR Our PU-Net EAR
Camera
1.31
1.91
2.43 1.99
2.74
3.75
Sofa
1.72
3.20
1.56 2.40
4.68
2.87
Chair
0.88
1.70
1.93 1.27
2.50
3.54
Fandisk
1.09
2.86
2.33 1.77
4.50
5.63
Switch-pot 1.36
2.00
1.76 1.83
3.07
2.44
Headphone 0.81
1.83
3.71 1.19
2.89
6.93
Table
1.15
2.14
2.74 1.62
3.12
5.34
Monitor
0.61
2.01
2.58 0.97
2.71
5.73
case of severe noise or under-sampling, it may over-smooth the geometric details
and sharp features, or over-sharpen the edges. It is because the method depends
on the quality of the estimated normals; see the limitation paragraph in [20]. Our
method, which is based on an edge-aware neural network model, is able to learn
and capture edge information with high learning generalization, our point cloud
consolidation results can lead to surface reconstructions that are closer to the
original meshes. Figure 7 also demonstrates that our method helps enhance the
Poisson reconstruction quality on real scans in terms of preserving sharp edges
compared with CLOP [21] and GPF [51].
We also quantitatively compare with EAR and PU-Net by calculating the mini-
mum distances from output points to the associated original mesh (as ground
truth) in the test dataset. Table 1 lists the mean and root mean square (RMS)
values of diﬀerent methods on the test models; see supplemental material for
visual comparison. We can see from the table that points generated from our
method are closer to the original meshes compared to others. The PU-Net uses
the EMD loss to encourage output points to be located on the underlying surfaces,
so this comparison shows that the results are sub-optimal compared with our
method, which directly minimizes the distances from output points to surfaces.
Our results under varying quantization noise. In real scans, the acquired
depth values are generally quantized nonlinearly depending on the distance from
the scanner. In short, objects far from the scanner would have less precise depth
values in fewer quantization levels. During the virtual scanning process, we mimic
real scans by quantizing the depth values in Nq levels. This means that there are
only Nq unique depth values over all the foreground pixels. In this subsection, we
investigate the robustness of our method under diﬀerent amount of quantization
noise by producing three sets of point cloud data using virtual scan on a testing
model with Nq=120 (low noise), Nq=80 (medium noise), and Nq=50 (high noise)
and then applying our method to consolidate the resulting point clouds.
Figure 8 presents the results, where the left, middle and right columns correspond
to low, medium and high noise levels, respectively. From the blown-up views, we


--- Page 14 ---

EC-Net
13
Fig. 8: Results of using our method to consolidate point clouds with diﬀerent
amount of quantization noise. Top row: the testing Monitor model; 2nd and 3rd
rows: blown-up views of the input point clouds with diﬀerent noise level (low to
high) and our consolidated results, respectively; 4th and 5th rows: statistics of
point-to-surface and point-to-edge distances, respectively.
can see more points above the object surface for the point cloud with high noise.
The statistics about the point-to-surface distances also reveal the noise level; see
the blue plots in the 4th row. After the consolidation, the points are steered to
the left (see the red plots in 4th row), meaning that the points are now all closer
to the ground truth surface under diﬀerent noise levels. Similar pattern can also
be observed from the statistical results for the point-to-edge distances.
Results on real scans. We also apply our method to point clouds produced
from real scans downloaded from Aim@Shape and obtained from the EAR
project [20]. Figure 7 has shown some results on real scans, and Figure 9 shows
more consolidation and reconstruction results. As we can see, real scan point
clouds are often noisy and have inhomogeneous point distribution. Comparing
with the input point clouds, our method is still able to generate more points near
the edges and on the surface, while better preserving the sharp features.


--- Page 15 ---

14
L. Yu et al.
Fig. 9: Three results from real scans (a). Note the diversity of the geometry, the
sparseness of the inputs (a), and how well the network locates the edges (in red);
see (b). The reconstruction results (c) are yet imperfect due to tiny regions that
are severely undersampled (see the blown-up views in (a)).
4
Discussion and future works
We presented EC-Net, the ﬁrst edge-aware network for consolidating point clouds.
The network was trained on synthetic data and tested on both virtual and
real data. To be edge-aware, we design a joint loss function to identify points
along edges, and to encourage points to be located close to the underlying
surface and edges, as well as being more evenly distributed over surface. We
compared our method with various state-of-the-art methods for point cloud
consolidation, showing improvement in the 3D reconstruction quality, especially
at sharp features. Our method has a number of limitations. First, it is not designed
for completion, so ﬁlling large holes and missing parts would be a separate and
diﬀerent problem. In future, we plan to investigate how to enhance the network
and the training process for point cloud completion. For tiny structures that are
severely undersampled, our network may not be able to reconstruct the sharp
edges around them. With insuﬃcient points, the patch could become too large
compared to the tiny structure. Moreover, our current implementation considers
patches with a ﬁxed number of points, and it cannot adapt structures of varying
scales. It would be an interesting problem to try to dynamically control the patch
size and explore the use of larger context in the training and inference.
Acknowledgments. We thank anonymous reviewers for the comments and
suggestions. The work is supported by the Research Grants Council of the
Hong Kong Special Administrative Region (Project no. GRF 14225616), the
Shenzhen Science and Technology Program (No. JCYJ20170413162617606 and
No. JCYJ20160429190300857), and the CUHK strategic recruitment fund.


--- Page 16 ---

EC-Net
15
References
1. Alexa, M., Behr, J., Cohen-Or, D., Fleishman, S., Levin, D., Silva, C.T.: Computing
and rendering point set surfaces. IEEE Trans. Vis. & Comp. Graphics 9(1) (2003)
3–15
2. Gal, R., Shamir, A., Hassner, T., Pauly, M., Cohen-Or, D.: Surface reconstruction
using local shape priors. In: Symposium on Geometry Processing. Number EPFL-
CONF-149318 (2007) 253–262
3. Sung, M., Kim, V.G., Angst, R., Guibas, L.J.: Data-driven structural priors for
shape completion. ACM Trans. on Graphics (SIGGRAPH Asia) 34(6) (2015)
175:1–175:11
4. Xu, K., Kim, V.G., Huang, Q., Kalogerakis, E.: Data-driven shape analysis and
processing. Computer Graphics Forum 36(1) (2017) 101–132
5. Remil, O., Xie, Q., Xie, X., Xu, K., Wang, J.: Surface reconstruction with data-
driven exemplar priors. Computer-Aided Design 88 (2017) 31–41
6. Guerrero, P., Kleiman, Y., Ovsjanikov, M., Mitra, N.J.: PCPNet: Learning local
shape properties from raw point clouds. arXiv preprint arXiv:1710.04954 (2017)
7. Yu, L., Li, X., Fu, C.W., Cohen-Or, D., Heng, P.A.: PU-Net: Point cloud upsampling
network. arXiv preprint arXiv:1801.06761 (2018)
8. Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: AtlasNet: A papier-
mˆach´e approach to learning 3D surface generation. In: IEEE CVPR. (2018) 216–224
9. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: PointNet: deep learning on point sets for
3D classiﬁcation and segmentation. In: IEEE CVPR. (2017) 77–85
10. Hoppe, H., DeRose, T., Duchamp, T., McDonald, J., Stuetzle, W.: Surface recon-
struction from unorganized points. In: Proc. of SIGGRAPH. (1992) 71–78
11. Turk, G., Levoy, M.: Zippered polygon meshes from range images. In: Proc. of
SIGGRAPH. (1994) 311–318
12. Amenta, N., Bern, M., Kamvysselis, M.: A new voronoi-based surface reconstruction
algorithm. In: Proc. of SIGGRAPH. (1998) 415–422
13. Berger, M., Tagliasacchi, A., Seversky, L.M., Alliez, P., Guennebaud, G., Levine,
J.A., Sharf, A., Silva, C.T.: A survey of surface reconstruction from point clouds.
In: Computer Graphics Forum. Volume 36., Wiley Online Library (2017) 301–329
14. Lipman, Y., Cohen-Or, D., Levin, D., Tal-Ezer, H.: Parameterization-free projection
for geometry reconstruction. ACM Trans. on Graphics (SIGGRAPH) 26(3) (2007)
22:1–22:5
15. Huang, H., Li, D., Zhang, H.R., Ascher, U., Cohen-Or, D.:
Consolidation of
unorganized point clouds for surface reconstruction. ACM Trans. on Graphics
(SIGGRAPH Asia) 28(5) (2009) 176:1–176:8
16. Pauly, M., Keiser, R., Kobbelt, L.P., Gross, M.: Shape modeling with point-sampled
geometry. ACM Trans. on Graphics (SIGGRAPH) 22(3) (2003) 641–650
17. Guennebaud, G., Barthe, L., Paulin, M.: Real-time point cloud reﬁnement. In:
Symposium on Point Based Graphics. (2004) 41–48
18. Fleishman, S., Cohen-Or, D., Silva, C.T.: Robust moving least-squares ﬁtting with
sharp features. ACM Trans. on Graphics (SIGGRAPH) 24(3) (2005) 544–552
19. ¨Oztireli, A.C., Guennebaud, G., Gross, M.: Feature preserving point set surfaces
based on non-linear kernel regression. Computer Graphics Forum (Eurographics)
28(2) (2009) 493–501
20. Huang, H., Wu, S., Gong, M., Cohen-Or, D., Ascher, U., Zhang, H.R.: Edge-aware
point set resampling. ACM Trans. on Graphics 32(1) (2013) 9:1–9:12


--- Page 17 ---

16
L. Yu et al.
21. Preiner, R., Mattausch, O., Arikan, M., Pajarola, R., Wimmer, M.: Continuous
projection for fast l1 reconstruction. ACM Trans. on Graphics (SIGGRAPH) 33(4)
(2014) 47:1–47:13
22. Guo, K., Zou, D., Chen, X.:
3D mesh labeling via deep convolutional neural
networks. ACM Trans. on Graphics 35(1) (2015) 3:1–3:12
23. Boulch, A., Marlet, R.: Deep learning for robust normal estimation in unstructured
point clouds. Computer Graphics Forum (SGP) 35(5) (2016) 281–290
24. Qi, C.R., Su, H., Niessner, M., Dai, A., Yan, M., Guibas, L.J.: Volumetric and
multi-view CNNs for object classiﬁcation on 3D data. In: IEEE CVPR. (2016)
5648–5656
25. Dai, A., Qi, C.R., Nießner, M.: Shape completion using 3D-encoder-predictor CNNs
and shape synthesis. In: IEEE CVPR. (2017) 5868–5877
26. Han, X., Li, Z., Huang, H., Kalogerakis, E., Yu, Y.: High-resolution shape completion
using deep neural networks for global structure and local geometry inference. In:
IEEE ICCV. (2017) 85–93
27. Wang, P., Liu, Y., Guo, Y., Sun, C., Tong, X.: O-CNN: Octree-based convolutional
neural networks for 3D shape analysis. ACM Trans. on Graphics 36(4) (2017)
72:1–72:11
28. Riegler, G., Ulusoy, A.O., Geiger, A.: OctNet: Learning deep 3D representations at
high resolutions. In: IEEE CVPR. (2017) 6620–6629
29. Liu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., Lu, J.: 3DCNN-DQN-RNN:
a deep reinforcement learning framework for semantic parsing of large-scale 3D
point clouds. In: IEEE ICCV. (2017) 5678–5687
30. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: PointNet++: deep hierarchical feature
learning on point sets in a metric space. In: Advances in Neural Information
Processing Systems 30. (2017) 5105–5114
31. Hua, B.S., Tran, M.K., Yeung, S.K.: Point-wise convolutional neural network.
(2017)
32. Klokov, R., Lempitsky, V.: Escape from cells: deep Kd-Networks for the recognition
of 3D point cloud models. In: IEEE ICCV. (2017) 863–872
33. Landrieu, L., Simonovsky, M.: Large-scale point cloud semantic segmentation with
superpoint graphs. arXiv preprint arXiv:1711.09869 (2017)
34. Xu, D., Anguelov, D., Jain, A.: PointFusion: Deep sensor fusion for 3D bounding
box estimation. arXiv preprint arXiv:1711.10871 (2017)
35. Wang, W., Yu, R., Huang, Q., Neumann, U.: SGPN: Similarity group proposal
network for 3D point cloud instance segmentation. arXiv preprint arXiv:1711.08588
(2017)
36. Yang, Y., Feng, C., Shen, Y., Tian, D.: FoldingNet: Interpretable unsupervised
learning on 3D point clouds. In: IEEE CVPR. (2018) 206–215
37. Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J.: Frustum PointNets for 3D object
detection from RGB-D data. arXiv preprint arXiv:1711.08488 (2017)
38. Li, Y., Bu, R., Sun, M., Chen, B.: PointCNN. arXiv preprint arXiv:1801.07791
(2018)
39. Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic
graph CNN for learning on point clouds. arXiv preprint arXiv:1801.07829 (2018)
40. Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang, M.H., Kautz, J.:
SPLATNet: Sparse lattice networks for point cloud processing. In: IEEE CVPR.
(2018) 2530–2539
41. Fan, H., Su, H., Guibas, L.J.:
A point set generation network for 3D object
reconstruction from a single image. In: IEEE CVPR. (2017) 2463–2471


--- Page 18 ---

EC-Net
17
42. Lin, C.H., Kong, C., Lucey, S.: Learning eﬃcient point cloud generation for dense
3D object reconstruction. In: AAAI. (2018) to appear
43. Chang, A.X., Funkhouser, T., Guibas, L.J., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: an
information-rich 3D model repository. arXiv preprint arXiv:1512.03012 (2015)
44. Eberly,
D.:
Distance
between
point
and
triangle
in
3D.
https://www.geometrictools.com/Documentation/DistancePoint3Triangle3.pdf
(1999)
45. Eberly,
D.:
Distance
between
point
and
line,
ray,
or
line
segment.
https://www.geometrictools.com/Documentation/DistancePointLine.pdf (1999)
46. Kingma, D., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
47. Bernardini, F., Mittleman, J., Rushmeier, H., Silva, C., Taubin, G.: The ball-
pivoting algorithm for surface reconstruction. IEEE Trans. Vis. & Comp. Graphics
5(4) (1999) 349–359
48. Kazhdan, M., Hoppe, H.: Screened poisson surface reconstruction. ACM Trans. on
Graphics 32(3) (2013) 29:1–29:13
49. Zheng, Y., Fu, H., Au, O.K.C., Tai, C.L.: Bilateral normal ﬁltering for mesh
denoising. IEEE Trans. Vis. & Comp. Graphics 17(10) (2011) 1521–1530
50. Berger, M., Levine, J.A., Nonato, L.G., et al.: A benchmark for surface reconstruc-
tion. ACM Trans. on Graphics 32(2) (2013) 20
51. Lu, X., Wu, S., Chen, H., Yeung, S.K., Chen, W., Zwicker, M.: GPF: GMM-inspired
feature-preserving point set ﬁltering. IEEE Trans. Vis. & Comp. Graphics 24(8)
(2018) 2315–2326
View publication stats
```

---

## Section 1.10: Edge and Corner Detection for Unorganized 3D Point Clouds with

Source File: Edge and Corner Detection for Unorganized 3D Point Clouds with.txt

### Paper Content:

```
# Edge and Corner Detection for Unorganized 3D Point Clouds with.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Edge and Corner Detection for Unorganized 3D Point Clouds with.pdf
# Output: ../layer2_completion/txt/Edge and Corner Detection for Unorganized 3D Point Clouds with.txt


--- Page 1 ---

 
 
Abstract—In this paper, we propose novel edge and corner 
detection algorithms for unorganized point clouds. Our edge 
detection method evaluates symmetry in a local neighborhood 
and uses an adaptive density based threshold to differentiate 3D 
edge points. We extend this algorithm to propose a novel corner 
detector that clusters curvature vectors and uses their 
geometrical statistics to classify a point as corner. We perform 
rigorous evaluation of the algorithms on RGB-D semantic 
segmentation and 3D washer models from the ShapeNet dataset 
and report higher precision and recall scores. Finally, we also 
demonstrate how our edge and corner detectors can be used as 
a novel approach towards automatic weld seam detection for 
robotic welding. We propose to generate weld seams directly 
from a point cloud as opposed to using 3D models for offline 
planning of welding paths. For this application, we show a 
comparison between Harris 3D and our proposed approach on 
a panel workpiece.  
I. INTRODUCTION 
Visual features like edge, corner and texture are widely 
used in computer vision and robotic perception. They can be 
detected through low-level cues and then subsequently used 
for higher level perception tasks. A rapidly growing 
application is robotic welding which depends on extraction of 
corners and edges to generate welding paths for the robot. An 
example of a welding workpiece, known as panel workpiece, 
is shown in Fig. 1. Existing approaches for such welding tasks 
are based on offline simulation and planning using 3D CAD 
models. With the growing demand for automation, alternative 
solutions are required that are more open to adaptation to 
previously unseen and complicated welding tasks.   
Though a 3D point cloud is more informative as compared 
to a 2D image, extracting edge and corner features in 3D is 
much more challenging. In case of edge detection for 
unorganized 3D point clouds, conventional methods perform 
surface reconstruction to form a mesh [1, 8, 16, 17] or build a 
graph [3] to analyze the neighborhood of each point through 
principal 
component 
analysis 
(PCA). 
However, 
reconstruction tends to wash out sharp edges and intricate 
features while graph based methods are computationally 
expensive [7]. The most recent approach, applicable to 
unorganized point clouds, is based on discriminative learning 
[2]. The authors train a random forest based binary classifier 
on a set of features that learn to classify points into edge 
versus non-edge points. One drawback of their method is poor 
performance on unseen data.  
Real-time state-of-the-art methods for 3D edge detection 
are much more efficient for organized point clouds [5, 6]. This 
is due to the fast nearest neighbor search that can be 
performed in an organized structure and makes them suitable 
 
Syeda Mariam Ahmed1 and Chee Meng Chew1 are with the Department of 
Mechanical Engineering, National University of Singapore (email: 
mpesyed@nus.edu.sg ). Yan Zhi Tan2 and Abdullah Al Mamun2 are with the 
 
Fig. 1. Demonstration of our edge and corner detectors on 3D pointcloud of 
panel workpiece.  
for real-time applications. However, their fast speed is a result 
of the nearest neighbor search in a regular 3D grid and thus 
their performance is restricted to organized point clouds.    
 Similarly corner detection methods in 3D are often an 
extension of 2D techniques [9] and require an organized 
structure or a depth image for detection [12]. Additionally, 
various keypoint detection algorithms exist in 3D, 
however,they aim to find stable and repeatable points that are 
not directly on the edge [10, 11]. Thus it is not possible to use 
such methods for accurate corner detection.  
This paper has two main contributions: the first is a new 
edge detection algorithm; second is a novel corner detector. 
The edge detection method evaluates the level of symmetry 
in a local region of a 3D point. A main advantage of this 
approach is that it is independent of surface normals due to 
which the detected edges show a higher accuracy. We extend 
the edge detection algorithm to differentiate between line-
type or sharp features and propose a novel corner detector. 
Our approach is most similar to the Gauss map clustering 
method, proposed by Weber [3]. They observe that sharp 
features separate two or more surfaces and in presence of a 
discontinuity, the points of a sphere will build more than one 
distinct cluster. Using a similar intuition, our corner detection 
approach is also based on clustering of curvature vectors. 
However the key differences are: 1) we only evaluate edge 
points in the point cloud, thus reducing the number of points 
to search for corners. 2) We compare features extracted from 
the size and average curvature vector associated with each 
cluster to detect a corner, as opposed to evaluating the number 
of clusters formed. As a result, our proposed method can 
detect accurate corners.  
II. 3D EDGE DETECTION  
The proposed edge detection method is based on the mean 
shift algorithm [18] which is a non-parametric iterative 
Department of Electrical Engineering, National University of Singapore. 
Fook Seng Wong3 is with Keppel Technology and Innovation, Singapore (e-
mail: fookseng.wong@komtech.com.sg).  
Edge and Corner Detection for Unorganized 3D Point Clouds with 
Application to Robotic Welding  
Syeda Mariam Ahmed1, Yan Zhi Tan2, Chee Meng Chew1, Abdullah Al Mamun2, Fook Seng Wong 3  
Detected 
Edges 
Detected 
Corners 


--- Page 2 ---

 
technique to locate the maximum density/modes of a function 
in feature space. Our algorithm for edge detection in 3D point 
cloud uses the same intuition, but instead of finding the local 
maxima, we evaluate the degree of shift in centroid from its 
initial position to classify points as edge or non-edge.  
More formally, we define the approach as follows. Given a 
query point 𝑝𝑖, we determine its k-nearest neighbors. For an 
unorganized point cloud, this is achieved through a k-
dimensional (K-d) tree [13]. For an organized point cloud, the 
nearest neighbors are all points in a square of size 𝑠, while 𝑝𝑖 
is the centroid. These neighboring points of 𝑝𝑖 are given as 
Ѵ𝑖= {𝑛1, 𝑛2, … . 𝑛𝑘 }. Initially we assume that the centroid of 
Ѵ𝑖 is the query point itself, while a new centroid 𝐶𝑖 is 
computed by taking the mean of the neighboring points as 
follows:  
𝐶𝑖=
1
|Ѵ𝑖| ∑
𝑛𝑗
𝑘
𝑗=1
                                (1) 
To cater variation in density, we compute the resolution 
𝑍𝑖(Ѵ𝑖), as defined by (2), of the neighboring points. This is 
achieved by determining the distance of the nearest neighbor 
of  𝑝𝑖 among all the k neighbors.  
𝑍𝑖(Ѵ𝑖) = min
𝑛∈Ѵ𝑖
‖𝑝𝑖−𝑛𝑖‖                            (2) 
‖𝐶𝑖−𝑝𝑖‖ > 𝜆∙𝑍𝑖(Ѵ𝑖)                             (3) 
Evaluation of 𝑍𝑖 ensures scale invariance as the local density 
of points is considered for each point individually. Finally, 𝑍𝑖 
is multiplied by a fixed parameter 𝜆 which serves as the 
classification threshold. If the distance between the new 
centroid 𝐶𝑖 and the query point 𝑝𝑖 is greater than this threshold 
times 𝑍𝑖(Ѵ𝑖) as defined by (3), the point is classified as an 
edge.  
Fig. 2 illustrates the concept behind the proposed approach 
on a partial view of the Stanford bunny model. It can be seen 
that the neighboring points Ѵ𝑖 (shown in green) of the query 
point 𝑝𝑖 (shown in red), do not have a symmetrical circular 
shape, but rather ends abruptly at the edge. This results in a 
larger shift in the position of the centroid 𝐶𝑖 (shown in blue). 
As opposed to this, a point on a smoother surface will have a 
shift in centroid position that is comparatively small. This 
fundamental idea of symmetry among neighboring points can 
enable quick classification of 3D data. 
The only variable parameters in the algorithm are k and 𝜆, 
that determine the number of nearest neighbors and the 
classification threshold, both of which can be easily tuned for 
any point cloud. For data with lower signal to noise ratio, a 
higher value of k and 𝜆 will be required and vice versa.  
III. CORNER DETECTION 
Efficient corner detection algorithms are the basis for many 
vision based applications. A 3D corner can be defined at the 
intersection of two or more edges. Theoretically, there should 
not be ambiguity in the location of the corner as it is spatially 
constrained, however noisy point clouds can make it difficult 
to accurately localize the corner point. To this end we propose 
a novel corner detector for 3D point clouds.   
The corner detection algorithm is mathematically 
formulated as follows. Given a point cloud 𝒫, we detect edges  
using the method described in Section II. As a result the data  
Fig. 2. Demonstration of shift in centroid for edge and non-edge points for a 
partial view of the Stanford bunny model. The query point 𝑝𝑖 is shown in red, 
the neighboring points Ѵ𝑖 are shown in green and the new centroid 𝐶𝑖 is shown 
in blue.  
points are classified as edge 𝐸, and non edge points 𝒩 such 
that 𝒫= (𝐸, 𝒩). For all points in 𝐸, we compute their 
curvature defined by the Eigenvector 𝑒 of the smallest 
Eigenvalue, 𝜆1, determined from the covariance tensor 𝛴𝐸 of 
the neighboring points in 𝐸. The covariance tensor used is a 
weighted linear combination of neighboring points proposed 
by [19]. The covariance tensor is defined as follows:  
𝛴𝐸=
1
∑(𝑅−𝑑𝑖) ∑
(𝑅−𝑑𝑖)(𝑝𝑖−𝑝̅)(𝑝𝑖−𝑝̅)𝑇
𝑛
𝑖=1
 ∀ 𝑝 ϵ 𝐸   (4) 
where 𝑅 is a predefined radius, 𝑑𝑖= ‖𝑝𝑖−𝑝‖2 and 𝑝̅ is the 
centroid of the neighboring points 𝑝 ϵ 𝐸. The advantage of 
this tensor is that it assigns smaller weights to distant points 
so as to increase repeatability in presence of clutter.  
Next, for each point 𝑝𝑖 𝜖 𝐸, we find its k nearest neighbors 
from 𝐸 defined as follows: 
𝒦= {𝑢1, 𝑢2, … . 𝑢𝑘 }   𝑢 ϵ 𝐸, |𝒦| = 𝑘              (5) 
Since corners in 3D may exist at the intersection of numerous 
edges, we assume to be dealing only with objects where the 
corners lie at intersection of either two or three edges. Thus it 
is important to distinguish between the type of corner, so as to 
define the required number of clusters. This check is 
performed by evaluating the maximum variation in Euclidean 
distance within each of the x, y and z directions as follows: .  
𝑁(𝒦𝑖)
{  
  
  
 | max
1<𝑖<𝑘𝑢𝑖(𝑥) −min
1<𝑖<𝑘𝑢𝑖(𝑥)| > 𝜌
  | max
1<𝑖<𝑘𝑢𝑖(𝑦) −min
1<𝑖<𝑘𝑢𝑖(𝑦)| > 𝜌
  | max
1<𝑖<𝑘𝑢𝑖(𝑧) −min
1<𝑖<𝑘𝑢𝑖(𝑧)| > 𝜌
            (6) 
where 𝑁(𝒦𝑖) is the number of clusters for the group of points 
that belong to 𝒦𝑖 while 𝑢𝑖(𝑥), 𝑢𝑖(𝑦) and 𝑢𝑖(𝑧) are the x, y 
and z values of each point 𝑢𝑖. 𝜌 is a fixed parameter that 
determines the minimum variance along each direction to 
indicate the existence of a corner at that particular location. If 
the variation in all three directions is above 𝜌, the corner is 
predicted to be located at the intersection of three edges. 
Similarly, if the variation is significant in two directions, the 
given corner will most likely exist at the intersection of two 
edges. Since our corner detector is based on a clustering 
approach, this check determines the number of clusters 𝑁, for  
splitting features. 


--- Page 3 ---

 
Our features associated with each cluster are 1) size of the 
cluster and 2) mean curvature vector of the cluster. These are 
shown in Fig. 3a) and 3b) where a corner of a washer (from 
the ShapeNet dataset [14]) lies at the intersection of three 
edges. In this case, since the corner belongs to a regular 
shaped cuboid, the mean curvature vectors 𝜇𝑖 associated with 
each cluster should be approximately orthogonal to each 
other. Thus, an angular variation between curvature vectors of 
the clusters in this range will indicate existence of such 
corners.  
Similarly, the size of clusters is an indication of how far the 
query point is from the actual corner. Considering the cube in 
Fig. 3a), a corner point will consist of k-nearest neighbors that 
have an equally distributed number of points with curvature 
vectors in three unique directions. The equal sized clusters are 
shown in red, yellow and purple. When the size of the three 
clusters is relatively equal, we consider the query point to be 
close to the actual corner. However, as shown in Fig. 3b), as 
the query point moves away from the real corner, its k-nearest 
neighbors will have points that consist of a bias towards a 
particular curvature direction. Thus, one of the clusters will 
be comparatively larger than the other two. This indicates that 
the query point is far away from the actual position of the 
corner. Thus, these properties can be used to distinguish well 
localized corners from edge points.        
Mathematically, we formulate these concepts as follows. 
Given the number of clusters from our initial condition, the 
neighboring points 𝑢𝑖 𝜖 𝐸 are partitioned into 𝑁 sets 𝑆=
{𝑆1, . 𝑆𝑛}, 𝑛= 2, 3 using k-means algorithm, based on the 
associated curvature vectors 𝑒. K-means algorithm will 
minimize the within-cluster variance among the curvature 
vectors which is defined as:  
min
𝑠
∑
∑
‖𝑒−𝜇𝑖‖2
𝑒∈𝑆𝑖
𝑘
𝑖=1
                       (7) 
where 𝜇𝑖 is the mean of curvature vectors for its respective 
cluster 𝑆𝑖. The geometrical properties associated with each 
cluster are their mean curvature vectors 𝜇𝑖 and the size of each 
cluster 𝑆𝑖. We use these properties to extract two prominent 
features a) angular variation in 𝜇𝑖 defined as 𝜙𝑖𝑗 in (9) and b) 
difference in size of the clusters denoted as |𝑆𝑖−𝑆𝑗|. We 
evaluate the angular variation in curvature between two 
clusters as follows: 
𝜙𝑖𝑗= 𝑎𝑡𝑎𝑛2 (‖𝜇𝑖⃑⃑⃑ × 𝜇𝑗
⃑⃑⃑ ‖, 𝜇𝑖⃑⃑⃑ ∙𝜇𝑗
⃑⃑⃑ )  𝑖, 𝑗= {1,2,3}, 𝑖≠𝑗    (8) 
where 𝜇𝑖⃑⃑⃑  and 𝜇𝑗
⃑⃑⃑  are mean curvature vectors for 𝑆𝑖 and 𝑆𝑗 
respectively. Thus the feature vector 𝜙𝑖𝑗 represents the 
angular variation in curvature for all N clusters. The second 
property is the difference in size of the sets given as |𝑆𝑖−𝑆𝑗| 
where 𝑖, 𝑗= {1,2,3}, 𝑖≠𝑗. Using these features, we classify 
the query point 𝑝𝑖 as a corner if it satisfies the following 
conditions: 
𝐶(𝑝𝑖) {1, { |𝑆𝑖−𝑆𝑗| < 𝜀,     𝑖, 𝑗= {1,2,3}, 𝑖≠𝑗 
 𝜃1 < 𝜙𝑖𝑗< 𝜃2,        𝜃1, 𝜃2 ∈[0, 𝜋]    
0,         otherwise
        (9) 
where 𝐶(𝑝𝑖) = 1 indicates a corner, 𝜀 is a slack variable while 
𝜃1, 𝜃2 define the allowable range of angular variation between 
Fig. 3. Demonstration of a corner that lies at the intersection of three edges, 
a) accurate localization of corner as the size of clusters 𝑺𝑖 is similar and b) 
poor localization of corner as the yellow cluster is significantly larger. 
the Eigenvectors. The introduction of the slack variable 𝜀 is 
meant to add robustness in case of noisy point clouds and 
allows the flexibility to classify a group of points as corners 
and finally use their centroid as the true corner.     
IV. EXPERIMENTAL RESULTS 
In the first section of our results, we evaluate the proposed 
algorithm for edge detection against state-of-the-art edge 
detection algorithms for organized and unorganized point 
clouds. We demonstrate our results on the RGB-D semantic 
segmentation dataset [20] for comparison. In the next section 
we evaluate the repeatability and accuracy of the corner 
detector on 3D models of washers from the ShapeNet dataset 
[14]. Finally, we show how the algorithms proposed above 
can be used to automate welding of a panel workpiece. All 
experiments described in the following sections are run on an 
Intel i7-4600M CPU with 2.9 GHz and 8GB RAM. No multi-
threading or any other parallelism such as OpenMP or GPU 
was used in our implementation.  
A. Evaluation of Edge Detection 
The 3D edge detection algorithm was evaluated using the 
RGB-D semantic segmentation dataset [20] that is acquired 
using the Microsoft Kinect device. The dataset provides 3D 
meshes and yaml files for 16 different scenes that includes 5 
categories of common grocery products such as packets of 
biscuits, juice bottles, coffee cans and boxes of salt. We use 
the 3D meshes to generate ground truth using Meshlab [15] 
that directly allows edge selection for the dataset. In addition, 
we use the yaml files to extract organized point clouds for 
comparative evaluation.   
We use three edge detection algorithms for comparison [5- 
7]. We will refer to the first two algorithms as D+HC and 
D+SC, respectively, throughout the rest of the paper. Both of 
these algorithms are available online as part of the PCL [4]. 
Their limitation is that they can only be implemented on 
organized point clouds. The third method is based on 
evaluating Eigenvalues of the covariance matrix that is 
defined by each point’s local neighborhood [7]. We will refer 
to this algorithm as EV. Unlike the first two methods, this alg- 
𝑺𝑖 
𝝁𝒊 
𝒑𝒊 
a) 
b) 


--- Page 4 ---

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 5. Comparison of all four edge detection algorithms on a scene from the RGB-D semantic segmentation dataset where a) ground truth extracted from the 
mesh of the scene, b) results of edge detection from the proposed approach, c) results of edge detection using EV [7], d) results of edge detection using D+HC 
[6] and e) results of edge detection using D+SC [5]. 
 
orithm can be implemented on both organized and 
unorganized point clouds. Finally, we will refer to our 
proposed edge detection algorithm as MS, Mean Shift based 
edge detector. 
Quantitave analysis of the results is performed using the 
precision and recall estimates. We conduct experiments with 
a nearest neighborhood of k = {10, 50, 100, and 150} (this 
parameter is consistent for all algorithms). For the particular 
dataset, it was observed that best results were obtained for k 
= 100 for all algorithms. For D+HC and D+SC, the main 
parameter is the depth discontinuity, d, which is evaluated 
over the range of 0.0002 < d < 0.5 and 0.0001 < d < 1.5 for 
each method respectively. In addition, D+SC also requires 
parameter tuning for Gaussian filter, detectable edge angle 
and line smoothing parameters. We experimented with 
Gaussian kernel sizes of {3x3, 7x7, 10x10, 20x20 and 
40x40}. Similarly we varied edge angle for a range of 
𝜃 𝜖 {10°, 20°, 25°, 30°, and 40°} while after experimenting 
with a few different values, the   minimum and maximum line 
width was kept at 10 and 30 respectively.  
From our experiments, we obtained the best results at d 
= 0.0002 and 0.0001 for D+HC and D+SC respectively. 
Additionally, a Gaussian kernel size of 40x40, 𝜃= 10°  and 
maximum and minimum line width at 10 and 30 were 
found to be optimal parameters for the given dataset. For 
EV, the only variable parameter is the factor of surface 
variation 𝜎𝑘(𝑝) that is used to distinguish between edge 
versus non- edge points. This parameter was varied 
between 0 < 𝜎𝑘(𝑝) < 27 and highest precision was achieved 
at 𝜎𝑘(𝑝) = 27. Finally, our proposed algorithm MS was 
evaluated for the threshold parameter varying between 
0 < 𝜆< 8 and optimal precision was achieved at 𝜆= 8.      
Precision and recall results for the parameters described 
above, are shown in Fig. 4. The graph clearly shows that MS 
has a consistently higher precision and recall as compared to 
all other algorithms. D+HC shows high precision in a very 
small range after which it quickly drops. Similarly EV and 
D+SC demonstrate even lower precision values that indicate 
a low level of TPs for generally majority of their parameter  
 
 
Fig. 4. Precision recall curve for proposed method, EV, D+HC and D+SC, 
with the fixed parameter of k = 100.  
range. These results are validated by a visual analysis of 
detected edges, shown in Fig. 5. It can be seen that EV and 
D+HC tend to miss out boundary edges, which results in 
lower precision. Additionally, D+SC generates noisier edges 
that further reduces TP and increases FP. As opposed to all 
three algorithms, results from the proposed algorithm are 
more precise and most similar to the ground truth edges.  
D+HC and D+SC still demonstrate faster computation 
times of an average time of 217 ms and 700 ms for one point 
cloud of size 640x480x3. After downsampling the cloud with 
a voxel grid leaf size of 0.005x0.005x0.005 and employing 
Kd tree search with k=100, for unorganized point clouds of 
size 307,200 points, EV and MS took an average of 4303 ms 
and 3515 ms respectively. We believe that the code can be 
optimized to reach much faster computations in future.  
B. Evaluation of Corner Detector 
In this section we evaluate the accuracy of the corner 
detector. For testing we use ten 3D models from the washer 
category of the ShapeNet dataset [14]. The corners in these 
3D models are manually labeled using Meshlab.  
The first experiment analyzes the accuracy of corner 
detection by varying 𝜀 for various 𝒦 nearest neighbors from 
the edge cloud 𝐸. For this experiment, the fixed parameters   
a) 
b) 
c) 
d) 
e) 
Precision 
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 
 
D+HC
D+SC
MS
EV
Recall 


--- Page 5 ---

 
 
Fig. 6 a) Precision recall curve versus 𝜀 for different values of 𝒦 nearest 
neighbours and b) precision recall curve for variable 𝜀= {1­6}, 30° < 𝜃1 <
80° and 110° < 𝜃2 < 140°.     
were 𝜌= 0.005, 𝜃1 = 60° and 𝜃2 = 130° while the variable 
parameters were 1 < 𝜀< 6 and 𝒦= {15, 20, 30}. As seen 
from the graph in Fig. 6a, since the washer models are noise 
free, even a very small value of epsilon results in high 
precision. This intuitively implies that the constraint of 
clusters being relatively equal in size holds true and the 
algorithm is able to detect corners precisely. However, the 
condition may be too strict to detect all corners. Thus, as slack 
variable 𝜀 increases to 3, the recall for detection quickly rises 
to 80%. When the 𝒦 nearest neighborhood is 20-30, a slack 
of 𝜀= 3 reaches a maximum recall, however, for 𝒦= 15 this 
value does not improve recall scores. Additionally, it is 
observed that for the entire parameter range, for 𝒦= 15, 
recall value continues to be low. This can be logically 
explained as for 𝒦= 15, if the number of clusters are N = 3, 
each cluster needs to have 5 points to be an exact corner. In 
this case, a slack of 3 points will result in a much larger bias 
in the cluster size with atleast one cluster left with only 2 
points. This size of a cluster is not significant enough to detect 
a corner correctly. Thus, for a very small neighborhood, large 
values for 𝜀 will result in poor performance. On the otherhand, 
for reasonably large neighborhoods of 𝒦= 20, 30, varying 𝜀 
by a few points will drastically improve results.  
In the next experiment, we vary 𝜃1 and 𝜃2 with 𝜀 to show 
that different types of corners can have a variable variation in 
their mean curvature values. We vary 30° < 𝜃1 < 80° and 
110° < 𝜃2 < 140° for 1 < 𝜀< 6 . All other parameters are 
kept fixed at 𝒦= 20, and  𝜌= 0.005. The precision recall 
curve is shown in the graph in Fig. 6b which indicates that 
with a broader range for angular variation, the recall value 
immediately rises to 100%, ensuring that all types of corners 
are detected. 
The corner detection algorithm is currently implemented in 
MATLAB R2016b and takes an average of 1.5 seconds for a 
point cloud of 27,000 points. We aim to increase this speed in 
future by implementing the algorithm in C++.    
C. Automatic Weld Seam Detection 
In this section we introduce an application of the proposed 
algorithms for automation of robotic welding. Traditionally, 
welding seams (path that the robot follows during welding) 
for straight line joints are determined by importing an 
accurately drawn 3D CAD model of the workpiece into a 
CAD/CAM based software. The software extracts edges and 
the user is able to select the sequence for the robot to weld the  
joint. We present a novel approach that is based on detecting 
edges and corners directly from a point cloud that can be used 
to generate weld seams, eliminating the requirement of 
accurate 3D drawing and offline programming.   
We test our method on 3D pointcloud of panel workpiece, 
captured using Asus Xtion. The panel workpiece only consist 
of straight line seams. For experimental evaluation, the point 
clouds have been cropped at a fixed size of 0.5x0.5x0.5m, to 
remove the surrounding walls and floor. We label the corners 
manually in Meshlab for groundtruth. Additionally, we 
compare our corner detection results with Harris 3D as a 
baseline, available from PCL.  
For the proposed corner detector, 𝜌 was fixed at 0.005m, 
𝜃1 = 60°, 𝜃2 = 140° and 1 < 𝜀< 6 for 𝒦= 20  nearest 
neighbors. For Harris 3D, we found the optimal threshold to 
be 0.01 while the radius for search was varied from 0.01 <
𝑟< 0.03. We observed that highest precision for Harris 3D 
was achieved at 𝑟= 0.01 but with in a very small parameter 
shift for 𝑟= 0.03, the precision drops as a high number of 
FPs are generated. However, recall grows as the algorithm is 
able to detect almost all corners. As opposed to this, the 
precision for our proposed algorithm continues to stay high 
for the entire range of  𝜀 indicating that very few FP corners 
were generated. Additionally, with a smaller value of 𝜀, only 
a few corners were detected, however, as 𝜀 is increased, all 
corners from the workpiece are successfully detected. The 
precision and recall curve for these parameters is shown in 
Fig. 7. It can be concluded that our proposed approach can be 
used to select specific type of corners much more accuarately 
while the Harris 3D is more suitable for detecting all kinds of 
sharp features together. Visually, this is demonstrated in Fig. 
8 where Harris 3D can detect corners but misses out a few 
desired corners for weld seam generation. On the other hand, 
our proposed corner detector can detect all desired corners 
from the panel point cloud.    
V. CONCLUSION  
We present a novel edge and corner detection algorithm 
applicable for unorganized point clouds. We demonstrate 
comparative evaluations on RGB-D semantic segmentation 
and 3D washer models from ShapeNet dataset for edge and 
corner detection respectively. Both of our methods show high 
accuracy as compared to state-of-the-art algorithms. We also 
implemented the two algorithms on a 3D pointcloud of panel  
workpiece, demonstrating that weld seams can be generated  
without the traditional CAD models.  


--- Page 6 ---

 
 
Fig. 7. Precision recall curve for Harris 3D and proposed corner detector on 
the panel dataset.   
ACKNOWLEDGMENT 
The authors would like to thank the National Research 
Foundation, Keppel Corporation and National University of 
Singapore for supporting this research that is carried out in the 
Keppel-NUS Corporate Laboratory. The conclusions put 
forward reflect the views of the authors alone, and not 
necessarily those of the institutions within the Corporate 
Laboratory.  
REFERENCES 
[1] M. Kazhdan and H. Hoppe, “Screened poisson surface reconstruction,” In ACM 
Transactions on Graphics, vol. 32(3), pp. 29, 2013.  
[2] T. Hackel, J. D. Wegner, and K. Schindler, “Contour detection in 
unstructured 3d point clouds,” In Proc. of the IEEE Conference on 
Computer Vision and Pattern Recognition, pp. 1610-1618, 2016. 
[3]  C. Weber, S Hahmann and H Hagen, "Sharp feature detection in point 
clouds." In IEEE Shape Modeling International Conference (SMI), pp. 
175-186, 2010. 
[4] R. B. Rusu, and S. Cousins, “3d is here: Point cloud library (pcl),” In 
IEEE International Conference on Robotics and automation (ICRA), 
pp. 1-4, 2011.  
[5] R. Bormann, J. Hampp, M. Hägele, and M. Vincze, “Fast and accurate 
normal estimation by efficient 3d edge detection,” In IEEE/RSJ 
International Conference on Intelligent Robots and Systems (IROS), 
pp. 3930-3937, 2015.  
[6] C. Choi, A. J. Trevor, and H. I. Christensen, “RGB-D edge detection 
and edge-based registration,” In IEEE/RSJ International Conference on 
Intelligent Robots and Systems (IROS), pp. 1568-1575, 2013. 
[7] D. Bazazian, J. R. Casas, and J. Ruiz-Hidalgo, “Fast and robust edge 
extraction in unorganized point clouds,” In International Conference on 
Digital Image Computing: Techniques and Applications (DICTA), pp. 
1-8, 2015.  
[8] H. Ni, X. Lin, X. Ning, and J. Zhang, “Edge detection and feature line tracing in 
3d-point clouds by analyzing geometric properties of neighborhoods,” Remote 
Sensing, vol. 8(9), pp.710, 2016.  
[9] C. Harris, C and M. Stephens, “ A combined corner and edge detector,” 
In Alvey Vision Conference, Vol. 15, No. 50, pp. 10-5244, 1988. 
[10] E. Mair, G. D. Hager, D. Burschka, M. Suppa and G. Hirzinger, 
“Adaptive and generic corner detection based on the accelerated 
segment test,” In European conference on Computer vision (ECCV), 
pp. 183-196, 2010.  
[11] T. Fiolka, J. Stückler, D. A. Klein, D. Schulz and S. Behnke, “Sure: 
Surface entropy for distinctive 3d features,” In International Conference 
on Spatial Cognition, pp. 74-93, 2012. 
 
Fig. 8. Corner detection for panel workpiece using point clouds captured from 
Asus Xtion. a) results of corner detection using Harris 3D, b) results of corner 
detection from the proposed approach, c) manually labelled corners. 
 
[12] B. Steder, R. B. Rusu, K. Konolige and W. Burgard, “NARF: 3D range 
image features for object recognition,” In IEEE/RSJ International 
Conference on Intelligent Robots and Systems (IROS), Vol. 44, 2010. 
[13] M. Greenspan and M. Yurick "Approximate kd tree search for efficient 
ICP, " pp. 442-448, 2003. 
[14] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, 
S. Savarese, M. Savva, S. Song, H. Su and J. Xiao, “Shapenet: An 
information-rich 
3d 
model 
repository,” 
arXiv 
preprint 
arXiv:1512.03012. 
[15] Cignoni, Paolo, M. Callieri, M. Corsini, M. Dellepiane, F. Ganovelli, 
and G. Ranzuglia. "Meshlab: an open-source mesh processing tool." In 
Eurographics Italian Chapter Conference, pp. 129-136. 2008. 
[16] K. Demarsin, D. Vanderstraeten, T. Volodine, and D. Roose, 
“Detection of closed sharp edges in point clouds using normal 
estimation and graph theory,” Computer Aided Design, vol. 39(4), pp. 
276–283, 2007. 
[17] P. Benkő, R.R. Martin, and T. Várady, “Algorithms for reverse 
engineering boundary representation models,” Computer-Aided 
Design, vol 33(11), pp.839-851, 2001. 
[18] G. R. Bradski, "Computer vision face tracking for use in a perceptual 
user interface," 1998. 
[19] F. Tombari, S. Salti, and L. Di Stefano, “Unique signatures of 
histograms for local surface description,” In European conference on 
computer vision (ECCV), pp. 356-369, 2010. 
[20] F. Tombari, L. Di Stefano, S. Giardino, "Online Learning for Automatic 
Segmentation of 3D Data", IEEE/RSJ Int. Conf. on Intelligent Robots 
and Systems (IROS '11), 2011.  
Corners  
Ground 
Truth 
Proposed 
Method
TP Corners  
Harris 3D 
FN Corners  
```

---

## Section 1.11: Edge_aware_point_set_resampling

Source File: Edge_aware_point_set_resampling.txt

### Paper Content:

```
# Edge_aware_point_set_resampling.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Edge_aware_point_set_resampling.pdf
# Output: ../layer2_completion/txt/Edge_aware_point_set_resampling.txt


--- Page 1 ---

Edge-Aware Point Set Resampling
HUI HUANG
Shenzhen Key Lab of Visual Computing and Visual Analytics / SIAT
and
SHIHAO WU
South China University of Technology
and
MINGLUN GONG
Memorial University of Newfoundland
and
DANIEL COHEN-OR
Tel-Aviv University
and
URI ASCHER
University of British Columbia
and
HAO (RICHARD) ZHANG
Simon Fraser University
Points acquired by laser scanners are not intrinsically equipped with nor-
mals, which are essential to surface reconstruction and point set render-
ing using surfels. Normal estimation is notoriously sensitive to noise. Near
sharp features, the computation of noise-free normals becomes even more
challenging due to the inherent under-sampling problem at edge singular-
ities. As a result, common edge-aware consolidation techniques such as
bilateral smoothing may still produce erroneous normals near the edges.
We propose a resampling approach to process a noisy and possibly outlier-
ridden point set in an edge-aware manner. Our key idea is to ﬁrst resample
away from the edges so that reliable normals can be computed at the sam-
ples, and then based on reliable data, we progressively resample the point
set while approaching the edge singularities. We demonstrate that our edge-
aware resampling (EAR) algorithm is capable of producing consolidated
point sets with noise-free normals and clean preservation of sharp features.
Authors’ addresses: hhzhiyan@gmail.com, shihao.wu312@gmail.com,
gongml@gmail.com,
cohenor@gmail.com,
ascher@cs.ubc.ca,
hao.r.zhang@gmail.com
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
show this notice on the ﬁrst page or initial screen of a display along with
the full citation. Copyrights for components of this work owned by others
than ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, to republish, to post on servers, to redistribute to lists, or to use
any component of this work in other works requires prior speciﬁc permis-
sion and/or a fee. Permissions may be requested from Publications Dept.,
ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax
+1 (212) 869-0481, or permissions@acm.org.
c⃝YYYY ACM 0730-0301/YYYY/12-ARTXXX $10.00
DOI 10.1145/XXXXXXX.YYYYYYY
http://doi.acm.org/10.1145/XXXXXXX.YYYYYYY
We also show that EAR leads to improved performance of edge-aware re-
construction methods and point set rendering techniques.
Categories and Subject Descriptors: I.3.5 [Computer Graphics]: Compu-
tational Geometry and Object Modeling—Curve, surface, solid, and object
representations
General Terms: Point set, Sampling, Edge-aware
Additional Key Words and Phrases: Point set resampling, normal estima-
tion, upsampling, surface reconstruction, surfel point set rendering
ACM Reference Format:
Huang, H., Wu, S., Gong, M., Cohen-Or D., Ascher U. and Zhang H. 2012.
Edge-Aware Point Set Resampling. ACM Trans. Graph. XX, X, Article X
(XX 2012), 12 pages.
DOI = 10.1145/XXXXXXX.YYYYYYY
http://doi.acm.org/10.1145/XXXXXXX.YYYYYYY
1.
INTRODUCTION
The last two decades have seen a considerable amount of work
on surface reconstruction from scanned point clouds. The use of
points as a modeling and rendering primitive has also been stud-
ied extensively [Gross and Pﬁster 2007]. Both tasks heavily rely
on having a quality normal associated with each point sample. In
particular, popular surface reconstruction techniques such as Pois-
son [Kazhdan et al. 2006] and RBF [Carr et al. 2001] are guided by
normal information and the well-known surfel-based point set ren-
dering [Pﬁster et al. 2000] operates on oriented samples. However,
points acquired by laser scanners are not intrinsically equipped with
normals; they must be estimated from acquired image or geometry
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 2 ---

2
•
H. Huang et al.
(a)
(b)
(c)
(d)
Fig. 1.
Points (222K) acquired by a laser scan (b) are corrupted with noise
and not intrinsically equipped with normals. Resampling the data without
accounting for surface singularities may smear the sharp features after sur-
face reconstruction (c). Our edge-aware resampling (EAR) leads to a piece-
wise smooth reconstruction (d) while preserving the sharp edges. Point col-
ors are the result of normal maps and the original object is shown in (a).
data. Acquired data are often tempered with noise and even outliers
which hinder the computation of normals. We stress that normal
estimation is sensitive to noise since normals are measured as ﬁrst-
order derivatives and numerical differentiation ampliﬁes noise.
Computing noise-free normals is much more challenging in the
presence of sharp features, e.g., see Figs. 1 and 2. The desire to pre-
serve the sharp features disallows the use of a na¨ıve point smooth-
ing operator prior to normal estimation, since such smoothing blurs
the edges. Alternatively, one can compute normals over the noisy
point set, typically via principal component analysis (PCA), and
then apply an edge-aware robust smoothing operator, such as bilat-
eral ﬁltering [Jones et al. 2004] or ℓ1-minimization [Avron et al.
2010], to the normals. While such solutions can generally “sepa-
rate” the two sides of an edge, in the vicinity of the edge some er-
roneous normals may still persist since the accuracy of these meth-
ods is limited by the noise level and sampling rate. In practice,
data near an edge are often unavoidably under-sampled and con-
tain more noise than smooth regions, making it difﬁcult to recover
sharpness directly at the edges.
Given a noisy point cloud with, possibly noisy, normals computed
by PCA, as shown in Figs. 2(a) and 2(b), respectively, Fig. 2(c)
and 2(d) show results of applying bilateral ﬁltering over the points
and PCA normals. As can be seen, near the edge some points have
incorrectly assigned normal directions to agree with point normals
at the wrong side of the edge. These seemingly small errors may
be ampliﬁed on the reconstructed surface, leading to visible arti-
facts, as demonstrated in Fig. 2(e) by an edge-aware surface re-
construction scheme using robust implicit moving least squares
(RIMLS) [ ¨Oztireli et al. 2009].
To circumvent this inherent problem, our strategy is to resample
the noisy point set judiciously and in an edge-aware manner. In
particular, since normal estimation close to edges is not reliable,
our key idea is to ﬁrst resample away from the edges. The result is
a set of oriented points away from the edges that are endowed with
reliable normals. Then based on these oriented points, we progres-
sively resample the point set while approaching the edges. Thus,
normal estimation proceeds from more reliable regions to less re-
liable regions (close to edges). Using the resampling strategy, we
avoid having to compute noise-sensitive derivative measures in dif-
ﬁcult regions. Resampling and normal estimation near edges are
guided by reliable data, in particular, reliable normals.
Our resampling algorithm is built on a robust edge-aware projection
operator which produces samples away from edges and a novel bi-
lateral projection operator which upsamples progressively so as to
ﬁll the edge regions. Resampling away from edges is enabled by
incorporating normal information into the projection, allowing the
projector to be edge-aware. Robustness to noise and outliers is en-
abled by the use of the ℓ1-median for data ﬁtting. As we approach
the edges, the bilateral projector considers both positional and cur-
rent normal information when computing the base, direction, and
distance of the projection. While the two latter attributes are the re-
sult of optimizing a bilateral objective function, the base location
is chosen to achieve even point distribution and fast convergence.
The result of our resampling algorithm is a consolidated point set
with noise-free normals and uniform point distribution throughout
and clean preservation of sharp features; see Fig. 2(g).
Previous works which also resort to resampling for point cloud
consolidation assume that the underlying surface is smooth [Alexa
et al. 2003; Lipman et al. 2007; Huang et al. 2009; Miao et al.
2009; ¨Oztireli et al. 2010]. Our edge-aware resampling (EAR)
scheme respects singularities, allowing effective handling of piece-
wise smooth surfaces. We show that EAR and its associated normal
estimation facilitate the reconstruction of such surfaces by edge-
oblivious methods such as Poisson [Kazhdan et al. 2006], alge-
braic point set surfaces (APSS) [Guennebaud and Gross 2007], and
Delaunay-based Cocone [Dey and Giesen 2001]. At the same time,
it enhances the performance of edge-aware reconstruction meth-
ods such as RIMLS; see Fig. 2(h). We also show that EAR can
be applied to upsample a point set, leading to superior rendering
results [Pﬁster et al. 2000; Vergne et al. 2010] near sharp features.
2.
RELATED WORK
Most point set resampling schemes aimed at consolidating a raw
point scan assume that the underlying surface is smooth. Early
work by Alexa et al. [2001] upsamples a point set through Voronoi
point insertion in local tangent spaces followed by moving least
squares (MLS) projection. Lipman et al. [2007] introduce the
parameterization-free, locally optimal projector (LOP), which is
driven by the use of the ℓ1-median. LOP is shown to be effective
in enhancing point sets while being robust to outliers and noise.
Huang et al. [2009] propose a weighted version of LOP which bet-
ter deals with non-homogeneous point density in the input. Miao et
al. [2009] develop a simpliﬁcation scheme for non-uniformly dis-
tributed point samples, which adaptively reﬂects the intrinsic geo-
metric features of the underlying shape. Recent work by ¨Oztireli
et al. [2010] presents a high quality isotropic point sampling tech-
nique for a given surface representation based on spectral analy-
sis, kernel methods and matrix perturbation theory. None of these
schemes were designed to handle sharp features, and some required
reliable normals as part of the input. Our resampling approach also
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 3 ---

Edge-Aware Point Set Resampling
•
3
(a) Noisy input with an edge.
(b) PCA normals.
(c) Bilateral point smoothing.
(d) Bilateral normal smoothing.
(e) RIMLS + Marching Cubes on (d).
(f) Resampling (b) away from edge.
(g) Resampling (f) close to edge.
(h) RIMLS + Marching Cubes on (g).
Fig. 2.
Comparison between bilateral smoothing and the proposed EAR approach. Each point is colored according to its normal direction and such normal
maps are used throughout the paper for rendering the results. The input (a) represents a ﬁn shape containing a sharp edge; the point cloud (1K points) is
corrupted with noise. In each subsequent result (b-h), we show a 2D cross-section view taken at the blue curve indicated in (a), as well as a zoomed-in 3D
view into the red window shown in (a). The results demonstrate that our resampling approach can effectively remove noise, provide reliable normals, preserve
sharp features, and facilitate edge-aware reconstruction methods such as RIMLS [ ¨Oztireli et al. 2009].
relies on ℓ1-median, but unlike LOP, it estimates and utilizes nor-
mals, doing so in an edge-aware manner.
Existing edge-aware point set resampling schemes either assume
that the sharp features are given or resort to explicit construction
of surface patches. For an accurate display of intersection curves
between point set surface sheets, the works in [Pauly et al. 2003;
Guennebaud et al. 2004; Guennebaud et al. 2008] present accu-
rate rendering of sharp creases and corners. These methods assume
that the intersection curves that deﬁne the sharp features are given.
Fleishman et al. [2005] develop a piecewise surface-reﬁtting algo-
rithm which allows projecting points and sampling the intersection
of surface patches from two sides of a sharp feature. Their method
requires a dense point set to start with while in practice, regions
near sharp features are often under-sampled [Salman et al. 2010].
Generally speaking, reliable ﬁtting of surfaces is computationally
expensive and sensitive to input data noise and outliers. Our method
does not rely on explicit feature identiﬁcation or patch ﬁtting; it
deals with heavy noise and under-sampling near edges by ﬁrst re-
sampling away from edges and then propagating reliable informa-
tion obtained there to the edges.
Sharp features are dealt with extensively in the mesh processing
literature. A typical task is to smooth a mesh surface while preserv-
ing sharp features, e.g., [Hildebrandt and Polthier 2004; Sun et al.
2007; Huang and Ascher 2008]. Bilateral ﬁltering [Fleishman et al.
2003; Jones et al. 2003] for mesh smoothing is related to our bi-
lateral projector since both perform optimization under a bilateral
objective function. However, mesh smoothing assumes an explicit
connectivity among the points.
More relevant are meshless methods which smooth point set sur-
faces that contain sharp features. The methods of [Adamson and
Alexa 2006; Guennebaud and Gross 2007] assume explicit repre-
sentation of sharp features. Salman et al. [2010] ﬁrst extract sharp
features from the point set and then directly generate piecewise
smooth surface triangle meshes. Fleishman et al. [2005] and Lip-
man et al. [2007] analyze the point set around sharp features and ﬁt
local surface patches over which points are projected. Algorithms
proposed in [Merigot et al. 2009; Weber et al. 2010] mainly focus
on detecting sharp features of a piecewise smooth surface from its
point cloud sampling. Other recent robust methods [ ¨Oztireli et al.
2009; Avron et al. 2010] reconstruct surfaces from point clouds
while respecting sharp features. All of these methods can produce
less than satisfactory results, e.g., erroneous normal estimates, near
sharp features as a result of severe noise and under-sampling. Re-
sampling the point set prior to feature extraction, patch ﬁtting, or
surface reconstruction enhances the performance of these methods
on a raw point scan.
Given the importance of normals in surface reconstruction and
point rendering, it is not surprising that there has been a tremen-
dous amount of work on normal estimation from raw point data.
Most methods resort to PCA or its variants [Hoppe et al. 1992;
Alexa et al. 2001; Pauly et al. 2002; Mitra et al. 2004; Lange and
Polthier 2005; Huang et al. 2009]. Near or on sharp features, PCA
normals tend to smear information across discontinuities. Bilateral
smoothing of PCA normals [ ¨Oztireli et al. 2009] provides some
remedy but can still produce erroneous results (see Figs. 2(d) and
2(e)). Moreover, sharp features are often under-sampled in point
scans which further hinders the performance of PCA. Another set
of techniques use Voronoi poles [Dey and Sun 2006] or Voronoi-
PCA [Alliez et al. 2007] to estimate normals. With under-sampling,
such interpolation-based methods cannot infer accurate normals
near sharp features. In our work, we also rely on bilateral process-
ing of PCA normals, but only to roughly detect edge locations so as
to enable a resampling away from edges. Normals at points close to
edges are then derived from reliable normals estimated away from
edges instead of using PCA.
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 4 ---

4
•
H. Huang et al.
(a) Noisy input.
(b) Resampling away from edges.
(c) Edge-aware upsampling.
(d) Upsampling for rendering.
Fig. 3.
Overview of EAR scheme. Given a noisy point scan (a) with 163K points, we ﬁrst resample away from edges, leaving gaps near sharp features (b).
Based on reliable normals associated with the point set thus obtained, we upsample while approaching the edges and ﬁlling the gaps (c). Point density can be
further increased through upsampling to obtain a quality point set rendering (d).
(c) 
(d) 
 
1
2
3
(b) 
(a) 
Fig. 4.
Upsampling via bilateral projection near a sharp feature: (a) us-
ing both positions and normals yields a better point insertion location (solid
green dot) than using positions alone (dashed green dot); (b) both projection
directions shown ﬁt nearby normals, but the one leading to solid red dot bet-
ter preserves point uniformity; (c) using bilateral weights properly projects
the point onto the latent surface (solid red dot), whereas using unilateral
weight pulls the point away (dashed red dot); (d) a series of progressive
projection operations upsample the sharp feature.
3.
OVERVIEW
Our EAR algorithm takes as input an unorganized and unoriented
3D point scan corrupted with noise, outliers, and under-sampling.
It produces a clean, uniform, and feature-preserving set of oriented
points that well approximates the underlying surface. An advantage
of the resampling approach is that the density of the point set can be
adjusted; e.g., for point set rendering, a dense point set is obtained.
To obtain noise-free normals especially near the possibly under-
sampled edge singularities, our resampling algorithm is separated
into two phases, as shown in Fig. 3. The ﬁrst phase resamples a
set of points away from edges, for which reliable normals can be
computed. In the second phase, we upsample to increase point den-
sity, progressively ﬁlling regions near the edges. Point insertion ap-
proaching the edges relies on the reliable normals generated in the
ﬁrst phase, leading to clean reconstruction of the sharp features.
Resampling away from edges. Starting with a noisy scan, such
as the one shown in Fig. 3(a), we ﬁrst perform PCA to estimate
normal directions and their consistent orientations as in [Huang
et al. 2009]. Next we apply bilateral smoothing of these normals
where the weighting scheme accounts for both positional and nor-
mal information. Iterative bilateral smoothing alone can generally
distinguish normals near edges and roughly reveal edge locations.
However, the normals may still smear across edges as shown in
Fig. 2(d). Our strategy is then to not compute normals too close
to edges but ﬁrst resample away, where normals can be estimated
reliably. The resampling is accomplished by a locally anisotropic
projection which accounts for the current normals. The result of
this phase is a set of oriented points with reliable normals, but leav-
ing gaps close to sharp features, as shown in Fig. 3(b).
Edge-preserving upsampling. Based on the reliable normals, ori-
ented points are inserted and projected onto the latent surface,
which is the unknown underlying surface deﬁned by the input point
set. For each inserted point, we ﬁrst select its base location at a mid-
point between two existing points, see Fig. 4(a). Then, the critical
question is along which direction to project the point, the green dot
in Fig. 4(b), to better preserve the sharp feature. To simplify the
problem, we constrain the normal of the inserted point to be the
same as the projection direction. Hence, a careful determination of
the direction serves two important objectives: (i) it ﬁts the normal
distribution of nearby points, and (ii) it helps maintain local point
uniformity. Once the projection direction is determined, we com-
pute an optimal projection distance so that the inserted point can be
moved onto the latent surface; see Fig. 4(c).
The steps for determining the base, direction, and distance of the
projection offer an integrated solution for inserting an oriented
point at a sparse spot anywhere on the surface. To properly han-
dle sharp features, both positional and normal information are ac-
counted for in all steps, making the projection operator bilateral and
edge-aware. As shown in Figs 4(d) and 3(c), repeating the above
upsampling process incrementally ﬁlls the gaps along edge singu-
larities and reconstructs the sharp features cleanly. The upsampling
process can continue to increase point densities to facilitate render-
ing of the point set surface, as shown in Fig. 3(d).
4.
RESAMPLING AWAY FROM EDGES
The input to our algorithm is an unorganized set of points Q =
{qj}j∈J ⊂R3, typically unevenly distributed and containing noise
and outliers. The output of the resampling step described in this
section is an oriented point set S = {si}i∈I = {(pi, ni)}i∈I ⊂
R6, consisting of cleaned point locations pi that better represent
the underlying smooth surface away from edges, as well as their
associated reliable and edge-aware normals ni.
The process of estimating normals near sharp features is particu-
larly delicate and challenging. Traditional normal estimation meth-
ods, e.g., [Hoppe et al. 1992], usually work accurately when the
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 5 ---

Edge-Aware Point Set Resampling
•
5
(a) 1% noise.
(b) LOP.
(c) σp = h.
(d) Upsampling.
(e) 2% noise.
(f) σp = h.
(g) σp = 3h.
(h) Upsampling.
Fig. 5.
EAR under different noise levels and the effect of neighborhood
size σp. Given a noisy input (a) with 18K points, LOP effectively removes
noise (b) but the points are smeared across the edges. In contrast, our re-
sampling (c) yields reliable data including both point locations and normals
away from edges, which facilitates the subsequent upsampling (d) towards
the edges. When a higher level of noise is present in the input (e), the resam-
pling should employ a larger local neighborhood size σp to ensure a clearer
patch separation about the edges and better smoothness away from edges,
e.g., contrasting (g) with (f), leading to higher quality upsampling (h).
underlying surface is smooth, but tend to smear information across
singularities such as corners or intersecting planes. Taking it as an
initial input, our approach amounts to an iteration between (i) sep-
arating and smoothing normals over the obtained point set, and (ii)
resampling the points away from edges while holding the normals
ﬁxed. Step (i) reveals the location of edges via normal separation
around edge regions, which enables an effective anisotropic projec-
tion operator in step (ii). The latter resampling away from edges, in
turn, can emphasize the edge locations (by sparsity) so that the pro-
cess in step (i) in the next iteration operates more accurately.
Separating and smoothing normals. The popular PCA-based
method for computing surface normal approximations from point
cloud data [Hoppe et al. 1992] uses local point neighborhoods. Cer-
tain errors may thus be retained when the input is highly noisy, as is
typically the case near sharp features; see Fig. 2(b). In order to pre-
serve sharp features, we therefore estimate the normal based on an
anisotropic neighborhood, as in the framework of bilateral normal
smoothing [Jones et al. 2004; ¨Oztireli et al. 2009]. In particular, for
a given point si = (pi, ni), we measure the difference between its
assigned normal ni and other normals in its neighborhood as
f(pi, ni) =
X
si′ ∈Nsi
∥ni −ni′∥2θ(∥pi −pi′∥)ψ(ni, ni′),
(1)
where ∥· ∥is the ℓ2-norm, Nsi = {si′|si′ ∈S ∧∥pi −pi′∥< σp}
under a given neighborhood size σp. The spatial and normal weight
functions are deﬁned by
θ(r) = e−r2/σ2p,
ψ(ni, ni′) = e
−
 
1−n⊤
i ni′
1−cos(σn)
!2
,
where the angle parameter σn scales the similarity of neighboring
normals and we set σn = 15◦by default. Our goal is to minimize
P
i∈I f(pi, ni), i.e., the normal differences between all points on
the surface and their neighbors. This can be achieved through iter-
atively updating ni for all i with
ni ←
P
si′ ∈Nsi θ(∥pi −pi′∥)ψ(ni, ni′)ni′
P
si′ ∈Nsi θ(∥pi −pi′∥)ψ(ni, ni′) .
(2)
The above formula works well to distinguish normals across dis-
continuities, classifying their directions into two disjoint clusters
near each sharp edge due to the high variance of PCA normals in
the vicinity; see Fig. 2(c). However, we can also see that direction
clustering does not really work for the point locations themselves,
and several points are incorrectly assigned a wrong normal direc-
tion. Noise and outliers in the input make it impossible to directly
obtain a clear classiﬁcation with respect to an edge in both spa-
tial and directional senses. Indeed, given such an oriented point
set, RIMLS may fail to reconstruct a surface that preserves sharp
features; see Figs. 2(d) and 2(e). Thus, with the normals {ni}i∈I
obtained by (2), we now turn to adjusting the locations {pi}i∈I to
complete a split-step iteration for the set S.
Resampling. Several efﬁcient resampling operators have been de-
signed to consolidate a noisy point set [Alexa et al. 2003; Lipman
et al. 2007] while being oblivious to normals. These work well
when the underlying surface is smooth. For a resampling opera-
tor to be edge-aware, however, it must account for normals, even
if they may not be entirely accurate. We seek such an operator that
would be easily implemented, robust to heavy noise, and utilize the
estimated normals around edges. To this end, we alter the LOP op-
erator [Lipman et al. 2007; Huang et al. 2009] and make it normal-
or edge-aware, allowing for resampling away from edges.
LOP takes as input a noisy point cloud, possibly with outliers, and
outputs a new point set which more faithfully adheres to the un-
derlying shape. The strength of LOP is that it operates well on raw
data, without relying on a local parameterization of the points or
on their local orientation. In intuitive terms LOP distributes a set
of points, within an optimization framework, to approximate their
ℓ1-median so as to achieve robustness to outliers and noise. At the
same time, a repulsion force is integrated into the optimization for-
mulation to obtain an even point distribution.
Although LOP works robustly for geometry reconstruction from
raw data, it is still an isotropic operator since the spatial weight
function θ(r) used there does not consider sharp geometric fea-
tures, see Fig. 5(b). However, in our context, following the bilateral
smoothing step (2), the processed normal directions indicate where
the edges approximately are. Thus we deﬁne
φ(ni, pi −qj) = e−(n⊤
i (pi−qj))2/σ2p,
and adjust the locations of the point pi by replacing θ(∥pi −qj∥)
with φ(ni, pi −qj) in the ﬁrst term of the LOP expressions of
[Huang et al. 2009].
Speciﬁcally, given a set of points Q as in the beginning of this sec-
tion, our anisotropic LOP algorithm deﬁnes a set of projected points
P = {pi}i∈I ⊂R3 by a ﬁxed point iteration P k+1 = G(P k), k =
0, 1, . . ., where
G(P k) = argmin
P ={pi}
(X
i∈I
X
j∈J
∥pi −qj∥φ(ni, pk
i −qj)
(3)
+
X
i∈I
λi
X
i′∈I\{i}
η(∥pi −pk
i′∥)θ(∥pk
i −pk
i′∥)


.
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 6 ---

6
•
H. Huang et al.
si 
(a) 
Surface   
v 
(b)   
sj 
si
bk
 
sk   
 
sk 
 
v 
Fig. 6.
Base location selection: (a) Inserting a point sk at the center of the
V-cell v then projecting it onto the surface may push it out of the V-cell; (b)
selecting midpoint under orthogonal distance picks bk instead of v. After
inserting sk and projecting it along its normal, the clearance at bk becomes
zero, preventing other points being inserted at bk.
The repulsion function is η(r) = −r. We choose the initial set
P 0 as a random subset of Q and its initial normal estimates are
computed using PCA and smoothed using (2). The balancing terms
{λi}i∈I vary at each point but depend on just one parameter, 0 ≤
µ < .5, controlling the repulsion force, as explained in [Lipman
et al. 2007; Huang et al. 2009]. In general, larger values of µ impose
higher penalties on points that get close to other points. We set
µ = 0.45 by default. Besides the repulsion parameter µ, another
important parameter σp used in both spatial weighting functions φ
and θ is tunable based on a rough density measure h = dbb/√m,
where dbb is the diagonal length of the bounding box and m the
size of the input set. The default setting is σp = h.
The resulting operator is therefore edge-aware and has the visible
effect of gently pushing points away from edges, see Fig. 5(c) for
example. This is because the normal-dependent weight function φ
down-weights large variation in geometric similarity, deﬁned as the
height difference of point qj over the tangent plane of the point pi.
If the noise level is high, we should increase the supporting neigh-
borhood size σp to ensure the necessary pushing-away “strength”
for a clearer point patch separation about the edges and piecewise
smoothness away from the edges, e.g., compare the resampling re-
sults in Figs. 5(f) and 5(g). The latter leads to higher quality up-
sampling and hence surface reconstruction results.
The bilateral normal adjusting formula (2) and the anisotropic LOP
(3) can be applied alternately several times (our default value is 3),
beneﬁting each other. Referring to the examples in Fig. 5, starting
from our resampling with reliable normals, we are able to progres-
sively upsample the point set towards the edges, see Figs. 5(d) and
5(h). Fig. 2 further shows that such resampling enables the RIMLS
procedure to retain sharp features, see Fig. 2(h), in marked im-
provement over Fig. 2(e).
5.
EDGE-PRESERVING UPSAMPLING
Given an oriented point set S with visually bald patches along
edges as, e.g., in Fig. 5(c), we now describe how to carefully re-
sample points, approaching edges with reliable data. This is done
through a sequence of insertion operations. For each insertion we
add a new oriented point (pk, nk) that fulﬁlls three objectives: (i)
pk lies on the underlying surface; (ii) nk is perpendicular to the
underlying surface at pk; and (iii) points are evenly distributed in
the local neighborhood after insertion.
(a) Input (750 points).
(b) LOP (375 points). (c) LOP (1,500 points).
(d) small σp (1,500 points).
(e) large σp (1500 points)
Fig. 7.
Resampling on a planar point set, which extends the example in
Fig. 6 of [Lipman et al. 2007]. While LOP excels at downsampling (b),
using it for upsampling (c) leads to uneven point distributions. Our approach
inserts at midpoints of (b) with the largest clearances, yielding an evenly
distributed point set (d). The inner boundary of (d) is well preserved under
a small neighborhood size. Using a large neighborhood respects the outer
boundary only and ﬁlls the interior with evenly distributed points (e).
Finding pk and nk directly under the above objectives requires
searching within the 5D solution space, which can be difﬁcult,
especially in the vicinity of sharp features. To make the problem
tractable, we design a novel projector, where the projection direc-
tion is constrained to be along the normal of the inserted point, i.e.,
pk = bk + dknk. The computation of the oriented point (pk, nk)
is then realized in three steps: ﬁnding a near-sparsest insertion base
bk (objective (iii)), optimizing the projection distance dk to move
the point onto the latent surface (objective (i)), and computing the
normal direction nk so that it ﬁts the neighborhood normal distri-
bution and best preserves sharp features (objective (ii)).
Base selection. The goal of choosing a good base bk is to en-
sure fast convergence to an even point distribution within the local
neighborhood, where the ensuing projection must be taken into ac-
count. Here we ﬁrst discuss how to choose a base location in the
neighborhood of an existing point si. Discussion on how to pick si
is deferred to the end of this subsection.
When ﬁnding the base location for point insertion the classical ap-
proach, e.g., [Alexa et al. 2003], uses a local Voronoi construc-
tion and picks the center of the largest empty Euclidean ball. How-
ever, the Euclidean distance does not take into account the neigh-
boring normals which dictate the ensuing projection. As shown in
Fig. 6(a), in the vicinity of sharp features the projection may push
the inserted point outside its clearing space, keeping the ball empty.
This would attract subsequent points being inserted into the same
base, namely the center of the ball, thus stalling the upsampling.
Therefore, we incorporate projection direction information into the
distance consideration when searching for the largest clearance
ball. Speciﬁcally, considering an existing point si and its neighbors
in the set Nsi, we wish to insert at a location b in si’s neighborhood
that maximizes C(b) = minsi′ ∈Nsi D(b, si′). Rather than using the
Euclidean distance for the distance function D(b, si′), we deﬁne it
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 7 ---

Edge-Aware Point Set Resampling
•
7
(a) Input (850 points).
(b) ρ = 1.
(c) ρ = 2.
(d) ρ = 3.
(e) ρ = 4.
(f) ρ = 5.
Fig. 8.
Upsampling results by EAR for different edge-sensitivity parame-
ter values ρ. The input (a) contains 850 points and was upsampled to 1,500
points in all the cases shown in (b - f). Larger values of ρ give higher priority
to inserting points along the sharp edge.
(a) Input.
(b) RIMLS.
(c) EAR.
(d) RIMLS.
Fig. 9.
EAR facilitates surface reconstruction. Given a sparse set (a) with
only 2,500 points, RIMLS reconstruction (b) does not produce a quality
piecewise smooth surface. With upsampling by EAR to 7,500 points (c),
the same RIMLS scheme results in a reconstruction of the Fandisk with
better quality and sharp feature preservation.
as the orthogonal distance between b and the normal extension at
si′,
D(b, si′) = ∥b −pi′ −n⊤
i′(b −pi′)ni′∥.
Since the distance calculation considers the normal directions of in-
serted points, which are also their projection directions, the stalling
problem is solved. Once a new oriented point sk is inserted at b, we
have C(b) = D(b, sk) = 0. The chance of further insertion near
b is signiﬁcantly reduced since even after projection, the normal
extension at point sk passes through b; see Fig. 6(b).
Computing the optimal position b in the neighborhood of si based
on D(·, ·) requires solving a constrained quadratic equation. To
quickly ﬁnd an approximate solution, here we limit our search to
the midpoints between si and its neighbors in Nsi and select the
midpoint with the largest clearance as the base bk. Fig. 7 shows
that while selecting among midpoints is only an approximation, it
is able to resample points with even distribution.
During the base selection we need to decide globally into which
neighborhood the next point should be inserted. Depending on the
application we may either want to insert the point where the density
is lowest (for uniform resampling) or place it along sharp bound-
aries (for sharp feature enhancement). To achieve both objectives
in a uniﬁed approach, we deﬁne the priority for the neighborhood
 
s1   s2 s3   
s4   s5 s6   
s1,s2,s3 
𝜃 
𝜓 
(b) 
(c)   
bk 
Surface 
nk 
s1   
s2   
s3   
s4 
s5 
s6 
(a)   
dk   
s4   s5 s6   
1 −𝐧𝑘
T𝐧
||b𝑘− || 
pk   
pi
i
Fig. 10.
Projection distance calculation. For each point si in (a), the Eu-
clidean distance weight and range space weight are obtained using the cor-
responding weight functions shown in (b) and (c), respectively. The ﬁnal
projected location pk is a weighted average of the projections of these
points on the straight line deﬁned by bk and nk.
 bk 
 
 
(a) 
(b) 
(c) 
 bk   
  
 bk 
 
0 
  
0 
0 
0 
 
n 
n 
n 
n 
n 
n 
0 
0 
 
nk   
  
nk 
  
nk 
  
nk 
  
nk 
 
nk   
  
sj   
si   
sj 
si 
sj 
si   
 dk
 dk
 dk
 fk
 fk
 fk
Fig. 11.
A 2D example for normal determination. The ﬁrst row shows lo-
cally optimal normal directions (red arrow) under different situations. The
next two rows plot the directional difference and the projection distance,
respectively. Note that (b) and (c), while appearing similar, have different
desired normal directions for preserving even distribution after projection.
of a given point si as
P(si) = max
si′ ∈Nsi
(2 −n⊤
i ni′)ρC(pi + pi′
2
),
where ρ is an edge-sensitivity parameter. When ρ = 0, the neigh-
borhood priority for si is determined solely by the largest midpoint
clearance. Hence, inserting a new point in the neighborhood with
the highest priority actually places its base at the midpoint that has
the largest clearance over the whole surface. When ρ > 0, higher
priority is given to insert points along sharp edges where the nor-
mals vary, see Fig. 8. In our context, as we ﬁrst resample away from
potential edges to obtain reliable normals, at this resampling step
for edge regeneration, we use the default value ρ = 5 to give higher
priority near edges, achieving the effect of sharp feature enhance-
ment, as shown, e.g., in Figs. 3(c) and 9(c).
It is worth noting that the ability to adaptively sample the latent
surface is an important feature of our method. Careful inspection
of Fig. 8 reveals that, with a high ρ value, our approach places all
newly inserted points along sharp edges and corners. Hence, if the
goal is to facilitate sharp feature reconstruction rather than direct
point set rendering as in Fig. 3(d), only a small number of point
samples need to be inserted; see Figs. 9 and 13 for examples. This
allows our approach to handle very large models.
Projection distance under a given normal. With the base location
bk selected, we now discuss how to project it onto the latent surface
along a given direction n, through determining the step size dk.
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 8 ---

8
•
H. Huang et al.
Input 1: 120◦angle at edge; 1.0% noise.
Input 2: 150◦angle at edge; 1.0% noise.
Input 3: 150◦angle at edge; 1.5% noise.
(a) A proﬁle showing normals computed by classical PCA are still noisy (red boxes show zoom-ins near the edge).
(b) PCA normals after bilateral normal smoothing do not reveal the edges.
(c) Normals after applying RIMLS to (b) showing quality degradation as noise and edge angle increase.
(d) Normals after applying ℓ1-minimization to (a) showing quality degradation as noise and edge angle increase.
(e) Normals after applying EAR to (a) showing better handling of noise and revelation of soft edges.
Fig. 12.
A comparison between EAR (e) and several well-known normal estimation and processing schemes including PCA (a), bilateral normal smoothing
(b), RIMLS (c), and ℓ1-minimization (d) [Avron et al. 2010] (results courtesy of Haim Avron). Three synthetic input point sets (shown in the ﬁrst row) were
tested at two noise levels. For each input, the underlying shape is characterized by a soft edge (120◦or 150◦angle) with 1K points. The results shown via 2D
cross-sections demonstrate that our resampling approach is not only robust to noise but also capable of handling soft edges.
This is achieved by minimizing a weighted total projection distance
between p = bk +dkn and existing points in the neighborhood. To
handle points near sharp features, the weights of neighboring points
are determined based on distances in both Euclidean and directional
spaces, giving the objective function
X
si∈Nbk
(n⊤(p −pi))2θ(∥p −pi∥)ψ(n, ni).
By ﬁxing the spatial weight θ at bk, we thus obtain the step size
dk(bk, n) =
P
si∈Nbk (n⊤(bk −pi))θ(∥bk −pi∥)ψ(n, ni)
P
si∈Nbk θ(∥bk −pi∥)ψ(n, ni)
.
(4)
As shown in Fig. 10, the above calculation is equivalent to pro-
jecting all existing points onto the straight line deﬁned by bk and
n = nk and then computing the weighted average location based
on weights determined using both Euclidean and directional terms.
In particular, the directional term ensures that points with normals
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 9 ---

Edge-Aware Point Set Resampling
•
9
Table I. Timing for EAR on four raw scans.
IP-N
FP-N
RA-T
OP-N
RC-T
Figure 1
222,543
22K
49 sec
800K
42 sec
Figure 14
161,994
16K
31 sec
700K
36 sec
Figure 16
99,416
10K
17 sec
600K
29 sec
Figure 17
291,365
30K
58 sec
1.8M
73 sec
IP-N: number of input points; FP-N: number of noise-free points; RA-T:
time for resampling away from edges; OP-N: number of output points; RC-
T: time for upsampling close to edges. All examples were run on an Intel(R)
Core(TM) i7 CPU 860@2.80GHz with 2GB RAM.
(a) Input (515 points).
(b) APSS.
(c) RIMLS.
(d) MLS
upsampling
(4,000 points).
(e) APSS over MLS.
(f) RIMLS over MLS.
(g) EAR
upsampling
(4,000 points).
(h) APSS over EAR.
(i) RIMLS over EAR.
Fig. 13.
Power of edge-preserving upsampling in our EAR scheme. For
a clean and oriented point set with a rather low density (a), APSS (b) and
RIMLS (c) cannot provide a good surface deﬁnition without upsampling.
MLS upsampling (d) improves the performances of APSS (e) and RIMLS
(f), but smear sharp features. With EAR (g), APSS (h) and RIMLS (i) suc-
cessfully preserve the sharp features.
more similar to the input n have higher weights. When the input
normal n is accurate, this property ensures that the inserted point
can be projected onto the latent surface even in the vicinity of sharp
features. Finding a good normal direction is therefore critical.
Normal determination. To ﬁnally determine pk = bk + dknk, we
now compute the normal vector nk. As shown in Fig. 11, here we
have two selection criteria: (i) nk ﬁts the normal distribution in the
local neighborhood of bk, i.e., f(bk, nk) of (1) is small; and (ii)
the moving distance dk(bk, nk) deﬁned by (4) is also kept small so
that the even distribution we had sought can be better preserved.
To compute nk efﬁciently, we limit our search to the two neighbor-
hoods surrounding ni and nj, respectively, where ni and nj are the
normals of the two endpoints used for generating bk; see Fig. 11.
We ﬁrst decide which neighborhood to use based on moving dis-
tance dk, that is,
l = argmin
l∈{i,j}
dk(bk, nl),
and then compute the normal nk by minimizing f(bk, n) while
holding the directional weight ψ(n, ·) at the ﬁxed normal direction
n = nl. This allows us to compute nk using (2) with a single it-
eration. Note that when both endpoints si and sj are on the same
smooth surface, for example in Fig. 11(a), the two neighborhoods
surrounding ni and nj overlap. Therefore, the ﬁnal solution nk
may be close to the average of ni and nj, providing a smooth in-
terpolation of existing normals.
(a) Raw scan.
(b) Direct Cocone.
(c) Cocone over MLS.
(d) Cocone over EAR.
Fig. 14.
Result comparison using the edge-oblivious Cocone surface re-
construction [Dey and Giesen 2001].
(a) Input.
(b) σn = 60◦. (c) σn = 30◦. (d) σn = 15◦.
Fig. 15.
Results of applying EAR with different parameters before Poisson
surface reconstruction on a dense noisy Ramesses model (800K points). As
expected, small values of σn tend to better enhance sharp features, but may
over-sharpen highly curved areas.
6.
RESULTS AND DISCUSSION
The presented EAR algorithm was tested on a variety of raw and
synthetic point scans. Processing times on raw scans are provided
in Table I. Unless speciﬁcally indicated in the captions, the default
parameter values as given in Sections 4 and 5 were applied for ob-
taining the presented results. Below we ﬁrst elaborate on some typ-
ical obtained results and then discuss limitations.
Results. We ﬁrst show EAR at work on a few synthetic models
containing both sharp and soft features separated by smooth sur-
face patches. In Figure 3, we use the Fandisk model to demon-
strate the capability of EAR to handle noisy input data; the input
point cloud was corrupted with both noise (2% of the bounding
box) and outliers (10% of the bounding box). The results show that
EAR not only smooths out noise in point positions, but also ef-
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 10 ---

10
•
H. Huang et al.
(a) Raw scan.
(b) Poisson over PCA normals.
(c) Poisson over MLS.
(d) Poisson over EAR.
(e) RIMLS over PCA normals.
(f) RIMLS over S with σp = h
(g) RIMLS over S with σp = 10h. (h) RIMLS over EAR with σp = h.
Fig. 16.
Result comparison on a raw scan (a) using the edge-oblivious Poisson [Kazhdan et al. 2006] and edge-aware RIMLS surface reconstructions. (b)
Poisson over oriented PCA normals. (c) Poisson over a ﬁltered and upsampled point set using MLS. (d) Poisson over output of EAR. (e) RIMLS over the same
oriented PCA normals used in (b). (f) RIMLS over an oriented point set S after resampling away from edges with σp = h. (g) RIMLS over the same point
set S with a much larger σp. (h) RIMLS over the output of the complete EAR with the same σp as in (f). The ability of EAR to lead to piecewise smooth and
feature preserving reconstructions in both scenarios is evident.
(a) Raw scan.
(b) MLS.
(c) Radiance scaling over (b).
(d) EAR.
(e) Radiance scaling over (d).
Fig. 17.
EAR for surfel point set rendering, where each output surfel is displayed using a single pixel and colored by its normal direction. The input scan
(a) of a shutter blind is noisy and unevenly distributed. MLS resampling (b) smears the edges whereas EAR (d) preserves them well. Comparing (c) and (e),
dominating edges are enhanced on EAR point set surface using radiance scaling [Vergne et al. 2010].
fectively handles smooth variations and sharp changes in normals.
Fig. 12 compares our resampling scheme with several well-known
normal estimation and smoothing schemes including PCA, bilateral
normal smoothing, RIMLS and ℓ1-minimization ﬁltering on a set
of shallow ﬁn-shapes with different edge angles and noise levels.
These results demonstrate superior performance of our method in
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 11 ---

Edge-Aware Point Set Resampling
•
11
(a) Raw scan (82K points).
(b) EAR results.
Fig. 18.
With accurate normals estimated near regions of close-by surface
sheets (a), our resampling algorithm performs well; see the middle zoom-
in in (b). However, the upsampling step is limited by the choice of the σp
parameter and may not ﬁll sufﬁciently large holes present in the point sam-
pling; see zoom-in’s with black borders in (b).
terms of robustness to noise and effective recovery of soft edge fea-
tures. Fig. 13 demonstrates the power of EAR, compared to MLS,
in resampling and more speciﬁcally, edge-preserving upsampling,
a highly sparse point set surface containing sharp features. With a
small amount of new insertions, the performance of reconstruction
methods such as APSS and RIMLS can be noticeably improved.
Figs. 1 and 13-16 all demonstrate how our EAR scheme can en-
hance the performance of existing surface reconstruction methods.
Four representative techniques are chosen: RIMLS, APSS, Poisson
and Cocone. The implementations of the ﬁrst three are from Mesh-
Lab and the last was provided by its authors. We also implemented
the well-known MLS projector which uses Voronoi-based upsam-
pling [Alexa et al. 2003], as the classical resampling operator to
compare our EAR scheme with. For all the existing algorithms, we
show the best results we were able to generate, following the guide-
lines provided by the distributed codes and published parameters.
The inputs in Figs. 1, 14 and 16 are raw scans of real objects. The
typical imperfections associated with digital scans, such as noise,
outliers, non-uniform point distribution, and missing data, are ubiq-
uitous in these data sets. EAR removes noise and outliers, ﬁlls in the
missing parts, preserves sharp features and, at the same time, facil-
itates various surface reconstruction methods. In particular, we can
observe the beneﬁts of applying resampling prior to surface recon-
struction and EAR is evidently out-performing MLS with respect
to preservation of sharp features. The proposed two steps, resam-
pling away from edges and edge-preserving upsampling, are both
indispensable for a satisfactory reconstruction, as shown in Fig. 16
for a wide variety of parameter values.
Fig. 15 presents results of applying EAR with different σn before
Poisson surface reconstruction on a large noisy data set, which is
more challenging and contains geometric details of different sizes.
By decreasing the parameter value σn, we are capable of better
enhancing sharp features. However, if σn is set too small, some
highly curved areas may be over-sharpened.
We also show the results of converting noisy and unoriented point
sets into clean point set surfaces with reliable, edge-aware normals
for direct surfel-based point set rendering. Both synthetic (Fig. 3)
and raw (Fig. 17) point scans were employed, demonstrating the
capability of EAR for handling smooth surfaces as well as sharp
features under a uniﬁed framework.
Limitations. When the noise level in the input point cloud is high
or the captured object contains close-by surface sheets, the initial
normal orientation using oriented PCA may be erroneous, subse-
quently causing errors in the resampling. This problem can be alle-
Fig. 19.
One limitation of our resampling scheme is that it is not designed
to handle open point sets — it may produce rough boundaries as a result.
viated by a more capable normal estimation technique, e.g., [Huang
et al. 2009], which can handle the close-by surface sheet problem;
this is illustrated in Fig. 18. In the same ﬁgure, we also see that
relatively large holes in the input point sampling are not ﬁlled by
our upsampling step in EAR since this step is limited by the choice
of σp, which measures the size of the neighborhood. A speciﬁcally
chosen large σp would lead to better gap ﬁlling. Also, we currently
use a ﬁxed neighborhood radius σp and angle parameter σn. Al-
though the default parameter setting has worked well for most of
our test cases, certain special situations may require a careful tun-
ing of these two parameters to deal with elusive sharp features.
Another limitation of our EAR scheme is that it is not designed
to handle open boundaries and may produce less than satisfactory
results in that situation, as shown in Fig. 19. Finally, under severe
noise or under-sampling, our scheme may over-smooth geometric
details and sharp features or over-sharpen an edge depending on the
edge sharpness, as can be observed in Fig. 16 by comparing (e) with
(h) and in Fig. 14(d); e.g., see edges in the black boxes. Currently,
our scheme does not have an adaptive parameter to locally control
the sharpness of the resampling results.
7.
CONCLUSIONS
We have described a novel resampling algorithm for converting
noisy scan data into a clean point set surface endowed with reli-
able normals. The core of our algorithm consists of two phases: a
robust edge-aware method which computes reliable normals away
from surface singularities is followed by a novel bilateral projec-
tor which progressively upsamples toward these singularities. Both
phases are edge-aware and hence the resampling results preserve
sharp features. Speciﬁcally, the presented resampling algorithm en-
joys the following properties: (i) sharp edges are preserved and
smooth surfaces are maintained under a uniﬁed approach; (ii) the
resampling can reach any density requirement speciﬁed by users;
(iii) the reliable normals generated greatly facilitate existing sur-
face reconstruction methods.
In the future, besides research on open boundary handling, we aim
to develop a point set surface modeling system, where users can
perform local editing such as smoothing, edge enhancing and hole-
ﬁlling directly using resampling tools presented in this paper. We
would also like to design a real-time GPU-based resampling al-
gorithm, providing users with unlimited zoom capabilities when
viewing point set surfaces, without the need for mesh generation.
Finally, it is desirable to include an adaptive parameter to locally
control the sharpness of our resampling results, in particular when
sharp regions are severely corrupted and under-sampled.
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.


--- Page 12 ---

12
•
H. Huang et al.
Acknowledgments
The
authors
would
like
to
thank
all
the
reviewers
for
their valuable comments. This work is supported in part by
grants from NSFC (61103166), Guangdong Science and Tech-
nology Program (2011B050200007), National 863 Program
(2011AA010503), Shenzhen Science and Innovation Program
(CXB201104220029A), NSERC (293127, 84306 and 611370) and
the Israel Science Foundation. The models in Figures 14 and 16 are
courtesy of Andrei Sharf and the models in Figures 3, 13 and 15
are courtesy of AIM@SHAPE Shape Repository.
REFERENCES
ADAMSON, A. AND ALEXA, M. 2006.
Point-sampled cell complexes.
ACM Trans. on Graphics (Proc. of SIGGRAPH) 25, 3, 671–689.
ALEXA, M., BEHR, J., COHEN-OR, D., FLEISHMAN, S., LEVIN, D., AND
SILVA, C. T. 2001. Point set surfaces. IEEE Trans. Vis. & Comp. Graph-
ics, 21–28.
ALEXA, M., BEHR, J., COHEN-OR, D., FLEISHMAN, S., LEVIN, D., AND
SILVA, C. T. 2003. Computing and rendering point set surfaces. IEEE
Trans. Vis. & Comp. Graphics 9, 1, 3–15.
ALLIEZ, P., COHEN-STEINER, D., TONG, Y., AND DESBRUN, M. 2007.
Voronoi-based variational reconstruction of unoriented point sets. Proc.
Eurographics Symp. on Geometry Processing, 39–48.
AVRON, H., SHARF, A., GREIF, C., AND COHEN-OR, D. 2010. ℓ1-sparse
reconstruction of sharp point set surfaces. ACM Trans. on Graphics 20, 5,
135.
CARR, J. C., BEATSON, R. K., CHERRIE, J. B., MITCHELL, T. J.,
FRIGHT, W. R., MCCALLUM, B. C., AND EVANS, T. R. 2001. Re-
construction and representation of 3D objects with radial basis functions.
ACM Trans. on Graphics (Proc. of SIGGRAPH), 67–76.
DEY, T. AND SUN, J. 2006. Normal and feature approximations from noisy
point clouds. In FSTTCS 2006: Foundations of Software Technology and
Theoretical Computer Science. Lecture Notes in Computer Science, vol.
4337. 21–32.
DEY, T. K. AND GIESEN, J. 2001. Detecting undersampling in surface
reconstruction. In Symp. on Comp. Geom. 257–263.
FLEISHMAN, S., COHEN-OR, D., AND SILVA, C. T. 2005. Robust moving
least-squares ﬁtting with sharp features. ACM Trans. on Graphics (Proc.
of SIGGRAPH) 24, 3, 544–552.
FLEISHMAN, S., DRORI, I., AND COHEN-OR, D. 2003. Bilateral mesh
denoising. ACM Trans. on Graphics (Proc. of SIGGRAPH) 22, 3, 950–
953.
GROSS, M. AND PFISTER, H. 2007. Point-Based Graphics. Morgan Kauf-
man.
GUENNEBAUD, G., BARTHE, L., AND PAULIN, M. 2004. Real-time point
cloud reﬁnement. Proc. of Symposium on Point-Based Graphics, 41–49.
GUENNEBAUD, G., GERMANN, M., AND GROSS, M. 2008.
Dynamic
sampling and rendering of algebraic point set surfaces. Computer Graph-
ics Forum (special issue of Eurographics) 27, 3, 653–662.
GUENNEBAUD, G. AND GROSS, M. 2007. Algebraic point set surfaces.
ACM Trans. on Graphics (Proc. of SIGGRAPH) 26, 3, 23.
HILDEBRANDT, K. AND POLTHIER, K. 2004. Anisotropic ﬁltering of non-
linear surface features. Computer Graphics Forum (special issue of Eu-
rographics) 23, 3, 391–400.
HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, J., AND STUET-
ZLE, W. 1992. Surface reconstruction from unorganized points. ACM
Trans. on Graphics (Proc. of SIGGRAPH), 71–78.
HUANG, H. AND ASCHER, U. 2008. Surface mesh smoothing, regulariza-
tion and feature detection. SIAM J. Scient. Comput. 31, 1, 74–93.
HUANG, H., LI, D., ZHANG, H., ASCHER, U., AND COHEN-OR, D. 2009.
Consolidation of unorganized point clouds for surface reconstruction.
ACM Trans. on Graphics (Proc. of SIGGRAPH ASIA) 28, 5, 176.
JONES, T., DURAND, F., AND DESBRUN, M. 2003. Non-iterative, feature
preserving mesh smoothing. ACM Trans. on Graphics (Proc. of SIG-
GRAPH) 22, 3, 943–949.
JONES, T. R., DURAND, F., AND ZWICKER, M. 2004. Normal improve-
ment for point rendering. IEEE Computer Graphics and Applications 24,
53–56.
KAZHDAN, M., BOLITHO, M., AND HOPPE, H. 2006. Poisson surface
reconstruction. Proc. Eurographics Symp. on Geometry Processing, 61–
70.
LANGE, C. AND POLTHIER, K. 2005. Anisotropic smoothing of point sets.
Comput. Aided Geom. Des. 22, 7, 680–692.
LIPMAN, Y., COHEN-OR, D., AND LEVIN, D. 2007. Data-dependent MLS
for faithful surface approximation. Proc. Eurographics Symp. on Geom-
etry Processing, 59–67.
LIPMAN, Y., COHEN-OR, D., LEVIN, D., AND TAL-EZER, H. 2007.
Parameterization-free projection for geometry reconstruction.
ACM
Trans. on Graphics (Proc. of SIGGRAPH) 26, 3, 22.
MERIGOT, Q., OVSJANIKOV, M., AND GUIBAS, L. 2009. Robust voronoi-
based curvature and feature estimation. SIAM/ACM Joint Conf. on Geo-
metric and Physical Modeling, 1–12.
MIAO, Y., DIAZ-GUTIERREZ, P., PAJAROLA, R., GOPI, M., AND FENG,
J. 2009. Shape isophotic error metric controllable re-sampling for point-
sampled surfaces. Proc. IEEE Conf. on Shape Modeling and Applica-
tions, 28–35.
MITRA, N. J., NGUYEN, A., AND GUIBAS, L. 2004. Estimating surface
normals in noisy point cloud data. Int. J. Comput. Geom. and Appl. 14, 4–
5, 261–276.
¨OZTIRELI, C., ALEXA, M., AND GROSS, M. 2010. Spectral sampling of
manifolds. ACM Trans. on Graphics (Proc. of SIGGRAPH ASIA) 29, 5,
168.
¨OZTIRELI, C., GUENNEBAUD, G., AND GROSS, M. 2009. Feature pre-
serving point set surfaces based on non-linear kernel regression. Comp.
Graphics Forum 28, 2, 493–501.
PAULY, M., GROSS, M., AND KOBBELT, L. P. 2002. Efﬁcient simpliﬁca-
tion of point-sampled surfaces. Proc. of IEEE Visualization, 163–170.
PAULY, M., KEISER, R., KOBBELT, L. P., AND GROSS, M. 2003. Shape
modeling with point-sampled geometry. ACM Trans. on Graphics (Proc.
of SIGGRAPH) 22, 3, 641–650.
PFISTER, H., ZWICKER, M., VAN BAAR, J., AND GROSS, M. 2000. Sur-
fels: surface elements as rendering primitives. In ACM Trans. on Graph-
ics (Proc. of SIGGRAPH). 335–342.
SALMAN, N., YVINEC, M., AND MERIGOT, Q. 2010. Feature preserving
mesh generation from 3D point clouds. Proc. Eurographics Symp. on
Geometry Processing 29, 5, 1623–1632.
SUN, X., ROSIN, P. L., MARTIN, R. R., AND LANGBEIN, F. C. 2007.
Fast and effective feature-preserving mesh denoising. IEEE Trans. Vis.
& Comp. Graphics 13, 5, 925–938.
VERGNE, R., PACANOWSKI, R., BARLA, P., GRANIER, X.,
AND
SCHLICK, C. 2010. Radiance scaling for versatile surface enhancement.
Proc. symposium on Interactive 3D graphics and games.
WEBER, C., HAHMANN, S., AND HAGEN, H. 2010. Sharp feature detec-
tion in point clouds. Proc. IEEE Conf. on Shape Modeling and Applica-
tions, 175–186.
Received September 2011; accepted Aug 2012
ACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.
```

---

## Section 1.12: Edge-crease detection and surface reconstruction from point clouds using a second-order variational model

Source File: Edge-crease detection and surface reconstruction from point clouds using a second-order variational model.txt

### Paper Content:

```
# Edge-crease detection and surface reconstruction from point clouds using a second-order variational model.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Edge-crease detection and surface reconstruction from point clouds using a second-order variational model.pdf
# Output: ../layer2_completion/txt/Edge-crease detection and surface reconstruction from point clouds using a second-order variational model.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/267367007
Edge-crease detection and surface reconstruction from point clouds using a
second-order variational model
Conference Paper  in  Proceedings of SPIE - The International Society for Optical Engineering · September 2014
DOI: 10.1117/12.2069666
CITATION
1
READS
369
2 authors:
Massimo Zanetti
Fondazione Bruno Kessler
26 PUBLICATIONS   579 CITATIONS   
SEE PROFILE
Lorenzo Bruzzone
University of Trento
737 PUBLICATIONS   46,519 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Massimo Zanetti on 26 March 2015.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

Edge-crease detection and surface reconstruction from point
clouds using a second-order variational model
Massimo Zanetti and Lorenzo Bruzzone
University of Trento, Department of Information and Communication Technology,
via Sommarive 5, Trento 38123, Italy
ABSTRACT
The automatic detection of geometric features, such as edges and creases, from objects represented by 3D point
clouds (e.g., LiDAR measurements, Tomographic SAR) is a very important issue in diﬀerent application domains
including urban monitoring and building reconstruction. A limitation of many methods in the literature is that
they rely on rasterization or interpolation of the original grid, with consequent potential loss of detail. Recently,
a second-order variational model for edge and crease detection and surface regularization has been presented
in literature and succesfully applied to DSMs. In this paper we address the generalization of this model to
unstructured grids. The model is based on the Blake-Zisserman energy and allows to obtain a regularization of
the original data (noise reduction) which does not aﬀect crucial regions containing jumps and creases. Speciﬁcally,
we focus on the detection of these features by means of two auxiliary functions that are computable by solving
speciﬁc diﬀerential equations. Results obtained on LiDAR data by solving the equations via Finite Element
Method are presented.
Keywords: Point clouds, edge detection, crease detection, variational methods, Blake-Zisserman functional,
remote sensing
1. INTRODUCTION
Nowadays, the increasing availability of remote sensing active systems (e.g. LiDAR and SAR with Tomography
capability) makes it possible to produce detailed representations of objects on the ground in the form of dense 3D
point clouds. For example, airborne LiDAR data acquisitions are now available with point densities ranging from
1 to 50 pts/m2. On the other hand, according to the recent developments on multi-pass InSAR techniques such
as Tomographic SAR Inversion (TomoSAR), densities comparable to LiDAR can be achieved by using multiple
very high resolution (VHR) SAR images acquired at diﬀerent viewing angles.1,2 In terms of data availability
this is of great relevance, in particular thanks to the large amount of images provided by new spaceborne
VHR SAR sensors like those mounted on Cosmo-SkyMed and TerraSAR-X satellites. This scenario oﬀers the
possibility to achieve signiﬁcant improvements in all those human activities where information extracted from
the real 3D geometry of objects is of major importance, e.g. urban monitoring, identiﬁcation of man-made
structures, building reconstruction. In order to concretize this potential, the increasing level of detail available
in the produced data must be rigorously followed by an increasing accuracy, eﬀectiveness and automation, of
the methods of analysis. Moreover, working directly on the original point cloud is a non-trivial task. Most of
the techniques available in the literature cope with the problem of segmenting the scene and recognizing objects
by means of model-based or context-driven approaches.3,4 These methods are shown to be eﬀective mostly on
speciﬁc contexts, and basically they rely on prior knowledge. Other techniques (most of them coming from
the literature of image processing) are based on segmentation steps where more general geometric features are
detected (e.g. boundaries, edges, creases, slopes) without assuming any predeﬁned model.5–7 A limitation of
these methods is that most of them require the unstructured point cloud to be rasterized or interpolated into
a regular grid. When the density of the point cloud is low this may result in a potential loss of detail. On the
other hand, the generalization of these methods to unstructured grid of points is not straightforward.
In this paper we address the problem of edge-crease detection and surface reconstruction from point clouds.
This work is intended to be a ﬁrst step for the generalization of a recently proposed variational model of second
Address all correspondence to: massimo.zanetti@unitn.it


--- Page 3 ---

order based on the Blake-Zisserman functional, which has been successfully applied to the automatic segmentation
of digital surface models (DSMs).8 The Blake-Zisserman model represents a generalization of the well-known
Mumford-Shah, and provides a general framework for both data segmentation and surface reconstruction. The
model is based on a cost-functional made up of several terms which interaction allows to obtain a smooth
approximation of the data (more precisely a piecewise linear approximation) that preserves the morphology of
constant gradient areas. These reconstructed regions are explicitly detected by partitioning the data using the
graph of two auxiliary functions: the edge-detector function (which detects discontinuities of the data) and
the edge/crease-detector function (which also detects discontinuities of the gradient). This model overcomes
several drawbacks of the widely used ﬁrst order model, such as, over-segmentation of steep gradients, lack of
second order information, degraded resolution on multiple junctions of boundaries. We aim at extending this
approach to the case of unstructured clouds of points by ﬁnding proper ways for generalizing it by taking
advantage from the intrinsic non-homogeneous characteristics of the data. To address this challenging problem,
in this paper we investigate the deﬁnition of a general algorithm based on Finite Element Methods (FEM).
After some manipulation of the original fomulation by Blake-Zisserman aimed at producing a new model for
which a numerical solution can be computed, we come up to a model that depends on weakly diﬀerentiable
Sobolev functions. Therefore, the original datum (the points to be segmented) and the unknowns of the problem
(the smoothed datum and the auxiliary functions) must be represented as surfaces. Firstly, the 3D points are
projected onto a plane and the original datum is represented as a surface deﬁned on such a plane. The projected
planar points are considerd as the new unstructured domain on which the problem is deﬁned. Secondly, from the
given optimization problem we derive diﬀerential equations whose solutions represent the discretization of the
two auxiliary functions that detect edges and creases. The solution of the diﬀerential equations is numerically
approximated by triangulating the unstructured planar domain and by splitting the variational weak formulation
of the equations over the triangles. The paper presents the analytical details of this methodology and preliminary
results obtained on LiDAR data.
2. THEORETICAL MODEL
The Blake-Zisserman variational model for image segmentation is a second order model introduced to overcome
the drawbacks of the well-known Mumford-Shah (MS), which is a ﬁrst order model.9
Because of intrinsic
properties of the functional, by minimizing the integral sum with the gradient component the approximation of
the original datum results in a piecewise constant function which is discontinuous on C1 arcs which represent the
boundary of the object in the scene (e.g., building facets). Unfortunately, in the regions where the gradient is
too steep, the solution of the minimization is often a step function that represents a degraded over-segmentation
of the original datum.10
In the Blake-Zisserman formulation,11 the gradient component is replaced with the squared norm of the
Hessian matrix. The minimization of this second order information allows the solution to have variations of
ﬁrst order, leading to a piecewise linear approximation of the original datum. In this way the problem of the
over-segmentation is solved. Moreover, in the context of surface reconstruction the linear components of the
solution are the recovered surfaces components of the building (facades, roof pitches, etc.). Another property of
this model, which is of great importance for the segmentation of DSMs8 or point clouds in general (rather than
images12), is that also second order geometric features such as creases between roof pitches are detectable.
In the sequel we will present the variational approach to segmentation in the Blake-Zisserman sense and
a numerical strategy for the solution of the related minimization problem. From the theory it turns out that
the original formulation by Blake-Zisserman is too strong to prove the existence of minima. Thus, the problem
is moved into a weaker space of functions that makes it possible to ﬁnd only solutions that make sense for
visual/surface reconstruction purposes.13 Since a direct minimization of the weak functional is not possible, a
relaxation of the problem by means of Γ-convergence is given.13 Moreover, the Γ-convergence approximation
gives rise to two auxiliary functions that explicitly detect features of ﬁrst and second order of the data (data
discontinuity and gradient discontinuity).
2.1 A second order variational model for segmentation
The variational formulation of the segmentation is given for open domains. The discretization step takes place
after the formulation of the problem. Let us consider the datum to be segmented as a function g : Ω→[0, M],


--- Page 4 ---

where Ω⊂R2 is an open bounded domain and M any positive constant. The idea is to ﬁnd u, a regularized
version of g, by minimizing the functional
BZ(u, K0, K1) := µ
Z
Ω
|u −g|2 dx +
Z
Ω\(K0∪K1)
|∇2u|2 dx
+ αH1(K0 ∩Ω) + βH1((K1 \ K0) ∩Ω)
(1)
among all u ∈C2(Ω\ (K0 ∪K1)) ∩C1(Ω\ K0) and compact sets K0, K1 such that K0 ∪K1 is a closed subset
of Ω. H1 is the one-dimensionall Hausdorﬀmeasure and µ, α, β are positive parameters. The solution u (if
it exists) is twice diﬀerentiable outside K0 ∪K1 and has to be at least diﬀerentiable outside K0. Therefore
discontinuities of u are allowed both on K0 and K1, whereas discontinuities of the gradient are allowed only on
K1. Let us heuristically analyze the properties of a minimizing triplet (u, K0, K1) for the functional (1). The
minimization of the term
R
Ω|u −g|2 forces u to be close to g, whereas by minimizing the term containing |∇2u|2
the solution tends to be smooth (a linear behaviour is expected) outside K0 ∪K1. The terms αH1(K0 ∩Ω)
and βH1((K1 \ K0) ∩Ω) are introduced to control the size of the sets K0, K1. In optimization problems of this
kind both bulk (here 2-dimensonal) and surface (here 1-dimensional) energies are minimized simultaneously on
unknown supports, henceforth the minimization is carried on both in a function domain (unknown u) and a
set domain (compact sets K0, K1). Such problems are well-known as free discontinuity problems14 because the
discontinuity sets K0, K1 are not known a priori.
Since the original formulation by Blake-Zisserman is too strong to prove the existence of minima (i.e., for
justifying the validity of the heuristical approach given above), the problem is moved into a weaker space
of functions where the classical results of Calculus of Variations (the branch of Mathematics that deals with
minimization of integral functionals) can be used for proving the existence of minima that are also suﬃciently
regular for applying in real problems. The weak formulation of the Blake-Zisserman functional is given in the
space of the generalized special functions of bounded variation, GSBV (Ω),15 as:
F(u) =
Z
Ω
µ|u −g|2 + |∇2u|2 dx + αH1(Su) + βH1(S∇u \ Su),
(2)
where Su, S∇u denote the discontinuity set for u and for the gradient of u, respectively. The existence of minima
for this functional can be stated, moreover, after a suitable identiﬁcation a correspondence with a minimum
for the strong formulation is possible.16 Because of the one-dimensional measure terms deﬁned on unknown
supports, the functional F(u) is not diﬀerentiable. Thus the classical methods based on gradient descent cannot
be directly applied for computing a minimizer. In order to overcome this problem a Γ-convergence approximation
of F(u) by the family of elliptical functionals Fϵ, deﬁned on proper Sobolev spaces, is given13
Fϵ(u, s, z) := δ
Z
Ω
z2|∇2u|2 dx + ξϵ
Z
Ω
(s2 + oϵ)|∇u|2 dx
+ (α −β)
Z
Ω
ϵ|∇s|2 + 1
4ϵ(s −1)2 dx
+ β
Z
Ω
ϵ|∇z|2 + 1
4ϵ(z −1)2 dx
+ µ
Z
Ω
|u −g|2 dx
(3)
for ϵ →0 (ϵ is called the Γ-convergence parameter). The size of the discontinuity sets Su, S∇u is approximated
by means of two auxiliary functions s, z : Ω→[0, 1], respectively. Here, δ, α, β, µ are positive parameters, ξϵ, oϵ
are inﬁnitesimals faster than ϵ. For each ϵ > 0, the functional Fϵ admits a minimizer (uϵ, sϵ, zϵ). By sending
ϵ →0, thanks to the properties of the Γ-convergence, the sequence {(uϵ, sϵ, zϵ)}ϵ converges to a triplet (u, s, z)
where u is a minimizer of the limit functional F. In order to understand what the functions s and z represent,
let us see more in detail the properties of a minimizer (uϵ, sϵ, zϵ) of (3). Because of the minimization, in order
to keep the term 1/4ϵ(sϵ −1)2 bounded, sϵ is forced to be 1 almost everywhere over Ω, except where big values
of |∇uϵ|2 are achieved (i.e., in presence of a discontinuity), where s2
ϵ (hence sϵ) is forced to take the value 0.


--- Page 5 ---

Moreover, the minimization of the term ϵ|∇sϵ| avoids big oscillations of the function sϵ, hence the transition of
sϵ between 0 and 1 turns out to be smooth. Similar arguments stand for zϵ, which goes to 0 in correspondence
of big values of |∇2uϵ|2 (i.e., in presence of a discontinuity of the gradient), and for |∇zϵ|, which causes smooth
variations of zϵ between 0 and 1. From the discussion above it follows that for small values of ϵ, a minimizer uϵ
is an approximation (in the Γ-convergence sense) of a real minimizer u, whereas sϵ, zϵ are approximations of the
indicator functions of the discontinuity sets Su, S∇u.
2.2 Minimization and discretization approaches
By following the framework given above, because of the Γ-approximation given by the family of functionals Fϵ,
an approximation of a minimizer of the funcitonal (2) can be found by minimizing (3), where ϵ is ﬁxed. The
cross terms z2|∇2u|2 and (s2 + oϵ)|∇u|2 do not allow a direct minimization with respect to the global variable
(u, s, z). Since the functional Fϵ is strictly convex in the directions (·, s, z), (u, ·, z) and (u, s, ·), a general block
iterative scheme can be used8



s1 = arg mins Fϵ(u0, s, z0)
z1 = arg minz Fϵ(u0, s1, z)
u1 = arg mins Fϵ(u, s1, z1)
(4)
where the iterations are repeated subsequently, until small variations of the functional according to a given
threshold, are achieved. For each partial minimization problem in (4) only one of the three components u, s, z is
considered as a variable, and the remaining two are considered as constants. Thus, this formulation is equivalent
to



s1 = arg mins F 1
ϵ (s, u0)
z1 = arg minz F 2
ϵ (z, u0)
u1 = arg mins F 3
ϵ (u, s1, z1)
(5)
where
F 1
ϵ (s, u) := ξϵ
Z
Ω
(s2 + oϵ)|∇u|2 dx + (α −β)
Z
Ω
ϵ|∇s|2 + 1
4ϵ(s −1)2 dx + k1,
(6)
F 2
ϵ (z, u) := δ
Z
Ω
z2|∇2u|2 dx + β
Z
Ω
ϵ|∇z|2 + 1
4ϵ(z −1)2 dx + k2,
(7)
F 3
ϵ (u, s, z) := δ
Z
Ω
z2|∇2u|2 dx + ξϵ
Z
Ω
(s2 + oϵ)|∇u|2 dx + µ
Z
Ω
|u −g|2 dx + k3,
(8)
and k1, k2, k3 are constants that are irrelevant for ﬁnding the arguments of the minima. This general approach
allows to exploit the convexity of the functional in its three components u, s, z in such a way that, after a
proper discretization of the domain, the computation of each partial minimizer becomes an optimization problem
characterized by a sparse positive deﬁnite matrix.
In the case of structured domains (e.g., DSMs, images) the open set Ω⊂R2 is a rectangle and it is approx-
imated by a lattice of coordinates, whereas the diﬀerential operators of the ﬁrst and second order appearing
in the functional are approximated by using classical diﬀerence-schemes.8 The advantage of this approach is
that each functional F i
ϵ, i = 1, 2, 3, can be ﬁrstly discretized and then derived in its discrete form, leading to a
straightforward and compact implementation.
When the domain is an unstructured grid of points diﬀerence-schemes cannot be used, hence diﬀerent discretiza-
tion strategies of the optimization problem must be used. The minimization of the functionals F i
ϵ, i = 1, 2
(i.e., the ones related to the detection of edges and creases) is in both cases a problem described by an elliptic
Partial Diﬀerential Equation (PDE) which is a modiﬁcation of the well-known Poisson’s equation (in literature
is sometimes referred to as scalar elliptic problem). Since one of the most eﬀective approaches to solve such kind
of elliptical problems on unstructured domains is the Finite Element Method (FEM),17 we will give in the sequel
some preliminay results on edge-crease detection (with consequent detection of planar surfaces) on synthetic and
LiDAR data by using the FEM method.


--- Page 6 ---

3. NUMERICAL SOLUTION TO THE THEORETICAL MODEL
Since in the proposed modelization the variables are weakly diﬀerentiable Sobolev functions deﬁned on a two-
dimensional domain Ω⊂R2, the data g, u, s, z must be considered as two-dimensional surfaces in a three-
dimensional space.
It follows that the discretized representations of the data are uniquely determined by a
(possibly) unstructured two-dimensional grid of points {(xi, yi)}i, which represents the discretization of Ω, and
four families of real non-negative values {gi}i, {ui}i, {si}i, {zi}i, which arethe values of the surfaces on the grid
points. It is worth noting that, since LiDAR points are more generally clouds of 3D points, the assumption
of being surfaces is not always satisﬁed.
Accordingly, one of the pre-processing steps needed to apply our
model to the problem of edge-crease detection on LiDAR points consists in representing the original datum
as a surface. Once the discrete domain is deﬁned, the solutions of the optimization problems (6) and (7) are
numerically computed by solving the associated Euler Lagrange equations via the FEM. To this aim the domain
Ωis subdivided into triangles whose vertices are triplets of points extracted from {(xi, yi)}i and the equation
is splitted over each triangular component. The ﬂexibility of the triangulation approach can be exploited also
for further reﬁnement of the unstructured grid by adding points only in certain areas where higher resolution is
needed, such as in proximity of edges and creases.
3.1 Domain and surface representation
Let us consider a set of n points {Pi}i=1,...,n where each point is characterized by its east, north and height
coordinate Pi = (Ei, Ni, Hi). The ﬁrst step of the processing consists in representing the area of interest as a
surface. Here we propose a strategy for point selection and coordinate change that returns a subset of points
{pik}k ⊂{Pi}i in local coordinates pik = (xik, yik, gik) in such a way that the surface to be segmented, g, is
described by the values {gik}k on the 2-dimensional points {(xik, yik)}k. This technique is based on the projection
of the points {Pi}i onto an aﬃne plane Π. We caractherize this plane by its normal versor v ∈R3, |v| = 1, and
its displacement d with respect to the origin (0, 0, 0), thus Π = Πv
d is given by
Πv
d := {p ∈R3 | p · v = d},
where · denotes the standard Euclidean scalar product. Let M : R3 →R3 be a linear function (that we have
ﬁxed) which isometrically maps Πv
d into Πe
0, where e = (0, 0, 1). For each point Pi let di be its displacement with
respect to the plane Πv
d, i.e, di = Pi · v −d. If di is non-negative we consider its orthogonal projecton Qi ∈Πv
d,
otherwise the point is discarted. By deﬁnition the third coordinate of the point M(Qi) is 0, we deﬁne xi and
yi as the ﬁrst and the second coordinate of M(Qi), respectively. The third coordinate of the projected point is
deﬁned as gi := di. It is worth noting that, if v = (0, 0, 1) and M is the identity map then the points pi are just
the points Pi for which the height Hi is greater (or lower, according to the sign of d) than d. For the sake of
simplicity and without loss of generality we can rename the family of points {pik}k as {pi}i.
The last step to represent the data g as a surface consists in running a simple thinning algorithm, which
removes the points that underlie the surface layer. The algorithm places a uniform grid over the points {(xi, yi)}i
and within each grid cell keeps only the point with the highest g coordinate. In order to mantain the same density
of the original data the grid width should be chosen accordingly. For example for a density of 25 pts/m2 a suitable
grid width is 0.2 m.
3.2 Numerical minimization
We give here a brief explanation on how the numerical minimization of the functionals (6),(7), is addressed.
According to the theory of Euler Lagrange Equations, similarly to what happens by minimizing functions, it is
expected that in proximity of a minimizer the variations of a functional will approach zero. In general, the Euler
Lagrange equation of a functional is derived in such a way that its solution (a Sobolev function in this case)
corresponds to a point where the variations of the objective functional are null, hence a potential minimizer.
In general, being a solution of the Euler Lagrange equation is just a necessary condition for being a minimizer;
however, since in our case the objective functional is convex, it is also suﬃcient. After some computations we


--- Page 7 ---

get the following diﬀerential equations
ξϵ|∇u|2s + α −β
4ϵ
(s −1) −ϵ(α −β)∆s = 0,
(9)
δ|∇2u|2z + β
4ϵ(z −1) −ϵβ∆z = 0,
(10)
where ∆denotes the Laplacian operator, over the open domain Ω. Since these equations do not have trivial
solutions we do not consider boundary conditions here. It is important to point out here that, since in this paper
we do not perform the minimization of the functional (8), the values of the function u are not known. Anyway, u
is the smooth approximation of the original datum g; therefore, in order to give a preliminary qualitative analysis
of the proposed method, we will just replace u by g in equations (9) and (10). Of course, results will be aﬀected
by the noise.
The numerical solution of the equations is computed by means of Finite Element Method based on Riesz-
Galerkin approximation.17
The Sobolev spaces in which the problem is deﬁned are approximated by ﬁnite
dimensional subspaces that are generated by a family of elementary basis functions (usually called shape func-
tions) whose support is the union of few simple planar elements. The only information we have about the domain
Ωis in its discrete representation given by the family of points {(xi, yi)}i; thus, let us consider a (Delaunay)
triangulation Ωh = ∪kτk where each triangle τk has its vertices in {(xi, yi)}i. Here h is a discretization param-
eter and it equals the largest edge length among all the triangles. The triangles in Ωh are the planar elements
in which the domain is splitted. By considering the variational formulation of equations (9) and (10) on the
above-mentioned ﬁnite dimensional subspaces we get two symmetric positve deﬁnite sparse linear systems whose
solutions are {si}i and {zi}i.
4. EXPERIMENTAL RESULTS
In this section we show the results of edge and crease detection on a 3D point cloud. The study area is a portion
of a large dataset acquired over Pellizzano, Trento (Italy), in September 2012, by the airborne Riegl LMS-Q680i
LiDAR sensor. Points have a density greater than 25 pts/m2. The main object represented in the considered
scene is a building with diﬀerent roof pitches (see in Figure 1a a plot of the original point cloud).
Being the building in a ﬂat area we choose as a projecting plane Πv
d, where v = (0, 0, 1) and d equals the
lowest height among all the points in the study area. After the projection step the point cloud is represented as
(a)
(b)
996 m
980 m
Figure 1. Dataset preparation. (a) Plot of the original LiDAR data representing a complex building. (b) Same dataset
represented as a surface, i.e., the g function. Notice that points belonging to balconies and walls have been signiﬁcantly
removed. We can see noticeable jumps due to chimneys and garrets. Creases are mainly due to the connection between
roof pitches.


--- Page 8 ---

(a)
(b)
983 m
980 m
Figure 2. Example of domain representation. The plots are related to a small portion of the dataset. (a) Vertical look
of the triangulation; each point (xi, yi) is colored according to the gi value. (b) 3D perspective of the same particular
represented in (a).
a surface by running a standard thinning algorithm. We can see in Figure 1b that many of the points belonging
to the balconies and walls are removed by the algorithm, but still a few of them remain. The discretized surface
domain {(xi, yi)}i is then triangulated according to a Delaunay criterion, i.e., each triangle has the property
that its circumcircle does not contain any other point of the unstructured grid.
A Delaunay triangulation
is a preferable choice for representing the domain of diﬀerential problems because of its very nice properties
such as: (1) each vertex of a Delaunay triangulation has on average six surrounding triangles (a topological
regularity property); (2) a Delaunay triangulation maximizes the minimum interior angle when compared to
other triangulation methods (a geometrical regularity property). Figure 2 shows a particular of the triangulation
and related surface representation on the considered dataset.
4.1 Edge detection
Let us present the results of edge-detection obtained by approximating the solution of equation (9), where u is
replaced by g. We mention here that the squared norm of the gradient |∇g|2 is computed by assuming that g
(a)
(b)
Figure 3. Edge detection. Purple points are the detected edges, i.e., the points (pi, gi) such that si = 0. Yellow points are
the ones where si = 1. (a) Look of the whole dataset from above. (b) Diﬀerent perspective of the edge detection on the
whole dataset.


--- Page 9 ---

(a)
(b)
Figure 4. Particular of edge/crease detection related to the same points represented in Figure 2. (a) Look from above.
(b) Diﬀerent perspective of the edge/crease detection.
is a surface made up of triangular facets denoted by τ g
k . For each triangle τk let us denote its vertices (in local
enumeration) as pa, pb, pc. We consider the trianular facet τ g
k as the planar triangle (in a 3D space) with vertices
(pa, ga), (pb, gb), (pc, gc). The gradient of each τ g
k is a constant vector, over the whole τk, pointing towards the
direction of maximum slope and it is computed by elementary operations involving pj, gj, j = a, b, c.
The numerical solution of equation (9) is computed by means of the Riesz-Galerkin approximation. The varia-
tional equation of (9) is considered in a ﬁnite dimensional space of piecewise linear functions and the problem
is reduced to a linear system of equations which solution is the vector {si}i. It is possible to prove that the
associated matrix (the so-called stiﬀness matrix) is sparse, symmetric and positive deﬁnite. Thus, the solution
exists, is unique and computationally convenient. In Figure 3 we can see the plots of the solution: the color
information is related to si, whereas the spatial information is related to (pi, gi). The points colored purple are
the detected edges, namely, the points (pi, gi) such that si = 0. The yellow points are the homogeneous regions,
i.e., the points such that si = 1. From the particular in Figure 4 we can see that the edges are detected with a
precision of one point for each side of the jump. This mainly happens because the portion of space occupied by
a jump is fully described by its triangular facet, which has a high gradient information. Notice that the purple
points are all vertices of common triangles.
4.2 Edge and crease detection
The joint edge and crease detection is performed by approximating the solution of equation (10), where u is
replaced by g. Again, a numerical method based on Riesz-Galerkin approximation is used. We recall that also
(a)
(b)
Figure 5. Approximation of |∇2g| over a triangle by superposition of a regular grid. (a) Vertical look of the piecewise linear
approximation. (b) 3D perspective of the same particular, the second-order information is catched by the interpolated
points.


--- Page 10 ---

(a)
(b)
Figure 6. Edge/crease detection. Red points are the detected edge/creases, i.e., the points (pi, gi) such that zi = 0. Yellow
points represent homogeneous areas, i.e., the points where zi = 1. (a) Look of the whole dataset from above. (b) Diﬀerent
perspective of the edge/crease detection on the whole dataset.
in this case the associated matrix is sparse, symmetric and positive deﬁnite. An important remark regarding the
approximation of the second-order operator must be done. Since in the proposed modelization the function g
is a piecewise linear function, the second-order information given by |∇2g| over a triangle must be recovered by
using neighboring points (with respect to the triangle). To this aim we used a strategy based on superposition
of a regular grid around each triangle τk. Once the grid is deﬁned, the corresponding values of g are interpolated
and their values are used to compute an approximation of |∇2g| over the triangle. An example of regular grid
superposition is represented in Figure 5.
Results on edge/crease detection attained by solving the equation are illustrated in Figure 3, where the red and
yellow points represent the detected edges/creases and the homogeneous areas, respectively. As we can see, some
irragularities on the roof pitches, especially due to noise, are wrongly detected by the algorithm. Indeed this
behaviour was expected because, as we have already remarked, no smoothing of the data is done. We recall
here that these preliminary tests are concerned with the problem of solving the partial optimization problems
(6) and (7), which are related to edge and crease detection. The minimization of (8), the smoothing funtional, is
not considered here. Lastly, we want to point out that the approximation of the second-order operator |∇2g| by
means of regular grid superposition is purely an empirical technique, aimed at exploiting only the piecewise linear
approximation of g, without making use of polynomial approximations. Anyway, in order to give a qualitative
look on the general behaviour of the proposed method, our choise has shown to be eﬀective.
Regarding the order of complexity of the computations we reacall that the method is based on the solution
of two sparse, symmetric, and positive deﬁnite linear systems. The size of each matrix side equals the number
of points of the cloud, which is 16840 in the considered case. For each row of the matrix there are on average
7 non-zero elements. The main tasks of the algorithm are two: (1) the population of the matrix (hard work),
and (2) the solution of the system (light work). The time required for populating the matrices strictly depends
on how well-optimized is the approximation of the operators |∇g|2 and |∇2g|2 over the triangles. In the ﬁrst
case the time was approximately 6 seconds, while in the second case it was 30 seconds. The signiﬁcant increase
in time of the second case is mainly due to the approximation of the second-order operator by the empirical
technique described before. Once the system is deﬁned, the solution can be computed by using standard libraries
for sparse systems. The time of computation in the considered case is in the order of 10−1 seconds for both
systems.
Computations are performed in MATLAB R
⃝, hardware is Intel(R) Core(TM) i5-4570 CPU @ 3.20 GHz, 8.00
GB Ram.


--- Page 11 ---

5. CONCLUSIONS
In this paper we addressed the problem of edge and crease detection and surface reconstruction from 3D point
clouds. Recently, a second-order variational model based on the Blake-Zisserman functional has been succesfully
applied to the segmentation of DSMs. This work is intended to be a ﬁrst step for the generalization of this model
from regular to unstructured grid of points. The model is of interest since it allows to: (1) detect geometrical
features such as steps and creases in order to discriminate objects in the scene; (2) smooth the original data (noise
reduction) by preserving ﬁrst and second order features; (3) control the level of detection (length of countours).
In the paper we presented a numerical approach to the solution of the related optimization problem based
on Euler-Lagrange equations via the Finite Element Method. We give two (of the overall three) diﬀerential
equations whose solutions determine the map of edges and edges/creases of the objects in the scene represented
by the cloud of points. In the experimental part we presented the results of edge and crease detection on LiDAR
data. The results have shown promise and gave us the conﬁdence that a complete formal generalization of the
method can be reached by adding the smoothing component to the algorithm.
In a future perspective we aim at: (1) completing the proposed method by adding the numerical implemen-
tation of the smoothing component; (2) exploiting the ﬂexibility of the proposed approach for further reﬁnement
of the unstructured grid by adding points only in certain areas where higher resolution is needed (for instance
in proximity of edges and creases); (3) performing a full quantitative assessment of the proposed method by
comparing it with others thechniques presented in the literature.
REFERENCES
[1] X. Zhu and R. Bamler, “Very high resolution spaceborne SAR tomography in urban environment,” IEEE
Transactions on Geoscience and Remote Sensing 48(12), 4296–4308 (2010).
[2] M. Shahzad and X. X. Zhu and R. Bamler, “Facade structure reconstruction using spaceborne TomoSAR
point clouds,” in [IEEE Geoscience and Remote Sensing Symposium (IGARSS)’12], 467–470 (July 2012).
[3] G. Forlani and C. Nardinocchi and M. Scaioni and P. Zingaretti, “Complete classiﬁcation of raw LiDAR data
and 3D reconstruction of buildings,” Pattern Analysis and Applications 8(4), 357–374 (2006).
[4] A. Sampath and J. Shan, “Segmentation and reconstruction of polyhedral building roofs from aerial lidar
point clouds,” IEEE Transactions on Geoscience and Remote Sensing 48(3), 1554–1567 (2010).
[5] P. Perona and J. Malik, “Scale-space and edge detection using anisotropic diﬀusion,” IEEE Trans. Pat. Anal.
Machine Intel. (12)7, 629–639 (1990).
[6] L. A. Vese and T. F. Chan, “A multiphase level set framework for image segmentation using the Mumford
and Shah model,” International journal of computer vision 50(3), 271–293 (2002).
[7] G. Dogan and P. Morin and R. H. Nochetto and M. Verani, “Discrete gradient ﬂows for shape optimization
and applications,” Computer methods in applied mechanics and engineering 196(37), 3898–3914 (2007).
[8] M. Zanetti and A. Vitti, “The Blake-Zisserman model for digital surface models segmentation,” ISPRS
Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences II-5-W2, 355–360 (2013).
[9] D. Mumford and J. Shah, “Optimal approximations by piecewise smooth functions and associated variational
problems,” Comm. Pure Appl. Math. 42, 577–685 (1989).
[10] A. Vitti, “The Mumford-Shah variational model for image segmentation: an overview of the theory, imple-
mentation and use,” ISPRS Journal of Photogrammetry and Remote Sensing 69, 50–64 (2012).
[11] A. Blake and A. Zisserman, [Visual Reconstruction], MIT Press (1987).
[12] M. Carriero and A. Farina and I. Sgura, “Blake & Zisserman variational model for image segmentation and
discrete approximation,” Dept. of Mathematics “Ennio De Giorgi” Lecce - Italy Preprint (11), . (2002).
[13] L. Ambrosio and L. Faina and R. March, “Variational approximation of a second order free discontinuity
problem in computer vision,” Siam J. Math. Anal. 32, 1171–1197 (2001).
[14] E. De Giorgi, [Frontiers in Pure and Applied Mathemathics, a collection of papers dedicated to J.L. Lions
on the occasion of his 60th birthday], ch. Free discontinuity problems in calculus of variations, 55–62, North-
Holland P.C. (1991).
[15] L. Ambrosio and N. Fusco and D. Pallara, [Functions of bounded variation and free discontinuity problems],
Oxford University Press (2000).


--- Page 12 ---

[16] M. Carriero and A. Leaci and V.M. Tomarelli, “Strong minimizers of Blake & Zisserman functional,” Ann.
Scuola Norm. Sup. Pisa Cl. Sci. 25, 257–285 (1997).
[17] B. Szabo and I. Babuska, [Introduction to ﬁnite element analysis: formulation, veriﬁcation, and validation],
John Wiley & Sons, Ltd (2011).
View publication stats
```

---

## Section 1.13: Edge Preserving Implicit Surface Representation of Point Clouds

Source File: Edge Preserving Implicit Surface Representation of Point Clouds.txt

### Paper Content:

```
# Edge Preserving Implicit Surface Representation of Point Clouds.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Edge Preserving Implicit Surface Representation of Point Clouds.pdf
# Output: ../layer2_completion/txt/Edge Preserving Implicit Surface Representation of Point Clouds.txt


--- Page 1 ---

JOURNAL OF LATEX CLASS FILES, 2019
1
Edge Preserving Implicit Surface
Representation of Point Clouds
Xiaogang Wang, Yuhang Cheng, Liang Wang, Jiangbo Lu, Senior Member, IEEE , Kai Xu, Senior
Member, IEEE , Guoqiang Xiao
Abstract— Learning implicit surface directly from raw data recently has become a very attractive representation method for 3D
reconstruction tasks due to its excellent performance. However, as the raw data quality deteriorates, the implicit functions often lead
to unsatisfactory reconstruction results. To this end, we propose a novel edge-preserving implicit surface reconstruction method, which
mainly consists of a differentiable Laplican regularizer and a dynamic edge sampling strategy. Among them, the differential Laplican
regularizer can effectively alleviate the implicit surface unsmoothness caused by the point cloud quality deteriorates; Meanwhile, in order
to reduce the excessive smoothing at the edge regions of implicit suface, we proposed a dynamic edge extract strategy for sampling near
the sharp edge of point cloud, which can effectively avoid the Laplacian regularizer from smoothing all regions. Finally, we combine them
with a simple regularization term for robust implicit surface reconstruction. Compared with the state-of-the-art methods, experimental
results show that our method signiﬁcantly improves the quality of 3D reconstruction results. Moreover, we demonstrate through several
experiments that our method can be conveniently and effectively applied to some point cloud analysis tasks, including point cloud edge
feature extraction, normal estimation,etc.
Index Terms—Implicit surface representation, Differential Laplacian regularizer, Dynamic edge sampling, Point cloud, Geometric
modeling, Shape analysis.
!
1
INTRODUCTION
Recently, Implicit Neural Representations (INRs) has gained made
great strides in the ﬁeld of 3D reconstruction [1]–[8]. In contrast
to traditional explicit representations such as point clouds
[9],
voxels [10], [11] and mesh
[12]–[15], implicit neural repre-
sentations represent surface function primarily through neural
networks, providing higher quality, ﬂexibility, and ﬁdelity without
discretization errors, and signiﬁcantly save amounts of storage
space to store high-quality results.
However, most of these methods need ground truth data as
supervision [1]–[3], which have difﬁculty in generalizing well to
unseen shapes that are dissimilar to the training samples. Recently,
some methods [16]–[20] have been proposed to reconstruct im-
plicit neural representations directly from raw data (point clouds,
triangle soups, unoriented meshes, etc.). Compared to data-driven
approaches, building implicit neural representations directly from
raw data is obviously more appealing. Generally speaking, the
core idea of such methods is to impose explicit/implicit regularity
constraints to reduce reliance on dataset. SAL [18] proposed a
unsigned regression loss to a given unsigned distance function
to raw data, which can produce signed solutions of implicit
functions. Speciﬁcally, starting from raw data (e.g., point clouds,
real scanned grids, etc.), implicit neural representations learn in a
self-supervised manner and can be trained reliably relying only
on raw input data by minimizing unsigned regression. Subse-
quently, SALD
[17], a generalized version of SAL
[18] was
proposed, which can obtain higher quality reconstruction results
•
Xiaogang Wang, Yuhang Cheng, Liang Wang and Guoqiang Xiao are
with College of Computer and Information Science, Southwest University,
China.
•
Jiangbo Lu is with the SmartMore Co., Ltd.
•
Kai Xu is with the National University of Defense Technology, China.
Fig. 1: Effect of edge preserving differential Laplacian regularizer.
(b) are the optimization results of the edge preserving differential
Laplacian regularizer which is incorporated into the state-of-the-
art methods IGR [19] . (a) and (c) are the results of IGR and
Ground truth respectively.
arXiv:2301.04860v1  [cs.CV]  12 Jan 2023


--- Page 2 ---

JOURNAL OF LATEX CLASS FILES, 2019
2
by incorporating an explicit gradient constraint on SAL. Gropp et
al. [19] proposed a novel implicit geometric regularization (IGR)
method to directly learn an implicit neural representation from
raw data and achieved surprising results. Different from SAL [18]
and SALD [17], IGR only relies on implicit regularization con-
straints, without the need for a unsigned distance function. More
speciﬁcally, IGR proposes an implicit geometric regularization,
which amounts to solving a particular Eikonal boundary value
problem that constrains the norm of spatial gradients to be 1
almost everywhere. Yet, when the normal information cannot be
available and the number of input points is not dense enough,
the above algorithms often lead to unsatisfactory reconstruction
results (See Figure 1(a)).
We observed that the main reason for the unsatisfactory re-
construction results is that the implicit function needs to ﬁt the
input point cloud as much as possible, and the noise information
in the point cloud tends to cause the implicit surface to be very
unsmooth. In other words, the main reason for this phenomenon is
the inconsistency of normal in the local region of the reconstructed
surface. Therefore, it is an intuitive idea to keep the local normal
of the surface consistent as much as possible; Meanwhile, it
should be noted that not all regions are restricted in their normal
consistency, for example, obviously sharp edges often exist in the
surface (as shown in Figure 1). In the reconstruction process,
we hope that this part of the area will not be overly smoothed.
Therefore, The edge preserving local normal consistency is more
accurate for implicit surface representation .
In view of the above problem, it can be visually viewed
as a standard Laplacian minimization problem; Meanwhile, we
can also use the Laplacian operator to identify the edge region
effectively, which has achieved good results in many image
processing tasks. Therefore, in other words, we can design an
intuitive Laplacian regularization, which can effectively improve
the quality of reconstruction results.
However, in this task, the raw data type we consider is point
cloud data, and the difference method cannot be directly used to
approximate high-order derivatives, mainly because point cloud
data does not have a clear topological relationship like mesh
or image. If the algorithm similar to KNN is used, the nearest
neighbor points searched cannot guarantee the correct topology
structure (as shown in Figure 3), especially when the point cloud
is not dense and the normal are not available , such wrong nearest
neighbor results will easily lead to the anti-optimization results (as
shown in Figure 4(a)).
Recently, there is growing interest in differentiable optimiza-
tion of implicit neural representations that enable differential
nature as supervision in learning frameworks
[3], [19], [21]–
[25]. The advantage of differentiable implicit neural represen-
tations is that it can directly solve the higher derivative of the
input signal instead of discretization approximation, which greatly
improves its optimization performance and application range.
Thanks to the analytically-differentiable nature of implicit neural
representation, we can easily design a differentiable Laplacian
regularizer. Meanwhile, the differentiable Laplacian regularizer
can be easily and intuitively incorporated into implicit neural
surface representations (as shown in Figure 1). We show that it
signiﬁcantly improve the quality of 3D reconstruction. Meanwhile,
in order to facilitate qualitative and quantitative comparisons in
this paper, unless otherwise stated, in this paper, all experimental
results are obtained by incorporating them into IGR
[19]. We
carefully evaluate its performance through a series of ablation
studies. Meanwhile, we demonstrate through several experiments
that our method can be conveniently and effectively applied to
some point cloud analysis tasks, including point cloud edge feature
extraction, normal estimation, etc.
In summary, we make the following contributions: In this pa-
per, we use the inﬁnite differentiability property of implicit neural
representation to propose a novel edge-preserving implicit surface
reconstruction method, which mainly consists of a differentiable
Laplican regularizer and a dynamic edge sampling strategy. 1),
Among them, the differential Laplican regularizer can effectively
alleviate the implicit surface unsmoothness caused by the point
cloud quality deteriorates; 2), Meanwhile, in order to reduce
the excessive smoothing at the edge regions of implicit suface,
we proposed a dynamic edge extract strategy for sampling near
the sharp edge of point cloud, which can effectively avoid the
Laplacian regularizer from smoothing all regions.
2
RELATED WORK
2.1
Data-driven based Implicit surface reconstruction
3D surface reconstruction from raw data has gained signiﬁcant
research progress in recent year, beneﬁting from the advances in
machine learning techniques
[1]–[8]. Early studies
[26]–[28]
most utilize predeﬁned geometric priors (such as local linearity
and smoothness) towards speciﬁc tasks. These geometric priors
often encode statistical properties of raw data and are designed
to be optimized, such as poisson equation
[28], [29], radius
basis function [26], moving least squares [27]. Recently, implicit
neural representation has gained signiﬁcant research progress for
geometry reconstruction
[1]–[3], [6], [7], [16], [30]–[34] and
object representation [3], [23], [35]–[45] due to their simplicity
and excellent performance, which learn an approximate implicit
function with multi-layer perceptron (MLP). Compared to the
traditional continuous and discrete representations (grid, point
cloud and voxel), implicit neural representations have many poten-
tial beneﬁts, which can provide higher modeling quality without
discretization errors, ﬂexibility and ﬁdelity, and save storage
space. However, most of these methods need ground truth data
as supervision [1]–[3], which have difﬁculty in generalizing well
to unseen shapes that are dissimilar to the training samples.
In addition, there are hybridization-based methods [46]–[50]
that combine data-driven priors with optimization strategy that can
achieve state-of-the-art performance. However, the above methods
also require additional ground truth data as supervision, which
seriously limits their applicability.
2.2
Sign Agnostic Implicit surface reconstruction
Recently, some methods [17]–[20] have been proposed to re-
construct implicit neural representations directly from raw data.
Compared to big data-driven approaches, building implicit neural
representations directly from raw data is obviously more appeal-
ing. These methods can avoid the need for a large number of
ground truth signed distance representation of training data as
supervision. SAL
[18] introduces a sign agnostic regression
loss to a given unsigned distance function to raw data, which
is the signed version of unsigned distance function. Meanwhile,
that avoids the use of surface normals by properly initializing
implicit decoder networks so that they can only produce signed
solutions of implicit functions using unsigned distance function.
Subsequently, SALD
[17], a generalized version of SAL
[18]


--- Page 3 ---

JOURNAL OF LATEX CLASS FILES, 2019
3
was proposed, which can obtain higher quality reconstruction
results by incorporating an explicit gradient constraint on SAL.
Similarly, in this paper, our approach also uses implicit neural
representation to estimate level set functions directly from raw
data. The major difference is that our proposed regularization
terms are directly based on differentiable implicit optimization,
and does not explicitly enforce some regularization on the zero
level set, such constraints, when the normal information cannot be
available and the number of input point cloud is not dense enough,
the implicit neural representation often lead to unsatisfactory
reconstruction results.
2.3
Differentiable implicit neural representation
Compared with general implicit neural representation, differen-
tiable implicit neural representation has the advantage that it
can directly use various properties of differential geometry in-
stead of discretization approximation, which can lead to more
stable solutions in many optimization problems. Recently, there is
growing interest in differentiable optimization of implicit neural
representation that enable differential nature as supervision in
learning frameworks [3], [19], [21], [21]–[25]. General numerical
optimization often uses the discrete approximation of differential
geometry, for example, ﬁnite difference method is often used to
enhance the smoothness between adjacent samples in space. But
thanks to the analytically-differentiable nature of implicit neural
representation, differentiable implicit neural representations can
make direct use of many properties in differential geometry, such
as gradients [19], [21], [23], curvatures [24], and the solution of
partial differential equations [22], [25]. Recently, Gropp et al. [19]
proposed to use the differentiable implicit neural representation
to directly reconstruct surface from raw data. More speciﬁcally,
it proposes an implicit regularization constraint, which amounts
to solving a particular Eikonal boundary value problem that
constrains the norm of spatial gradients to be 1 almost everywhere.
Similarly, Sitzmann et al. [21] uses the proposed a differentiable
periodic activation functions to represent signed distance ﬁelds
in a fully-differentiable manner. Both of these works [19], [21]
, however, when the normal information cannot be available and
the number of input points is not dense enough, often lead to
unsatisfactory reconstruction results. In this paper, our work is also
based on the differentiability of implicit neural representations
to optimize implicit level set function estimated directly from
the input point cloud. Speciﬁcally, we designed an implicit dif-
ferentiable Laplacian regularizer, which effectively alleviated the
problem of unsatisfactory reconstruction results caused by direct
ﬁtting of input point cloud by implicit neural function.
3
METHOD
We present a differentiable laplacian regularizer for neural implicit
representation directly from input point cloud without normal
supervision. Note that our differential Laplacian regularizer can
be incorporated into any implicit neural representation, such as
IGR [19],SAL [18],SALD [17]. In this paper, unless otherwise
noted, we incorporate it in the IGR, which use level sets of neural
network to represent 3D shape (Sec. 3.1). More speciﬁcally, IGR
proposes an implicit geometric regularization, which amounts to
solving a particular Eikonal boundary value problem that con-
strains the norm of spatial gradients to be 1 almost everywhere.
Yet, when the normal information cannot be available and the
number of input points is not dense enough, IGR often lead
Fig. 2: Illustrations of the local normal consistency.
to unsatisfactory reconstruction results (See Figure 1(a)). We
observed that the main reason for the unsatisfactory reconstruction
results is that the implicit function needs to ﬁt the input point cloud
as much as possible, and the noise information in the point cloud
tends to cause the implicit surface to be very unsmooth.
To
overcome
this
problem,
we
use
the
analytically-
differentiable nature of implicit neural representation, to propose
a differential Laplacian regularizer, which can effectively alleviate
the unsatisfactory reconstruction results (Sec. 3.2). Meanwhile, in
order to reduce the excessive smoothing at the edge regions of
3D shape (such as man-made shapes), a dynamic edge extraction
strategy (Sec. 3.2) is introduced for sampling near the sharp edge
of input point cloud, which can effectively avoid the Laplacian
regularizer from smoothing all regions, so as to effectively im-
prove the quality of reconstruction results while maintaining the
edge.
3.1
Background
A neural implicit representations is a continuous function that
approximate the signed distance function. The underlying surface
of 3D shape is implicitly represented by the zero level set of this
function,
fθ(x) = 0, ∀x ∈X.
(1)
where θ indicates the parameters to be learned and X indicates the
set of input point cloud. In general, one parameterize this function
using a multi-layer perceptron (MLP). Meanwhile, in order to
conveniently use the analytically-differentiable (such as, gradi-
ents,etc.) nature of implicit neural representation, recent works
[19], [21] usually replace the commonly used ReLU activation
function with a non-linear differentiable activation functions, thus
transforming MLP into a continuous and inﬁnitely differentiable
function.
In IGR, the training is done by minimizing the loss that
encourages f to vanish on X:
Lvanish =
1
N(X)
X
x∈X
|fθ(x)|
(2)
where N(X) is the number of point set X, | • | indicates abso-
lute value. if the input point cloud includes normal information
ngt(x), the corresponding loss function can be designed to make
the predicted normal (the differentiable gradient ▽fθ(x) of the
implicit function) as close as possible to the ground truth normal
ngt(x):
Lnormal =
1
N(X)
X
x∈X
||▽fθ(x) −ngt(x)||2
(3)
In addition to the above two intuitive ﬁtting loss terms, IGR
[19] based on the Eikonal partial differential equation presents
an additional loss (Eikonal loss), which is equivalent to solve


--- Page 4 ---

JOURNAL OF LATEX CLASS FILES, 2019
4
Fig. 3: Illustrations of two different N nearest neighbors of non-
topological preservation (b) and topological preservation (c) for
geometric structure (a).
boundary value problems of a particular Eikonal that constrains
the norm of spatial gradients ▽fθ(x) to be 1 almost everywhere:
Leikonal =
1
N(X)
X
x∈X
(||▽fθ(x)||2 −1)2
(4)
Note that, in our approach, we do not consider normal infor-
mation as supervision, so we will not consider Lnormal term in
all subsequent experiments. More speciﬁcally, our approach builds
upon the above two items Lvanish and Leikonal.
3.2
Differentiable laplace regularization
Neighborhood normal consistency. A high-quality result can be
generated based on the above two terms (Lvanish and Leikonal)
when the input point data is large enough, however, when the
normal information cannot be available and the number of input
points is not dense enough, often lead to unsatisfactory reconstruc-
tion results (See Figure 1(a)).
We observed that the main reason for the unsatisfactory re-
construction results is that the implicit function needs to ﬁt the
input point cloud as much as possible, and the noise information
in the point cloud tends to cause the implicit surface to be
very unsmooth. More speciﬁcally, the optimization results are
not guaranteed to provide a high-quality reconstruction result,
which is intuitively reﬂected by the possibility that the normal
of reconstruction result is inconsistent in the neighborhood.
From another perspective, it is well known that 3D shapes
tend to be piecewise smooth, that is, ﬂat surfaces are more
likely than high-frequency structures [51]. For this purpose, we
incorporate this prior into implicit neural function by encouraging
the geometric smoothness of the reconstructed results. Therefore,
an intuitive solution is to constrain the consistency of the neighbor-
hood normal of the reconstruction results (as shown in Figure 2):
Lneibor =
X
x∈X
X
xi∈nei(x)
||▽fθ(x) −▽fθ(xi)||2
(5)
where nei(x) indicates the neighbor point set of point x.
However, in this paper, the raw data type we consider is point
cloud data, which does not have a clear topological structure
like mesh or voxels. If the algorithm similar to KNN is used,
the nearest neighbor points searched cannot guarantee that they
maintain the correct topology structure, especially when the point
cloud is not dense and the normal are not available, as shown in
Figure 3(b) where the three points P4, P5 and P6 do not meet the
nearest neighbor result of N = 5 under the maintenance of the
topology structure, and the correct set of nearest neighbor points
Fig. 4: The comparison of Lneibor (a) and Llaplacian (b).
should be {P1, P2, P3, P8, P9}. Moreover, it is difﬁcult to get a
reasonable value for this parameter nei(x) in practice. As shown
in Figure 4, we can easily see that the wrong reconstructed results,
which is mainly caused by the above reasons.
Differentiable Laplacian regularizer. In fact, the above con-
straint Lneibor is mainly used to constrain the normal consistency
in the local domain, which can be easily interpreted as a discrete
Laplace operator. The Laplacian operator △f is a second-order
differential operator in n-dimensional euclidean space, deﬁned as
the divergence (▽· f) of the gradient (▽f). Thanks to the inﬁnite
differentiability of implicit neural representation, we can design a
simple but effective differentiable Laplacian regularizer:
Llaplacian =
X
x∈X
△fθ(x)2
(6)
where △fθ(x) indicates the differentiable Laplace operator of
point x.
As shown in Figure 4(b), compared with the explicit regular-
ization constraint Lneibor based on the nearest neighbor normal
consistency, the differentiable Laplacian regularizer can obtain
more stable results without introducing hyperparameter nearest
neighbors N.
3.3
Dynamic edge sampling
However, while the differentiable Laplacian regularizer restricts
the normal consistency, it also brings a new problem: It imposes
undifferentiated constraints on all 3D regions, even in the sharp-
edge regions, as shown in Figure 6. As we know, complex 3D
shapes are generally constructed by multiple piecewise smooth
surfaces, which may not be differentiable at the joints, and are
more likely to form sharp edges. Therefore, in essence, a complex
3D shape (piecewise smooth model with sharp edges) cannot
be accurately represented by an implicit function, because it is
obviously not differentiable at sharp edges, so if it is forced to be
represented by an implicit function, especially only sparse point
sets without normal information are used as supervision, it is easy
to form an overly smooth reconstruction at the sharp edges (as
shown in Figure 6).
The most intuitive solution is to implicitly represent each
piecewise smooth surface separately, but this is difﬁcult to do in
practice because it ﬁrst requires the segmentation of the input point
set, which is difﬁcult to do accurately in unsupervised conditions.
Therefore, we propose a novel dynamic edge sampling
strategy to effectively extract sharp edge regions in the training


--- Page 5 ---

JOURNAL OF LATEX CLASS FILES, 2019
5
Fig. 5: Statistics of Laplacian operators |△fθ(x)| and edge thresh-
old τ selection.
process. In theory, the remaining regions not only satisfy the
differentiable property, but also conform to the normal consistency
constraint, which can effectively avoid the indifference smoothing
of all regions, including the edge regions, of the laplace regular-
izer.
Speciﬁcally, for each point p in the input point set, we may
quickly determine whether it is an edge point according to its
differentiable Laplacian operator △fθ(x). Essentially, Laplacian
is mainly used to describe the rate of change of gradient, and
is often used for edge detection in image processing. From the
perspective of differential geometry, it is used to describe the
change rate of spatial position normal. Therefore, the larger the
laplacian of the point, the stronger the possibility that the point is
an edge point. We threshold the Laplacian |△fθ(x)| < τ to obtain
a corresponding set of non-edge points X′. According to statistics
(as shown in Figure 5), we set the parameter τ = 20 throughout
our experiments. This operation is performed before the back-
propagation of each iteration, therefore, we call it dynamic edge
sampling.
Llaplacian =
X
x∈X′
△fθ(x)2
(7)
where X′ indicates the non-edge subset of the input point cloud
X. Finally, we optimize the total loss:
Ltotal = Lvanish + λ1Leikonal + λ2Llaplacian
(8)
In which, we set λ1 = 0.1 and λ2 = 0.001 throughout our
experiments.
4
DETAILS, RESULTS AND EVALUATIONS
4.1
Implementation details
Data preparation. To facilitate quantitative evaluation of our
method on multiple tasks, including reconstruction , edge ex-
traction and normal estimation, we selected 100 3D shapes with
rich geometric topologies to construct the evaluation dataset (See
Figure 8) from ABC dataset [52], which provides more than 1
million standard 3D CAD models with multiple types of standard
CAD format ﬁles. In addition to 3D geometry and normal informa-
tion, the geometric edges information mentioned above does not
provide us explicitly. To this end, we have developed a tool that,
Fig. 6: The comparison of with (b) and without Dynamic Edge
Sampling (DES) (a).
for each 3D shape, can quickly and easily extract the geometric
edge information from the multiple CAD ﬁles, thus fully meeting
the needs of our method for multi-task quantitative evaluation.
Point sampling. For each model, we sample it into a point
cloud containing 16, 384 points by uniform point sampling.
Meanwhile, in order to simulate the real point cloud noise, we
added Gaussian noise with mean µ = 0 and standard devia-
tion δ = 0.005 to each sampling point. In each case, except
where otherwise stated, the network is trained on the noisy data
throughout our experiments. A few metrics on point cloud multi-
tasks accuracy are deﬁned to support quantitative evaluation of our
approach; see the following subsections for details.
4.2
Metrics
In our experiments, both qualitative and quantitative evaluations
are provided. We evaluate our approach via ablation studies
(Section 4.6), comparisons to state-of-the-art methods for 3D
reconstruction (Section 4.3) , edge detection (Section 4.4) and
normal estimation (Section 4.5). For the quantitative assessment of
the 3D reconstruction results, we used the two-sided Chamfer dC
and Hausdorff distances dH introduced by [19]. For the evaluation
of the normal estimation, we use the angle dangle between the
predicted normal and the groudtruth normal as the metric. To
evaluate edge detection, we measure precision/recall and the
IoU between predictions and ground truth, while to evaluate the
geometric accuracy of the reconstructed edges, we employ the
Edge Chamfer Distance (ECD) introduced by [1].
4.3
Reconstruction
Comparison with IGR [19]. To facilitate a fair comparison with
IGR [19], our network architecture is consistent with IGR [19]. In
all experiments, we used the default training procedure speciﬁed
in IGR to train our network, except that we did not use normal
information in the training and set iterations to 10000. We set the
loss parameters (see equation (8)) λ2 = 0.1 and λ3 = 0.001
throughout our experiments. Qualitative and quantitative experi-
ments are reported in Table 1 and Figure 7 we can also see that
the performance of our method is signiﬁcantly better.
Comparison with state-of-the-art methods SAL [18] and
SALD [17]. In addition to IGR [19], our method is also compared
with SAL [18] and SALD [17], two state-of-the-art sign agnostic
learning based methods from raw data. The results shown in
Table 1(row 1 and 2) are inferior to those of our method. As shown
in Figure 7, the results demonstrate the signiﬁcant advantage of


--- Page 6 ---

JOURNAL OF LATEX CLASS FILES, 2019
6
Fig. 7: Qualitative comparison with state-of-the-art methods IGR [19], SAL [18] and SALD [17].
Fig. 8: An overview of multi-task evaluation dataset.
dC
dH
Mean
Median
Mean
Median
SAL [18]
0.019
0.016
0.094
0.050
SALD [17]
0.016
0.015
0.053
0.042
IGR [19]
0.028
0.011
0.111
0.034
Our (Llaplace)
0.017
0.009
0.068
0.026
Our (Llaplace + DES)
0.007
0.007
0.021
0.021
TABLE 1: A quantitative comparison of our method and ablation
against IGR [19], SAL [18] and SALD
[17] on multi-task
evaluation dataset.
our approach, due to the fact that differential Laplacian regularizer
can effectively alleviate the unsatisfactory reconstruction results.
4.4
Edge recognition
Speciﬁcally, for each point p in the input point set, we may quickly
determine whether it is an edge point according to its differentiable
laplace operator △fθ(x) . Essentially, laplace operator is mainly
used to describe the rate of change of gradient, and is often used
for edge detection in image processing. From the perspective of
differential geometry, it is used to describe the change rate of
spatial position normal. Therefore, the larger the laplace operator
of the point, the stronger the possibility that the point is an edge
point. We threshold the laplace operator |△fθ(x)| > τ to obtain a
corresponding set of non-edge points Xedge. We set the parameter
τ = 20 throughout our experiments, as shown in Figure 10.
In addition to IGR [19], we also choose two representative
classical non-learning based methods: Voronoi Covariance Mea-
sure (VCM) [53], and Edge-Aware Resampling (EAR) [54], as
both have been adopted in the point-set processing routines of the
well known CGAL library. As reported in Table 4, our method
completely outperforms these classical methods, This is mainly
because we use the differentiable Laplacian operator of each
sampling point as the metric, which can be approximate to the
average curvature in the implicit surface representation. Note that,
there are a large number of high-quality edge detection methods
based on data-driven. We do not use these methods as references
here, mainly because ours is a self-supervised learning approach.
4.5
Normal estimation
Essentially, an implicitly represented MLP with softplus activation
funtion represents a differentiable Signed Distance Functions d =
fθ(x). According to the properties of differential geometry, the
gradient operator of each point on the implicit surface fθ(x) = 0
can be regarded as the normal vector of the current point x.
Therefore, after the training, for each point in the input point
cloud, we can directly calculate the gradient operator ▽fθ(x) of
the differentiable function fθ(x) at the current point x, that is, the
normal vector of the current point x. The experimental results are
reported in Table 1. The comparison results demonstrate how our
method achieves signiﬁcantly better performance; as immediately
quantiﬁed by the fact that dangle is larger than the one reported
for our method.


--- Page 7 ---

JOURNAL OF LATEX CLASS FILES, 2019
7
Fig. 9: Visualization normal estimation of differential Laplacian regularizer (c) and dynamic edge sampling strategy (d).
Fig. 10: Visualization edge recognition of differential Laplacian regularizer (c) and dynamic edge sampling strategy (d).
dC
dH
Mean
Median
Mean
Median
X = 0.010
0.0102
0.0108
0.0509
0.0543
X = 0.005
0.0069
0.0069
0.0206
0.0209
X = 0.000
0.0055
0.0057
0.0148
0.0153
D = 4, 096
0.0075
0.0075
0.0350
0.0328
D = 8, 192
0.0071
0.0072
0.0352
0.0269
D = 16, 384
0.0069
0.0069
0.0206
0.0209
TABLE 2: Algorithm performance with respect to noise X and
sampling density D.
4.6
Analysis of parameters and networks
Effect of noise. We stress test Laplacian regularizer by increasing
the level of noise. Speciﬁcally, we randomly add a Gaussian noise
whose mean is 0 and variance is X to each sampling point on
the surface of the 3D shape, where we tested four values of
X = {0, 0.005, 0.01, 0.02}. In each case, the implicit neural
surface was trained with the noise-added data. Table 2 shows
the quantitative results. As we can observe that, the Laplacian
regularizer, even when trained with noisy data, can still out-
perform these state-of-the-art methods [17]–[19] when they are
tested on point cloud with 0.005 noise.
Effect of density. We also train our method on point clouds
at a reduced density. Speciﬁcally, for each 3D shape, we sam-
pled a different number D of points to verify whether our
network could handle the sparser point clouds, where D =


--- Page 8 ---

JOURNAL OF LATEX CLASS FILES, 2019
8
Fig. 11: Effect of edge preserving differential Laplacian regu-
larizer. (b) are the optimization results of the edge preserving
differential Laplacian regularizer which is incorporated into the
state-of-the-art method SALD [17]. (a) and (c) are the results of
SALD and Ground truth respectively.
dC
dH
dangle
IGR [19]
0.028
0.111
0.514
Our (+Llaplacian)
0.017
0.068
0.274
Our (+Llaplacian + DES)
0.009
0.036
0.133
TABLE 3: Ablation studies – We evaluate the quantitative per-
formance of our method with/without components Llaplacian and
dynamic edge sampling (DES).
{4, 096, 8, 192, 16, 384}. (Results in Table 2 reveal a similar
trend as from the previous stress test. Namely, our network, when
trained on sparser point clouds, can still outperform these state-of-
the-art methods [17]–[19] when they are tested on or trained on
data at full resolution (16,384 points).
Effect of Llaplacian. To evaluate the effectiveness of loss
Llaplacian, We incorporate this into another state-of-the-art
method, SALD [17], This qualitative result is shown in Figure 11,
we can ﬁnd that, compared with the original algorithm, the re-
construction quality can be effectively improved by incorporating
Laplacian. This is mainly because the differentiable Laplacian reg-
ularizer can effectively alleviate the unsatisfactory reconstruction
results.
Dynamic edge sampling. We evaluate the effect of dynamic
edge sampling strategy on reconstruction quality. We experiment
with the dynamic edge sampling, while keeping all other
parameters the same. From Table 1 and Figure 7 and 12 , we can
see that at the sharp edges, we can effectively improve the quality
of modeling compared with state-of-the-art methods (Table 1
(rows 1 3)) and the baseline method without dynamic edge
sampling, this is largely due to thedynamic edge sampling
strategy for sampling near the sharp edge of input point cloud,
which can effectively avoid the regularizer from smoothing all
regions.
ECD
IoU
Precision
Recall
VCM [53]
0.0017
0.1925
0.2238
0.5998
EAR [54]
0.0071
0.1146
0.2399
0.1933
IGR [19]
0.0063
0.0880
0.0958
0.5620
Our
0.0015
0.2375
0.2665
0.6934
TABLE 4: Comparison state-of-the-art edge recognition tech-
niques - VCM [53], EAR [54], and IGR [19].
5
CONCLUSION AND LIMITATION
We present a differential Laplacian regularizer for neural implicit
representation directly from input point cloud without normal
supervision. More speciﬁcally, we use the inﬁnite differentiability
property of implicit neural representation to propose a differen-
tiable Laplacian regularizer, which can effectively alleviate the
unsatisfactory reconstruction results. Meanwhile, we propose a
dynamic edge sampling strategy for sampling near the sharp
edge of input point cloud, which can effectively avoid the Lapla-
cian regularizer from smoothing all regions, so as to effectively
improve the quality of reconstruction results while maintaining
the edge. Moreover, the differentiable Laplacian regularizer can
be easily and intuitively incorporated into implicit neural sur-
face representations. We carefully evaluate its generation quality
through a series of ablation studies, which show that our method
signiﬁcantly improve the quality of 3D reconstruction. In addition
to 3D reconstruction, our method can also be conveniently applied
to other point cloud analysis tasks, including edge extraction and
normal estimation, etc.
Limitation. Our approach has a few limitations, which point
out the directions of future study. Some representative failure cases
are shown in Figure 13. First, our method is prone to problems
in the reconstruction of ultra-thin geometric structures, probably
because the point cloud data is noisy, resulting in the geometric
structure has been completely destroyed. Second, Our method
for extremely detailed structure may be overlooked, resulting in
incorrect reconstruction results.
ACKNOWLEDGEMENT
We thank the anonymous reviewers for their valuable comments.
This work was supported in part by Natural Science Foundation
of China (62102328), and Fundamental Research Funds for the
Central Universities (SWU120076).
REFERENCES
[1]
Z. Chen and H. Zhang, “Learning implicit ﬁelds for generative shape
modeling,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2019, pp. 5939–5948.
[2]
L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger,
“Occupancy networks: Learning 3d reconstruction in function space,” in
Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, 2019, pp. 4460–4470.
[3]
J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove,
“Deepsdf: Learning continuous signed distance functions for shape
representation,” in Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 2019, pp. 165–174.
[4]
J. Chibane, T. Alldieck, and G. Pons-Moll, “Implicit functions in feature
space for 3d shape reconstruction and completion,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 6970–6981.
[5]
P. Erler, P. Guerrero, S. Ohrhallinger, N. J. Mitra, and M. Wimmer,
“Points2surf learning implicit surfaces from point clouds,” in European
Conference on Computer Vision.
Springer, 2020, pp. 108–124.


--- Page 9 ---

JOURNAL OF LATEX CLASS FILES, 2019
9
Fig. 12: Visualization examples of differential Laplacian regularizer (c) and dynamic edge sampling strategy (d).
Fig. 13: Failure cases.
[6]
S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger, “Con-
volutional occupancy networks,” in European Conference on Computer
Vision.
Springer, 2020, pp. 523–540.
[7]
S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and H. Li,
“Pifu: Pixel-aligned implicit function for high-resolution clothed human
digitization,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2019, pp. 2304–2314.
[8]
Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann, “Disn: Deep
implicit surface network for high-quality single-view 3d reconstruction,”
Advances in Neural Information Processing Systems, vol. 32, 2019.
[9]
H. Fan, H. Su, and L. J. Guibas, “A point set generation network for 3d
object reconstruction from a single image,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017, pp. 605–
613.
[10] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, “3d-r2n2: A
uniﬁed approach for single and multi-view 3d object reconstruction,” in
European conference on computer vision.
Springer, 2016, pp. 628–644.
[11] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum, “Learning a
probabilistic latent space of object shapes via 3d generative-adversarial
modeling,” Advances in neural information processing systems, vol. 29,
2016.
[12] T. Groueix, M. Fisher, V. Kim, B. Russell, and M. Aubry, “Atlasnet: A
papier-mˆach´e approach to learning 3d surface generation. arxiv 2018,”
arXiv preprint arXiv:1802.05384, 1802.
[13] H. Kato, Y. Ushiku, and T. Harada, “Neural 3d mesh renderer,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 3907–3916.
[14] J. Tang, X. Han, J. Pan, K. Jia, and X. Tong, “A skeleton-bridged deep
learning approach for generating meshes of complex topologies from
single rgb images,” in Proceedings of the ieee/cvf conference on computer
vision and pattern recognition, 2019, pp. 4541–4550.
[15] J. Tang, X. Han, M. Tan, X. Tong, and K. Jia, “Skeletonnet: A topology-
preserving solution for learning mesh reconstruction of object surfaces
from rgb images,” IEEE transactions on pattern analysis and machine
intelligence, 2021.
[16] M. Atzmon, N. Haim, L. Yariv, O. Israelov, H. Maron, and Y. Lipman,
“Controlling neural level sets,” Advances in Neural Information Process-
ing Systems, vol. 32, 2019.
[17] M. Atzmon and Y. Lipman, “Sald: Sign agnostic learning with deriva-
tives,” arXiv preprint arXiv:2006.05400, 2020.
[18] ——, “Sal: Sign agnostic learning of shapes from raw data,” 2020, pp.
2565–2574.
[19] A. Gropp, L. Yariv, N. Haim, M. Atzmon, and Y. Lipman, “Im-
plicit geometric regularization for learning shapes,” arXiv preprint
arXiv:2002.10099, 2020.
[20] W. Zhao, J. Lei, Y. Wen, J. Zhang, and K. Jia, “Sign-agnostic implicit
learning of surface self-similarities for shape modeling and reconstruc-
tion from raw point clouds,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2021, pp. 10 256–10 265.
[21] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein, “Im-
plicit neural representations with periodic activation functions,” Advances
in Neural Information Processing Systems, vol. 33, pp. 7462–7473, 2020.
[22] H. Chen, R. Wu, E. Grinspun, C. Zheng, and P. Y. Chen, “Implicit
neural spatial representations for time-dependent pdes,” arXiv preprint
arXiv:2210.00124, 2022.
[23] L. Yariv, J. Gu, Y. Kasten, and Y. Lipman, “Volume rendering of neural
implicit surfaces,” Advances in Neural Information Processing Systems,
vol. 34, pp. 4805–4815, 2021.
[24] T. Ehret, R. Mar´ı, and G. Facciolo, “Nerf, meet differential geometry!”
arXiv preprint arXiv:2206.14938, 2022.
[25] J. Zehnder, Y. Li, S. Coros, and B. Thomaszewski, “Ntopo: Mesh-free
topology optimization using implicit neural representations,” Advances
in Neural Information Processing Systems, vol. 34, pp. 10 368–10 381,
2021.
[26] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C.
McCallum, and T. R. Evans, “Reconstruction and representation of 3d
objects with radial basis functions,” in Proceedings of the 28th annual
conference on Computer graphics and interactive techniques, 2001, pp.
67–76.
[27] M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin, and C. T. Silva,
“Computing and rendering point set surfaces,” IEEE Transactions on
visualization and computer graphics, vol. 9, no. 1, pp. 3–15, 2003.
[28] M. Kazhdan, M. Bolitho, and H. Hoppe, “Poisson surface reconstruc-


--- Page 10 ---

JOURNAL OF LATEX CLASS FILES, 2019
10
tion,” in Proceedings of the fourth Eurographics symposium on Geometry
processing, vol. 7, 2006.
[29] M. Kazhdan and H. Hoppe, “Screened poisson surface reconstruction,”
ACM Transactions on Graphics (ToG), vol. 32, no. 3, pp. 1–13, 2013.
[30] K. Genova, F. Cole, D. Vlasic, A. Sarna, W. T. Freeman, and
T. Funkhouser, “Learning shape templates with structured implicit func-
tions,” in Proceedings of the IEEE/CVF International Conference on
Computer Vision, 2019, pp. 7154–7164.
[31] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger, “Occupancy
ﬂow: 4d reconstruction by learning particle dynamics,” in Proceedings
of the IEEE/CVF international conference on computer vision, 2019, pp.
5379–5389.
[32] M. Oechsle, S. Peng, and A. Geiger, “Unisurf: Unifying neural implicit
surfaces and radiance ﬁelds for multi-view reconstruction,” in Proceed-
ings of the IEEE/CVF International Conference on Computer Vision,
2021, pp. 5589–5599.
[33] M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan,
U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng, “Fourier features let
networks learn high frequency functions in low dimensional domains,”
Advances in Neural Information Processing Systems, vol. 33, pp. 7537–
7547, 2020.
[34] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, “Neus:
Learning neural implicit surfaces by volume rendering for multi-view
reconstruction,” arXiv preprint arXiv:2106.10689, 2021.
[35] V. Sitzmann, M. Zollh¨ofer, and G. Wetzstein, “Scene representation
networks: Continuous 3d-structure-aware neural scene representations,”
Advances in Neural Information Processing Systems, vol. 32, 2019.
[36] A. Bergman, P. Kellnhofer, and G. Wetzstein, “Fast training of neural
lumigraph representations using meta learning,” Advances in Neural
Information Processing Systems, vol. 34, pp. 172–186, 2021.
[37] J. Chibane, A. Bansal, V. Lazova, and G. Pons-Moll, “Stereo radiance
ﬁelds (srf): Learning view synthesis for sparse views of novel scenes,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2021, pp. 7911–7920.
[38] C. Gao, Y. Shih, W.-S. Lai, C.-K. Liang, and J.-B. Huang, “Portrait neural
radiance ﬁelds from a single image,” arXiv preprint arXiv:2012.05903,
2020.
[39] Y. Jiang, D. Ji, Z. Han, and M. Zwicker, “Sdfdiff: Differentiable render-
ing of signed distance ﬁelds for 3d shape optimization,” in Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition,
2020, pp. 1251–1261.
[40] P. Kellnhofer, L. C. Jebe, A. Jones, R. Spicer, K. Pulli, and G. Wetzstein,
“Neural lumigraph rendering,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, 2021, pp. 4287–4297.
[41] L. Liu, J. Gu, K. Zaw Lin, T.-S. Chua, and C. Theobalt, “Neural
sparse voxel ﬁelds,” Advances in Neural Information Processing Systems,
vol. 33, pp. 15 651–15 663, 2020.
[49] M. Yang, Y. Wen, W. Chen, Y. Chen, and K. Jia, “Deep optimized
priors for 3d shape modeling and reconstruction,” in Proceedings of
[42] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Doso-
vitskiy, and D. Duckworth, “Nerf in the wild: Neural radiance ﬁelds
for unconstrained photo collections,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2021, pp.
7210–7219.
[43] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
and R. Ng, “Nerf: Representing scenes as neural radiance ﬁelds for view
synthesis,” arXiv preprint arXiv:2003.08934, 2020.
[44] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger, “Differentiable
volumetric rendering: Learning implicit 3d representations without 3d
supervision,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020, pp. 3504–3515.
[45] L. Yariv, Y. Kasten, D. Moran, M. Galun, M. Atzmon, B. Ronen, and
Y. Lipman, “Multiview neural surface reconstruction by disentangling
geometry and appearance,” Advances in Neural Information Processing
Systems, vol. 33, pp. 2492–2502, 2020.
[46] C. Jiang, A. Sud, A. Makadia, J. Huang, M. Nießner, T. Funkhouser et al.,
“Local implicit grid representations for 3d scenes,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 6001–6010.
[47] E. Tretschk, A. Tewari, V. Golyanik, M. Zollh¨ofer, C. Stoll, and
C. Theobalt, “Patchnets: Patch-based generalizable deep implicit 3d
shape representations,” in European Conference on Computer Vision.
Springer, 2020, pp. 293–309.
[48] R. Chabra, J. E. Lenssen, E. Ilg, T. Schmidt, J. Straub, S. Lovegrove,
and R. Newcombe, “Deep local shapes: Learning local sdf priors for
detailed 3d reconstruction,” in European Conference on Computer Vision.
Springer, 2020, pp. 608–625.
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 3269–3278.
[50] J. Tang, J. Lei, D. Xu, F. Ma, K. Jia, and L. Zhang, “Sa-convonet:
Sign-agnostic optimization of convolutional occupancy networks,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 6504–6513.
[51] J. Huang, A. B. Lee, and D. Mumford, “Statistics of range images,” in
Proceedings IEEE Conference on Computer Vision and Pattern Recogni-
tion. CVPR 2000 (Cat. No. PR00662), vol. 1.
IEEE, 2000, pp. 324–331.
[52] S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Artemov, E. Burnaev,
M. Alexa, D. Zorin, and D. Panozzo, “Abc: A big cad model dataset for
geometric deep learning,” in The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
[53] Q. M´erigot, M. Ovsjanikov, and L. J. Guibas, “Voronoi-based curvature
and feature estimation from point clouds,” IEEE Transactions on Visual-
ization and Computer Graphics, vol. 17, no. 6, pp. 743–756, 2010.
[54] H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and H. Zhang,
“Edge-aware point set resampling,” ACM transactions on graphics
(TOG), vol. 32, no. 1, pp. 1–12, 2013.
```

---

## Section 1.14: Escape from cells_ Deep kd-networks for the recognition of 3d point cloud models

Source File: Escape from cells_ Deep kd-networks for the recognition of 3d point cloud models.txt

### Paper Content:

```
# Escape from cells_ Deep kd-networks for the recognition of 3d point cloud models.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Escape from cells_ Deep kd-networks for the recognition of 3d point cloud models.pdf
# Output: ../layer2_completion/txt/Escape from cells_ Deep kd-networks for the recognition of 3d point cloud models.txt


--- Page 1 ---

Escape from Cells: Deep Kd-Networks for the Recognition of
3D Point Cloud Models
Roman Klokov
Skolkovo Institute of Science and Technology
roman.klokov@skoltech.ru
Victor Lempitsky
Skolkovo Insitute of Science and Technology
lempitsky@skoltech.ru
Abstract
We present a new deep learning architecture (called Kd-
network) that is designed for 3D model recognition tasks
and works with unstructured point clouds. The new archi-
tecture performs multiplicative transformations and shares
parameters of these transformations according to the sub-
divisions of the point clouds imposed onto them by kd-
trees. Unlike the currently dominant convolutional archi-
tectures that usually require rasterization on uniform two-
dimensional or three-dimensional grids, Kd-networks do
not rely on such grids in any way and therefore avoid poor
scaling behavior. In a series of experiments with popular
shape recognition benchmarks, Kd-networks demonstrate
competitive performance in a number of shape recognition
tasks such as shape classiﬁcation, shape retrieval and shape
part segmentation.
1. Introduction
As the 3D world around us is getting scanned and dig-
itized and as the archives of human-designed models are
growing in size, recognition and analysis of 3D geometric
models are gaining importance. Meanwhile, deep convolu-
tional networks (ConvNets) [15] have excelled at solving
analogous recognition tasks for 2D image datasets. It is
therefore natural that a lot of research currently aims at the
adaptation of deep ConvNets to 3D models [36, 18, 4, 35,
34, 21, 31, 2, 3].
Such adaptation is non-trivial. Indeed, the most straight-
forward way to make ConvNets applicable to 3D data, is to
rasterize 3D models onto uniform voxel grids. Such ap-
proach however leads to excessively large memory foot-
prints and slow processing times. Consequently, works that
follow this path [36, 18, 4, 35, 34, 16] use small spatial res-
olutions (e.g. 64 × 64 × 64), which clearly lag behind grid
resolutions typical for processing 2D data, and is likely to
be insufﬁcient for the recognition tasks that require atten-
tion to ﬁne details in the models.
To solve this problem, we take inspiration from the long
history of research in computer graphics and computational
geometry communities [25, 10], where a large number of
indexing structures that are far more scalable than uni-
form grids have been proposed, including kd-trees [1], oc-
trees [19], binary spatial partition trees [28], R-trees [11],
constructive solid geometry [22], etc. Our work was moti-
vated by the question, whether at least some of these index-
ing structures are amenable for forming the base for deep
architectures, in the same way as uniform grids form the
base for the computations, data alignment and parameter
sharing inside convolutional networks.
In this work, we pick one of the most common 3D in-
dexing structures (a kd-tree [1]) and design a deep architec-
ture (a Kd-network) that in many respects mimics ConvNets
but uses kd-tree structure to form the computational graph,
to share learnable parameters, and to compute a sequence
of hierarchical representations in a feed-forward bottom-
up fashion. In a series of experiments, we show that Kd-
networks come close (or even exceed) ConvNets in terms
of accuracy for recognition operations such as classiﬁca-
tion, retrieval and part segmentation. At the same time, Kd-
networks come with smaller memory footprints and more
efﬁcient computations at train and at test time thanks to the
improved ability of kd-trees to index and structure 3D data
as compared to uniform voxel grids.
Below, we ﬁrst review the related work on convolutional
networks for 3D models in Section 2. We then discuss the
Kd-network architecture in Section 3. An extensive evalua-
tion on toy data (a variation of MNIST) and standard bench-
marks (ModelNet10, ModelNet40, SHREC’16, ShapeNet
part datasets) is presented in Section 4. We summarize the
work in Section 5.
2. Related Work
Several groups investigated application of ConvNets
to the rasterizations of 3D models on uniform 3D grids
[36, 18]. The improvements include combinations of gener-
ative and very deep discriminative architectures [4, 35]. De-
arXiv:1704.01222v2  [cs.CV]  26 Oct 2017


--- Page 2 ---

spite considerable success in coarse-level classiﬁcation, the
reliance on uniform 3D grids for data representation makes
scaling of such approaches to ﬁne-grained tasks and high
spatial representations problematic. To improve the scala-
bility [34, 16] have considered sparse ways to deﬁne con-
volutions, while still using uniform 3D grids for representa-
tions.
Another approach [31, 21] is to avoid the use of 3D
grids, and instead apply two-dimensional ConvNets to 2D
projections of 3D objects, while pooling representations
corresponding to different views.
Despite gains in efﬁ-
ciency, such approach may not be optimal for hard 3D
shape recognition tasks due to the loss of information as-
sociated with the projection operation.
A group of ap-
proaches (such as spectral ConvNets [6, 2] and anisotropic
ConvNets [3]) generalize ConvNets to non-Euclidean ge-
ometries, such as mesh surfaces. These have shown very
good performance for local correspondence/matching tasks,
though their performance on standard shape recognition and
retrieval benchmarks has not been reported. Kd-networks
as well as the PointNet architecture [20] work directly with
points and therefore can take the representations computed
with intrinsic ConvNets as inputs. Such conﬁguration is
likely to combine at least some of the advantages of extrin-
sic and intrinsic ConvNets, but its investigation is left for
future work.
Aside from their connections to convolutional networks
that we discuss in detail below, Kd-networks are related to
recursive neural networks [30]. Both recursive neural net-
works and Kd-networks have tree-structured computational
graphs. However, the former share parameters across all
nodes in the computational tree graph, while sharing of pa-
rameters in Kd-networks is more structured, which allows
them to achieve competitive performance.
Finally, two approaches developed in parallel to ours
share important similarities.
OctNets [23] are modiﬁed
ConvNets that operate on non-uniform grids (shallow Oct-
Trees) and thus share the same idea of utilizing non-uniform
spatial structures within deep architectures. Even more re-
lated are graph-based ConvNets with edge-dependant ﬁlters
[29]. Kd-networks can be regarded as a particular instance
of their architecture with a kd-tree being an underlying
graph (whereas [29] evaluated nearest neighbor graphs for
point cloud classiﬁcation). Kd-networks outperform both
[23] and the setup in [29] on the ModelNet benchmarks
suggesting that deep architectures based on kd-trees may be
particularly well suited for coarse-level shape categoriza-
tion.
3. Shape Recognition with Kd-Networks
We now introduce Kd-networks, starting with the discus-
sion of their input format (kd-trees of certain size), then dis-
cussing the bottom-up computation of representations per-
8
9
10
11
12
13
15
14
0
1
2
3
4
5
6
7
8
9
10
15
11
12
13
14
Figure 1. A kd-tree built on the point cloud of eight points (left),
and the associated Kd-network built for classiﬁcation (right). We
number nodes in the kd-tree from the root to leaves. The arrows
indicate information ﬂow during forward pass (inference). The
leftmost bars correspond to leaf (point) representations. The right-
most bar corresponds to inferred class posteriors v0. Circles corre-
spond to afﬁne transformations with learnable parameters. Colors
of the circles indicate parameter sharing, as splits of the same type
(same orientation, same tree level – three “green” splits in this ex-
ample) share the transformation parameters.
formed by Kd-networks, and ﬁnally discussing supervised
parameter learning.
3.1. Input
The new deep architecture (the Kd-network) works with
kd-trees constructed for 3D point clouds.
Kd-networks
can also consider and utilize properties of individual input
points (such as color, reﬂectivity, normal direction) if they
are known. At train time, Kd-network works with point
clouds of a ﬁxed size N = 2D (point clouds of different
sizes can be reduced to this size using sub- or oversam-
pling). A kd-tree is constructed recursively in a top-down
fashion by picking the coordinate axis with the largest range
(span) of point coordinates, and splitting the set of points
into two equally-sized subsets, subsequently recursing to
each of them. As a result, a balanced kd-tree T of depth
D is produced that contains N−1 = 2D−1 non-leaf nodes.
Each non-leaf node Vi ∈T is thus associated with one
of three splitting directions di (along x, y or z-axis, i.e. di ∈
{x, y, z}) and a certain split position (threshold) τi. A tree
node is also characterized by the level li ∈{1, .., D −1},
with li=1 for the root node, and li=D for tree leaves that
contain individual 3D points. We assume that the nodes in
the balanced tree are numbered in the standard top-down
fashion, with the root being the ﬁrst node, and with the ith
node having children with numbers c1(i) = 2i and c2(i) =
2i + 1.
3.2. Processing data with Kd-networks
Given an input kd-tree T , a pretrained Kd-network
computes vectorial representations vi associated with each
node of the tree. For the leaf nodes these representations
are given as k-dimensional vectors describing the individ-


--- Page 3 ---

ual points, associated with those leaves. The representa-
tions corresponding to non-leaf nodes are computed in the
bottom-up fashion (Figure 1). Consider a non-leaf node i
at the level l(i) with children c1(i) and c2(i) at the level
l(i)+1, for which the representations vc1(i) and vc2(i) have
already been computed. Then, the vector representation vi
is computed as follows:
vi =





φ(W li
x [vc1(i); vc2(i)] + blix ), if di = x ,
φ(W li
y [vc1(i); vc2(i)] + bliy ), if di = y ,
φ(W li
z [vc1(i); vc2(i)] + bliz ), if di = z ,
(1)
or in short form:
vi = φ(W li
di[vc1(i); vc2(i)] + bli
di) .
(2)
Here, φ(·) is some non-linearity (e.g. REctiﬁed Linear Unit
φ(a) = max(a, 0)), and square brackets denote concate-
nation. The afﬁne transformation in (1) is deﬁned by the
learnable parameters {W li
x , W li
y , W li
z , blix , bliy , bliz } of the
layer li. Thus, depending on the splitting direction di of the
node, one of the three afﬁne transformations followed by a
simple non-linearity is applied.
The dimensionality of the matrices and the bias vectors
are determined by the dimensionalities m1, m2, . . . , mD of
representations at each level of the tree. The W l
x ,W l
y, and
W l
z matrices at the lth level thus have the dimensionality
ml×2ml+1 (recall that the levels are numbered from the
root to the leaves) and the bias vectors bl
x, bl
y, bl
z have the
dimensionality ml.
Once the transformations (1) are applied in a bottom-
up order, the root representation v1(T ) for the sample T
is obtained.
Naturally, it can be passed through several
additional linear and non-linear transformations (“fully-
connected layers”). In our classiﬁcation experiments, we
directly learn linear classiﬁers using v1(T ) representation
as an input. In this case, the classiﬁcation network output
the vector of unnormalized class odds:
v0(T ) = W 0v1(T ) + b0 ,
(3)
where W 0 and b0 are the parameters of the ﬁnal linear
multi-class classiﬁer.
3.3. Learning to classify
A Kd-network is a feed-forward neural network that has
the learnable parameters {W j
x , W j
y , W j
z , bj
x, bj
y, bj
z} at each
of the D−1 non-leaf levels j ∈{1..D−1}, as well as the
learnable parameters {W 0, b0} for the ﬁnal classiﬁer. Stan-
dard backpropagation method can be used to compute the
gradient of the loss function w.r.t. network parameters. The
network parameters can thus be learned from the dataset of
labeled kd-trees using standard stochastic optimization al-
gorithms and standard losses, such as cross-entropy on the
network outputs v0(T ) (3).
Figure 2. Kd-trees for MNIST clouds. We visualize several ex-
amples of 2D point clouds for MNIST (see text for description)
with constructed kd-trees. The type of split is encoded with color
and for each example the types of splits for the ﬁrst four levels of
the tree are shown below. Importantly, the structure of the kd-tree
serves as a shape descriptor (e.g. ‘ones’ are dominated by vertical
splits, and ‘zeroes’ tend to interleave vertical and horizontal splits
as a kd-tree is traversed from the root to a leaf).
3.4. Learning to retrieve
It is straightforward to learn the representation (3) to pro-
duce not the class odds, but a descriptor vector of a certain
dimensionality that characterizes the shape and can be used
for retrieval. The parameters of the Kd-network can then be
learned using backpropagation using any of the embedding-
learning losses that observe examples of matching (e.g.
same-class) and non-matching (e.g. different-class) shapes.
In our experiments, we use a recently proposed histogram
loss [33], but more traditional losses such as Siamese loss
[5, 8] or triplet loss [27] could be used as well.
3.5. Properties of Kd-networks
Here we discuss the properties of the Kd-networks and
also relate them to some of the properties of ConvNets.
Layerwise parameter sharing.
Similarly to Con-
vNets, Kd-networks process the inputs by applying a se-
quence of parallel spatially-localized multiplicative oper-
ations interleaved with non-linearities.
Importantly, just
as ConvNets share their parameters for localized multipli-
cations (convolution kernels) across different spatial loca-
tions, Kd-networks also share the multiplicative parameters
{W j
x , W j
y , W j
z , bj
x, bj
y, bj
z} across all nodes at the tree level
j.
Hierarchical representations. ConvNets apply bottom-
up processing and compute a sequence of representations
that correspond to progressively large parts of images. The


--- Page 4 ---

procedure is hierarchical, in the sense that a representation
of a spatial location at a certain layer is obtained from the
representations of multiple surrounding locations at the pre-
ceding layer using linear and non-linear operations. All this
is mimicked in Kd-networks, the only difference being that
the receptive ﬁelds of two different nodes at the same level
of the kd-tree are non-overlapping.
Partial invariance to jitter.
Convolutional networks
that use pooling operations and/or strides larger than one are
known to possess partial invariance to small spatial jitter in
the input. Kd-networks are also invariant to such jitter (un-
less such jitter strongly perturbs the representations of leaf
nodes). This is because the key forward-propagation opera-
tion (1) does ignore splitting thresholds τi. Thus, any small
spatial perturbation of input points that leave the topology
of the kd-tree intact can only affect the output of a Kd-
network via the leaf representations (which as will be re-
vealed in the experiments play only secondary role in kd-
networks).
Non-invariance to rotations. Similarly to ConvNets,
Kd-networks are not invariant to rotations, as the under-
lying kd-trees are not invariant to them.
In this aspect,
Kd-networks are inferior to intrinsic ConvNets [6, 2, 3].
Standard tricks to handle variable orientations include pre-
alignment (using heuristics or network branches that pre-
dict geometric transformations of the data [13, 20]) as well
as pooling over augmentations [14] (or simply training with
excessive augmentations).
Role of kd-tree structure. The role of the underlying
kd-trees in the process of Kd-network data processing is
two-fold. Firstly, the underlying kd-tree determines which
leaf representations are getting combined/merged together
and in which order. Secondly, the structure of the under-
lying kd-tree can be regarded as a shape descriptor itself
(Figure 2) and thus serves as the source of the information
irrespective of what the leaf representations are. The Kd-
network then serves as a mechanism for extracting the shape
information contained in the kd-tree structure. As will be
revealed in the experiments, the second aspect is of consid-
erable importance, as even in the absence of meaningful leaf
representations, Kd-networks are able to recognize shapes
well solely based on the kd-tree structure.
3.6. Extension for segmentation
Kd-network architecture can be extended to perform se-
mantic/part segmentation tasks in the same way as Con-
vNets.
In this work, we mimic the encoder-decoder
(hourglass-shaped) architecture with skip connections (Fig-
ure 3) that has been proposed for ConvNets in [17, 24].
More formally, during inference ﬁrstly the representations
vi are computed using (2), and then the second representa-
tion vector ˜vi is computed at each node i. The computations
of the second representation proceed by setting ˜v1=v1 (or
Figure 3. The architecture for parts segmentation (individual point
classiﬁcation) for the point cloud shown in Figure 1 (left). Arrows
indicate computations that transform the representations (bars)
of different nodes. Circles correspond to afﬁne transformations
followed by non-linearities. Similarly colored circles on top of
each other share parameters. Dashed lines correspond to skip-
connections (some “yellow” skip connections are not shown for
clarity). The input representations are processed by an additional
transformation (light-brown) and there are additional transforma-
tions applied to every leaf representation independently at the end
of the architecture (light-blue).
obtaining ˜v1 by one or several fully connected layers) and
then using the following chain of top-down computations:
˜vc1(i) = φ([ ˜W li
dc1(i) ˜vi + ˜bli
dc1(i); Slivc1(i) + tli]) ,
˜vc2(i) = φ([ ˜W li
dc2(i) ˜vi + ˜bli
dc2(i); Slivc2(i) + tli]) ,
(4)
where ˜W li
dc∗(i) and ˜bli
dc∗(i) are the parameters of the afﬁne
transformation that map the parent’s representation to the
children representations stacked on top of each other, while
Sli and tli are the parameters of the afﬁne transformation
within the skip connection from vc1(i) to ˜vc1(i) (as well as
from vc2(i) to ˜vc2(i)). In our implementation, the former set
of parameters depends on split orientation, while the latter
depends on the node layer only.
To increase the capacity of the model, additional multi-
plicative layers interleaved with non-linearities can be in-
serted in the beginning of the architecture or at the end of
architecture (with parameters shared across leaves making
these layers analogous to 1×1-convolutions in ConvNets).
Also, fully-connected multiplicative layers can be inserted
at the bottleneck.
3.7. Implementation details
Leaf representation. As mentioned above, for a leaf
node i a representation vi can be deﬁned in several ways.
In our experiments, unless stated otherwise, we use normal-
ized 3D coordinates obtained by putting the center of mass
of the shape at origin and rescaling the input point cloud to
ﬁt the [−1; 1]3 3D box.
Data augmentation. Similarly to other machine learn-
ing architectures, performance of Kd-networks can be im-
proved through training data augmentations.
Below, we


--- Page 5 ---

ModelNet
10-class
40-class
Accuracy averaging
class instance class instance
3DShapeNets [36]
83.5
-
77.3
-
MVCNN [31]
-
-
90.1
-
FusionNet [12]
-
93.1
-
90.8
VRN Single [4]
-
93.6
-
91.3
MVCNN [21]
-
-
89.7
92.0
PointNet [20]
-
-
86.2
89.2
OctNet [23]
90.1
90.9
83.8
86.5
ECC [29]
90.0
90.8
83.2
87.4
Kd-Net (depth 10)
92.8
93.3
86.3
90.6
Kd-Net (depth 15)
93.5
94.0
88.5
91.8
VRN Ensemble [4]
-
97.1
-
95.5
MVCNN-MultiRes [21]
-
-
91.4
93.8
Table 1. Classiﬁcation results on ModelNet benchmarks. Compar-
ison of accuracies of Kd-networks (depth 10 and 15) with state-
of-the-art. Kd-networks outperform all single model architectures
except MVCNNs, while performing worse than reported ensem-
bles.
experiment with applying perturbing geometric transforma-
tions to 3D point clouds. Additionally, we found the inject-
ing randomness into kd-tree construction very useful. For
that, we randomize the choice of split directions using the
following probabilities:
P (di = j|ˆri) =
exp γˆrj
i
P
j=x,y,z exp γˆrj
i
,
(5)
where ˆri is a vector of ranges normalized to unit sum.
4. Experiments
We now discuss the results of application of Kd-
networks to shape classiﬁcation, shape retrieval and part
segmentation tasks benchmarks. For classiﬁcation, we also
evaluate several variations and ablations of Kd-networks.
Our implementation of Kd-networks using Theano [32] and
Lasagne [9] as well as additional qualitative and quantita-
tive results are available at project webpage1.
4.1. Shape classiﬁcation
Datasets and data processing.
We evaluate Kd-
networks on datasets of 2D (for illustration purposes) as
well as 3D point clouds. 2D point clouds were produced
from the MNIST dataset [15] by turning centers of non-
zero pixels into 2D points. A point cloud of a needed size
was then sampled from the resulting set of points with an
addition of a small random noise. Figure 2 shows examples
of resulting point clouds.
The 10-class and the 40-class variations of Model-
Net [36] (ModelNet10 and ModelNet40) benchmarks, con-
taining 4899 and 12311 models respectively, were used for
1http://sites.skoltech.ru/compvision/kdnets/
MNIST ModelNet10 ModelNet40
Split-based linear
82.4
83.4
73.2
Kd-net RT+SA (no leaf)
98.6
92.7
89.8
Kd-net DT
98.9
89.2
85.7
Kd-net RT
99.1
92.8
89.9
Kd-net RT+TA
99.1
92.9
90.1
Kd-net RT+SA
99.1
93.2
90.6
Kd-net RT+SA+TA
99.1
93.3
90.6
Table 2. Classiﬁcation accuracy for baselines and different data
augmentations. The resulting accuracies for the baseline model,
the ablated model with trivial leaf representations, as well as Kd-
networks trained with various data augmentations. DT = deter-
ministic kd-trees, RT = randomized kd-trees, TA = translation aug-
mentation, SA = anisotropic scaling augmentation. All networks
are depth 10. See text for discussion.
3D shape classiﬁcations. The two datasets are split into
the training set (3991 and 9843 models) and the test set
(909 and 2468 models respectively). In this case, 3D point
clouds were computed as follows: ﬁrstly, a given number
of faces were sampled with the probability proportionate
to their surface areas. Then, for the sampled face a ran-
dom point was taken. The whole sampling procedure thus
closely approximated uniform sampling of model surfaces.
Training and test procedures.
Additionally we pre-
process each object by applying a geometric perturbation
and noise (as discussed below). Either a deterministic or
a randomized kd-tree is constructed and, ﬁnally, the result-
ing point cloud and leaf representations are used to perform
forward-backward pass in the Kd-Network. At test time,
we use the same augmentations as were used during train-
ing and average predicted class probabilities over ten runs.
We experimented with the following augmentations: (i)
proportional translations along every axis (TR) of up to
±0.1 in normalized coordinates; proportional anisotropic
rescaling over the two horizontal axes (AS) by the number
sampled from the 0.66 to 1.5 range. More global augmen-
tations like ﬂips or rotations did not improve results. Ad-
ditionally, we evaluated both deterministic (DT) and ran-
domized (RT) kd-trees. For our experiments we ﬁxed the
parameter γ in (5) to ten.
Benchmarking classiﬁcation performance. We com-
pare our approach to the state-of-the-art on the ModelNet10
and ModelNet40 benchmarks in Table 1. We give the re-
sults obtained with kd-trees of depth 10 and depth 15. For
depth 10, our architecture ﬁrstly obtains leaf representa-
tion of size 32 from initial points coordinates with an afﬁne
transformation with parameters shared across all the in-
put points interleaved with a ReLU non-linearity, then a
Kd-network obtains intermediate representations of sizes:
32 −64 −64 −128 −128 −256 −256 −512 −512 −128.
Resulting representation for a point cloud is directly used to
obtain class posteriors with a single fully connected layer.


--- Page 6 ---

For depth 15, the previous architecture has been modiﬁed
by changing the size of leaf representation to 8 and by
updated progression of intermediate representation sizes:
16 −16 −32 −32 −64 −64 −128 −128 −256 −256 −
512 −512 −1024 −1024 −128.
In both cases, we used translation-based and anisotropic
scaling-based augmentations as well as randomized kd-tree
generation at test and at train time. Note that despite the
use of random augmentations, a single model (i.e. a single
set of model weigths) was evaluated for each of the cases
(depth 10 and depth 15).
Our results are better than all
previous single-model results on these benchmarks except
MVCNNs. While being worse than the reported ensembles,
Kd-networks can be trained faster. VRN ensemble involves
6 models each trained over the course of 6 days on NVidia
Titan X. Our depth-10 model can be trained in 16 hours,
and our depth-15 model can be trained in 5 days using an
older NVidia Titan Black. Furthermore, more than 75% of
the time is spent on point cloud sampling and kd-tree ﬁt-
ting, while the training itself takes less then a quarter of the
mentioned times.
It is also interesting to note that the performance of Kd-
networks on the MNIST dataset reaches 99.1% (Table 2),
which is in the ballpark of the results obtained with Con-
vNets (without additional tricks).
Ablations and variants. Kd-networks use two sources
of information about each object, namely the leaf represen-
tations and the direction of the splits. Note, that the split
coordinates are not used in the classiﬁcation. We assess
the relative importance of the two sources of the informa-
tion using two baselines. Firstly, we consider the baseline
for both 2D and 3D point clouds that encode split informa-
tion from their kd-trees in the following way: every split
on every level is one-hot encoded and concatenated to re-
sulting feature vector. We then use a linear classiﬁer on
such a representation (which is also shown as red/blue bars
in Figure 2). This baseline evaluates how much information
can be recovered from the split orientation information with
very little effort.
We also evaluate a model ablation corresponding to our
full method with the exception that we remove the ﬁrst
source information. To this end, we make each leaf rep-
resentation equal a one-dimensional vector (i.e. scalar) that
equals one, effectively removing the ﬁrst source of informa-
tion.
The results in Table 2 suggest that the ﬁrst (linear clas-
siﬁcation) baseline performs much worse than Kd-network
(even without leaf information), which suggests that multi-
stage hierarchical data ﬂow and intricate weight sharing
mechanism of Kd-networks plays an important role (note,
however that this baseline performs considerably better than
chance suggesting that the orientation of splits in a kd-tree
can serve as shape descriptor). Most interestingly, the ab-
2
3
4
5
6
7
8
9 10 11 12 13
50
60
70
80
90
100
Kd-Tree Depth, N levels
Test Accuracy, %
MNIST
ModelNet10
ModelNet40
Figure 4. Kd-tree depth experiments.
Test accuracy for Kd-
networks trained on clouds of different size 2N (corresponding to
kd-tree depth N). Saturation without overﬁtting can be observed.
lated version of Kd-network comes very close to the full
method, highlighting that the second source of information
(split direction) dominates the ﬁrst in terms of importance
(conﬁrming the suitability of kd-trees for shape descrip-
tion).
Finally, in Table 2 we assess the importance of two dif-
ferent augmentations as well as the relative performance
of randomized and deterministic trees. These experiments
suggest that the randomization of kd-tree boosts the perfor-
mance (generalization) considerably, while the geometric
augmentations give a smaller effect.
Kd-tree depth experiments. For better understanding
of the effect of depth, we also conducted a series of ex-
periments corresponding to trees of different depths (Fig-
ure 4) of less or equal than ten. To obtain Kd-network archi-
tectures for smaller depths we simply remove initial layers
from our 10-depth architecture (described above).
Apart from the saturating performance, we observe that
the learning time for each epoch for smaller models be-
comes very short but the number of epochs to achieve con-
vergence increases. For bigger models the time of kd-tree
construction (and point sampling) becomes the bottleneck
in our implementation.
Degradation in the presence of non-uniform sampling
and jitter. We have also measured the degradation of Kd-
networks in the presence of non-uniform sampling and jit-
ter and provide the results in the supplementary material.
Overall, degradation from both effects on the ModelNet10
benchmark is surprisingly graceful.
4.2. Shape retrieval
Dataset and data processing. For the purpose of eval-
uation for 3D shape retrieval task we use ShapeNetCore
dataset [7].
ShapeNetCore is a subset of full ShapeNet
dataset of 3D shapes with manually veriﬁed category an-
notations and alignment. It consists of 51300 unique 3D
shapes divided into 55 categories each represented by its
triangular meshes. For our experiments we used a distri-


--- Page 7 ---

Micro
Macro
P@N R@N
F1@N mAP
NDCG@N
P@N
R@N
F1@N mAP
NDCG@N
Bai [26]
0.706 0.695 0.689 0.825
0.896
0.444 0.531 0.454 0.740
0.850
Su [26]
0.770 0.770 0.764 0.873
0.899
0.571 0.625 0.575 0.817
0.880
Kd-net (depth 15)
0.760 0.768 0.743 0.850
0.905
0.492 0.676 0.519 0.746
0.864
Bai [26]
0.678 0.667 0.661 0.811
0.889
0.414 0.496 0.423 0.730
0.843
Su [26]
0.632 0.613 0.612 0.734
0.843
0.405 0.484 0.416 0.662
0.793
Kd-net (depth 15)
0.473 0.519 0.451 0.617
0.814
0.205 0.529 0.241 0.484
0.726
MVKd-net (depth 10) 0.660 0.652 0.631 0.766
0.868
0.355 0.560 0.382 0.617
0.792
Table 3. Retrieval results on normal and perturbed (top and bottom respectively) version of ShapeNetCore dataset for the metrics introduced
in [26] (higher is better). See [26] for the details of metric and the presented systems (in general, all systems in [26] incorporated some
variants of 2D multi-view ConvNets). Kd-networks perform on par with the system of Su et al. that is based on multi-view ConvNet[31]
and generally better than other methods in case of pose normalized dataset. For the perturbed version of the dataset, Kd-network suffer
from degradation in performance due to sensitivity to global rotation. Multi-view (20 random views) version of Kd-network again perform
on par with most sophisticated multi-view ConvNets.
bution of the dataset and a training/validation/test split pro-
vided by the organizers of 3D Shape Retrieval Contest 2016
(SHREC16) [26]. Apart from the aligned shapes this distri-
bution contains a perturbed version of the dataset, which
consists of the same shapes each perturbed by a random ro-
tation. Also, there is an additional division into several sub-
categories available for each category. In our experiments
we evaluate on both versions of the dataset.
Training and test procedures. We used a two stage
training procedure for the object retrieval task. Firstly, the
network was trained to perform classiﬁcation task in the
manner described above. Secondly, the ﬁnal layer of the
network predicting the class posterior was removed, result-
ing representations of point clouds were normalized and
used as shape descriptors provided for the ﬁne-tuning of the
network with histogram loss. A mini-batch of size 110 was
used for training, each containing two randomly selected
shapes from each category of the dataset. Both training and
prediction was done with geometric perturbations and kd-
tree randomization applied. The parameters of the augmen-
tations were taken from the classiﬁcation task. To improve
stability and quality of prediction at test time for each model
the descriptors were averaged over several (16 in this exper-
iment) randomized kd-trees before normalization.
Benchmarking retrieval performance.
We compare
our results Table 3 with the results of the participants
of SHREC’16 for both normal and perturbed versions of
ShapeNetCore.
Most participating teams of SHREC’16
challenge used systems based on multi-view 2D ConvNets.
We use the metrics introduced in [26]. Macro averaged met-
rics are computed by simple averaging of a metric across all
shape categories, micro averaged metrics are computed by
weighted averaging with weights proportionate to the num-
ber of shapes in a category. A depth-15 Kd-network trained
with the histogram loss [33] was used for this task with leaf
representation of size 16 (obtained from the three coordi-
nates using an additional multiplicative layer) and interme-
diate representations of sizes 32−32−64−64−128−128−
256−256−512−512−1024−1024−2048−2048−512.
The obtained descriptors of size 512 were used to compute
similarity and make predictions for each shape. A similarity
cutoff was chosen from the results obtained on the valida-
tion part of the datasets.
In general our method performs on par with the sys-
tem based on multiview CNNs [31], and better than other
systems that participated in SHREC’16 for the ‘normal’
set. For the ‘perturbed’ version, the performance of Kd-
networks suffers from non-invariance to global rotations.
To address this, we implemented a simple modiﬁcation (in
the spirit of the TI-Pooling [14]) that applies Kd-network
(depth 10) to 20 different random rotations of a model and
performs max-pooling over the produced representations
followed by three fully connected layers to produce ﬁnal
shape descriptors. The resulting system achieved a compet-
itive performance on the ‘perturbed’ version of the bench-
mark (Table 3).
4.3. Part Segmentation
Finally, we used the architecture discussed in Section 3.6
to predict part labels for individual points within point
clouds (e.g. in an airplane each point can correspond to
body, wings, tail or engine).
Dataset and data processing.
We evaluate our ar-
chitecture for part segmentation on ShapeNet-part dataset
from [37]. It contains 16881 shapes represented as separate
point clouds from 16 categories with per point annotation
(with 50 parts in total). In this dataset, both the categories
and the parts within the categories are highly imbalanced,
which poses a challenge to all methods including ours.
Training and test procedures.
Since the number of
points representing each model differs in the dataset, we up-
sample each point cloud to size 4096 by duplicating random


--- Page 8 ---

mean aero
bag
cap
car
chair ear
guitar knife lamp laptop motor mug
pistol rocket skate
table
plane
phone
bike
board
Yi [37]
81.4
81.0
78.4 77.7 75.7 87.6 61.9
92.0
85.4 82.5 95.7
70.6
91.9 85.9
53.1
69.8
75.3
3DCNN [20]
79.4
75.1
72.8 73.3 70.0 87.2 63.5
88.4
79.6 74.4 93.9
58.7
91.8 76.4
51.2
65.3
77.1
PointNet [20] 83.7
83.4
78.7 82.5 74.9 89.6 73.0
91.5
85.9 80.8 95.3
65.2
93.0 81.2
57.9
72.8
80.6
Kd-network
82.3
80.1
74.6 74.3 70.3 88.6 73.5
90.2
87.2 81.0 94.9
57.4
86.7 78.1
51.8
69.9
80.3
Table 4. Part segmentation results on ShapeNet-core dataset. The Intersection-over-Union scores are presented for each category as well as
mean IoU are reported. Kd-network do not outperform PointNet, although for some classes the performance of Kd-networks is competitive
or better.
Figure 5. Examples of part segmentation resulting point labeling (use zoom-in for better viewing). Each pair of shapes contain ground truth
labeling on the left and predicted labeling on the right. The examples were randomly taken from the validation part of the ShapeNet-core
dataset.
points with an addition of a small noise. Apart from making
data feasible for our method, such upsampling helps with
rare classes. The upsampled point clouds then are fed to the
architecture shown in the Figure 3, which is optimized with
the mean cross entropy over all points in a cloud as a loss
function. During test time predictions are computed for the
upsampled clouds, then the original cloud is passed through
a constructed kd-tree to obtain a mapping of each leaf index
to corresponding set of original points. This is further used
to produce ﬁnal predictions for every point. Similar to other
tasks, we have used data augmentations both during train-
ing and test times and averaged predictions over multiple
kd-trees.
Benchmarking part segmentation performance. Our
results are compared to 3D-CNN (reproduced from [20]),
PointNet architecture [20], and the architecture of [37]. For
each category mean intersection over union (IoU) is con-
sidered as a metric: for each shape IoUs are computed as
an average of IoUs for each part which is possible to oc-
cur in this shape’s category. Resulting shape IoUs are aver-
aged over all the shapes in the category. A depth 12 variant
of Kd-network was used for this task with leaf representa-
tions of size 128 and intermediate representations of sizes
128 −128 −128 −256 −256 −256 −256 −512 −512 −
512 −512 −1024. Two additional fully connected layers
of sizes 512 and 1024 was used in the bottleneck of the ar-
chitecture. The output of segmentation network is further
processed by three afﬁne transformations interleaved with
ReLU non-linearities of sizes 512, 256, 128. The probabil-
ities of the 50 parts present in all classes in the dataset are
predicted (the probabilities of the parts that are not possible
for a given class are ignored following the protocol of [20]).
Batch-normalization is applied to each layer of the whole
architecture.
The performance of Kd-networks (Table 4) for the part
segmentation task is competitive though not improving over
state-of-the-art. We speculate that one of the reasons could
be insufﬁcient propagation of information across high-level
splits within kd-tree, although resulting segmentations do
not usually show the signs of underlying kd-tree structure
(Figure 5). A big advantage of Kd-networks for the seg-
mentation task is their low memory footprint. Thus, for our
particular architecture, the footprint of one example during
learning is less than 120 Mb.
5. Conclusion
In this work we propose new deep learning architecture
capable of production of representations suitable for differ-
ent 3D data recognition tasks which works directly with
point clouds. Our architecture has many similarities with
convolutional networks, however it uses kd-tree rather than
uniform grids to build the computational graphs and to share
learnable parameters. With our models we achieve results
comparable to current state-of-the-art for a variety of recog-
nition problems. Compared to the top-performing convolu-
tional architectures, kd-trees are also efﬁcient at test-time
and train-time.
The competitive performance of our deep architecture
based on kd-trees suggests that other hierarchical 3D space
partition structures, such as octrees, PCA-trees, bounding
volume hierarchies ould be investigated as underlying struc-
tures for deep architectures.
Acknowledgement: this work is supported by the Rus-
sian MES grant RFMEFI61516X0003.


--- Page 9 ---

References
[1] J. L. Bentley.
Multidimensional binary search trees used
for associative searching.
Communications of the ACM,
18(9):509–517, 1975.
[2] D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castel-
lani, and P. Vandergheynst. Learning class-speciﬁc descrip-
tors for deformable shapes using localized spectral convolu-
tional networks. Comput. Graph. Forum, 34(5):13–23, 2015.
[3] D. Boscaini, J. Masci, E. Rodol`a, and M. M. Bronstein.
Learning shape correspondence with anisotropic convolu-
tional neural networks. In Proc. NIPS, pages 3189–3197,
2016.
[4] A. Brock, T. Lim, J. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. arXiv preprint arXiv:1608.04236, 2016.
[5] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun,
C. Moore, E. S¨ackinger, and R. Shah. Signature veriﬁca-
tion using a siamese time delay neural network.
Interna-
tional Journal of Pattern Recognition and Artiﬁcial Intelli-
gence, 7(04):669–688, 1993.
[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral
networks and locally connected networks on graphs. arXiv
preprint arXiv:1312.6203, 2013.
[7] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015.
[8] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In Proc. CVPR, pages 539–546, 2005.
[9] S. Dieleman, J. Schlter, C. Raffel, E. Olson, et al. Lasagne:
First release., Aug. 2015.
[10] J. D. Foley, A. Van Dam, S. K. Feiner, J. F. Hughes, and R. L.
Phillips.
Introduction to computer graphics, volume 55.
Addison-Wesley Reading, 1994.
[11] A. Guttman, M. Stonebraker, and C. U. B. E. R. LAB. R-
trees: A Dynamic Index Structure for Spatial Searching.
Memorandum (University of California, Berkeley, Electron-
ics Research Laboratory). Defense Technical Information
Center, 1983.
[12] V. Hegde and R. Zadeh.
Fusionnet: 3d object classiﬁ-
cation using multiple data representations.
arXiv preprint
arXiv:1607.05695, 2016.
[13] M.
Jaderberg,
K.
Simonyan,
A.
Zisserman,
and
K. Kavukcuoglu.
Spatial transformer networks.
In
Proc. NIPS, pages 2017–2025, 2015.
[14] D. Laptev, N. Savinov, J. M. Buhmann, and M. Pollefeys.
TI-POOLING: transformation-invariant pooling for feature
learning in convolutional neural networks. In Proc. CVPR,
pages 289–297, 2016.
[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.
[16] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. Fpnn: Field
probing neural networks for 3d data. In Proc. NIPS, 2016.
[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proc. CVPR, pages
3431–3440, 2015.
[18] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
neural network for real-time object recognition.
In Proc.
IROS, pages 922–928. IEEE, 2015.
[19] D. J. Meagher. Octree encoding: A new technique for the
representation, manipulation and display of arbitrary 3-d
objects by computer.
Electrical and Systems Engineering
Department Rensseiaer Polytechnic Institute Image Process-
ing Laboratory, 1980.
[20] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
arXiv preprint arXiv:1612.00593, 2016.
[21] C. R. Qi, H. Su, M. Niessner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view cnns for object classiﬁ-
cation on 3d data. In Proc. CVPR, 2016.
[22] A. Requicha, H. Voelcker, and U. of Rochester. Production
Automation Project.
Constructive Solid Geometry.
TM
(Rochester, PAP). Production Automation Project, Univer-
sity of Rochester, 1977.
[23] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learn-
ing deep 3d representations at high resolutions. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017.
[24] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In Proc.
MICCAI, pages 234–241. Springer, 2015.
[25] H. Samet. The design and analysis of spatial data structures,
volume 199. Addison-Wesley Reading, MA, 1990.
[26] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or,
W. Deng, H. Su, S. Bai, X. Bai, et al. SHREC16 track large-
scale 3d shape retrieval from ShapeNet Core-55. In Proceed-
ings of the Eurographics Workshop on 3D Object Retrieval,
2016.
[27] M. Schultz and T. Joachims. Learning a distance metric from
relative comparisons. Advances in neural information pro-
cessing systems (NIPS), page 41, 2004.
[28] R. A. Schumacker, B. Brand, M. G. Gilliland, and W. H.
Sharp. Study for applying computer-generated images to vi-
sual simulation. Technical report, DTIC Document, 1969.
[29] M. Simonovsky and N. Komodakis.
Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In Proc. CVPR, 2017.
[30] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing nat-
ural scenes and natural language with recursive neural net-
works. In Proc. ICML, pages 129–136, 2011.
[31] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-
view convolutional neural networks for 3d shape recognition.
In Proc. ICCV, pages 945–953, 2015.
[32] Theano Development Team. Theano: A Python framework
for fast computation of mathematical expressions. arXiv e-
prints, abs/1605.02688, May 2016.
[33] E. Ustinova and V. S. Lempitsky. Learning deep embeddings
with histogram loss. In Proc. NIPS, pages 4170–4178, 2016.
[34] D. Z. Wang and I. Posner. Voting for voting in online point
cloud object detection. In Proc. RSS, 2015.


--- Page 10 ---

[35] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.
Learning a probabilistic latent space of object shapes via 3d
generative-adversarial modeling. In Proc. NIPS, pages 82–
90, 2016.
[36] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proc. CVPR, pages 1912–1920, 2015.
[37] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, A. Lu,
Q. Huang, A. Sheffer, L. Guibas, et al. A scalable active
framework for region annotation in 3d shape collections.
ACM Transactions on Graphics (TOG), 35(6):210, 2016.
```

---

## Section 1.15: Estimating surface normals in noisy point cloud data

Source File: Estimating surface normals in noisy point cloud data.txt

### Paper Content:

```
# Estimating surface normals in noisy point cloud data.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Estimating surface normals in noisy point cloud data.pdf
# Output: ../layer2_completion/txt/Estimating surface normals in noisy point cloud data.txt


--- Page 1 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
International Journal of Computational Geometry & Applications
c⃝World Scientiﬁc Publishing Company
Estimating Surface Normals in Noisy Point Cloud Data∗
NILOY J. MITRA
Stanford Graphics Laboratory, James H. Clark Center, Room S297,
318 Campus Drive, Stanford, CA 94305 USA
niloy@stanford.edu
AN NGUYEN
Stanford Graphics Laboratory, James H. Clark Center, Room S297,
318 Campus Drive, Stanford, CA 94305 USA
anguyen@cs.stanford.edu
LEONIDAS GUIBAS
Stanford Graphics Laboratory, James H. Clark Center, Room S293,
318 Campus Drive, Stanford, CA 94305 USA
guibas@cs.stanford.edu
Received 7 July 2003
Revised 16 November 2003
Communicated by Joe Mitchell
In this paper we describe and analyze a method based on local least square ﬁtting
for estimating the normals at all sample points of a point cloud data (PCD) set, in the
presence of noise. We study the eﬀects of neighborhood size, curvature, sampling density,
and noise on the normal estimation when the PCD is sampled from a smooth curve in
R2 or a smooth surface in R3, and noise is added. The analysis allows us to ﬁnd the
optimal neighborhood size using other local information from the PCD. Experimental
results are also provided.
Keywords: normal estimation; noisy point cloud data; eigen analysis; neighborhood size
estimation.
1. Introduction
Modern range sensing technology enables us to make detailed scans of complex
objects generating point cloud data (PCD) consisting of millions of points. The data
acquired is usually distorted by noise arising out of various physical measurement
processes and limitations of the acquisition technologies.
∗A preliminary version of this paper appeared in the Proc. of the 19th ACM Symp. on Computa-
tional Geometry, 2003. The work was supported by NSF CARGO grant 0138456 and a Stanford
Graduate Fellowship.
1


--- Page 2 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
2
Niloy J. Mitra, An Nguyen, Leonidas Guibas
The traditional way to use PCD is to reconstruct the underlying surface model
represented by the PCD, for example as a triangle mesh, and then apply well known
methods on that underlying manifold model. However, when the size of the PCD is
large, such methods may be expensive. To do surface reconstruction on a PCD, one
would ﬁrst need to ﬁlter out the noise from the PCD, usually by some smoothing
ﬁlter 15. Such a process may remove sharp features as well, and this may be unde-
sirable. A reconstruction algorithm, such as those proposed by Amenta et al. 2,4,
then computes a mesh that approximates the noise free PCD. Both the smooth-
ing and the surface reconstruction processes may be computationally expensive.
For certain applications like rendering or visualization, such a computation is often
unnecessary and direct rendering of PCD has been investigated by the graphics
community 17,18.
Alexa et al. 1 and Pauly et al. 17 have proposed to use PCD as a new modeling
primitive. Algorithms for such a paradigm often require information about the
normal at each of the points. For example, normals are used in rendering PCD,
making visibility computation, answering inside–outside queries, etc. Also some
curve (or surface) reconstruction algorithms 6,7 need to have the normal estimates
as a part of the input data.
The normal estimation problem has been studied by various communities such
as computer graphics, image processing, and mathematics, but mostly in the case
of manifold representations of the surface. We would like to estimate the normal at
each point in a PCD, given to us only as an unstructured set of points sampled from
a smooth curve in R2 or a smooth surface in R3, without any additional manifold
structure.
Hoppe et al. 13 proposed an algorithm where the normal at each point is es-
timated as the normal to the ﬁtting plane obtained by applying the total least
square method to the k-nearest neighbors of the point. This method is robust in
the presence of noise due to the inherent low pass ﬁltering. In this algorithm, the
value of k is a parameter and is chosen manually based on visual inspection of the
computed estimates of the normals, and diﬀerent trial values of k may be needed
before a good selection of k is found. Furthermore, the same value of k is used for
the normal estimation at all points in the PCD ignoring the variation in curvature
and sampling density along the PCD.
We note that the accuracy of the normal estimation using a total least square
method depends on (1) the noise in the PCD, (2) the curvature of the underlying
manifold, (3) the density and the distribution of the samples, and (4) the neigh-
borhood size used in the estimation process. In this paper, we make precise such
dependencies and study the contribution of each of these factors on the normal
estimation process. This analysis allows us to ﬁnd the optimal neighborhood size to
be used in the method. The neighborhood size can be computed adaptively at each
point based on local information, given some estimates about the noise, the local
sampling density, and bounds on the local curvature. The computational complexity
of estimating all normals of a PCD with m points is only O(m log m).


--- Page 3 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
3
1.1. Related Work
In this section, we summarize some of the previous works that are related to the
computation of the normal vectors of a PCD. Many current surface reconstruction
algorithms 2,4,10 can either compute the normals as part of the reconstruction, or
the normals can be trivially approximated once the surface has been reconstructed.
As the algorithms require that the input is noise free, a raw PCD with noise needs
to go through a smoothing process before these algorithms can be applied.
Fleishman et al. 9 and Jones et al. 14 have independently proposed the use of
edge-preserving ﬁlters for designing fast feature-preserving mesh denoising tech-
niques. These methods can be easily extended to PCDs using neighborhood graphs
to approximate connectivity information. However, these denoising algorithms have
parameters that have to be manually adjusted for good results.
The work of Hoppe et al. 13 for surface reconstruction suggests a method to
compute the normals for the PCD. The normal estimate at each point is done by
ﬁtting a least square plane to its k-nearest neighbors. The value of k is selected
experimentally. The same approach has also been adopted by Zwicker et al. 20
for local surface estimation. Higher order surfaces have been used by Welch and
Witkin 19 for local parameterization. However, as pointed out by Amenta and
Bern
3 such algorithms can fail even in cases with arbitrarily dense set of samples.
This problem can be resolved by assuming uniformly distributed samples which
prevents errors resulting from biased ﬁts. As noted before, all these algorithms
work well even in presence of noise because of the inherent ﬁltering eﬀect. The
success of these algorithms depends largely on selecting a suitable value for k, but
usually little guidance is provided for the selection of this crucial parameter.
1.2. Paper Overview
In section 2, we study the normal estimation for PCD which are samples of curves
in R2, and the eﬀects of diﬀerent parameters of the normal estimation algorithm
on the resulting error. In section 3, we derive similar results for PCD which come
from surfaces in R3. In section 4, we provide simulations to illustrate the results
obtained in sections 2 and 3. We also provide an algorithm for using our theoretical
results on practical data. We conclude in section 5.
2. Normal Estimation in R2
In this section, we consider the problem of approximating the normals for a point
cloud in R2. Given a set of points, which are noisy samples of a smooth curve in R2,
we can use the following method to estimate the normal to the curve at each of the
sample points. For each point O, we ﬁnd all the points of the PCD inside a circle
of radius r centered at O, and then compute the total least square line ﬁtting those
points. The normal to the ﬁtting line gives us an approximation to the undirected
normal of the curve at O. Note that the orientation of the normals is a global


--- Page 4 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
4
Niloy J. Mitra, An Nguyen, Leonidas Guibas
property of the PCD and thus cannot be computed locally. Once all the undirected
normals are computed, an approximation algorithm like the one suggested by Hoppe
et al. 13 can be applied to obtain a consistent global orientation for all the normals.
For the rest of this paper, we only consider the computation of the undirected
normals.
We analyze the error of the approximation when the noise is small and the
sampling density is high enough around O. Under these assumptions, which we will
make precise later, the computed normal approximates well the true normal. We
observe that if r is large, the neighborhood of the point cannot be well approximated
by a line in the presence of local curvature in the data and we may incur large
error. On the other hand, if r is small, the noise in the data can result in signiﬁcant
estimation error. We aim for the optimal r that strikes a balance between these
opposing sources of error.
2.1. Modeling
Without loss of generality, we consider O as the origin, and the y-axis to be along
the normal to the curve at O. We assume that the points of the PCD around O come
from a segment of a smooth curve (a 1-D topological disk) of bounded curvature.
More precisely, we assume that the segment of the curve near O is locally a graph
of a single valued C2 continuous function y = g(x) deﬁned over some interval R
containing [−r, r], and that |g′′(x)| < κ for all x ∈R where κ is some positive
constant.
Let pi = (xi, yi) for 1 ≤i ≤k be the points of the PCD that lie inside a circle of
radius r centered at O. We assume the following probabilistic model for the points
pi. Assume that xi’s are instances of a random variable X taking values in [−r, r],
and yi = g(xi)+ni, where the noise terms ni are independent instances of a random
variable N. X and N are assumed to be independent. We assume that the noise N
has zero mean and standard deviation σn, and takes values in [−n, n].
Using Taylor series, there are numbers ψi, 1 ≤i ≤k such that g(xi) =
g′′(ψi)x2
i /2 with |ψi| ≤|xi| ≤r. Let γi = g′′(ψi), then the bounded curvature
assumption implies that |γi| ≤κ.
Note that if κr is large, even when there is no noise in the PCD, the normal
to the best ﬁt line may not be a good approximation to the tangent as shown in
Figure 1. Similarly, if σn/r is large and the noise is biased, this normal may not be
a good approximation even if the original curve is a straight line, see Figure 2. In
order to keep the normal approximation error low, we assume a priori that κr and
σn/r are suﬃciently small.
We assume that the samples are evenly distributed; there is a radius r0 > 0
(possibly dependent on O) so that any neighborhood of size r0 in R contains at
least two points of the xi’s but no more than some small constant number of them.
We observe that the number of points k inside any disk of radius r is bounded from
above by Θ(1)rρ, and also is bounded from below by another Θ(1)rρ, where ρ is


--- Page 5 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
5
r
κr2
Fig. 1. Curvature causes error in the estimated normal
n
n
r
Fig. 2. Noise causes error in the estimated normal
the sampling density of the point cloud. Throughout this paper, we use Θ(1) to
denote some positive constant, and for notational simplicity, diﬀerent appearances
of Θ(1) may denote diﬀerent constants. We note that distributions satisfying the
(ϵ, δ) sampling condition proposed by Dey et al. 8 are evenly distributed in our
sense.
Under the above assumptions, we would like to bound the normal estimation
error and study the eﬀects of diﬀerent parameters on the estimation error. The
analysis involves probabilistic arguments to account for the random nature of the
noise.
2.2. Total Least Square Method
The normal to the total least square ﬁtting line (or hyper-plane) of a set of k
points pi, 1 ≤i ≤k in Rd for d ≥2 can be obtained by computing the eigenvector
corresponding to the smallest eigenvalue of the covariance matrix M deﬁned as
M = 1
k
Pk
i=1 (pi −¯p)(pi −¯p)T
13, where ¯p = 1
k
Pk
i=1 pi. We observe that M is
always symmetric positive semi-deﬁnite, and thus M has non-negative eigenvalues
and non-negative diagonal entries.
2.3. Eigen-analysis of M
We can write the 2×2 symmetric matrix M deﬁned in the previous section as
· m11 m12
m12 m22
¸
. Note that in the absence of noise and curvature, m12 = m22 = 0,
and thus zero is the smallest eigenvalue of M with [0 1]T as the corresponding
eigenvector. Under our assumption that the noise and the curvature are small, yi’s
are small, and thus m12 and m22 are small. Let α = (|m12| + m22)/m11. We would
like to estimate the smallest eigenvalue of M and its corresponding eigenvector
when α is small.
From the Gershgorin Circle Theorem 11 it follows that there is an eigenvalue λ1


--- Page 6 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
6
Niloy J. Mitra, An Nguyen, Leonidas Guibas
such that |m11 −λ1| ≤|m12|, and an eigenvalue λ2 such that |m22 −λ2| ≤|m12|.
When α < 1/2, we have that λ1 ≥m11 −|m12| > m22 + |m12| ≥λ2. It follows that
the two eigenvalues are distinct, and λ2 is the smallest eigenvalue of M. Let [v 1]T
be the eigenvector corresponding to λ2, then
· m11 m12
m12 m22
¸ · v
1
¸
= λ2
· v
1
¸
,
· m11 −λ2
m12
¸
v = −
·
m12
m22 −λ2
¸
.
Thus,
v = −(m11 −λ2)m12 + m12(m22 −λ2)
(m11 −λ2)2 + m2
12
,
|v| = |m12|(m11 + m22 −2λ2)
(m11 −λ2)2 + m2
12
≤α(1 + α)
(1 −α)2 .
(1)
Thus, the estimation error, which is the angle between the estimated normal and the
true normal (which is [0 1]T in this case), is less than tan−1(α(1+α)/(1−α)2) ≈α,
for small α. Note that we could write the error explicitly in closed form, then
bound it. Our approach is more complicated, though as we will show later, it can
be extended to obtain the error bound for the 3D case. To bound the estimation
error, we need to bound α.
2.4. Estimating Entries of M
The assumption that the sample points are evenly distributed in the interval [−r, r]
implies that, given any number x in that interval, the number of points pi’s satisfy-
ing |xi −x| ≥r/4 is at least Θ(1)k. It follows easily that m11 = 1
k
Pk
i=1 (xi −¯x)2 ≥
Θ(1)r2. The constant Θ(1) depends only on the distribution of the random variable
X.
For the entries m12 and m22, we use |xi| ≤r and |yi| ≤κr2/2 + n to obtain the
following trivial bound:
|m12| =
¯¯¯¯¯
1
k
k
X
i=1
xiyi −1
k2
k
X
i=1
xi
k
X
i=1
yi
¯¯¯¯¯
≤2r(κr2/2 + n) ,
m22 ≤1
k
k
X
i=1
y2
i
≤2((κr2/2)2 + n2) .


--- Page 7 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
7
Thus,
α ≤Θ(1)
µ
κr + n
r + κ2r2 + n2
r2
¶
≤Θ(1)
³
κr + n
r
´
.
(2)
This bound illustrates the eﬀects of r, κ and n on the error. For large values of r,
the error caused by the curvature κr dominates, while for small neighborhoods the
term n/r dictates the error. Nevertheless, the expression depends on the absolute
bound n of the noise N. This bound n can be unnecessarily large or unbounded
for many distribution models of N. We would like to use our assumption on the
distribution of the noise N to further improve our bound on α.
Note that,
|m12| =
¯¯¯¯¯
1
k
k
X
i=1
xiyi −1
k2
k
X
i=1
xi
k
X
i=1
yi
¯¯¯¯¯
≤
¯¯¯¯¯
1
k
k
X
i=1
(γix3
i /2 + xini)
¯¯¯¯¯ +
¯¯¯¯¯
1
k2
k
X
i=1
xi
k
X
i=1
(γix2
i /2 + ni)
¯¯¯¯¯
≤κr3 +
¯¯¯¯¯
1
k
k
X
i=1
xini
¯¯¯¯¯ + r
¯¯¯¯¯
1
k
k
X
i=1
ni
¯¯¯¯¯ .
Furthermore, under the assumption that X and N are independent, we have
E[xini] = E[xi]E[ni] = 0 since E[ni] = 0, and Var(xini) = Θ(1)r2σ2
n since
Var(ni) = σ2
n. Let ϵ be some small positive number. Using the Chebyshev Inequal-
ity 16, the following bound on |m12| holds with probability at least 1 −ϵ:
|m12| ≤κr3 + Θ(1)
r
r2σ2n
ϵk
+ Θ(1)r
r
σ2n
ϵk
≤κr3 + Θ(1)
s
r2σ2n
ϵrρ + Θ(1)r
s
σ2n
ϵrρ
≤κr3 + Θ(1)σn
r r
ϵρ .
(3)
For reasonable noise models, we also have that:
m22 ≤1
k
k
X
i=1
2(γ2
i x4
i /4 + n2
i )
≤Θ(1)κ2r4 + Θ(1)σ2
n .


--- Page 8 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
8
Niloy J. Mitra, An Nguyen, Leonidas Guibas
2.5. Error Bound for the Estimated Normal
From the previous estimates of the entries of M, we obtain the following bound on
α with probability at least 1 −ϵ:
α ≤c1κr + c2
σn
p
ϵρr3 + c3
σ2
n
r2
(4)
for some small positive constants c1, c2, and c3 which depend only on the distribu-
tion of the points. Note that this bound depends on the standard deviation σn of
the noise N rather than its magnitude bound n.
For a given set of parameters κ, σn, ρ, and ϵ, we can ﬁnd the optimal r that
minimizes the right hand side of the inequality (4). As this optimal value of r is
not easily expressed in closed form, let us consider a few extreme cases:
• When there is no curvature (κ = 0) we can make the bound on α arbitrarily
small by increasing r. For suﬃciently large r, the bound is linear in σn and
it decreases as r−3/2.
• When there is no noise, we can make the error bound small by choosing r
as small as possible, say r = r0.
• When both noise and curvature are present, the error bound cannot
be arbitrarily reduced. When the density ρ of the PCD is suﬃciently
high, α ≤c1κr + (c2 + c3)σ2
n/r2. This error bound is minimized when
r = Θ(1)σ2/3
n
κ−1/3, in which case α ≤Θ(1)κ2/3σ2/3
n .
• When there are both noise and curvature, and the density ρ is suﬃciently
low, α ≤c1κr + (c2 + c3)σn/
p
ϵρr3. This bound is minimized when r =
Θ(1)(σ2
n/(ϵρκ2))1/5, in which case, α ≤Θ(1)(κ3σ2
n/(ϵρ))1/5.
3. Normal Estimation in R3
We can extend the results obtained for curves in R2 to surfaces in R3. Given a point
cloud obtained from a smooth 2-manifold in R3 and a point O on the surface, we
can estimate the normal to the surface at O as follows: ﬁnd all the points of the
PCD inside a sphere of radius r centered at O, then compute the total least square
plane ﬁtting those points. The normal vector to the ﬁtting plane is our estimate of
the undirected normal at O.
Given a set of k points pi, 1 ≤i ≤k, let M =
1
k
Pk
i=1(pi −¯p)(pi −¯p)T
where ¯p = 1
k
Pk
i=1 pi. As pointed out in subsection 2.2, the normal to the total
least square plane for this set of k points is the eigenvector corresponding to the
minimum eigenvalue of the covariance matrix M. Again, we would like to bound
the angle between this eigenvector and the true normal to the surface at O.
3.1. Modeling
We model the PCD in a similar fashion as in the R2 case. We assume that O is
the origin, the z-axis is the normal to the surface at O, and that the points of the


--- Page 9 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
9
PCD in the sphere of radius r around O are samples of a topological disk on the
underlying surface that has bounded curvature. We locally represent the surface
as the graph of a smooth single valued C2 continuous function z = g(x) where
x = [x, y]T . Using Taylor Theorem, we can write g(x) = 1
2xT Hx where H is the
Hessian of g at some point ψ such that |ψ| ≤|x|.
The assumption that the surface has bounded curvature in some neighborhood
around O implies that there exists a positive constant κ such that the Hessian H
of g satisﬁes ||H||2 ≤κ in that neighborhood.
We write the points pi as pi = (xi, yi, zi) = (xi, zi). We assume that zi =
g(xi) + ni, where the ni’s are independent instances of some random variable N
with zero mean and standard deviation σn. We similarly assume that the points
xi’s are evenly distributed in the xy-plane on a disk D of radius r centered at O,
i.e. there is a radius r0 such that any disk of size r0 inside D contains at least three
points among the xi’s but no more than some small constant number of them. We
also assume that the noise and the surface curvature are both small.
3.2. Eigen-analysis in R3
We write the covariance matrix M as


m11 m12 m13
m12 m22 m23
m13 m23 m33

≜
· M11 M13
M T
13 m33
¸
. As pointed
out in subsection 2.2, M is symmetric and positive semi-deﬁnite. Under the assump-
tion that the noise and the curvature are small, and that the points xi are evenly
distributed, m11 and m22 are the two dominant entries in M. We assume, without
loss of generality, that m11 ≤m22. Let α = (|m13| + |m23| + m33)/(m11 −|m12|).
As in the R2 case, we would like to bound the angle between the computed normal
and the true normal to the point cloud in term of α.
Denote by λ1 ≤λ2 the eigenvalues of the 2 × 2 symmetric matrix M11. Using
again the Gershgorin Circle Theorem, it is easy to see that m11 −|m12| ≤λ1 ≤
λ2 ≤m22 + |m12|.
Let λ be the smallest eigenvalue of M. From the Gershgorin Circle Theorem
we have λ ≤|m13| + |m23| + m33 = α(m11 −|m12|) ≤αλ1. Let [vT 1]T be the
eigenvector of M corresponding to the eigenvalue λ. Then, as with Equation (1),
we have that:
v = −
¡
(M11 −λI)2 + M13M T
13
¢−1 ((M11 −λI)M13 + M13(m33 −λ))
= −(M11 −λI)−2 ¡
I + (M11 −λI)−2M13M T
13
¢−1 ×
((M11 −λI)M13 + M13(m33 −λ)) ,
||v||2 ≤||(M11 −λI)−2||2
¯¯¯
¯¯¯
¡
I + (M11 −λI)−2M13M T
13
¢−1¯¯¯
¯¯¯
2 ×
(||(M11 −λI)||2||M13||2 + ||M13||2|m33 −λ|) .


--- Page 10 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
10
Niloy J. Mitra, An Nguyen, Leonidas Guibas
Note that,
||(M11 −λI)−2M13M T
13||2 ≤||(M11 −λI)−2||2||M13||2||M T
13||2
≤(λ1 −λ)−2(m2
13 + m2
23)
≤(1 −α)−2α2 .
Thus,
¯¯¯
¯¯¯
¡
I + (M11 −λI)−2M13M T
13
¢−1¯¯¯
¯¯¯
2 ≤
1
1 −(1 −α)−2α2
≤(1 −α)2
1 −2α .
It follows that:
||v||2 ≤
1
(1 −α)2λ2
1
(1 −α)2
1 −2α (λ2αλ1 + αλ1αλ1)
≤α(1 + α)
1 −2α
λ2
λ1
.
When α is small, the right hand side is approximately (λ2/λ1)α, and thus the
angle between the computed normal and the true normal, tan−1 ||v||2, is approxi-
mately bounded by (λ2/λ1)α ≤((m22 + |m12|)/(m11 −|m12|))α.
3.3. Estimation of the entries of M
As in the R2 case, from the assumption that the samples are evenly distributed, we
can show that Θ(1)r2 ≤m11, m22 ≤r2. We can also show that m33 ≤Θ(1)κ2r4 +
Θ(1)σ2
n. Let ρ be the sampling density of the PCD at O, then k = Θ(1)ρr2. Again,
let ϵ be some small positive number. Using the Chebyshev inequality, we have that
m13, m23 ≤Θ(1)κr3 + Θ(1)σnr/
√
ϵk ≤Θ(1)κr3 + Θ(1)σn/√ϵρ with probability at
least 1 −ϵ. For the term m12, we note that E[xiyi] = 0 and V ar(xiyi) = Θ(1)r4,
and so, by the Chebyshev inequality, m12 ≤Θ(1)r/√ϵρ with probability at least
1 −ϵ.
3.4. Error Bound for the Estimated Normal
Let β = m12/m11. We restrict our analysis to the cases when β is suﬃciently less
than 1, say β < 1/2. This restriction simply means that the points xi’s are not
degenerate, i.e. not all of the points xi’s are lying on or near any given line on the
xy-plane. With this restriction, it is clear that (λ2/λ1)α ≤(m22/m11)((1 + β)/(1 −
β))α = Θ(1)α.
From the estimations of the entries of M, we obtain the following bound with
probability at least 1 −ϵ:
λ2
λ1
α ≤Θ(1)κr + Θ(1)
σn
r2√ϵρ + Θ(1)κ2r2 + Θ(1)σ2
n
r2
≤Θ(1)κr + Θ(1)
σn
r2√ϵρ + Θ(1)σ2
n
r2 .


--- Page 11 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
11
This is an approximate bound on the angle between the estimated normal and
the true normal. To minimize this error bound, it is clear that we should pick
r =
µ 1
κ
µ
d1
σn
√ϵρ + d2σ2
n
¶¶1/3
,
(5)
for some constants d1, d2. The constants d1 and d2 are small and they depend only
on the distribution of the PCD.
We notice from the above result, when there is no noise, we should pick the radius
r to be as small as possible, say r = r0. When there is no curvature, the radius
r should be as large as possible. When the sampling density is high, the optimal
value of r that minimizes the error bound is approximately r = d1/3
2
(σ2
n/κ)1/3. This
result is similar to that for curves in R2, and it is not that intuitive.
4. Experiments
In this section, we describe simulations to validate our theoretical results. We then
show how to use the theoretical results to obtain good neighborhood sizes for the
normal computation using the least square method.
4.1. Validation
We consider a family of PCDs whose points are noisy samples of the curves
(x, κx3/6), for x ∈[−1, 1] for diﬀerent choices of κ. We estimate the normals to
the curves at the origin by applying the least square method on their correspond-
ing PCD. As the y-axis is known to be the true normal to the curves, the angles
between the computed normals and the y-axis give the estimation errors.
To obtain the PCDs in our experiments, we let the sampling density ρ be 100
points per unit length, and x to be uniformly distributed in the interval [−1, 1].
The y-components of the data have been polluted with uniformly random noise in
the interval [−n, n], for some value n.
Figure 3(a) shows the error as a function of the neighborhood size r when
n = 0.05 for three diﬀerent values of κ, κ = 0.4, 0.8, and 1.2. As predicted by Equa-
tion (4) for large values of r, the error increases as r increases. In the experiments,
it can be seen that the error increases as κr for r > 0.4.
Figure 3(b) shows the estimation error as a function of the neighborhood size
r for small r when κ = 1.2 for three diﬀerent values of n, n = 0.017, 0.033, and
0.05. We observe that the error tends to decrease as r increases for r < 0.15. This
is expected as from Equation (4), the bound on the error is a decreasing function
of r when r is small. To factor out the random eﬀect of noise, the estimation error
curves have been averaged over 50 runs of the experiment.
4.2. Estimating Neighborhood Size for the Normal Computation
In this section, we use the results obtained in Section 3 to estimate the normals at
all the sample points of a PCD. The data points in the PCD are assumed to be noisy


--- Page 12 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
12
Niloy J. Mitra, An Nguyen, Leonidas Guibas
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
0.3
Radius
Error Angle
0.4
0.8
1.2
(a) Error due to curvature dominates for r >
0.4. The error–curves behave similarly for
diﬀerent choices of κ.
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Radius
Error Angle
0.017
0.033
0.05
(b) Error due to noise dominates for r <
0.15. The error–curves behave similarly un-
der diﬀerent amounts of noise.
Fig. 3. Eﬀects of curvature and noise on estimation error for diﬀerent choices of r.
samples of a smooth surface in R3. This is the case, for example, for PCD obtained
by range scanners. We would like to use Equation (5) to obtain the neighborhood
size for the normal computation using the least square method.
We assume that the standard deviation σn of the noise has been provided as
a part of the input. We estimate the other local parameters in Equation (5), then
compute r. Note that this value of r minimizes the bound of the normal computation
error, and there is no guarantee that this would minimize the error itself. The
constants d1 and d2 in the equation depend on the sampling distribution of the
PCD. While we can attempt to compute the exact values of d1 and d2, we try to
estimate the values of d1 and d2.
Given a PCD, we estimate the local sampling density ρ as follows. For a given
point p in the PCD, we use the approximate nearest neighbor library ANN 5 to
ﬁnd the distance s from p to its k0-th nearest neighbor for some small number k0.
The local sampling density at p can then approximated as ρ = k0/(πs2) samples
per unit area.
To estimate the maximum local curvature κ, we use the method proposed by
Gumhold et al. 12. Let pj, 1 ≤j ≤k be the k-nearest sample points around p,
and let µ be the average distance from p to all the points pj. We compute the best
ﬁt least square plane for those k points, and let d be the distance from p to that
best ﬁt plane. The local curvature at p can then be approximated as κ = 2d/µ2.
This method gives an estimate of the local curvature without any guarantees on
the approximation quality.
Once all the parameters are obtained, we compute the neighborhood size r using
Equation (5). Note that the estimated value of r can be used to obtain a good value


--- Page 13 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
13
Algorithm 1 Estimates good normals for all the points of a noisy PCD
1: estimate d1, d2
2: choose ϵ
3: for each point p do
4:
k ←k0
5:
count ←MaxCount
6:
repeat
7:
rold ≈distance from p to its k-th nearest neighbor
8:
ρ ←k/πr2
old
9:
given k, compute κ locally (Gumhold et al. 12)
10:
compute rnew using Equation (5)
11:
k ←⌈πρr2
new⌉
12:
if k > kthreshold then
13:
break
14:
end if
15:
count ←count −1
16:
until count ̸= 0
17:
the normal to the least square ﬁt plane to the k-nearest neighbors of p gives
a good estimate of the normal at p
18: end for
for k, which, in turn, can be used to re-estimate the local density and the local
curvature. This suggests an iterative scheme in which we repeatedly estimate the
local density, the local curvature, and the neighborhood size. In our experiments,
we found that only a small number of iterations were enough to obtain good values
for all the quantities. Algorithm 1 illustrates this iterative scheme. For the following
experiments, k0 was set to 15, and MaxCount was set to 10. The value of ϵ was
ﬁxed at 0.1.
We still have the problem of obtaining good estimates for the constants d1 and
d2. Fortunately, we only have to estimate the constants once for a given PCD, and
we can use the same constants for other PCDs with a similar point distribution. We
used Figure 6(a) for choosing d1 and d2. The PCD was created such that underlying
model and hence the exact normals at all points (except those on the edges) are
known. Estimation errors can then be computed exactly at almost all the points
and this information used to estimate the constants. We found that d1 = 1, d2 = 4
is a good pair of values and the same pair has been be used for the other data sets.
Noisy PCD used in our experiments were obtained by adding noise to the original
data. The x, y, and z components of the noise were chosen independently and
uniformly random. The magnitude of the added noise was measured in a scale
where the average spacing between neighboring points in the mesh representation
of the original data was taken as one unit.
Figure 4 shows the eﬀects of curvature and noise on the choice of neighborhood


--- Page 14 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
14
Niloy J. Mitra, An Nguyen, Leonidas Guibas
(a) 1x noise level
(b) 2x noise level
Fig. 4. Eﬀects of curvature and noise on the choice of neighborhood size under diﬀerent amounts
of noise in the input data. The neighbors determined by Algorithm 1 for a few points on the bunny
have been highlighted.
size. The neighborhood of a few points are shown in the ﬁgures. Figure 4(a) demon-
strates that bigger neighborhoods have been selected in ﬂatter regions compared
to neighborhoods in regions with more local curvature. Figure 4(b), in compari-
son to Figure 4(a), shows that a higher noise level results in selection of larger
neighborhoods.
(a) 1x noise level
(b) 2x noise level
Fig. 5. Normal estimation errors for the bunny PCD with noise added. Points with more than 5◦
estimation error have been highlighted.
We compute the normals of the noisy PCD, and use the angles between those
normals and the normals of the original PCD as estimates of the normal compu-


--- Page 15 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
15
tation errors. The normals computed from the mesh representation of the original
data set are considered to be the true normals. The mesh representation of the
original data sets is not available to Algorithm 1. In Figure 5, we highlight the
points with estimation error more than 5◦under two diﬀerent amounts of noise.
Figure 6 shows the performance of the algorithm under diﬀerent noise condi-
tions. In Figure 6(c), we observe that even in presence of signiﬁcant noise, the
algorithm performs well in ﬂat faces of the object. As noted before, since the un-
derlying surface model is known for this PCD, the true normals used for computing
the estimation errors, are speciﬁed at almost all the points.
(a) 1x noise
(b) 2x noise
(c) 4x noise
Fig. 6. Performance of the algorithm under various noisy conditions. Points with more than 5◦
estimation error have been highlighted.
5. Conclusions
We have analyzed the method of least square ﬁtting to a neighborhood in estimat-
ing the normals to a point cloud data derived either from a smooth curve in R2
or a smooth surface in R3, with noise added. In both cases, we provided theoret-
ical bounds on the maximum angle between the estimated normal and the true
normal of the underlying manifold. This theoretical study allowed us to ﬁnd an
optimal neighborhood size to be used in the least square method. Application of
the theoretical study on practical data resulted in satisfactory behavior.
6. Acknowledgments
We are grateful to Tamal K. Dey for his remarks and suggestions. Special thanks
to Marc Levoy, Ron Fedkiw for their feedback and comments.
We also want to thank the numerous referees of the previous versions of this
paper for their extremely useful suggestions.


--- Page 16 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
16
Niloy J. Mitra, An Nguyen, Leonidas Guibas
References
1. Marc Alexa, Johannes Behr, Daniel Cohen-Or, Shachar Fleishman, David Levin, and
Claudio T. Silva. Point set surfaces. In Proceedings of the Conference on Visualization,
pages 21–28. IEEE Computer Society, 2001.
2. N. Amenta, S. Choi, T. K. Dey, and N. Leekha. A simple algorithm for homeomorphic
surface reconstruction. In Proceedings of the Symposium on Computational Geometry,
pages 213–222. ACM Press, 2000.
3. Nina Amenta and Marshall Bern. Surface reconstruction by voronoi ﬁltering. In Pro-
ceedings of the Symposium on Computational Geometry, pages 39–48, 1998.
4. Nina Amenta, Marshall Bern, and Manolis Kamvysselis. A new Voronoi-based surface
reconstruction algorithm. Computer Graphics, 32(Annual Conference Series):415–421,
1998.
5. Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y.
Wu. An optimal algorithm for approximate nearest neighbor searching ﬁxed dimen-
sions. Journal of the ACM, 45(6):891–923, 1998.
6. Jean-Daniel Boissonnat and Frederic Cazals. Smooth surface reconstruction via nat-
ural neighbour interpolation of distance functions. In Proceedings of the Symposium
on Computational Geometry, pages 223–232, 2000.
7. J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum,
and T. R. Evans. Reconstruction and representation of 3d objects with radial basis
functions. In Proceedings of the 28th Annual Conference on Computer Graphics and
Interactive Techniques, pages 67–76. ACM Press, 2001.
8. T. K. Dey, J. Giesen, S. Goswami, and W. Zhao. Shape dimension and approximation
from samples. In Proceedings of the Symposium on Discrete Algorithms, pages 772–
780, 2002.
9. Shachar Fleishman, Iddo Drori, and Daniel Cohen-Or. Bilateral mesh denoising. ACM
Transactions on Graphics (TOG), 22(3):950–953, 2003.
10. Stefan Funke and Edgar A. Ramos. Smooth-surface reconstruction in near-linear time.
In Proceedings of the Symposium on Computational Geometry, pages 781–790. Society
for Industrial and Applied Mathematics, 2002.
11. Gene H. Golub and Charles F. Van Loan. Matrix computations (3rd ed.). Johns Hop-
kins University Press, 1996.
12. S. Gumhold, X. Wang, and R. MacLeod. Feature extraction from point clouds. In
10th International Meshing Roundtable, Sandia National Laboratories, pages 293–305,
October 2001.
13. Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuet-
zle. Surface reconstruction from unorganized points. Computer Graphics, 26(2):71–78,
1992.
14. Thouis R. Jones, Fr´edo Durand, and Mathieu Desbrun. Non-iterative, feature-
preserving mesh smoothing. ACM Transactions on Graphics (TOG), 22(3):943–949,
2003.
15. In-Kwon Lee. Curve reconstruction from unorganized points. Computer Aided Geo-
metric Design, 17(2):161–177, 2000.
16. A. Leon-Garcia. Probability and Random Processes for Electrical Engineering. Addi-
son Wesley, 1994.
17. Mark Pauly, Richard Keiser, Leif P. Kobbelt, and Markus Gross. Shape modeling
with point-sampled geometry. ACM Transaction on Graphics, 22(3):641–650, 2003.
18. Szymon Rusinkiewicz and Marc Levoy. Qsplat: a multiresolution point rendering sys-
tem for large meshes. In Proceedings of the 27th Annual Conference on Computer
Graphics and Interactive Techniques, pages 343–352. ACM Press/Addison-Wesley


--- Page 17 ---

January 23, 2004
15:13
WSPC/Guidelines
normalEst
Estimating Surface Normals in Noisy Point Cloud Data
17
Publishing Co., 2000.
19. William Welch and Andrew Witkin. Free-form shape design using triangulated sur-
faces. Computer Graphics, 28(Annual Conference Series):247–256, 1994.
20. Matthias Zwicker, Mark Pauly, Oliver Knoll, and Markus Gross. Pointshop 3d: an
interactive system for point-based surface editing. In Proceedings of the 29th Annual
Conference on Computer Graphics and Interactive Techniques, pages 322–329. ACM
Press, 2002.
```

---

## Section 1.16: Fast and Accurate Computation of surface normals from range images

Source File: Fast and Accurate Computation of surface normals from range images.txt

### Paper Content:

```
# Fast and Accurate Computation of surface normals from range images.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Fast and Accurate Computation of surface normals from range images.pdf
# Output: ../layer2_completion/txt/Fast and Accurate Computation of surface normals from range images.txt


--- Page 1 ---

2011 IEEE International Conference on Robotics and Automation.
May 9-13, 2011. Shanghai, China.
Fast and Accurate Computation of
Surface Normals from Range Images
H. Badino, D. Huber, Y. Park and T. Kanade
Abstract— The fast and accurate computation of surface
normals from a point cloud is a critical step for many 3D
robotics and automotive problems, including terrain estimation,
mapping, navigation, object segmentation, and object recogni-
tion. To obtain the tangent plane to the surface at a point,
the traditional approach applies total least squares to its small
neighborhood. However, least squares becomes computationally
very expensive when applied to the millions of measurements
per second that current range sensors can generate. We refor-
mulate the traditional least squares solution to allow the fast
computation of surface normals, and propose a new approach
that obtains the normals by calculating the derivatives of the
surface from a spherical range image. Furthermore, we show
that the traditional least squares problem is very sensitive to
range noise and must be normalized to obtain accurate results.
Experimental results with synthetic and real data demonstrate
that our proposed method is not only more efﬁcient by up to
two orders of magnitude, but provides better accuracy than the
traditional least squares for practical neighborhood sizes.
I. INTRODUCTION
For most robotics and automotive problems, computational
power is a limited resource, which must be distributed wisely.
Surprisingly, the efﬁcient computation of normals from a
point cloud is a topic that has had very little attention in
the literature considering that surface normals are used for
a large number of problems in the computer vision domain,
such as terrain estimation [4], mapping [24], navigation [25],
object segmentation [10], and object recognition [9].
In the robotics and automotive domain, the environment is
usually measured as the sensors move small displacements
between views, allowing the use of range images to store
and organize the data [10]. In this paper, we use Spherical
Range Images (SRI) to efﬁciently compute surface normals.
Figure 1 shows an example of the extracted surface normals
from noisy sparse data obtained from a 3D imager.
The most commonly used method for obtaining surface
normals from point clouds is total linear least squares [11]
(least squares, hereafter, for brevity), since it is relatively
cheap to compute and easy to implement. However, least
squares becomes computationally very expensive when ap-
plied to the millions of measurements per second that current
range sensors such as stereo or high-deﬁnition LIDAR can
generate.
In this paper, we make three main contributions towards
the efﬁcient and accurate computation of surface normals.
First, we propose two new approaches for computing the
H. Badino, D Huber and T. Kanade are with the Robotics Institute,
Carnegie Mellon University, Pittsburgh, PA, USA. Y. Park is with the
Agency for Defence Development, Daejeon, Korea.
Fig. 1.
Normals obtained from noisy sparse 3D data. The data corresponds
to the marked region of the spherical range image shown in Figure 6c.
normals with linear least squares. For this purpose, the
traditional least squares loss function is reformulated in such
a way, that box-ﬁltering techniques can be applied to mini-
mize the number of operations required. The new algorithms
present speed up factors of up to two orders of magnitude
over the traditional method. Second, we propose a novel
method for solving the normals by computing derivatives of
the surface deﬁned by the spherical range image. The new
method is not only computationally very efﬁcient, but also
more accurate than least squares for small window sizes.
Third, we show that without a proper normalization, the
traditional least squares fails to compute accurate normal
estimates. This problem is solved by applying a non-isotropic
scaling to the data matrix.
II. RELATED WORK
The computation of surface normals has been widely
addressed by surface reconstruction approaches, where the
3D points are usually unorganized. The surface can be
reconstructed up to the ﬁrst order with the normal vectors at
the measured points. In [11], the computation of the tangent
plane at each point with least squares was ﬁrst proposed. The
least squares solution was also addressed by [18], where the
authors evaluated the effects of neighborhood size, measure-
ment noise, and surface curvature when estimating planes
with least squares and derived error bounds for the estimated
normals. An alternative solution to the normal computation
is based on the construction of Voronoi diagrams [1]. In this
approach, the normals can be estimated from the center of
large polar balls. Since the initial approaches to the normal
estimation problem using polar balls assumed noise free data
International Conference on Robotics and Automation (ICRA) 
Shanghai, China, May 9-13, 2011
H. Badino, D. Huber, Y. Park and T. Kanade, "Fast and Accurate Computation of Surface Normals from Range Images," 
International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011.


--- Page 2 ---

[20], the method was adapted to the case of noisy point
clouds [5], [7]. Both, the least squares and the polar balls
methods were compared in accuracy and computation time
in [6]. The experiments showed that the polar ball method is
more robust to noise but that least squares is more accurate
for low noise levels. It also showed that least squares is by
far the fastest method for the computation of the normals.
In the robotics and automotive domain, the points are
usually organized in graphs or range images, and the com-
putation of surface normals focuses almost exclusively on
least squares solutions [9], [10], [16], [21], [22], [24], [28].
Alternative methods for computing normals from organized
point clouds rely on averaging the normals of adjacent
triangles [2], [13], [19], [26], but the obtained normals are
sensitive to noise and to the proper construction of the graph.
Least squares and some of these averaging methods were
compared in quality and computation time in [13], [21], [27].
In all these experiments, least squares shows the best results
in terms of accuracy and speed.
III. DEFINITIONS
In this section, we deﬁne the variables, transformation
equations, and the range image that are used in the next
sections.
A. Spherical Coordinates
A coordinate vector in the spherical coordinate system
is deﬁned as m = (r, θ, φ)T , where r is the range, θ
the azimuth, and φ the elevation component. The coor-
dinates are constrained so that r ≥0, −π < θ ≤π, and
−π/2 < φ ≤π/2.
A point in Cartesian coordinates is represented by
p = (x, y, z)T . The transformation from the spherical to the
Cartesian coordinate system is given by
p = rv
(1)
where v is a unit direction vector deﬁned as:
v =


sin θ cos φ
sin φ
cos θ cos φ


(2)
The transformation from Cartesian to spherical coordinate
system is given by
m =


r
θ
φ

=


p
x2 + y2 + z2
arctan (x/z)
arcsin (y/
p
x2 + y2 + z2)

.
(3)
Given a set of n measurements mi of a scene observed
from a single viewpoint, we seek to estimate the unit surface
normals ni at positions pi. The next sections are dedicated
to solving this problem.
B. Spherical Range Images
A Spherical Range Image (SRI) is a function s(θ, φ) in
which the domain (θ, φ) represents the azimuth and elevation
components, and the codomain s() is a real value that
represents a distance or range. The three components of the
image −azimuth (column), elevation (row) and range (image
value) −deﬁne the coordinate vector of a 3D point, i.e.,
(θ, φ, r)T with r = s(θ, φ). In the actual implementation
of the SRI, the domain is discretized and the codomain is
approximated by a ﬂoating point variable. In this way, an
SRI stores an image as observed from a single viewpoint,
providing a 2.5D representation of the scene.
Range images are widely used for data processing and vi-
sualization purposes [4], [9], [10], [19], [27]. Figure 3 shows
some examples of synthetic SRIs with their corresponding
3D model, and Figure 6c shows an example of a real SRI
obtained from a Velodyne LIDAR.
C. Procedure
As noted by [20], most methods for computing normals
follow three main steps: 1) identify neighbors, 2) estimate
the normal vector using the neighbors, and 3) deﬁne the
direction of the obtained normal. For the ﬁrst step we use a
small rectangular window of size k = w×h around the point
to be estimated. The second step corresponds to the proper
estimation of the normals using the extracted neighbors
points. The next two sections deal with this step, and Section
VI compares the accuracies and computation times. The
third step is required to be able to identify the insides
and outsides of complex objects. When estimating normals
of surfaces observed from a single viewpoint, a globally
consistent normal orientation is obtained by constraining the
dot product pT n to be negative (i.e., if the obtained normal
is such that pT n > 0, then n is negated).
IV. LEAST SQUARES APPROACHES
This section presents three different least squares formu-
lations to the surface normal estimation problem.
A. Traditional Total Least Squares
In the simplest case, least squares is formulated to ﬁnd
the plane parameters that optimally ﬁt some small area of
the surface [11], [18]. A plane is deﬁned by the equa-
tion nxx + nyy + nzz −d = 0, where (x, y, z)T lies on the
plane and (nx, ny, nz, d) are the sought plane parameters.
Given a subset of k 3D points pi, i
=
1, 2, ..., k of
the surface, least squares ﬁnds the optimal normal vector
n = (nx, ny, nz)T and scalar d that minimizes
e =
k
X
i=1
 pT
i n −d
2
subject to
|n| = 1.
(4)
The
closed
form
solution
to
Equation
4
for
n
is
given by ﬁnding the eigenvector corresponding to the
smallest
eigenvalue
of
the
sample
covariance
matrix
M = 1/k Pk
i=1(pi −¯p)(pi −¯p)T with ¯p = 1/k Pk
i=1 pi.
The component d is found as the normal distance to the
plane, i.e., d = ¯pT n. This is the most widely used method
in the literature [10], [11], [14], [16], [18], [21], [22], [23],
[24], [28].
The computational complexity of the implementation of
the above solution is linear in the number of neighbors
k. The main problem with the above algorithm is that
the computation of the matrix M, and its corresponding


--- Page 3 ---

eigen analysis, makes the algorithm extremely slow for
practical image sizes. Table Ia shows the minimum number
of operations required for this method, which we will call
“traditional least squares.”
B. Normalized Least Squares
The traditional least squares assumes independent iden-
tically distributed noise. Nevertheless, the noise is caused
mainly due to range measurement error, which propagates
linearly in the viewing direction v of the 3D vector p
(see Equation 1). In consequence, obtaining the eigenvectors
directly from M produces bad results. The solution to this
problem is to normalize the coordinates of the 3D points
so that the three principal moments of the M becomes all
equal to unity while ensuring a good conditioning of the
data matrix. This is analogous to the conditioning method
proposed by Hartley [8], but in a different domain. Before
computing the eigenvalues, the matrix M is normalized by
(a) TRADITIONAL LEAST SQUARES
Operation
Mult.
Add.
Calculation of pi
3
0
Calculation of M
5 × k + 7
9 × (k −1) + 6
Eigen analysis
66
38
Total
5 × k + 76
9 × (k −1) + 44
(b) NORMALIZED LEAST SQUARES
Operation
Mult.
Add.
Traditional LS
5 × k + 76
9 × k + 35
3x3 Cholesky Fact.
7
4
Inversion of K
11
1
Scaling
18 × 4
18 × 4
Total
5 × k + 166
9 × k + 112
(c) UNCONSTRAINED LEAST SQUARES
Operation
Mult.
Add.
Calculation of pi
3
0
Product pipT
i
6
0
Box-ﬁlt. for f
M
0
4 × 6
Box-ﬁlt. for ˜b
0
4 × 3
Inversion of f
M
24
10
Equation 7
9
6
Total
42
52
(d) FAST LEAST SQUARES
Operation
Mult.
Add.
Box-ﬁltering for ˆb
0
4 × 3
Calculation of vi/ri
3
0
Equation 10
9
2 × 3
Total
12
18
(e) SRI DERIVATIVE METHOD
Operation
Mult.
Add.
Prewitt k × k
k −2
k
Gauss 3 × 3
9
8
Equation 13
11
6
Total
18 + k
14 + k
TABLE I
NUMBER OF MULTIPLICATIONS AND ADDITIONS REQUIRED FOR ONE NORMAL
COMPUTATION USING k NEIGHBOR POINTS.
Cholesky factorization, i.e.:
M ′ = K−1MK−T
(5)
where
K
is
a
lower
triangular
matrix
such
that
Pk
i=1 pipT
i = KKT . The eigenvectors are then obtained
from the normalized matrix M ′. Table Ib shows the total
number of operations required for the normalized least
squares. As it can be seen, the Cholesky factorization does
not depend on the number of neighbors k, but it adds a
signiﬁcant overhead to the traditional least squares.
C. Unconstrained Least Squares
We present now a computationally efﬁcient solution that
does not require eigen-analysis and the corresponding fac-
torization. We ﬁrst divide both parts of the loss function in
Equation 4 by d2 and eliminate the constraint on the normal
vector to obtain:
˜e =
k
X
i=1
 pT
i ˜n −1
2
(6)
where ˜n is the sought normal vector, deﬁned up to a scale
factor. Note that this formulation degenerates for planes
passing through the origin (for d = 0), but this situation
cannot happen for range images. We will show in the
experimental results that, although this formulation is not
theoretically optimal, it is more accurate than the traditional
least squares in practical real situations. The closed form
solution for ˜n is given by
˜n = f
M
−1˜b
(7)
where f
M = Pk
i=1 pipT
i and ˜b = Pk
i=1 pi. The ﬁnal unit
normal is found by normalization, i.e., n = ˜n/|˜n|.
The matrices f
M and ˜b can be computed efﬁciently. First,
an image I that contains the outer products is built, i.e.,
I(l, m) = pl,mpT
l,m, where pl,m is the 3D point correspond-
ing to image location (l, m). Then, box-ﬁltering is applied
to obtain a new image with the sums of the outer products
within the window (Fig. 2). The same procedure is applied
to obtain an image of vectors for ˜b. Box-ﬁltering is a well-
known technique in computer vision [17]. This technique not
only minimizes the number of operations required, but is also
computationally independent of the window size.
Most of the computational power required for the above
method is spent in the summation and inversion of matrices,
which, although independent of k, represents a considerable
computational burden (see Table Ic)1.
D. Fast Approximate Least Squares
In this section, we simplify the loss function to completely
avoid the computation of the matrix f
M −1 every time. By
multiplying and dividing the right hand side of Equation 6
by r2
i we obtain:
˜e =
k
X
i=1
r2
i
 vT
i ˜n −r−1
i
2
(8)
1Observe that Cholesky factorization could be use to solve Eq. 7, but
it would require more operations than the single matrix inversion of f
M.


--- Page 4 ---

Fig. 2.
Box-ﬁltering technique. The box-ﬁltering technique consists in three main steps. An initialization, where a vector of column sums is built and
the initial sum within the window is calculated. After initialization, 2 steps are performed iterating from top to bottom and from left to right. The ﬁrst
step consists in updating the previous window sum by a subtraction and addition of vector sums. The second step consist in updating the vector sum that
is not longer required in the current row by an addition and subtraction of original image components. Observe that, independent of the window size,
two additions and two subtractions are required to compute the total sum within the window. There is an overhead in the initialization that is linearly
dependent on k, but it becomes negligible for large image size n and small window size k. Observe that the codomain of the image can be scalars, vectors
or matrices.
with vi as deﬁned in Equation 2. Since all points pi are in a
small neighborhood, all ri are similar. Dropping the r2
i from
the above equation leads us to the following approximate
formulation of the loss function:
ˆe =
k
X
i=1
 vT
i ˆn −r−1
i
2
(9)
whose solution for ˆn is given by
ˆn = c
M
−1ˆb
(10)
with c
M = Pk
i=1 vivT
i and ˆb = Pk
i=1 vi/ri. In this new,
approximate formulation, the matrix c
M −1 is independent
of the ranges (i.e., the measurements), and depends only on
the image parameters, so that it can be precomputed. The
vector ˆb is obtained using the same box-ﬁltering technique
described above. This simpliﬁcation greatly reduces the com-
putational requirements and is independent of the window
size k (see Table Id). Observe that this method works directly
with spherical coordinates, not requiring the pre-computation
of 3D points in Cartesian coordinates as in the three previous
formulations. A similar formulation was presented in [9].
V. SRI DERIVATIVE APPROACH
We propose now a method, which obtains the normals
by performing calculations directly in the spherical space.
Instead of ﬁtting a plane to obtain the normal vector, we
compute the normal directly from the surface deﬁned by the
SRI.
A. Derivation of the del Operator
We will ﬁrst demonstrate how transform the Cartesian del
operator to spherical coordinates. In the next section, we will
apply the resulting operator to the SRI to obtain the normal
vector. The del operator in the Cartesian coordinate system
is given by
∇≡ˆx ∂
∂x + ˆy ∂
∂y + ˆz ∂
∂z
(11)
where ˆx, ˆy, and ˆz are the unit vectors in the respective
coordinate directions. Applying the chain rule to the partial
derivatives in 11 gives
∂
∂x
=
∂
∂r
∂r
∂x + ∂
∂θ
∂θ
∂x + ∂
∂φ
∂φ
∂x
∂
∂y
=
∂
∂r
∂r
∂y + ∂
∂θ
∂θ
∂y + ∂
∂φ
∂φ
∂y
∂
∂z
=
∂
∂r
∂r
∂z + ∂
∂θ
∂θ
∂z + ∂
∂φ
∂φ
∂z .
We apply ﬁrst the above partial derivatives to Equation 3 and
substitute the results into 11 to obtain:
∇
≡
ˆx
 ∂
∂r sin θ cos φ + ∂
∂θ
cos θ
r cos φ −∂
∂φ
sin θ sin φ
r

+
ˆy
 ∂
∂r sin φ + ∂
∂φ
cos φ
r

+
ˆz
 ∂
∂r cos θ cos φ −∂
∂θ
sin θ
r cos φ −∂
∂φ
cos θ sin φ
r

The expressions in parentheses deﬁne a rotation of a vector
so that the previous equation can be expressed as
∇≡
 ˆz
ˆx
ˆy 
Rθ,φ


∂/∂r
1
r cos φ ∂/∂θ
1
r ∂/∂φ


(12)
where
Rθ,φ =


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1




cos φ
0
−sin φ
0
1
0
sin φ
0
cos φ

.
Equation 12 expresses the Cartesian del operator as a func-
tion of variables and derivatives in spherical coordinates.


--- Page 5 ---

(a) Sphere
(b) Cylinder
(c) Prism
(d) Floor and Ceiling
Fig. 3.
Synthetic Data sets. The top images show the modeled object of each data set. The grids have a size of 2x2 meters. The sensor is located at the
center of the objects and the color encodes the range. The bottom images show the Spherical Range Image for the corresponding top image.
B. Obtaining the Surface Normals
The SRI implies a functional dependence of the range on
the azimuth and elevation components, i.e., r = s(θ, φ). The
normal vectors of a surface can be obtained by computing
the derivatives of the function deﬁning the surface. For this,
we apply the del operator of Equation 12 to the function s()
to obtain
n = ∇s(θ, φ) = ˆ
Rθ,φ


1
1
r cos φ∂r/∂θ
1
r∂r/∂φ


(13)
where ˆ
Rθ,φ =
 ˆz
ˆx
ˆy 
Rθ,φ. Observe that ˆ
Rθ,φ is
independent of the ranges and can be precomputed and stored
in a look-up table.
The partial derivatives are obtained by applying standard
image processing convolution kernels on the SRI, as is
usually done for edge extraction in images [3]: the image is
pre-ﬁltered with a Gaussian 3 × 3 mask, and then a Prewitt
operator is applied. The Prewitt computational complexity
depends linearly on the number of neighbors k (see Table Ie),
but it can be implemented very efﬁciently for practical
window sizes, as we demonstrate in the next section.
VI. EXPERIMENTAL RESULTS
We have conducted experiments with simulated and real
data in order to evaluate the accuracy of each method under
different types of objects and window sizes.
A. Synthetic Data
Figure 3 shows four synthetic SRIs with corresponding
3D models that were used for the evaluation. Four data sets
were generated:
• Sphere: a centered sphere with a radius of 10 m.
• Cylinder: an open cylinder with planar radius of 10 m
and height of 20 m.
• Prism: a uniform triangular open prism of 22 m maxi-
mal height.
• Floor and Ceiling: two parallel planes emulating a ﬂoor
and a ceiling, each at a normal distance of 2 m from
the sensor.
The Sphere data set covers the full ﬁeld of view with an
angular resolution of approximately 0.5◦for the azimuth
and elevation components forming a range image size of
750 × 375. The last three data sets cover a ﬁeld of view of
360◦× 86◦with the same angular resolution and an image
size of 750 × 175. The lower part of Figure 3 shows the
corresponding SRIs for each object. The parameter conﬁgu-
ration was chosen to simulate a typical LIDAR sensor. The
data sets are available online [12].
The traditional LS and the three new proposed meth-
ods were used to estimate the normals under different
levels of noise and window sizes. The evaluation was
performed obtaining the average angular error in the es-
timation of the normals. The angular error is deﬁned as
ei = arccos(nT
i ¯ni) where ni is the estimated normal and
¯ni is the known truth normal to the surface at point pi. The
average error in degrees is then 180
πn
Pn
j=1 ei. Furthermore,
30 trials were averaged to obtain the ﬁnal error.
In the experiments, Gaussian noise with σ up to 1.0
meter was added to the SRIs. The magnitude of the noise
might appear large for standard LIDAR sensors, but it is the
typical level for distant points measured with stereo systems.
Choosing a wide noise interval allows us to analyze the
accuracy of the estimation at all levels of noise.
Figures 4a to 4d show the results when window sizes
are varied from 3 × 3 to 9 × 9. The obtained results were
consistent between all data sets and window sizes. Because
of space limitations, only the most representative plot for
each data set and window size is shown. The results for
the normalized LS and the unconstrained LS were exactly
equivalent. For readability, only one curve is shown for both
approaches.
The most remarkable result obtained from the plots of
Figure 4 is that the traditional least squares performs badly
for all objects and window sizes. The normalized least
squares provides better results by just normalizing the data
matrix before the eigen-analysis, as addressed in Section IV-
B. The fast least squares shows a very similar estimation
error to the unconstrained LS, conﬁrming that the elimination
of the ranges from Equation 8 is reasonable. Only a very
small difference can be observed between those two curves
at very large noise levels. As expected, the difference is
less marked on the Sphere data set, where the assumption
of constant ranges within the mask does actually hold.
The SRI derivative approach performs better than all LS
approaches for small window sizes. For large window sizes,


--- Page 6 ---

0
10
20
30
40
50
60
70
80
90
0.00
0.20
0.40
0.60
0.80
1.00
Angular Error [deg]
Noise Sandard Deviation [m]
SRI
Norm. and Unc. LS
Fast LS
Trad. LS
(a) Sphere 3 × 3
0
10
20
30
40
50
60
70
80
90
0.00
0.20
0.40
0.60
0.80
1.00
Angular Error [deg]
Noise Sandard Deviation [m]
SRI
Norm. and Unc. LS
Fast LS
Trad. LS
(b) Cylinder 5 × 5
0
10
20
30
40
50
60
70
80
90
0.00
0.20
0.40
0.60
0.80
1.00
Angular Error [deg]
Noise Sandard Deviation [m]
SRI
Norm. and Unc. LS
Fast LS
Trad. LS
(c) Prism 7 × 7
0
10
20
30
40
50
60
70
80
90
0.00
0.20
0.40
0.60
0.80
1.00
Angular Error [deg]
Noise Sandard Deviation [m]
SRI
Norm. and Unc. LS
Fast LS
Trad. LS
(d) Floor and ceiling 9 × 9
Fig. 4.
Accuracy results with synthetic data for different window sizes.
the new proposed LS formulations perform better. The reason
for this is that the derivative is a local property of the surface
and will not be estimated with much greater accuracy as the
window size increases. This leads to a limited improvement
of the normal estimate when using larger window sizes.
Least squares, on the other hand, fully beneﬁts from more
measurements, increasing the estimation accuracy.
Figure 5 shows the results for all methods, data sets and
window sizes for the noise level σ = 0.2 m. The four charts
show that the results for all ﬁve methods are consistent for
all objects and window sizes.
As a general guideline, we can conclude that SRI deriva-
tive method is the best for small window sizes. For large
window sizes the least squares methods produce better results
at the expense of a higher computation time.
B. Computation Times
Table II shows the obtained computation times for each
method. For the test, each algorithm was implemented in
C++ using OpenMP and executed 50 times with the Cylinder
data set on an Intel Core 2 Duo 2.66GHz CPU. The times
are shown in milliseconds and they correspond to the average
time for the whole range image. As predicted in Table I, the
M. Size
Methods
Time (ms) ± σ
SUF
3 × 3
Trad. LS
192.25 ± 5.92
1
Norm. LS
227.52 ± 4.04
0.85
Unc. LS
18.87 ± 0.18
10.19
Fast LS
7.02 ± 0.05
27.39
SRI
3.66 ± 0.01
52.53
5 × 5
Trad. LS
195.00 ± 3.04
1
Norm. LS
237.2 ± 4.04
0.82
Unc. LS
19.14 ± 0.16
10.18
Fast LS
7.04 ± 0.03
27.70
SRI
3.71± < 0.01
52.56
7 × 7
Trad. LS
436.12 ± 7.2
1
Norm. LS
483.5 ± 3.1
0.9
Unc. LS
19.14 ± 0.14
15.57
Fast LS
7.07 ± 0.03
42.14
SRI
4.33 ± 0.01
68.81
9 × 9
Trad. LS
436.12 ± 2.58
1
Norm. LS
489.96 ± 4.8
0.89
Unc. LS
19.35 ± 0.16
22.54
Fast LS
7.18 ± 0.12
60.74
SRI
4.40 ± 0.01
99.12
TABLE II
ACTUAL COMPUTATION TIMES WITH SPEED UP FACTORS (SUF) OVER THE
TRADITIONAL LS.


--- Page 7 ---

0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
Sphere
Cylinder
Prism
F. and C.
Angular Error [deg]
SRI
Norm. And Unc. LS
Fast LS
Trad. LS
(a) 3 × 3
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
Sphere
Cylinder
Prism
F. and C.
Angular Error [deg]
SRI
Norm. And Unc. LS
Fast LS
Trad. LS
(b) 5 × 5
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
Sphere
Cylinder
Prism
F. and C.
Angular Error [deg]
SRI
Norm. And Unc. LS
Fast LS
Trad. LS
(c) 7 × 7
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
Sphere
Cylinder
Prism
F. and C.
Angular Error [deg]
SRI
Norm. And Unc. LS
Fast LS
Trad. LS
(d) 9 × 9
Fig. 5.
Estimation errors with the synthetic data sets for σ = 0.2m.
Method
Angular Difference
SRI & Unc. LS
3.71◦± 1.51◦
SRI & Fast LS
3.75◦± 1.51◦
Unc. & Fast LS
0.39◦± 0.11◦
TABLE III
AVERAGE ANGULAR DIFFERENCES FOR THE REAL DATA SEQUENCE.
computation time for the fast and unconstrained LS does
not increase with the window size. Observe that according
to Table I, the fast LS method should be faster than the
SRI derivative method. However, Table I shows theoretical
values of an optimal implementation and does not count for
overhead in the form of indexing operations and memory
accesses. The SRI derivative method is the fastest to compute
and provides a speed up factor of up to two orders of
magnitude with respect to the traditional least squares.
C. Results with Real Data
We have also conducted experiments with real data ob-
tained from a Velodyne HDL-64E High Deﬁnition LIDAR
scanner [15] mounted on the top of a vehicle. An SRI
is deﬁned with the sensor speciﬁcations, and the sensor
measurements are registered in the range image as they are
acquired. The registration of the sensor measurements in the
SRI leads to a sparse image, since not every image position
is occupied with a measurement. A linear interpolation step
is used to ﬁll those holes. To avoid interpolating image
locations at depth discontinuities, an empty image position
is only ﬁlled if the ranges to be interpolated are proportional
up to a scale. An example of the resulting SRI for a real
scenario is shown in Figure 6c.
All methods were tested with 6 sequences of data acquired
on different scenarios. Figure 1 shows a snapshot of the nor-
mals obtained with the SRI derivative method on one of the
sequences containing more than 23 million measurements.
The sequence was acquired as the vehicle was traveling
in rough terrain containing some human-made structures.
Table III shows the average angular differences between the
proposed methods for the whole sequence, from where it
can be seen that all three methods provide consistent normal
estimates.
VII. CONCLUSIONS
Based on the experiments performed in this paper we
can draw the following conclusions when computing sur-
face normals with range images. First, the traditional least


--- Page 8 ---

(a) Visual snapshot
(b) Bird’s eye view. red: vertical, green: horizontal.
(c) Spherical range image: the color encodes the range. The size of the image is 3300 × 100 covering a ﬁeld of view of 360◦× 30◦with angular
resolution of 0.11 × 0.2 deg/px. The rectangle corresponds to the snapshot of Figure (a) and the normal estimates of Figure 1.
Fig. 6.
Experiment results with real data
squares is very sensitive to noise and does not perform
accurately without a proper normalization. This problem is
easily solved by an appropriate scaling of the data matrix.
Most remarkably, the literature on normal estimation usually
does not specify if such a normalization step is performed,
even though this is a critical step for the accurate estimation
of surface normals. Second, the proposed unconstrained least
squares and the fast approximate least squares show the same
accuracy level as the normalized version, while being up
to 17 times faster. Third and last, the new proposed SRI
derivative approach is the fastest and more accurate method
for small window sizes. It is also clearly the best option
for large window sizes when time constraints prevail over
accuracy requirements.
REFERENCES
[1] N. Amenta and M. Bern. Surface reconstruction by Voronoi ﬁltering.
Discrete and Computational Geometry, 22:481–504, 1999.
[2] M. Bosse and R. Zlot. Map matching and data association for large-
scale two-dimensional laser scan-based SLAM. International Journal
of Robotics Research, 27(6):667–691, 2008.
[3] J. Canny. A computational approach to edge detection. Transactions
on Pattern Analysis and Machine Intelligence, 8(6):679–698, 1986.
[4] C. Castej´on, B. L. Boada, D. Blanco, and L. Moreno. Traversable
region modeling for outdoor navigation. Journal of Intelligent and
Robotic Systems, 43(2-4):175–216, 2005.
[5] T. K. Dey and S. Goswami.
Provable surface reconstruction from
noisy samples. Computational Geometry: Theory and Applications,
35(1):124–141, 2006.
[6] T. K. Dey, G. Li, and J. Sun. Normal estimation for point clouds:
a comparison study for a Voronoi based method.
In Point-Based
Graphics, Eurographics/IEEE VGTC Symposium, pages 39–46, 2005.
[7] T. K. Dey and J. Sun. Normal and feature approximations from noisy
point clouds. In Foundations of Software Technology and Theoretical
Computer Science, pages 21–32, 2006.
[8] R. I. Hartley. In defense of the 8-point algorithm. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 19:580–593, 1997.
[9] M. Hebert and T. Kanade. Outdoor scene analysis using range data. In
International Conference on Robotics and Automation, pages 1426–
1432, 1986.
[10] R. Hoffman and A. K. Jain.
Segmentation and classiﬁcation of
range images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 9:608–620, September 1987.
[11] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle.
Surface reconstruction from unorganized points. SIGGRAPH Comput.
Graph., 26:71–78, July 1992.
[12] http://www.cs.cmu.edu/vmr/datasets/normals, 2011.
[13] K. Klasing, D. Althoff, D. Wollherr, and M. Buss. Comparison of
surface normal estimation methods for range sensing applications. In
Conference on Robotics and Automation, pages 1977–1982, 2009.
[14] L. Kobbelt and M. Botsch. A survey of point-based techniques in
computer graphics. Journal of Computer and Graphics, 28(6):801–
814, 2004.
[15] http://www.velodyne.com/lidar/products/manual/hdl-64e, 2011.
[16] Z. C. Marton, . B. Rusu, and M. Beetz. On fast surface reconstruction
methods for large and noisy point clouds. In International Conference
on Robotics and Automation, pages 2829–2834, 2009.
[17] M. J. McDonnell. Box-ﬁltering techniques. Computer Graphics and
Image Processing, 17(1):65–70, September 1981.
[18] N. J. Mitra, A. Nguyen, and L. Guibas. Estimating surface normals
in noisy point cloud data.
International Journal of Computational
Geometry & Applications, 14(4 & 5):261–276, 2004.
[19] F. Moosmann, O. Pink, and C. Stiller. Segmentation of 3D lidar data
in non-ﬂat urban environments using a local convexity criterion. In
Intelligent Vehicles Symposium, June 2009.
[20] D. OuYang and H. Y. Feng. On the normal vector estimation for point
cloud data from smooth surfaces.
Computed Aided Design, pages
1071–1079, 2005.
[21] K. Pathak, N. Vaskevicius, and A. Birk.
Uncertainty analysis for
optimum plane extraction from noisy 3D range-sensor point-clouds.
Intelligent Service Robotics, 3(1):37–48, 2009.
[22] R. B. Rusu, N. Blodow, Z. C. M., and M. Beetz. Aligning point cloud
views using persistent feature histograms. In International Conference
on Intelligent Robots and Systems, 2008.
[23] O. Schall, A. Belyaev, and H.-P. Seidel.
Robust ﬁltering of
noisy scattered point data.
In M. Pauly and M. Zwicker, editors,
IEEE/Eurographics Symposium on Point-Based Graphics, pages 71–
77, Stony Brook, New York, USA, 2005.
[24] A. Segal, D. Haehnel, and S. Thrun. Generalized ICP. In Robotics:
Science and Systems, Seattle, USA, June 2009.
[25] H. Sekkati and S. Negahdaripour. 3-D motion estimation for position-
ing from 2-d acoustic video imagery. In Iberian conference on Pattern
Recognition and Image Analysis, pages 80–88, 2007.
[26] L. Spinello, R. Triebel, and R. Siegwart. Multimodal detection and
tracking of pedestrians in urban environments with explicit ground
plane extraction. In International Conference on Intelligent Robots
and Systems, pages 1823–1829, 2008.
[27] C. Wang, H. Tanahashi, H. Hirayu, Y. Niwa, and K. Yamamoto.
Comparison of local plane ﬁtting methods for range data. Conference
on Computer Vision and Pattern Recognition, 1:663–669, 2001.
[28] J. W. Weingarten, G. Gruener, and A. Dorf.
Probabilistic plane
ﬁtting in 3D and an application to robotic mapping. In International
Conference on Robotics and Automation, pages 927–932, 2004.
```

---

## Section 1.17: Fast and Robust Edge Extraction in Unorganized Point Clouds

Source File: Fast and Robust Edge Extraction in Unorganized Point Clouds.txt

### Paper Content:

```
# Fast and Robust Edge Extraction in Unorganized Point Clouds.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Fast and Robust Edge Extraction in Unorganized Point Clouds.pdf
# Output: ../layer2_completion/txt/Fast and Robust Edge Extraction in Unorganized Point Clouds.txt


--- Page 1 ---

Fast and Robust Edge Extraction in Unorganized
Point Clouds
Dena Bazazian∗, Josep R. Casas†, Javier Ruiz-Hidalgo‡
Signal Theory and Communications Department, Universitat Politecnica de Catalunya
Barcelona, Spain
Email: ∗dena.bazazian@tsc.upc.edu,
†josep.ramon.casas, ‡j.ruiz

@upc.edu
Abstract—Edges provide important visual information in
scene surfaces. The need for fast and robust feature extraction
from 3D data is nowadays fostered by the widespread availability
of cheap commercial depth sensors and multi-camera setups.
This article investigates the challenge of detecting edges in
surfaces represented by unorganized point clouds. Generally,
edge recognition requires the extraction of geometric features
such as normal vectors and curvatures. Since the normals alone
do not provide enough information about the geometry of the
cloud, further analysis of extracted normals is needed for edge
extraction, such as a clustering method. Edge extraction through
these techniques consists of several steps with parameters which
depend on the density and the scale of the point cloud. In this
paper we propose a fast and precise method to detect sharp edge
features by analysing the eigenvalues of the covariance matrix
that are deﬁned by each point’s k-nearest neighbors. Moreover,
we evaluate quantitatively, and qualitatively the proposed meth-
ods for sharp edge extraction using several dihedral angles and
well known examples of unorganized point clouds. Furthermore,
we demonstrate the robustness of our approach in the noisier
real-world datasets.
I.
INTRODUCTION
Edge extraction has attracted a lot of attention in com-
puter vision. Many applications are built around this con-
cept. Examples include object recognition, similarity, regis-
tration, matching, down sampling, and visualization. In non-
photorealistic rendering, sharp edges are used to enhance the
visual perception. Additionally, in the case of segmentation,
knowledge about the position of features can be of great help.
The computer vision community has drawn their attention to
3D scene analysis in recent years using stereo and multi-
camera systems, and specially after the success of commercial
depth sensors, such as MS kinect or Asus Xtion.
Most of the existing research on extracting edges in point
clouds are considering either statistical and geometrical meth-
ods or estimating the normals on sharp edges.
The challenge for estimating normals on the edge feature
points is related to the neighbourhood employed for the normal
estimation. The neighborhood may enclose points belonging to
different surface patches across the edge feature.
In [1]–[3], Weber et al. estimate normals by triangulation.
This technique is quite sensitive, particularly for the relevant
points located around edge. Furthermore, triangulation for
normal estimation is computationally expensive and difﬁcult
to implement in real time for large scale point clouds.
In this paper, we ﬁrst explore the challenges of sharp edge
extraction in small dihedral angles of a 3D surface. In general
most techniques to extract sharp edges are prone to error
when the dihedral angle is small, due to the local analysis
neighborhood taking points on both sides of the surface edge.
The main contribution of this paper is to evaluate techniques
of sharp edge extraction quantitatively for toy examples and
qualitatively for some 3D shapes. Our intention is to extract
sharp edges through different techniques and then quantita-
tively compare the accuracy of results and their computational
load.
The remainder of the article is organized as follows. Section
II presents related work, followed by a description of our
approach and architecture in Section III. Section IV reports
the experimental results of our approach, and conclusions are
drawn in section V.
II.
RELATED WORK
Sharp feature extraction is a key issue in many scientiﬁc
ﬁelds, such as computer graphics, medical imaging, computer
vision and computational ﬂuid dynamics. Some research
efforts focus on extracting sharp features on point clouds (3D
data).
A. Edge extraction
There are multiple techniques for the edge and sharp
feature extraction in point clouds, which can be categorized
into the classes hereafter: In [6]–[8] the authors have employed
robust statistics to extract sharp features. In [9], [10] surface
segmentation and in [13] line segmentation has been explored
to extract sharp features. Alternately, [1], [2], [11], [12] pro-
pose a region growing method that segments the point cloud
into clusters and identify the regions with sharp features based
on the analysis of the normals of the points.
Fleischman et al. [6] use statistical techniques in order to
identify sharp features. Neighborhoods of points are segmented
into regions corresponding to the same surface part, and the
creation of neighborhoods is guided by the moving least
squares (MLS) computation. A development of this work by
Daniels et al. [7] extracts feature curves on the reconstructed
MLS surface. The beneﬁt is that points on the sharp feature
can be identiﬁed in the case of noisy and rough input data.
Following this idea, Oztireli et al. [8] adopted a robust implicit
moving least-squares (RIMLS) method to locally approximate
the scanned surface and to preserve sharp features. They
have employed kernel regression to extend the moving least
squares (MLS) surface reconstruction with sharp features.
Their method increases the presentation of sharp features by
combining the MLS and local kernel regression.
Demarsin et al. [9] also searched for sharp features in point


--- Page 2 ---

cloud data; they are interested in closed sharp features. They
use segmentation to identify the regions of sharp features.
The output is a set of points with many points representing
the feature line. Therefore, they use a graph approach with a
minimum spanning tree for closed feature lines.
Xu et al. [10] proposed a method to segment surface and
extract edge feature lines of irregular fractured fragments,
an accurate surface segmentation is implemented by merging
faces based on face normal vector and roughness, and edge
feature lines are extracted based on the surface segmentation.
Gumhold et al. [11] present a method that uses the Rieman-
nian tree to build the connectivity information in the point
cloud. Then, they analyze the neighborhood of each point via
principal component analysis (PCA). The Eigen values of the
correlation matrix are used to determine a probability of a point
belonging to a feature, and the kind of feature. This method
can differentiate between line-type features, border and corner
points. The result is a quite dense set of points covering the
feature, independently of whether the feature is sharp or not,
since points with high curvature values are detected.
Weber et al. [1], [2] present a method for detecting sharp
features on an unstructured point cloud; this method computes
a Gauss map clustering on local neighborhoods in order to
discard all points that are unlikely to belong to a sharp feature.
Feng et al. [12] have presented an algorithm for reliably
detecting multiple planes in real time from point clouds. They
have constructed a graph whose node and edge represent
a group of points and their neighborhood respectively. An
agglomerative hierarchical clustering is performed on that
graph to systematically merge nodes belonging to the same
plane.
Noise reduction algorithms such as jump edge ﬁltering may be
suitable, especially for ﬁnding better boundaries [1] for each
region.
Lin et al. [13] proposed a method that is capable of accurately
extracting plane intersection line segments from large-scale
raw scan points. The 3D line-support region, namely, a point
set near a straight linear structure, is extracted simultaneously.
The 3D line-support region is ﬁtted by a Line-Segment-
Half-Planes (LSHP) structure, which provides a geometric
constraint for a line segment, making the line segment more
reliable and accurate.
In addition Wang et al. [24] employ the majority voting
scheme, in order to detect distinct geometric features such as
sharp edges and outliers in a scanned point cloud.
To obtain sharp edge features precisely it is essential to
estimate normals accurately using a convenient neighbourhood
points. In the following section we will summarize the normal
estimation techniques on the sharp edge features.
B. Normal estimation
Reliable estimation of normal vectors at each point in a
scanned point cloud has become a fundamental step in point
cloud data processing. Extracting sharp edge features from a
3D point cloud requires accurate normals as input in order to
generate high quality surfaces.
The performance of common point based rendering techniques
is much dependent on the accuracy of the input normals. In
this section, we review some research on the computation of
normals; speciﬁcally, some efforts on normal estimation to
extract sharp features from point clouds.
Park et al. [14] proposed EGG (Elliptic Gabriel Graph) which
is an intuitive extension of the Gabriel graph (GG), using an
elliptic inﬂuence region. EGG provides balanced neighbors
by considering both distance and directional spread and can
be used for normal vector estimation.
Holzer et al. [15] have presented two methods for fast
estimation of surface normals from organized point cloud data
using integral images. The use of integral images makes it
possible to adapt the considered neighborhood size according
to depth and object borders without any additional cost in
terms of processing speed.
Demarsin et al. [9] extract closed sharp feature lines to create
a closed curve network. They used a ﬁrst order segmentation
to extract candidate feature points and process them as a
graph to recover the sharp feature lines. They considered the
Delaunay triangulation to estimate normal and the normal
of each sample point is estimated as the normal of the least
squares plane through the neighbors.
Grim et al. [16] have concentrated on non-uniformly sampled
and noisy point data. The output of the algorithm they
propose is a surface normal for each data point, a local
surface approximation in the form of a one-ring, the local
shape (ﬂat, ridge, bowl, saddle, sharp edge, corner, boundary),
the feature size, and a conﬁdence value that can be used to
determine areas where the sampling is poor or not surface-like.
Li et al. [17] estimate normals on unorganized point clouds
by employing statistics methods to detect the local tangent
plane for each point. Their proposed algorithm is capable
of dealing with points located in high curvature regions or
near/on complex sharp features.
Zhang et al. [18] use neighbor points normals as prior
knowledge to carry out neighborhood clustering. Afterwards
they design an unsupervised learning process to represent
the prior knowledge as a guiding matrix. Thereupon by
low-rank subspace clustering with the guiding matrix, they
segment the anisotropic neighborhood into several isotropic
neighborhoods. Hence, the normal of the points near sharp
features is estimated as the normal of a plane ﬁtting the
consistent sub-neighborhood. Their method is capable of
estimating normals in noisy and anisotropic samplings, while
preserving sharp features within the original point data.
Wang et al. [19] have developed a normal estimation method
in order to establish effectively a proper neighborhood for
each point in the scanned point cloud. In particular, for a point
near sharp features, an anisotropic neighborhood is formed to
only enclose neighboring points located on the same surface
patch as the point. Neighboring points on the other surface
patches are discarded.
The challenge for estimating normals on the sharp feature
points is that the neighborhood employed for the normal
estimation would enclose points belonging to different surface
patches across the sharp feature. In particular, for a point near
sharp features, an anisotropic neighborhood is formed to only
enclose neighboring points located on the same surface patch
as the point. Neighboring points on the other surface patches
are discarded.
III.
PROPOSED APPROACH
Our idea is partly motivated by the sharp feature detection
method proposed by Weber et al. [1]. They propose a normal


--- Page 3 ---

clustering approach based on Gauss map clustering to detect
sharp feature points. This algorithm aims to classify points
into sharp feature and non-sharp feature points by clustering
the normals of potential triangles in the neighborhood.
In Weber’s method, each point p has k nearest neighbors.
Hence, there are k(k −1) possible triangles 1 built with p
and two neighborhood points as vertices. Hereon there are
k(k −1) normal vectors of these triangles. Feature detection
is performed by analysing the clustering behaviour of all these
normals in a given neighborhood of point p.
In this technique, the calculated normals are quite sensitive
to measurement noise. Moreover, if a sample point and two
neighbour points are aligned in a row, then these three points
will not form a triangle. The normal vector does not exist
in this case, as illustrated in Fig. 1. Furthermore, any neigh-
borhood formed by points in the two sides of a sharp edge
may provide normals which are neither reliable nor accurate.
Moreover, for large scale point clouds, this method of normal
estimation tends to present a high computational load in order
to process k(k−1) normals in the neighborhood of each point.
In order to extract edges, Weber proposes to map the set
of normals computed in the neighborhood of each point
to the Gaussian sphere (Gauss map). Then these normals
are clustered with a hierarchical agglomerative (bottom-up)
clustering method [21]. Initially, each point of the Gauss
map is considered as a separate cluster. Afterwards, clusters
are merged step by step into larger clusters. Weber deﬁnes
the criterion for the merging process as a distance measure
computed from the angle between normals in the Gauss map,
as deﬁned in (1).
Dc(S1, S2) =
1
|S1||S2|

x∈S1

y∈S2
d(x, y),
(1)
where S1, S2 are two clusters to be compared, |S| is the
number of elements in a cluster and d is the distance mea-
sure on the Gauss map. Each merging increases the distance
between the clusters. The clustering algorithm can stop when
the distance between the existing clusters exceeds a certain
threshold according to the angle between the normals in the
Gauss map, as it is presented in Algorithm 1. When the
agglomeration process results in a single cluster, the point is
considered to lay in a ﬂat plane. In the case of two to four
clusters, Weber proposes to classify the point as a feature point.
We propose a method in III-A to estimate normals using
the PCA (Principal Component Analysis) and then clustering
normals according to Weber’s method [1] to extract sharp edge
features.
However, these techniques of extracting sharp features by
classifying normals are prone to error when the dihedral angle
of the sharp edge is small. Hereupon, we propose a faster
method in III-B to extract sharp features merely by analysing
eigenvalues of covariance matrix that are deﬁned by each
point’s k-nearest neighbors.
A. PCA and agglomerative clustering
We propose to replace Weber’s normal estimation method
(triangulation) with PCA. For each point of the cloud, a least
1Note that Weber computes normals for the discrete Gauss map from the
triangle formed by the sample point p and two neighbors pi, pj as nij =
ppi × ppj, considering the two possible results nij = −nij per triangle.
squares local plane is ﬁtted to its k nearest neighbors. The
normal of each point is the eigenvector corresponding to the
smallest eigenvalue of the covariance matrix. After estimating
the normal of each point, then we consider k nearest neighbors
for the sample point and afterwards cluster the normals of
those k nearest neighbors with the agglomerative technique.
We have estimated normals, and implemented it as proposed
in [20]. An open source implementation of normal estimation
is made available in the Point Cloud Library (PCL) [23] 2.
Algorithm 1 Agglomerative Clustering algorithm
1: procedure AGGLOMERATIVE–CLUSTERING
2:
Consider each element as an individual Cluster at the
ﬁrst step
3:
for all the Clusters do
4:
Find the angle between each Clusters
5:
end for
6:
Find the pair of clusters with minimum angle
7:
Merge two clusters with the minimum angle as in (1)
8: While MinAngle < Threshold
9:
Compute number of clusters
10: end procedure
B. Eigenvalue analysis
To avoid the complexity of the process of edge extraction,
due to the two step of estimating normals and clustering, in this
section we propose a method to extract sharp edges without
clustering. Since estimating normals by PCA is based on the
eigenvalues of the covariance matrix, we extract edge features
merely with the variation of the eigenvalues for each point.
This statistical approach eliminates the sensitivity of the nor-
mal estimation of sharp edges, furthermore, it removes the
clustering step simplifying the edge estimation process.
Covariance is a measure of how much each of the dimensions
varies from the mean with respect to each other. For a 3-
dimensional data set (X, Y, Z), the 3×3 Covariance matrix C
for a sample point p(x, y, z) is given by:
C =
Cov(x, x)
Cov(x, y)
Cov(x, z)
Cov(y, x)
Cov(y, y)
Cov(y, z)
Cov(z, x)
Cov(z, y)
Cov(z, z)

(2)
where, for instance Cov(x, y) is the Covariance of x, y
computed as:
Cov(x, y) =
k
i=1(xi −¯x)(yi −¯y)
n −1
(3)
Afterwards we explore the Eigenvalues of C: λ0 ≤λ1 ≤
λ2.
In [4], [5] Pauly et. al. introduce the following concept of
surface variation σk(p):
σk(p) =
λ0
λ0 + λ1 + λ2
(4)
The surface variation, σk(p), for each sample point with k
neighbors allows us to distinguish whether the point belongs
2Available as a feature in the PCL trunk: http://www.pointclouds.org.


--- Page 4 ---

Fig. 1: Illustration of a case with a sample point
(in brown) and 8 neighbors (in red) used for
triangulation in normal estimation
Fig. 2: Sharp edge features for different dihedral
angles
to a ﬂat plane or to a salient point (edge) in the point cloud.
Since the smallest eigenvalue of covariance matrix for the ﬂat
surfaces is zero then the value of the surface variation for the
ﬂat surfaces would be zero.
IV.
EXPERIMENTAL RESULTS
In this section we explain the experimental data set used in
our research and the experimental results of edge extraction.
A. Experimental data
We propose to ﬁrst study the behavior of edge detectors
with simple synthetic (artiﬁcial) point clouds, for which we
can easily label the ground truth. The existence of ground truth
also allows to derive quantitative measures for the evaluation
of different edge extraction techniques.
1) Geometric shapes: The artiﬁcial point clouds in this
paper are based on geometric concepts. Since the aim of this
research is to extract sharp edges, we determine edges as the
junction points of two planes. In order to analyse the effect
of edge sharpness on the effectiveness of edge detection, we
sample a surface made of two planar rectangular patches joined
at various dihedral angles. As shown in Fig. 2.
In addition, in order to explore our approach at different scales,
we have designed on a synthetic curved wedge, which has
sharp and curve features. This shape is deﬁned as a para-
metric synthetic shape for objective evaluation of 3D feature
descriptors. The generated shape has the form of a curved
wedge represented by a point cloud, with varying sharpness
and scale at the geometrical edge. We have extended the sharp
edge of the symmetric curved wedge to a cylindrical section,
by considering a scale, which can be deﬁned as the radius of
the cylindrical section. Part of the plane surface is changed into
a cylindrical section at a certain point, keeping the continuity
at the surface transition both in position and in gradient. Fig.
3 shows a synthetic curved wedge for which the radius of the
cylindrical section is 0.2 cm and the angle between the planes
is 90◦.
The point distribution is uniform both for the curved and for
the sharp edge objects.
2) Ground truth estimate: The ground truth of geometric
shapes is deﬁned at the synthesis stage by labelling the points
located at the proximity of the sharp edges. The width of the
edge line for the ground truth, is set the equivalent to the
average distance between the points for each neighborhood
size.
Fig. 3: Synthetic curved wedge with an angle of 90◦and a
radios of the cylindrical section of 0.2cm
B. Experimental results
In order to quantitatively compare the results of the dif-
ferent techniques, we propose to use the F1-Score, as metrics,
deﬁned as:
F1 = 2 × Precision × Recall
Precision + Recall
(5)
where precision is deﬁned as the proportion of points correctly
detected by the edge extraction technique and recall is deﬁned
as the proportion of points labeled as edges in the ground truth.
Precision and recall are deﬁned as:
Precision =
TP
TP + FP
Recall =
TP
TP + FN
(6)
where TP stands for True Positives representing the number
of correctly detected points, FP stands for False Positives
representing the number of wrongly detected points, FN
stands for False Negatives, representing the number of false
rejections, i.e. points that belong to the ground truth but are
not detected by the edge extraction technique.
1) Edge extraction for a small dihedral angle: Our ﬁrst
variation with respect to Weber’s strategy is to implement
PCA to estimate normals instead of triangulation, according to
the explanation in III-A. A second variation implementation is
the proposed strategy for 3D edge detection from the analysis
of the eigenvalues according to the explanation in III-B. We
compare the detection of edge points on several dihedral angles
with the reference method [1] and the two proposed variations.
The values of precision, recall and F1-score for this experiment
are given in Table I.
When comparing triangulation and PCA methods in the two
ﬁrst column groups of Table I, F1-Scores are almost the same
for the largest angles (90◦and 67.5◦) whereas triangulation
is slighly more precise for smaller angles (45◦and 22.5◦).
The computational load of the triangulation method makes
it 60 times slower than PCA, as will be discussed below in


--- Page 5 ---

Triangulation Method and Clustering
PCA and Clustering
Eigenvalues Analysis
Dihedral Angle
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
90◦
0.886
0.956
0.919
0.933
0.896
0.914
0.902
0.909
0.906
67.5◦
0.890
0.935
0.912
0.925
0.912
0.918
0.914
0.888
0.901
45◦
0.755
0.943
0.839
0.671
0.745
0.706
0.907
0.873
0.890
22.5◦
0.589
0.936
0.723
0.602
0.795
0.685
0.795
0.962
0.870
TABLE I: Quantitative evaluation of methods for edge extraction for several dihedral angles
Fig. 4: Analysis of the eigenvalues of the covariance matrix. Behaviour of the eigenvalues λ0 ≤λ1 ≤λ2
paragraph IV-B2. The last column group in Table I allows com-
paring these results with the technique of surface variation [4],
[5] for the analysis of eigenvalues according to (4). As shown
in the Table, the results for the 45◦and 22.5◦dihedral angles
are more accurate compared to the two previous techniques.
The surface variation method does not employ normal vectors
explicitly, whereas for the two previous techniques we have
employed agglomerative clustering to group the normal vectors
in each neighborhood. The common challenge of the agglom-
erative clustering method is to ﬁnd a suitable angle threshold
for reliable edge extraction results.
The analysis of the eigenvalues of the covariance matrix in
the surface variation method has a certain advantage over the
two previous methods to face this challenge, both in terms of
precision and simplicity, and also in terms of computational
load, as we will show later in IV-B2. Since the covariance is
a measure of how much each of the dimensions varies from
the mean with respect to each other, the eigenvalues of the
covariance matrix measure the variation of the corresponding
point along the direction of the eigenvectors. Hence the largest
and smallest eigenvalues of the covariance matrix correspond
to the dimensions that have the strongest and smallest cor-
relation [22]. There are three eigenvalues for the covariance
matrix of the three dimensional data sets, and the eigenvector
of smallest eigenvalue is associated to the normal vector. Thus,
for a ﬂat surface, the amount of the smallest eigenvalue will
be zero and if any curvature is present in the surface deﬁned
by the neighborhood of the sample point, then the amount of
the corresponding smallest eigenvalue will be larger. Fig. 4
shows the amount of the eigenvalues of the covariance matrix
in a pseudocolor representation (red is high, yellow is mid-
high and green and blue represent low values).
For qualitative evaluation of the eigenvalue analysis technique,
we provide edge detection results on other 3D shapes, as shown
in Fig. 5. The Bunny and Dragon are from the Stanford 3D
Scanning Repository 3 and the Trim-Star is from the Aim @
SHAPE Shape Repository 4.
2) Fast
edge
extraction:
To
evaluate
the
eigenvalue
analysis technique’s performance in terms of time, the total
computation time for the three techniques is given in Table II.
We implemented, the three techniques in C++. All experiments
were conducted on a system with an Intel Core i5-3470 CPU
of 3.20GHz and 8GB of RAM. No multi-threading or any
other parallelism such as OpenMP or GPU was used in our
implementation. As shown in Table II, normal estimation with
PCA is about 60 times faster than triangulation. Moreover,
the eigenvalue analysis is two times faster than clustering in
the second step of the reference method to extract edges.
3) Robustness to noise: In order to test the robustness of
the eigenvalue analysis method to noise, we show the results
for noisy models perturbed with additive 10% and 20% of
Gaussian noise. Fig. 6 shows the edge extraction of the
surface by eigenvalue analysis in a noisy point cloud. In this
case we have computed the surface variation as in (4). Edges
are detected and even salient points in the corners are clearly
detected in both cases with 10% and 20% added noise. In
addition the F1-Scores for edge detection with Gaussian noise
in this ﬁgure are given in Table III. As shown, the F1-Score
is over 0.5 even for the point cloud with 20% Gaussian noise.
Increasing the percentage of the Gaussian noise, increases
recall and decreases precision, due to the scattered noisy
points around the surface, which are counted as the (false)
edge points.
4) Multi-scale analysis: In order to extend the proposed
eigenvalue analysis method to other scales, we propose a multi-
scale approach. In this last experiment, we work both with
3http://graphics.stanford.edu/data/3Dscanrep/
4http://www.aimatshape.net/


--- Page 6 ---

Fig. 5: Eigenvalue analysis for the Bunny, Dragon and Trim-Star in order to extract sharp features
Total Time
Dihedral Angle
Triangulation Method and Clustering
PCA and Clustering
Eigenvalues Analysis
90◦
2m58.618s
0m2.596s
0m1.260s
67.5◦
2m46.539s
0m2.432s
0m1.540s
45◦
2m19.363s
0m2.328s
0m1.552s
22.5◦
2m20.025s
0m2.480s
0m1.380s
TABLE II: Computation time of different methods of edge extraction
Fig. 6: Edge extraction with the eigenvalues of the covariance matrix with different levels of Gaussian noise in the values of
the coordinates of the points in the cloud
the sharp edge between two planes in a 90◦dihedral angle,
and with the curved wedge presented in IV-A1. We have
considered different number of local neighbors and computed
the surface variation according to (4). As shown in Fig. 7, we
have analysed, for different neighborhood sizes, the evolution
of the surface variation parameter σk(p) at each surface point
according to the distance of the point to the edge.
We consider K neighbors as a discrete scale parameter. As
explained in IV-B1 when the smallest eigenvalue is zero, means
that the sample point lies in a plane. For small neighborhood
sizes (K=10, 22), the smallest eigenvalue of the covariance
matrix is not zero only when the sample point is placed
at a very short distance to the edge, as some points in its
neighborhood may be placed after the edge.
For the case of two planes, there is a sharp change in the value
of the surface variation at a certain distance to the edge. This
turning point in each corresponds to the equivalent radius of
the neighborhood for the K nearest neighbors in a uniformly
sampled surface. Increasing the number of neighbors, the
distance for this turning point increases, which denotes that
more points in the neighborhood are located after the edge.
For the case of the curved wedge, the distance for this turning
point is constant because, according to IV-A1, it depends
on the radius of cylindrical section. In our experiment the
radius is 0.2 cm. As shown in Fig. 7, the turning point is
placed at 0.2 for all the scales. When the number of neighbors


--- Page 7 ---

Without Noise
10% Noise
20% Noise
Precision
0.875
0.531
0.410
Recall
0.881
0.901
0.922
F1-Score
0.878
0.668
0.568
TABLE III: F1-Score for the eigenvalue analysis in the point cloud without and with Gaussian noise
is small the cylindrical section surface can be considered
approximately ﬂat, and when increasing the number of neigh-
bors, the variation of the σk(p) in the cylindrical section
increases. Interestingly enough, a certain spread in the values
of surface variation can be observed, according to the number
of neighbors, for points placed closer than 0.2 cm to the edge.
V.
CONCLUSION
In this paper we have investigated the challenges of edge
extraction techniques in unorganized point clouds. We focused
on the quantitative results for several synthetic dihedral angles
and the total computation time for several edge detection
strategies.
We have proven that normal estimation for edge extraction
shows deﬁnitely smaller computation times when using PCA
than when normal estimation is done by triangulation in the
neighborhood, with similar detection efﬁciency in both cases.
We have also proven that edge extraction by the analysis
of the eigenvalues the covariance matrix (via the surface
variation parameter) is faster and more accurate in small
dihedral angles. In addition, it reduces user dependency by
eliminating any parameter (threshold) in the edge extraction
method.
For this, we have quantitatively compared the accuracy of
results and the total computation time in the analysis of
synthetic objects for which we do have ground truth.
Furthermore, by adding Gaussian noise to the artiﬁcial point
clouds, we have demonstrated that this approach how much
will be robust in the noisier real-world datasets.
The outcomes of our study favors the proposed strategy of
eigenvalues analysis, and can be summarized in three aspects.
First, it works for edges in very small dihedral angles.
Second, it is a fast procedure, and third, it reduces the user
dependency.
We plan to exploit the algorithm proposed in [24] in order to
develop a normal estimation method that rejects neighborhood
outliers. Furthermore, our future work aims at the design
of an adaptive threshold algorithm for multi-scale analysis.
We plan to complete semi-automatic edge extraction for 3D
point-clouds. Our main aim is a new classiﬁcation framework
that allows discrete surface analysis at multiple scales.
Moreover, with the grouping of edge points, we plan to
extract feature lines, for analysis or visualization to enhance
the the rendering of 3D objects.
ACKNOWLEDGMENT
This work has been developed in the framework of the
project TEC2013-43935-R, ﬁnanced by the Spanish Ministerio
de Economia y Competitividad and the European Regional
Development Fund (ERDF).
REFERENCES
[1]
Weber, C., Hahmann, S., Hagen, H. Sharp feature detection in point
clouds. Shape modelling international conference, pp. 175-186, 2010.
[2]
Weber, C., Hahmann, S., Hagen, H. Methods for feature detection in
point clouds. Visualization of Large and Unstructured Data Sets -IRTG
Workshop, pp. 90-99, 2010.
[3]
Weber, C., Hahmann, S., Hagen, H., Bonneau, G. Sharp feature preserv-
ing MLS surface reconstruction based on local feature line approxima-
tions. Graphical Models, 74 (6), pp. 335345, 2012.
[4]
Pauly, M., Gross, M., Kobbelt, L. Efﬁcient Simpliﬁcation of Point-
Sampled Surfaces. Visualization, VIS. IEEE, pp. 163-170, 2002.
[5]
Pauly, M., Keiser, R., Gross, M. Multi-scale feature extraction on point-
sampled surfaces. Computer Graphics Forum, 22(3), pp. 281-289, 2003.
[6]
Fleischman, S., Cohenor, D., Silva, T. Robust moving least-squares ﬁtting
with sharp features. ACM Trans Graph, pp. 37-49, 2005.
[7]
Daniels, J., Ochotta, T., Ha, L. K. Spline-based feature curves from point-
sampled geometry. Vis. Comput, 24(6), pp. 449-462, 2008.
[8]
Oztireli, C., Guennebaud, G., Gross, M. Feature preserving point set sur-
faces based on non-linear kernel regression. Computer Graphics Forum,
28(2), 2009.
[9]
Demarsin, K., Vanderstraeten, D., Volodine, T., Roose, D.
Detection
of closed sharp edges in point clouds using normal estimation and graph
theory. Computer-Aided Design, 39(4), pp. 276-283, 2007.
[10]
Xu, J., Zhou, M., Wu, Z., Shui, W., Ali, S. Robust surface segmentation
and edge feature lines extraction from fractured fragments of relics .
Journal of Computational Design and Engineering, 2(2), pp. 79-87, 2015.
[11]
Gumhold, S., Wang, X., Mcleod, R. Feature extraction from point
clouds. Proceedings of 10th International Meshing Roundtable, 2001.
[12]
Feng, C., Taguchi, Y., Kamat, V. Fast plane extraction in organized point
clouds using agglomerative hierarchical clustering. IEEE International
Conference on Robotics and Automation (ICRA), pp. 6218-6225, 2014.
[13]
Lin, Y., Wang, C., Cheng, J., Chen, B., Jia, C., Chen, Z., Li, J.
Line segment extraction for large scale unorganized point clouds. ISPRS
Journal of Photogrammetry and Remote Sensing, 102, pp. 172-183, 2015.
[14]
Park, J., Shin, H., Choi, B. Elliptic gabriel graph for ﬁnding neighbors
in a point set and its application to normal vector estimation. Computer
Aided Design, 38(6), pp. 619-626, 2006.
[15]
Holzer, S., Rusu, R.B., Dixon, M., Gedikli, S., Navab, N. Adaptive
neighborhood selection for real-time surface normal estimation from
organized point cloud data using integral images. IEEE/RSJ International
Conference on Intelligent Robots and Systems, pp. 2684-2689, 2012.
[16]
Grim, C., Smart, W. Shape classiﬁcation and normal estimation for non-
uniformly sampled, noisy point data. Computers Graphics, 35(4), pp. 904-
915, 2011.
[17]
Li, B., Schnabel, R., Klein, R., Cheng, Z., Dang, G., Jin, S. Robust nor-
mal estimation for point clouds with sharp features. Computers Graphics,
34(2), pp. 94-106, 2010.
[18]
Zhang, J., Cao, J., Liu, X., Wang, J., Liu, J., Shi, X. Point cloud
normal estimation via low-rank subspace clustering. Shape Modeling
International (SMI) Conference, 37(6), pp. 697-706, 2013.
[19]
Wang, Y., Feng, H., Yung, D., Felix, E., Engin, S. An adaptive
normal estimation method for scanned point clouds with sharp features.
Computer-Aided Design, 45 (11), pp. 1333-1348, 2013.
[20]
Rusu, R.B. Semantic 3D Object Maps for Everyday Manipulation in
Human Living Environments, PhD dessertation, Institut fr Informatik der
Technischen Universitt Mnchen, 2009.
[21]
Hastie, T., Tibshirani, R., Friedman. J. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. Springer, Boston, MA,
USA, 2nd edition, 2009.


--- Page 8 ---

Fig. 7: Scatter-plot of the evolution of surface variation parameter for different neighborhood sizes, over the distance of the
each point to the edge for the intersection of two planes (top) and the curved wedge surface (bottom)
[22]
Garland, M. Quadric-Based Polygonal Surface Simpliﬁcation. PhD the-
sis, Carnegie Mellon University, CS Dept., 1999. Tech. Rept. CMU-CS-
99-105.
[23]
Rusu, R.B., Cousins, S. 3D is here: Point Cloud Library (PCL). In IEEE
International Conference on Robotics and Automation (ICRA), pp. 1-4,
2011.
[24]
Wang, Y., Feng, H. Outlier detection for scanned point clouds using
majority voting. Computer-Aided Design, 62, pp. 31-43, 2015.
```

---

## Section 1.18: Feature-Preserving Surface Reconstruction

Source File: Feature-Preserving Surface Reconstruction.txt

### Paper Content:

```
# Feature-Preserving Surface Reconstruction.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Feature-Preserving Surface Reconstruction.pdf
# Output: ../layer2_completion/txt/Feature-Preserving Surface Reconstruction.txt


--- Page 1 ---

Noname manuscript No.
(will be inserted by the editor)
Feature-Preserving Surface Reconstruction
and Simpliﬁcation from Defect-Laden Point Sets
Julie Digne · David Cohen-Steiner · Pierre Alliez ·
Fernando de Goes · Mathieu Desbrun
the date of receipt and acceptance should be inserted later
Abstract We introduce a robust and feature-capturing
surface reconstruction and simpliﬁcation method that
turns an input point set into a low triangle-count sim-
plicial complex. Our approach starts with a (possibly
non-manifold) simplicial complex ﬁltered from a 3D
Delaunay triangulation of the input points. This ini-
tial approximation is iteratively simpliﬁed based on an
error metric that measures, through optimal transport,
the distance between the input points and the current
simplicial complex—both seen as mass distributions.
Our approach is shown to exhibit both robustness to
noise and outliers, as well as preservation of sharp fea-
tures and boundaries. Our new feature-sensitive metric
between point sets and triangle meshes can also be used
as a post-processing tool that, from the smooth output
of a reconstruction method, recovers sharp features and
boundaries present in the initial point set.
Keywords Optimal transportation · Wasserstein dis-
tance · Linear programming · Surface reconstruction ·
Shape simpliﬁcation · Feature recovery.
Mathematics Subject Classiﬁcation (2000) 65D17 ·
65D18
1 Introduction
Surface reconstruction is a multi-faceted challenge which
precise problem statement depends on the nature and
defects of the input data, the properties of the inferred
surface (smooth vs piecewise smooth, with or without
boundaries), and the desired level of detail one wishes to
J. Digne · D. Cohen-Steiner · P. Alliez
Inria Sophia Antipolis - M´editerran´ee
F. de Goes · M. Desbrun
California Institute of Technology
capture. Despite a number of major contributions over
the past decade [5,24], achieving both feature preser-
vation and robustness to measurement noise and out-
liers remains a scientiﬁc challenge—and a pressing re-
quirement for many reverse engineering and geometric
modeling applications. Furthermore, low polygon-count
reconstructions has only received limited attention de-
spite the increase of point density in 3D scanning tech-
nology and the need for eﬃcient subsequent geometry
processing.
In this paper we contribute a reconstruction method
that simpliﬁes an initial (possibly non-manifold) tri-
angulation of the input point set, based on an error
metric that quantiﬁes through optimal mass transport
the distance between the current simplicial complex and
the input points. Our reconstruction approach inherits
the qualities of our optimal transport based metric: it
is resilient to noise and outliers, can handle uneven
sampling, yet it ﬁnely captures boundaries and sharp
features. We demonstrate these distinguishing proper-
ties on a series of examples. Applied to the output of
a feature-lossy reconstruction method, our new metric
can also be used in order to recover sharp features and
boundaries through a vertex relocation process.
2 Previous Work
We ﬁrst discuss existing surface reconstruction meth-
ods, restricting our review to approaches that are ro-
bust to noise and outliers as well as feature preserving.
We then point out recent, relevant work on geometry
processing based on optimal transport.


--- Page 2 ---

2
Julie Digne et al.
2.1 Surface Reconstruction
A common approach to robust surface reconstruction
from defect-laden point sets involves denoising and ﬁl-
tering of outliers, and often requires an interactive ad-
justment of parameters. Automatic methods such as
spectral methods [25,47,4] and graph cut approaches [20,
26] have been proven extremely robust but are better
suited to the reconstruction of smooth, closed surfaces.
More recently, Cohen-Or and co-authors have proposed
a series of contributions based on robust norms and
sparse recovery [29,21,7]. An interpolating, yet noise
robust approach was alternatively proposed by Digne
et al. [13] through the construction of a scale space.
Feature preserving methods are typically based on
an implicit representation that approximates or inter-
polates the input points. In [14], for instance, sharp fea-
tures are captured through locally adapted anisotropic
basis functions. Adamson and Alexa [1] proposed an
anisotropic moving least squares (MLS) method instead,
using ellipsoidal mapping functions based on princi-
pal curvatures. More recently, Oztireli et al. [35] ex-
tended the MLS surface reconstruction through kernel
regression to allow for much sharper features. How-
ever, none of these techniques returns truly sharp fea-
tures: reconstructions are always semi-sharp, that is,
still rounded with various degrees of roundness depend-
ing on the approach and the sampling density. More-
over, the presence of sharpness in the geometry of a
point set is detected only locally, which often leads
to fragmented creases; the reconstruction quality thus
degrades quickly if defects and outliers are present.
Another way to detect local sharpness within a point
set consists in performing a local clustering of estimated
normals [34]: if this process reveals more than one clus-
ter of normals, then the algorithm ﬁts as many quadrics
as the number of clusters. Improved robustness was
achieved in [16] by segmenting neighborhoods through
region growing. Lipman et al. [28], instead, proposed
a systematic enrichment of the MLS projection frame-
work with sharp edges driven by the local error of the
MLS approximation. Again, the locality of the feature
detection can generate fragmented sharp edges, much
like general feature detection approaches (e.g., [19,36]).
To reduce crease fragmentation, a diﬀerent thread
of work aims at extracting long sharp features. Pauly
et al. [37], for instance, used a multi-scale approach
to detect feature points, and constructed a minimum-
spanning tree to recover the most likely feature graph.
Daniels et al. [12] used a robust projection operator
onto sharp creases, and grew a set of polylines through
projected points. Jenke et al. [23] extracted feature lines
by robustly ﬁtting local surface patches and by com-
puting the intersection of close patches with dissimilar
normals.
Shape simpliﬁcation has also been tackled in [8], but
in a coarse-to-ﬁne manner: a random initial subset of
the input point cloud and a signed distance function
over the set is built. Using this function, points are
added until a signiﬁcant number of points lie within
an error tolerance. The augmented set is triangulated
and a surface mesh is reconstructed. Thus the method
also interleaves reconstruction with simpliﬁcation. In
[2] and [3], a surface is reconstructed through point set
simpliﬁcation and local coarsening or reﬁnement of the
mesh. Salman et al. [45] proposed to detect features
within the point set (as in [33]) and combined Delaunay
reﬁnement over features and Poisson reconstruction on
smooth parts of the inferred surface [24].
Contributions. In this paper, we adopt a very dif-
ferent reconstruction methodology: we reconstruct a sha-
pe through an iterative, feature-preserving simpliﬁca-
tion of a simplicial complex constructed from the input
point set. To achieve noise and outlier robustness, an er-
ror metric driving the simpliﬁcation is derived in terms
of optimal transport between the input point set and
the reconstructed mesh, both seen as mass distributions
(or equivalently, probability measures) in R3.
Next we provide a brief review of optimal transport,
and mention its applications to various problems in
computer graphics and computer vision.
2.2 Optimal Transport
The problem of transporting a measure onto another
one as a way to quantify their similarity has a rich
scientiﬁc history. For two measures µ and ν deﬁned over
R3 and of equal total mass (i.e., their integrals are the
same), the L2 optimal transport from µ to ν consists
in ﬁnding a transport plan π that realizes the following
inﬁmum:
inf
Z
R3 ∥x −y∥2dπ(x, y)
 π ∈Π(µ, ν)

,
where Π(µ, ν) is the set of all possible transport plans
between µ and ν [46]. In a nutshell, a transport plan
π is a displacement that maps every inﬁnitesimal mass
from the input measure µ (here, the set of points) to the
target measure ν (here, the simplicial complex). This
formulation is particularly well suited to comparing 1D
measures such as histograms over the real line or on
the circle [40], and it has been used for transferring
color and contrast between images [43,39]. For applica-
tions in higher dimensions such as 2D or 3D shape re-
trieval [44,41] and segmentation [38], the optimal trans-
port formulation is notoriously less tractable: solving


--- Page 3 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
3
the optimal transport problem requires linear program-
ming (LP). The LP formulation of optimal transport
has been used in applications such as surface compar-
ison [30] and displacement interpolation [9]. Attempts
to design computationally simpler surrogates have also
been made; the sliced Wasserstein approach [42], for
instance, consists in approximating the transport prob-
lem by a series of 1D problems through projection.
Fig. 1 Transport plans. Top: Binary transport plan [18]. We
depict the input point set and simpliﬁed triangulation. Red
and green line segments depict the transport plan between
the input point set and the uniform measure on respectively
the vertices and edges of the triangulation. Each input point
is simply transported either to its closest edge or to the end
vertices of that edge. For each edge the transport plan favors a
transport to its end vertices instead of to the whole edge when
the corresponding transport cost is lower. A closeup reveals
a spurious tangential component of the transport near corner
vertices (as indicated by red segments pointing towards the
corner), artiﬁcially creating a higher total transport cost. For
more complex features such as concave corners on surfaces,
such behavior leads to reconstruction artifacts. Bottom: our
transport plan is, as expected, mostly normal to the edges.
Contributions. Our reconstruction method also re-
lies on a linear programming formulation. However, our
approach introduces a key distinctive property: our tar-
get measure ν is not given, but instead, solved for. More
speciﬁcally, we search for the simplicial complex of a
user-speciﬁed size that minimizes the cost of transport-
ing the input pointwise measure (i.e., the initial point
set) to the complex simplices. This speciﬁc setup bears
a resemblance to what is known as the optimal location
problem [31]), where the source measure is given but the
target measure is only partially known. Yet a signiﬁcant
diﬀerence lies in the type of constraints we are enforcing
on the target measure, rendering current computational
methods to solve this problem not appropriate to our
context. Another line of research for ﬁnding a trans-
portation plan between an input point set and a set
of discrete sites of various capacities [6,22,32] make
use of power diagrams, and are thus likely to be too
computationally costly for our context.
The closest work to ours was proposed by de Goes et
al. [18]. Their algorithm reconstructs and simpliﬁes 2D
shapes from point sets also based on optimal transport.
Nonetheless, our approach diﬀers from theirs in several
aspects:
1. their optimal transport involves only points and edges
and therefore can be computed in closed form. To
our knowledge, no such closed form exists when trans-
porting points to the facets of a simplicial complex.
Therefore we use a discretized formulation of the
optimal transport problem.
2. the authors of [18] propose to approximate the op-
timal transport plan by assigning each input point
to its closest edge in the triangulation. Such a sim-
plistic scheme can lead to a sub-optimal transport
plan and cost as illustrated in Figure 1(top)—even
more so in 3D. Our discretized formulation, com-
bined with a linear programming solver, provides
better approximations of both the optimal plan and
the optimal cost.
3. their method requires a valid embedding of a 2D
triangulation, which they achieved through a recur-
sive edge ﬂip procedure. Such an edge ﬂip proce-
dure can not, however, be generalized to 3D tri-
angulations. Instead, our method removes the em-
bedding requirement by only employing a (possibly
non-manifold) simplicial complex, initially chosen as
a subset of a 3D Delaunay triangulation.
2.3 Overview
Motivated by the concept of reconstruction introduced
in 2D by de Goes et al. [18], we present a ﬁne-to-coarse
algorithm which reconstructs a surface from a point set
through greedy simpliﬁcation of a 3D simplicial com-
plex. We initialize the complex with a (possibly non-
manifold) subset of the 3D Delaunay triangulation of
input points, then we perform repeated decimations
based on half-edge collapse operations. The error met-
ric guiding our simpliﬁcation is derived from the op-
timal cost to transport the input point set (seen as


--- Page 4 ---

4
Julie Digne et al.
Dirac measures) to a constant-per-facet measure de-
ﬁned over the simplicial complex. At each iteration,
we collapse the half-edge which minimizes the increase
of total transport cost between input points and re-
constructed triangulation. Just like for the formulation
presented in [18], our optimal transport driven metric
brings desirable properties that are rarely satisﬁed by
current reconstruction methods, such as resilience to
noise and outliers, and preservation of sharp features
and boundaries.
In the remainder of this paper, we ﬁrst discuss the
details of our optimal transport based metric (Sec. 3)
then describe our reconstruction algorithm step by step
(Sec. 4). Our method is summarized in Algorithm 1 and
its main stages are illustrated in Figure 2.
Algorithm 1: Algorithm overview.
Input
: Point set S, user-speciﬁed value V .
Output: Simplicial complex C with V vertices.
Construct 3D Delaunay Triangulation T from S;
Compute transport cost from S to facets of T ;
Construct simplicial complex C from facets of T with
non-zero measure;
Decimate C until desired number of vertices V ;
Filter out facets of C by thresholding mass density.
3 Transport Formulation of Reconstruction
We consider the reconstruction problem of turning an
input point set S into a coarse simplicial complex C. The
point set contains N points at locations {pi}i=1···N, and
each point is given a mass mi that reﬂects its measure-
ment conﬁdence (all masses are set to a constant if no
conﬁdence is provided). Our reconstruction method is
based on considering both the point set and the com-
plex as mass distributions (or equivalently, probability
measures), where the measure (mass density) of C is
constant per simplex and possibly 0. Our approach then
consists in ﬁnding a compact shape C that minimizes
the optimal transport cost between the input point set
S and a uniform measure on each facet and vertex of
C.
In [18], a similar, yet 2D optimal transport cost
between S and C was eﬃciently approximated based
on closed form expressions for the optimal cost be-
tween points and edges. However, to our knowledge,
such closed form can not be extended between points
and triangles. We present instead, using linear program-
ming (LP), a discretized formulation of the optimal
transport between S and C that we will solve for later
on through local relaxations.
Fig. 2 Steps of our algorithm: (a) Initial point set; (b)
3D Delaunay triangulation of a random subset containing
10% of the input points; (c) Initial simplicial complex
constructed from facets of the 3D triangulation with non-zero
measure; (d) Initial transport plan assigning point samples to
bin centroids (green arrows); (e-f) Intermediary decimation
steps; (g-i) Reconstruction with 100, 50, and 22 vertices,
respectively; (j-l) Final transport plan with 100, 50, and 22
vertices, respectively.
3.1 Discretization
We approximate the optimal transport cost between
the input point set S and the simplicial complex C
using quadrature. We start by deﬁning a set B of bins
(small regions of the complex) over C. As we aim at
reconstructing piecewise smooth surfaces from point
sets, facet bins are necessary—edge bins could be used
as well if curves in R3 were sought after as well; for
simplicity, we do not discuss this extension. However,
vertex bins are useful as well: when outliers are present,
vertices serve as garbage collectors. Vertex and facet
bins are thus used to evaluate the optimal cost between
S and C as a sum of squared distances between the
points in S and the centroids of the bins in B.


--- Page 5 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
5
First, every vertex of C is considered as (the cen-
ter of) its own bin; each triangular facet is, instead,
tiled with bins using a 2D Centroidal Voronoi Tessella-
tion (CVT) (Figure 3); note that our choice of a CVT
tiling stems from the fact that it minimizes the ap-
proximation error given by quadrature points put at
their centroids [15], which will thus provide optimal
approximation of our transport cost. The number of
bins per facet is set based on a user-deﬁned quadrature
parameter. In all our experiments, we used 200 bins
per unit area, the point sets being included in a half-
unit side size box (note that the facet bins of ﬁg 3 are
purposedly generated with a higher density). Finally,
to compensate for a slightly non-uniform distribution
of bins, we assign a capacity for each bin in B (i.e., ratio
of the total amount of mass that a bin can receive over
the total amount of mass transported to the simplex
the bins belongs to): vertex bins are set to unit capacity
(since there is only one vertex per bin), while each facet
bin is given a capacity equal to the ratio between its
area (i.e., the area of the associated centroidal Voronoi
cell) and the area of its containing facet. Finally, the
centroids of the bins in B are computed and stored as
representatives of their bins.
Fig. 3 Bins of a facet. Bins in a facet are deﬁned as cells of
a centroidal Voronoi tessellation. Bin centroids are depicted
as red dots. Capacities of the bins (set proportional to their
areas) are depicted using a thermal color ramp.
3.2 Linear Programming Formulation
We now present a linear programming formulation to
compute the optimal transport cost between the input
point set S and the bin set B. In the following, we
denote the simplices of C as {σj}j=1···L and the cen-
troids of the bins in B as {bj}j=1···M, where L and M
are the number of simplices and bins respectively. The
capacity of bin bj is denoted cj. We also deﬁne s(j) to
be the index of the simplex containing the bin bj (i.e.,
bj ∈σs(j)). Finally, we denote by mij the amount of
mass transported from a given input point pi ∈S to
the centroid bj (Figure 4).
With these deﬁnitions, we can now formally refer to
a transport plan between S and B as a set of N × M
variables mij such that:
∀ij : mij ≥0,
(1)
∀i :
X
j
mij = mi,
(2)
∀j1, j2 s.t. s(j1)=s(j2) :
P
i mij1
cj1
=
P
i mij2
cj2
,
(3)
where Equation 2 ensures that the entire measure of
an input point gets transported onto the mesh C, and
Equation 3 ensures a uniform measure over each facet
of C.
An optimal transport plan is then deﬁned as a trans-
port plan π that minimizes the associated transport
cost:
cost(π) =
X
ij
mij∥pi −bj∥2.
Finding a transport plan minimizing the transport cost
results in a linear program with respect to the mij,
with equality (Eq. 2 and 3) and inequality constraints
(Eq. 1). Note that the number of bins, their positions,
as well as the square distances between input points and
bin centroids are all precomputed. In order to enforce
the uniformity constraint (Eq. 3) more sparsely, we also
introduce L additional variables li (one per simplex σi)
indicating the target measure density of the correspond-
ing simplex. The ﬁnal problem formulation is thus:
Minimize
P
ij mij∥pi −bj∥2
w.r.t. the variables mij and ls(j), and subject to:













∀i :
X
j
mij = mi
∀j :
X
i
mij = cj · ls(j)
∀i, j : mij ≥0, ls(j) ≥0
3.3 Local Relaxation
Solving directly for the formulation described above is
compute-intensive due to the number of variables and
constraints involved: it requires instantiating a dense
matrix (representing the constraints) of size (M × N +
L) × (M + N). For example, computing the optimal
transport cost between an input point set of 2, 100 sam-
ples and a simplicial complex containing 782 simplices
on which 7, 300 bins are placed involves solving for a
linear program of over 15 million variables and 9, 000


--- Page 6 ---

6
Julie Digne et al.
constraints. Alas, linear programming solvers do not
scale up well to such large numbers.
In order to improve scalability we propose an iter-
ative and local relaxation strategy instead, as summa-
rized in Algorithm 2. A subset of the global solution
space is explored through local LP solves over small
stencils, until a local minimum of the objective function
is reached. Note that we cannot guarantee convergence
of this local procedure to the global minimum; but the
minima reached in practice have consistently provided
satisfactory results in all of our tests.
Our procedure starts with a trivial transport plan
which maps each input point to its nearest vertex of the
simplicial complex C. Since no uniformity constraints
are imposed on vertices, this transport plan is valid,
yet obviously suboptimal in general. Subsequent local
optimizations will only decrease the global transport
cost or leave it unchanged, as local re-assignments are
made only if they generate smaller or equal cost after
optimization. The transport cost found through our
local stencil updates is thus an upper bound of the
global optimal transport cost. Our experiments showed,
unsurprisingly, that the convergence rate of this proce-
dure depends on the shape of local stencils used: the
larger the stencil, the faster the convergence—but with
the unfortunate side eﬀect that large stencils increase
the size of the corresponding linear program. We found
in practice that simply using the 1-ring of a chosen
simplex is a rather reliable choice. More precisely, the
local stencil N of a facet σ is deﬁned as all the facets
incident to σ, along with their vertices.
Armed with this scalable approximation of the trans-
port cost, we describe next how we put it to work for
surface reconstruction through simpliﬁcation.
4 Reconstruction through Simpliﬁcation
4.1 Initialization
We begin our reconstruction process by randomly pick-
ing a subset of the input points S and computing a 3D
Delaunay triangulation. We then construct a simplicial
pi
mij4
mij1
mij2
mij3
bj1
bj2
bj4
bj3
Fig. 4 Transport plan for a single input sample point pi.
Variable mij models the transport of the mass mi at an input
point pi to the jth bin of the facet.
Algorithm 2: Local stencil relaxation overview.
Input
: Simplicial complex C, point set S, threshold ε
Output: Locally optimal transport plan π = {mij}
for pi ∈S do
Transport pi to nearest vertex v ∈C;
new cost ←0;
repeat
for σj ∈C do
old cost ←new cost;
Build the stencil N of the facet σj;
Collect sample points and partial measures
{pi, emi} transporting onto this stencil;
Solve the linear program to ﬁnd the optimal
transport plan of (pi, emi) onto the bins of N;
Update transport plan π and cost new cost;
δ = new cost −old cost;
until δ ≤ε;
complex C from a subset of facets of this 3D triangu-
lation. To select this subset of facets, we perform two
steps: (1) we reuse the local stencil relaxation method
(Algorithm 2) to estimate a transport cost from all
the input points onto the facets and vertices of the 3D
triangulation; (2) we then build C with only the facets
containing non-zero transported measure. For step (1),
we use a stencil centered at each facet and containing
vertices and edges of the two tetrahedra adjacent to
the facet. For an inside facet of the triangulation, for
instance, this stencil contains 7 facets and 5 vertices.
Optimization is then performed by going over all sten-
cils of the triangulation. This stencil-based optimization
is repeated until the decrease in transport cost is below
a user-speciﬁed threshold (set to 10−5 in all our tests).
In practice, the global transport cost decreases rapidly,
and we need to go over all stencils only 10 times at
most. For step (2), we convert our data structure to a
simplicial complex for two main reasons: ﬁrst to allow
our reconstruction to have long and anisotropic sim-
plices; and more importantly, to remove the diﬃcult
issue identiﬁed in [18] of keeping the embedding of the
triangulation valid during decimation.
The initial Delaunay is built by taking a random
subset of the point set for eﬃciency. However, if the sub-
set is small enough we risk not having enough degrees
of freedom for representing the shape. Though we do
not have a theoretical guarantee for this subsampling,
we start in practice with only 10 to 20% of the samples
as it is usually above our target number of vertices and
suﬃcient to capture enough details. (Another option
could be to ﬁlter the initial Delaunay not based on the
transport but on the edge length, but this would result
in a much larger initial simplicial complex.)


--- Page 7 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
7
4.2 Decimation
From the initial simplicial complex C, we further sim-
plify the reconstruction through a greedy decimation
based on half-edge collapse operations. Note, however,
that a conventional decimation algorithm (e.g., [17,27]
and variants) can not be applied in our setup: the pres-
ence of outliers and noise renders typical error metrics
inadequate.
Our optimal transport framework provides a robust
alternative: we pick the next half-edge to collapse as the
one that induces the least increase in global transport
cost. To this end, we simulate the collapse of a can-
didate half-edge e and evaluate the induced change of
transport cost ∆. Since this cost change mostly aﬀects a
neighborhood Ωe of e, we can restrict the computation
of ∆only to Ωe. More speciﬁcally, setting Ωe to the
closure of simplices in the 1-ring of e, we ﬁrst gather
the set of samples pi transporting (partially or entirely)
on Ωe (Figure 5), adding up the already computed
transport cost of this set of (possibly partial) samples to
Ωe, simulate the collapse of e, and recompute the cost
of transporting the set of samples onto the resulting
simplices. The change of transport cost ∆is then set
to the diﬀerence of transport cost before and after the
simulated collapse of e. Once a half-edge is selected
and collapsed, we also update the transport plan of
the edges for which their 1-rings intersect the one-ring
of this collapsed edge. Finally, we increase scalability
by employing a multiple choice approach [48]: the next
half-edge to be collapsed is selected from only a small
set of randomly selected edges as recommended in [18],
instead of maintaining an compute-intensive exhaustive
priority queue. This decimation process is summarized
in Algorithm 3.
Fig. 5 Local stencil of an edge. For better depiction we
represent a manifold neighborhood of an edge (in red) and do
not depict the bins. Simplices in the local stencil are depicted
in blue. Point samples are depicted in green. We only solve
for the measures transported to the stencil (solid green lines)
and not for the measures transported outside of the stencil
(dash green lines).
Algorithm 3: Decimation algorithm.
Input
: Simplicial complex C, input point set S,
target number of vertices V
Output: Simplicial complex Cfinal with V vertices
for each edge e ∈C do
Simulate two half-edge collapse operators;
Push these half-edges to a priority queue P, sorted
by change of transport cost ∆.
repeat
Pop half-edge e∗out of P;
Collect set E of edges whose neighborhoods
intersect neighborhood of e∗;
Collapse e∗and update transport plan on the
neighborhood of e∗;
Update P by recomputing the change of cost ∆for
all edges in E.
until the simplicial complex has V vertices;
4.3 Vertex Relocation
So far our method based on half-edge collapses results
in an interpolating reconstruction, since vertices of the
ﬁnal complex can only be a subset of the input points.
This may lead to suboptimal results, even more so in
the presence of noise and outliers. We thus couple our
decimation with an optimization procedure in order
to relocate the vertices in the reconstructed simplicial
complex C. After the collapse of a half-edge e, the re-
maining vertex v of e is relocated by iterating two steps:
(1) for a given transport plan π, we move v toward
the position that best improves the optimal cost of π;
(2) then we update π around v accordingly. For the
ﬁrst step, we compute the locally optimal position of
v when the transport plan π (i.e., the values mij for
all i and j) is kept ﬁxed. To this end, we express the
position of each centroid of the facet bins in barycentric
coordinates within its containing triangle. Then ﬁnding
the optimal position of vertex v of triangle t = (v, v1, v2)
amounts to minimizing:
min
v
X
i
X
j
mij∥pi −αjv −βjv1 −γjv2∥2,
where αj, βj, γj are the barycentric coordinates of the
centroid of bin bj with respect to vertices (v, v1, v2).
The optimal position with respect to triangle t is:
v⋆(t) =
P
i
P
j mijαj(pi −βjv1 −γjv2)
P
i mijα2
j
.
Thus each triangle t adjacent to v yields an optimal
position v⋆(t). Furthermore, the vertex itself may have
input points assigned to its bin, so that we must add
the vertex contribution to its own relocation:
v⋆(v) =
P
i mijpi
P
i mij
.


--- Page 8 ---

8
Julie Digne et al.
with mij being the mass portion of sample pi assigned
to vertex bin bj of v. Thus each simplex (vertex or facet)
adjacent to vertex v contributes an optimal position for
v. The ﬁnal position v⋆is then chosen as an average
of optimal positions weighted by their corresponding
mass:
v⋆=
m(v) · v⋆(v) +

t adjacent to v
m(t) · v⋆(t)
m(v) +

t adjacent to v
m(t)
,
where m(t) is the total mass transported to simplex t
(corresponding to variable li in the general LP formu-
lation provided i is the index of simplex t). Vertex v
is ﬁnally moved at the midpoint between its current
position and the optimal position v⋆.
For the second step, we freeze the vertex locations and
update the transport map π by solving the local lin-
ear program (Algorithm 2). By alternating these two
steps, the vertices move to their locally optimal po-
sition, allowing for a better recovery of sharp features
and surface boundaries. Figure 6 depicts a simple vertex
relocation sequence in 2D for clarity.
Fig. 6 Vertex relocation. For visual clarity we choose a 2D
example with a single triangle and only facet bins. We ﬁrst
depict the input point set, here uniformly sampled on a
triangle, the initial simplicial complex composed of one facet,
and the facet bins and their capacities. For all subsequent
images we depict the transport plan throughout the vertex
relocation process with blue edges connecting the source point
samples and their target bin centroids.
4.4 Facet Filtering
When the decimation terminates, we could return as
our ﬁnal reconstructed mesh the subset of facets from
C that carry a non-zero measure. However, facets may
have non-zero measure due the presence of noise and
outliers; we thus found convenient to sort the facets
based on their measure density (i.e., the ratio of facet
measure to its area) and provide the user with an inter-
active slider to decide which threshold is most appropri-
ate. Figure 7 shows the reconstructed surface obtained
with diﬀerent ﬁltering thresholds.
Fig. 7 Facet ﬁltering. For a noisy input point set, the
simplicial
complex
returned
by
our
decimation
scheme
contains facets with small, but non-zero measure (top left).
The other images (top to bottom, left to right) show the
result of gradually increasing the threshold during the ﬁnal
ﬁltering of the facets. The best reconstruction in this example
is highlighted in a frame.
4.5 Experimental Results
We implemented our algorithm in C++ using CGAL’s
3D Delaunay triangulation [10] to initialize the recon-
struction, and our own data structure for simplicial
complexes. We used the Coin-OR Clp library [11] as our
linear program solver. Our implementation is partially
parallelized to accelerate computations, exploiting the
fact that all half-edge collapse simulations are indepen-
dent. The initialization and update of the priority queue
are, by far, the most costly operations, as each collapse
involves around 120 simulations on average. When us-
ing the exhaustive priority queue on a laptop with a
two-core processor, a point set containing 30, 000 points
is reconstructed in around 10 hours (initial and ﬁnal
simplicial complexes containing respectively 3, 000 and


--- Page 9 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
9
200 vertices). On a 8-core computation server, this com-
putation reduces to 2 hours (note that the computation
time reduction is not only due to parallelization but
also to a faster clock). However, when using a multiple-
choice approach with random sets of 40 collapses (as
we did in all results shown), the timings are three times
faster on average. The typical breakdown of computa-
tional time spent on each phase of the algorithm is as
follows: building the initial Delaunay mesh and ﬁltering
it takes around 5% of the total computation time; in the
remaining iterative process, 70% of the time is spent in
solving linear problems (needed for collapse simulation
and reassignment), 20% of the time in assembling the
LP systems, and the remaining 10% is spent on per-
forming the collapses. Throughout these computations,
memory consumption remains low; e.g., for the par-
ticular experiment mentioned above, the peak memory
usage was around 80Mb.
Robustness to noise. We tested our method on a
point set sampling a staircase shape with an increasing
amount of synthetic, uniform noise (Figure 8). Even in
the presence of signiﬁcant noise, the method tends to
recover the creases of the input shape well. Only for
noise magnitudes larger than 5% of the bounding box
size does our method fail: for such high noise levels,
spurious facets cannot be discarded by a simple thresh-
olding based on mass density. Our method can however
robustly handle pointsets from current point acquisition
devices, as they generally contain noise magnitudes be-
low this failure regime.
Fig. 8 Robustness to noise. We increase the amount of
synthetic noise from σ = 1% to σ = 2% and σ = 5%,
expressed in percentage of the longest edge length of the
bounding box. The reconstruction starts failing at σ = 5%.
Robustness to outliers. We also tested our method
on a point set that samples a cylinder (Figure 9). Re-
sults are excellent up to 15% of outliers, but our method
can fail when the amount of outliers exceeds 20%—
again, current acquisition devices and stereophotogram-
metric methods are usually good enough not to reach
this amount of outliers.
Feature preservation. Figure 10 depicts the fea-
ture preservation property of our approach on the blade
Fig. 9 Robustness to outliers. The reconstruction is eﬀective
even with 10% outliers (left; compare to outlier-free input in
Fig. 12) but fails from 20%. The outliers are added randomly
within a loose bounding box (120%) of the input point set.
model. Our approach performs well even on thin fea-
tures subtending small angles, for which implicit ap-
proaches (here, the noise-robust Poisson surface recon-
struction method of [24]) tend to smooth out features
and create spurious topological artifacts on low point
density regions.
Fig.
10 Reconstruction
of
the
blade
model
containing
30K
sample
points.
Top:
our
reconstruction.
Bottom:
the output of the Poisson reconstruction method (with
Delaunay
reﬁnement
used
for
contouring
the
resulting
implicit function), and closeup on a sharp crease subtending
a small angle, where the implicit approach fails.
On the cone model in Figure 11, all features (tip,
boundaries) are preserved and the simpliﬁcation is very
eﬀective. Similarly, on a cylinder model (Figure 12) the
boundaries are preserved and the simpliﬁcation leads
to anisotropic triangles with most edges aligned with
minimum curvature directions as expected. Figure 13
also illustrates boundary and sharp feature preserva-
tion, this time on a twisted bar.
Figure 14 illustrates the behavior of our approach
on two intersecting planar polygons. The algorithm be-
haves well down to 10 vertices, and the simplicial com-


--- Page 10 ---

10
Julie Digne et al.
Fig. 11 Reconstruction of a cone. Left: input point set.
Middle: input point set and ﬁnal simplicial complex with
(nearly uniform) facet densities shown. Right: ﬁnal complex.
Fig. 12 Reconstruction and simpliﬁcation of a cylinder. Left:
10K noisy sample points and reconstruction with 12 vertices
(facet density shown). Middle: transport plan between point
samples and bin centroids. Right: simplicial complex and
facet density.
Fig. 13 Reconstruction of a twisted bar. Sharp features are
well preserved.
plex maintains the initial topology during decimation.
Going down to 8 vertices (the expected minimum num-
ber of vertices) would require a richer set of topological
operators in order to disconnect the intersecting edge
before pursuing decimation; we did not pursue this par-
ticular extension.
Fig. 14 Reconstructing and simpliﬁcation of two intersecting
planar polygons until 10 vertices.
Figure 15 shows the performance of the method
on a LIDAR point cloud. Even with these noisy data,
our method recovers the features of the shapes and
produces a low complexity mesh.
Fig. 15 Reconstruction of an aerial LIDAR point cloud
capturing the rooftop of a house. Top: input point set,
middle: ﬁnal reconstruction, bottom: two other views. The
reconstruction yields a very simpliﬁed mesh despite the noise.
Point set courtesy of Qian-Yi Zhou and Ulrich Neumann.
Weaknesses. Given the eﬃciency of current linear
program solvers, results of our approach come at the
price of intensive computations, currently preventing its
use on large point sets. Also, there is currently nothing
in our formulation that favors 2-manifoldness, as the
main data structure is a simplicial complex initialized
by the facets of a 3D triangulation; this can lead, in
rare occasions, to invalid embedding as well as mul-
tiple facets covering the same area (see Figure 16).
The latter issue is more complex than just ensuring
a 2-manifold reconstruction, as complex features may
correspond to non-manifold shapes. One could deﬁne a
notion of “eﬀectiveness” per facet, but this would lead
to a non-linear objective function and require a richer
set of topological operators such as facet deletion.


--- Page 11 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
11
Fig.
16 Reconstruction
and
simpliﬁcation
of
a
scene
composed of two boxes. Left: 10K noisy sample points.
Right: reconstruction with 16 vertices. The level of anisotropy
matches our expectations but some facets of the boxes are
covered twice.
5 Feature Recovery
Another application of our proposed metric is to recover
sharp features and boundaries from the output of recon-
struction methods that are designed to produce smooth,
closed surfaces (e.g., Poisson reconstruction [24]). These
approaches are in general scalable and robust to noise,
but they round oﬀsharp features and ﬁll up holes, even
if a data ﬁtting term is added. We can remediate these
artifacts via vertex relocation and facet ﬁltering; this
feature recovery method is summed up in Algorithm 4.
Algorithm 4: Feature recovery.
Input
: Point set S, reconstructed mesh T .
Output: Feature-capturing mesh T
Compute initial assignment;
for all vertices of T do
Compute the relocation force;
Move the vertex in the direction of the force;
Update the transport plan around the vertex;
Filter out facets of T by thresholding mass densities.
The input of the algorithm is a surface triangle mesh
(the output of a smooth reconstruction algorithm) and
the original point set used for reconstruction. Bins are
ﬁrst sampled on the mesh. The initial assignment is per-
formed through relaxation as described in Section 3.3:
each sample is assigned to the nearest mesh vertex, and
local reassignments are iterated until a local minimum
for the transport cost is reached. Each mesh vertex is
then relocated as described in Section 4.3, by com-
puting the relocation direction, moving the point in
this direction, and updating the transport plan. One
should notice that this process depends on the mesh
vertices traversal order: the ﬁrst vertex is moved at
the midpoint between its current position and the com-
Fig. 17 Anchor. Noisy point set (top), Poisson reconstruction
(middle left), improved reconstruction (middle right) and
associated closeups.
puted optimal position, then the local transport plan
is updated, and then the next vertex is handled. The
traversal order could be randomized between relocation
iterations to avoid potential artifacts. However, all our
experiments were obtained using the same traversal or-
der with no visual bias due to this ﬁxed order. Figure 17
demonstrates how sharpness is recovered with this sim-
ple post-processing phase.
For open surfaces this method recovers boundaries
of the surface through the last ﬁltering step (section 4.4)
as can be seen on the church example (Figure 19 and
20). On the latter, the relocation seems incorrect at ﬁrst
glance on the bell tower, but the seemingly spurious tri-
angles created by the relocation procedure correspond
to actual geometry in the point set: these details of the
shape were lost by the Poisson reconstruction. On the
challenging synthetic point set used in Figure 21 the
vertex relocation recovers the sharpness of the features
as well. In terms of computational cost applying the
vertex relocation algorithm on the church mesh (23K
vertices, 232K input points) takes around 10 minutes.


--- Page 12 ---

12
Julie Digne et al.
Fig. 18 Blade. Poisson reconstruction (2 top rows) and
improved reconstruction (2 bottom rows). The input point
set is depicted with black dots on the global views and is not
depicted on the close-ups for clarity. Neither remeshing nor
edge ﬂips are applied: the spurious topological handles shown
in Figure 10 are not repaired, triangles are only pulled closer
toward the point set.
6 Conclusion
We introduced a surface reconstruction and simpliﬁca-
tion method which exhibits both robustness to noise
and outliers, as well as preservation of sharp features
and boundaries. Our approach is based on the decima-
tion of a simplicial complex guided by an optimal trans-
portation error metric between the reconstruction and
the initial point set. This error metric was also shown
Fig.
19 Church. Point set (left), Poisson reconstruction
(middle) and relocated mesh (right).
Fig.
20 Church. Point set (top), Poisson reconstruction
(bottom left) and its improvement via vertex relocation
(bottom right): ﬁltering combined with vertex relocation
allows recovery of the surface boundaries.
useful as a post-processing phase to recover features
from smooth reconstructed shapes.
The main drawback of our approach is its computa-
tional cost: despite our eﬀorts to introduce local relax-
ation, parallelization, and multiple-choice accelerations,
we cannot reconstruct large point sets in reasonable
time. The main strength of our approach lies in the
simplicity of its formulation: it is expressed directly on
the simplicial complex being reconstructed, departing
from common robust operators that require subsequent
contouring to obtain the ﬁnal reconstructed (but not
simpliﬁed) surface mesh. In addition, our formulation
can be trivially extended to allow for the reconstruction
of curves embedded in R3 by simply adding edge bins
to vertex and facet bins. Furthermore, our formulation
provides us with a transport plan, which can be used for
further geometry processing of the resulting simplicial
complex.


--- Page 13 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
13
Fig.
21 Sharp sphere. Left column: global view, right
column: close-up. From top to bottom: point set, smooth
reconstruction, and vertex relocation. Features are recovered
through vertex relocation.
As future work we wish to improve scalability. The
multi-scale approach of M´erigot [32] is certainly an in-
teresting direction but we believe it requires signiﬁcant
work to be truly practical.
Acknowledgments.This work was funded by the Euro-
pean Research Council (ERC Starting Grant “Robust
Geometry Processing”, Grant agreement 257474). We
also thank the National Science Foundation for partial
support through the CCF grant 1011944.
References
1. Adamson, A., Alexa, M.: Anisotropic point set surfaces.
In: Conference on Computer Graphics, Virtual Reality,
Visualisation and Interaction in Africa, p. 13 (2006)
2. All`egre, R., Chaine, R., Akkouche, S.: Convection-driven
dynamic surface reconstruction.
In: Shape Modeling
International, pp. 33–42. Cambridge, MA, USA (2005)
3. All`egre, R., Chaine, R., Akkouche, S.: A Dynamic Surface
Reconstruction Framework for Large Unstructured Point
Sets. In: IEEE/Eurographics Symposium on Point-Based
Graphics 2006, pp. 17–26 (2006)
4. Alliez, P., Cohen-Steiner, D., Tong, Y., Desbrun, M.:
Voronoi-based variational reconstruction of unoriented
point sets.
In: Eurographics Symposium on Geometry
Processing, pp. 39–48 (2007)
5. Amenta,
N.:
The
Crust
algorithm
for
3d
surface
reconstruction.
In:
Symposium
on
Computational
geometry, pp. 423–424 (1999)
6. Aurenhammer,
F.,
Hoﬀmann,
F.,
Aronov,
B.:
Minkowski-type
theorems
and
least-squares
clustering.
Algorithmica
20, 61–76 (1998).
URL
http://dx.doi.org/10.1007/PL00009187
7. Avron, H., Sharf, A., Greif, C., Cohen-Or, D.: ℓ1-sparse
reconstruction of sharp point set surfaces. ACM Trans.
on Graphics 29(5), 1–12 (2010)
8. Boissonnat,
J.D.,
Cazals,
F.:
Coarse-to-ﬁne
surface
simpliﬁcation with geometric guarantees.
Computer
Graphics Forum 20(3), 490–499 (2001)
9. Bonneel, N., van de Panne, M., Paris, S., Heidrich,
W.: Displacement interpolation using Lagrangian mass
transport. ACM Transactions on Graphics (SIGGRAPH
Asia) (2011)
10. CGAL, Computational Geometry Algorithms Library.
Http://www.cgal.org
11. CLP, coin-or linear program solver.
Http://www.coin-
or.org/Clp/
12. Daniels, J.I., Ha, L.K., Ochotta, T., Silva, C.T.: Robust
smooth
feature
extraction
from
point
clouds.
In:
IEEE International Conference on Shape Modeling and
Applications, pp. 123–136 (2007)
13. Digne, J., Morel, J.M., Souzani, C.M., Lartigue, C.: Scale
space meshing of raw data point sets. Computer Graphics
Forum 30(6), 1630–1642 (2011)
14. Dinh, H.Q., Turk, G., Slabaugh, G.: Reconstructing sur-
faces using anisotropic basis functions. In: International
Conference on Computer Vision, pp. 606–613 (2001)
15. Du, Q., Faber, V., Gunzburger, M.: Centroidal Voronoi
Tessellations: Applications and algorithms. SIAM Rev.
41(4), 637–676 (1999)
16. Fleishman, S., Cohen-Or, D., Silva, C.: Robust moving
least-squares ﬁtting with sharp features.
In: ACM
SIGGRAPH 2005 Papers, p. 552 (2005)
17. Garland, M., Heckbert, P.S.: Surface simpliﬁcation using
quadric error metrics. In: ACM SIGGRAPH, pp. 209–216
(1997)
18. de Goes, F., Cohen-Steiner, D., Alliez, P., Desbrun, M.:
An optimal transport approach to robust reconstruction
and simpliﬁcation of 2d shapes.
Computer Graphics
Forum 30(5), 1593–1602 (2011)
19. Gumhold, S., Wang, X., MacLeod, R.: Feature extraction
from point clouds. In: International Meshing Roundtable,
pp. 293–305 (2001)
20. Hornung, A., Kobbelt, L.: Robust reconstruction of
watertight 3D models from non-uniformly sampled point
clouds without normal information.
In: Eurographics
Symposium on Geometry Processing, pp. 41–50 (2006)
21. Huang, H., Li, D., Zhang, H., Ascher, U., Cohen-Or,
D.: Consolidation of unorganized point clouds for surface
reconstruction.
ACM Transactions on Graphics 28(5)
(2009)
22. Huesmann, M.: Optimal transport between random
measures. ArXiv e-prints (2012)
23. Jenke, P., Wand, M., Straßer, W.: Patch-graph re-
construction for piecewise smooth surfaces.
Vision,
modeling, and visualization 2008: proceedings p. 3 (2008)
24. Kazhdan, M., Bolitho, M., Hoppe, H.: Poisson surface
reconstruction.
In:
Eurographics
Symposium
on
Geometry Processing, SGP ’06, pp. 61–70 (2006)
25. Kolluri, R., Shewchuk, J.R., O’Brien, J.F.: Spectral
surface reconstruction from noisy point clouds.
In:
Eurographics Symposium on Geometry Processing, pp.
11–21 (2004)
26. Labatut, P., Pons, J.P., Keriven, R.: Robust and eﬃcient
surface reconstruction from range data.
Computer
Graphics Forum 28(8), 2275–2290 (2009)
27. Lindstrom, P., Turk, G.: Evaluation of memoryless
simpliﬁcation. IEEE Transactions on Visualization and
Computer Graphics 5(2), 98–115 (1999)


--- Page 14 ---

14
Julie Digne et al.
28. Lipman, Y., Cohen-Or, D., Levin, D.: Data-dependent
MLS for faithful surface approximation. In: Eurographics
Symposium on Geometry Processing, p. 67 (2007)
29. Lipman, Y., Cohen-Or, D., Levin, D., Tal-Ezer, H.:
Parameterization free projection for geometry recon-
struction.
ACM Transactions on Graphics 26(3), 22
(2007)
30. Lipman, Y., Daubechies, I.: Surface comparison with
mass transportation (2010). ArXiv preprint 0912.3488
31. McAsey, M., Mou, L.: Optimal locations and the mass
transport
problem.,
pp.
131–148.
Providence,
RI:
American Mathematical Society (1999)
32. M´erigot, Q.: A multiscale approach to optimal transport.
Computer Graphics Forum 30(5), 1583–1592 (2011)
33. M´erigot,
Q.,
Ovsjanikov,
M.,
Guibas,
L.:
Robust
Voronoi-based curvature and feature estimation. In: 2009
SIAM/ACM Joint Conference on Geometric and Physical
Modeling, pp. 1–12 (2009)
34. Ohtake, Y., Belyaev, A., Alexa, M., Turk, G., Seidel,
H.P.: Multi-level partition of unity implicits. In: ACM
SIGGRAPH, vol. 22(3), pp. 463–470 (2003)
35. Oztireli,
C.,
Guennebaud,
G.,
Gross,
M.:
Feature
preserving point set surfaces based on non-linear kernel
regression. In: Computer Graphics Forum, vol. 28(2), pp.
493–501 (2009)
36. Pang, X.F., Pang, M.Y.: An algorithm for extracting
geometric features from point cloud.
International
Conference
on
Information
Management,
Innovation
Management and Industrial Engineering 4, 78–83 (2009)
37. Pauly, M., Keiser, R., Gross, M.: Multi-scale feature
extraction
on
point-sampled
surfaces.
Computer
Graphics Forum 22(3), 281–289 (2003)
38. Peyr´e, G., Fadili, J., Rabin, J.: Wasserstein active
contours. Tech. rep., Preprint Hal-00593424 (2011). URL
http://hal.archives-ouvertes.fr/hal-00593424/
39. Rabin, J., Delon, J., Gousseau, Y.: Regularization of
transportation maps for color and contrast transfer. In:
IEEE International Conference on Image Processing, pp.
1933 –1936 (2010)
40. Rabin, J., Delon, J., Gousseau, Y.: Transportation
distances on the circle. J. Math. Imaging Vis. 41(1-2),
147–167 (2011)
41. Rabin, J., Peyr´e, G., Cohen, L.D.: Geodesic shape
retrieval via optimal mass transport.
In: European
Conference on Computer Vision: Part V, ECCV’10, pp.
771–784. Springer-Verlag, Berlin, Heidelberg (2010)
42. Rabin, J., Peyr´e, G., Delon, J., Bernot, M.: Wasserstein
barycenter and its application to texture mixing.
In:
A. Bruckstein, B. ter Haar Romeny, A. Bronstein,
M. Bronstein (eds.) Scale Space and Variational Methods
in Computer Vision, Lecture Notes in Computer Science,
vol. 6667, pp. 435–446. Springer Berlin / Heidelberg
(2012)
43. Reinhard, E., Ashikhmin, M., Gooch, B., Shirley, P.:
Color transfer between images. IEEE Comput. Graph.
Appl. 21(5), 34–41 (2001)
44. Rubner, Y., Tomasi, C., Guibas, L.J.: The earth mover’s
distance as a metric for image retrieval. Int. J. Comput.
Vision 40(2), 99–121 (2000)
45. Salman, N., Yvinec, M., M´erigot, Q.: Feature Preserving
Mesh Generation from 3D Point Clouds. In: Computer
Graphics Forum, vol. 29, pp. 1623–1632 (2010)
46. Villani, C.: Topics in Optimal Transportation. American
Mathematical Society (2010)
47. Walder, C., Chapelle, O., Sch¨olkopf, B.: Implicit surface
modelling as an eigenvalue problem.
In: Machine
Learning ICML 2005, pp. 936–939 (2005)
48. Wu, J., Kobbelt, L.: Fast mesh decimation by multiple-
choice techniques. In: Vision, Modeling, and Visualiza-
tion, pp. 241–248 (2002)
```

---

## Section 1.19: Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets

Source File: Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets.txt

### Paper Content:

```
# Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets.pdf
# Output: ../layer2_completion/txt/Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/256230377
Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-
Laden Point Sets
Article  in  Journal of Mathematical Imaging and Vision · February 2013
DOI: 10.1007/s10851-013-0414-y
CITATIONS
94
READS
628
5 authors, including:
David Cohen-Steiner
National Institute for Research in Computer Science and Control
70 PUBLICATIONS   7,190 CITATIONS   
SEE PROFILE
Pierre Alliez
National Institute for Research in Computer Science and Control
167 PUBLICATIONS   13,573 CITATIONS   
SEE PROFILE
Fernando de Goes
California Institute of Technology
49 PUBLICATIONS   1,977 CITATIONS   
SEE PROFILE
Mathieu Desbrun
California Institute of Technology
247 PUBLICATIONS   20,951 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Pierre Alliez on 08 September 2014.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

Noname manuscript No.
(will be inserted by the editor)
Feature-Preserving Surface Reconstruction
and Simpliﬁcation from Defect-Laden Point Sets
Julie Digne · David Cohen-Steiner · Pierre Alliez ·
Fernando de Goes · Mathieu Desbrun
the date of receipt and acceptance should be inserted later
Abstract We introduce a robust and feature-capturing
surface reconstruction and simpliﬁcation method that
turns an input point set into a low triangle-count sim-
plicial complex. Our approach starts with a (possibly
non-manifold) simplicial complex ﬁltered from a 3D
Delaunay triangulation of the input points. This ini-
tial approximation is iteratively simpliﬁed based on an
error metric that measures, through optimal transport,
the distance between the input points and the current
simplicial complex—both seen as mass distributions.
Our approach is shown to exhibit both robustness to
noise and outliers, as well as preservation of sharp fea-
tures and boundaries. Our new feature-sensitive metric
between point sets and triangle meshes can also be used
as a post-processing tool that, from the smooth output
of a reconstruction method, recovers sharp features and
boundaries present in the initial point set.
Keywords Optimal transportation · Wasserstein dis-
tance · Linear programming · Surface reconstruction ·
Shape simpliﬁcation · Feature recovery.
Mathematics Subject Classiﬁcation (2000) 65D17 ·
65D18
1 Introduction
Surface reconstruction is a multi-faceted challenge which
precise problem statement depends on the nature and
defects of the input data, the properties of the inferred
surface (smooth vs piecewise smooth, with or without
boundaries), and the desired level of detail one wishes to
J. Digne · D. Cohen-Steiner · P. Alliez
Inria Sophia Antipolis - M´editerran´ee
F. de Goes · M. Desbrun
California Institute of Technology
capture. Despite a number of major contributions over
the past decade [5,24], achieving both feature preser-
vation and robustness to measurement noise and out-
liers remains a scientiﬁc challenge—and a pressing re-
quirement for many reverse engineering and geometric
modeling applications. Furthermore, low polygon-count
reconstructions has only received limited attention de-
spite the increase of point density in 3D scanning tech-
nology and the need for eﬃcient subsequent geometry
processing.
In this paper we contribute a reconstruction method
that simpliﬁes an initial (possibly non-manifold) tri-
angulation of the input point set, based on an error
metric that quantiﬁes through optimal mass transport
the distance between the current simplicial complex and
the input points. Our reconstruction approach inherits
the qualities of our optimal transport based metric: it
is resilient to noise and outliers, can handle uneven
sampling, yet it ﬁnely captures boundaries and sharp
features. We demonstrate these distinguishing proper-
ties on a series of examples. Applied to the output of
a feature-lossy reconstruction method, our new metric
can also be used in order to recover sharp features and
boundaries through a vertex relocation process.
2 Previous Work
We ﬁrst discuss existing surface reconstruction meth-
ods, restricting our review to approaches that are ro-
bust to noise and outliers as well as feature preserving.
We then point out recent, relevant work on geometry
processing based on optimal transport.


--- Page 3 ---

2
Julie Digne et al.
2.1 Surface Reconstruction
A common approach to robust surface reconstruction
from defect-laden point sets involves denoising and ﬁl-
tering of outliers, and often requires an interactive ad-
justment of parameters. Automatic methods such as
spectral methods [25,47,4] and graph cut approaches [20,
26] have been proven extremely robust but are better
suited to the reconstruction of smooth, closed surfaces.
More recently, Cohen-Or and co-authors have proposed
a series of contributions based on robust norms and
sparse recovery [29,21,7]. An interpolating, yet noise
robust approach was alternatively proposed by Digne
et al. [13] through the construction of a scale space.
Feature preserving methods are typically based on
an implicit representation that approximates or inter-
polates the input points. In [14], for instance, sharp fea-
tures are captured through locally adapted anisotropic
basis functions. Adamson and Alexa [1] proposed an
anisotropic moving least squares (MLS) method instead,
using ellipsoidal mapping functions based on princi-
pal curvatures. More recently, Oztireli et al. [35] ex-
tended the MLS surface reconstruction through kernel
regression to allow for much sharper features. How-
ever, none of these techniques returns truly sharp fea-
tures: reconstructions are always semi-sharp, that is,
still rounded with various degrees of roundness depend-
ing on the approach and the sampling density. More-
over, the presence of sharpness in the geometry of a
point set is detected only locally, which often leads
to fragmented creases; the reconstruction quality thus
degrades quickly if defects and outliers are present.
Another way to detect local sharpness within a point
set consists in performing a local clustering of estimated
normals [34]: if this process reveals more than one clus-
ter of normals, then the algorithm ﬁts as many quadrics
as the number of clusters. Improved robustness was
achieved in [16] by segmenting neighborhoods through
region growing. Lipman et al. [28], instead, proposed
a systematic enrichment of the MLS projection frame-
work with sharp edges driven by the local error of the
MLS approximation. Again, the locality of the feature
detection can generate fragmented sharp edges, much
like general feature detection approaches (e.g., [19,36]).
To reduce crease fragmentation, a diﬀerent thread
of work aims at extracting long sharp features. Pauly
et al. [37], for instance, used a multi-scale approach
to detect feature points, and constructed a minimum-
spanning tree to recover the most likely feature graph.
Daniels et al. [12] used a robust projection operator
onto sharp creases, and grew a set of polylines through
projected points. Jenke et al. [23] extracted feature lines
by robustly ﬁtting local surface patches and by com-
puting the intersection of close patches with dissimilar
normals.
Shape simpliﬁcation has also been tackled in [8], but
in a coarse-to-ﬁne manner: a random initial subset of
the input point cloud and a signed distance function
over the set is built. Using this function, points are
added until a signiﬁcant number of points lie within
an error tolerance. The augmented set is triangulated
and a surface mesh is reconstructed. Thus the method
also interleaves reconstruction with simpliﬁcation. In
[2] and [3], a surface is reconstructed through point set
simpliﬁcation and local coarsening or reﬁnement of the
mesh. Salman et al. [45] proposed to detect features
within the point set (as in [33]) and combined Delaunay
reﬁnement over features and Poisson reconstruction on
smooth parts of the inferred surface [24].
Contributions. In this paper, we adopt a very dif-
ferent reconstruction methodology: we reconstruct a sha-
pe through an iterative, feature-preserving simpliﬁca-
tion of a simplicial complex constructed from the input
point set. To achieve noise and outlier robustness, an er-
ror metric driving the simpliﬁcation is derived in terms
of optimal transport between the input point set and
the reconstructed mesh, both seen as mass distributions
(or equivalently, probability measures) in R3.
Next we provide a brief review of optimal transport,
and mention its applications to various problems in
computer graphics and computer vision.
2.2 Optimal Transport
The problem of transporting a measure onto another
one as a way to quantify their similarity has a rich
scientiﬁc history. For two measures µ and ν deﬁned over
R3 and of equal total mass (i.e., their integrals are the
same), the L2 optimal transport from µ to ν consists
in ﬁnding a transport plan π that realizes the following
inﬁmum:
inf
Z
R3 ∥x −y∥2dπ(x, y)
 π ∈Π(µ, ν)

,
where Π(µ, ν) is the set of all possible transport plans
between µ and ν [46]. In a nutshell, a transport plan
π is a displacement that maps every inﬁnitesimal mass
from the input measure µ (here, the set of points) to the
target measure ν (here, the simplicial complex). This
formulation is particularly well suited to comparing 1D
measures such as histograms over the real line or on
the circle [40], and it has been used for transferring
color and contrast between images [43,39]. For applica-
tions in higher dimensions such as 2D or 3D shape re-
trieval [44,41] and segmentation [38], the optimal trans-
port formulation is notoriously less tractable: solving


--- Page 4 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
3
the optimal transport problem requires linear program-
ming (LP). The LP formulation of optimal transport
has been used in applications such as surface compar-
ison [30] and displacement interpolation [9]. Attempts
to design computationally simpler surrogates have also
been made; the sliced Wasserstein approach [42], for
instance, consists in approximating the transport prob-
lem by a series of 1D problems through projection.
Fig. 1 Transport plans. Top: Binary transport plan [18]. We
depict the input point set and simpliﬁed triangulation. Red
and green line segments depict the transport plan between
the input point set and the uniform measure on respectively
the vertices and edges of the triangulation. Each input point
is simply transported either to its closest edge or to the end
vertices of that edge. For each edge the transport plan favors a
transport to its end vertices instead of to the whole edge when
the corresponding transport cost is lower. A closeup reveals
a spurious tangential component of the transport near corner
vertices (as indicated by red segments pointing towards the
corner), artiﬁcially creating a higher total transport cost. For
more complex features such as concave corners on surfaces,
such behavior leads to reconstruction artifacts. Bottom: our
transport plan is, as expected, mostly normal to the edges.
Contributions. Our reconstruction method also re-
lies on a linear programming formulation. However, our
approach introduces a key distinctive property: our tar-
get measure ν is not given, but instead, solved for. More
speciﬁcally, we search for the simplicial complex of a
user-speciﬁed size that minimizes the cost of transport-
ing the input pointwise measure (i.e., the initial point
set) to the complex simplices. This speciﬁc setup bears
a resemblance to what is known as the optimal location
problem [31]), where the source measure is given but the
target measure is only partially known. Yet a signiﬁcant
diﬀerence lies in the type of constraints we are enforcing
on the target measure, rendering current computational
methods to solve this problem not appropriate to our
context. Another line of research for ﬁnding a trans-
portation plan between an input point set and a set
of discrete sites of various capacities [6,22,32] make
use of power diagrams, and are thus likely to be too
computationally costly for our context.
The closest work to ours was proposed by de Goes et
al. [18]. Their algorithm reconstructs and simpliﬁes 2D
shapes from point sets also based on optimal transport.
Nonetheless, our approach diﬀers from theirs in several
aspects:
1. their optimal transport involves only points and edges
and therefore can be computed in closed form. To
our knowledge, no such closed form exists when trans-
porting points to the facets of a simplicial complex.
Therefore we use a discretized formulation of the
optimal transport problem.
2. the authors of [18] propose to approximate the op-
timal transport plan by assigning each input point
to its closest edge in the triangulation. Such a sim-
plistic scheme can lead to a sub-optimal transport
plan and cost as illustrated in Figure 1(top)—even
more so in 3D. Our discretized formulation, com-
bined with a linear programming solver, provides
better approximations of both the optimal plan and
the optimal cost.
3. their method requires a valid embedding of a 2D
triangulation, which they achieved through a recur-
sive edge ﬂip procedure. Such an edge ﬂip proce-
dure can not, however, be generalized to 3D tri-
angulations. Instead, our method removes the em-
bedding requirement by only employing a (possibly
non-manifold) simplicial complex, initially chosen as
a subset of a 3D Delaunay triangulation.
2.3 Overview
Motivated by the concept of reconstruction introduced
in 2D by de Goes et al. [18], we present a ﬁne-to-coarse
algorithm which reconstructs a surface from a point set
through greedy simpliﬁcation of a 3D simplicial com-
plex. We initialize the complex with a (possibly non-
manifold) subset of the 3D Delaunay triangulation of
input points, then we perform repeated decimations
based on half-edge collapse operations. The error met-
ric guiding our simpliﬁcation is derived from the op-
timal cost to transport the input point set (seen as


--- Page 5 ---

4
Julie Digne et al.
Dirac measures) to a constant-per-facet measure de-
ﬁned over the simplicial complex. At each iteration,
we collapse the half-edge which minimizes the increase
of total transport cost between input points and re-
constructed triangulation. Just like for the formulation
presented in [18], our optimal transport driven metric
brings desirable properties that are rarely satisﬁed by
current reconstruction methods, such as resilience to
noise and outliers, and preservation of sharp features
and boundaries.
In the remainder of this paper, we ﬁrst discuss the
details of our optimal transport based metric (Sec. 3)
then describe our reconstruction algorithm step by step
(Sec. 4). Our method is summarized in Algorithm 1 and
its main stages are illustrated in Figure 2.
Algorithm 1: Algorithm overview.
Input
: Point set S, user-speciﬁed value V .
Output: Simplicial complex C with V vertices.
Construct 3D Delaunay Triangulation T from S;
Compute transport cost from S to facets of T ;
Construct simplicial complex C from facets of T with
non-zero measure;
Decimate C until desired number of vertices V ;
Filter out facets of C by thresholding mass density.
3 Transport Formulation of Reconstruction
We consider the reconstruction problem of turning an
input point set S into a coarse simplicial complex C. The
point set contains N points at locations {pi}i=1···N, and
each point is given a mass mi that reﬂects its measure-
ment conﬁdence (all masses are set to a constant if no
conﬁdence is provided). Our reconstruction method is
based on considering both the point set and the com-
plex as mass distributions (or equivalently, probability
measures), where the measure (mass density) of C is
constant per simplex and possibly 0. Our approach then
consists in ﬁnding a compact shape C that minimizes
the optimal transport cost between the input point set
S and a uniform measure on each facet and vertex of
C.
In [18], a similar, yet 2D optimal transport cost
between S and C was eﬃciently approximated based
on closed form expressions for the optimal cost be-
tween points and edges. However, to our knowledge,
such closed form can not be extended between points
and triangles. We present instead, using linear program-
ming (LP), a discretized formulation of the optimal
transport between S and C that we will solve for later
on through local relaxations.
Fig. 2 Steps of our algorithm: (a) Initial point set; (b)
3D Delaunay triangulation of a random subset containing
10% of the input points; (c) Initial simplicial complex
constructed from facets of the 3D triangulation with non-zero
measure; (d) Initial transport plan assigning point samples to
bin centroids (green arrows); (e-f) Intermediary decimation
steps; (g-i) Reconstruction with 100, 50, and 22 vertices,
respectively; (j-l) Final transport plan with 100, 50, and 22
vertices, respectively.
3.1 Discretization
We approximate the optimal transport cost between
the input point set S and the simplicial complex C
using quadrature. We start by deﬁning a set B of bins
(small regions of the complex) over C. As we aim at
reconstructing piecewise smooth surfaces from point
sets, facet bins are necessary—edge bins could be used
as well if curves in R3 were sought after as well; for
simplicity, we do not discuss this extension. However,
vertex bins are useful as well: when outliers are present,
vertices serve as garbage collectors. Vertex and facet
bins are thus used to evaluate the optimal cost between
S and C as a sum of squared distances between the
points in S and the centroids of the bins in B.


--- Page 6 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
5
First, every vertex of C is considered as (the cen-
ter of) its own bin; each triangular facet is, instead,
tiled with bins using a 2D Centroidal Voronoi Tessella-
tion (CVT) (Figure 3); note that our choice of a CVT
tiling stems from the fact that it minimizes the ap-
proximation error given by quadrature points put at
their centroids [15], which will thus provide optimal
approximation of our transport cost. The number of
bins per facet is set based on a user-deﬁned quadrature
parameter. In all our experiments, we used 200 bins
per unit area, the point sets being included in a half-
unit side size box (note that the facet bins of ﬁg 3 are
purposedly generated with a higher density). Finally,
to compensate for a slightly non-uniform distribution
of bins, we assign a capacity for each bin in B (i.e., ratio
of the total amount of mass that a bin can receive over
the total amount of mass transported to the simplex
the bins belongs to): vertex bins are set to unit capacity
(since there is only one vertex per bin), while each facet
bin is given a capacity equal to the ratio between its
area (i.e., the area of the associated centroidal Voronoi
cell) and the area of its containing facet. Finally, the
centroids of the bins in B are computed and stored as
representatives of their bins.
Fig. 3 Bins of a facet. Bins in a facet are deﬁned as cells of
a centroidal Voronoi tessellation. Bin centroids are depicted
as red dots. Capacities of the bins (set proportional to their
areas) are depicted using a thermal color ramp.
3.2 Linear Programming Formulation
We now present a linear programming formulation to
compute the optimal transport cost between the input
point set S and the bin set B. In the following, we
denote the simplices of C as {σj}j=1···L and the cen-
troids of the bins in B as {bj}j=1···M, where L and M
are the number of simplices and bins respectively. The
capacity of bin bj is denoted cj. We also deﬁne s(j) to
be the index of the simplex containing the bin bj (i.e.,
bj ∈σs(j)). Finally, we denote by mij the amount of
mass transported from a given input point pi ∈S to
the centroid bj (Figure 4).
With these deﬁnitions, we can now formally refer to
a transport plan between S and B as a set of N × M
variables mij such that:
∀ij : mij ≥0,
(1)
∀i :
X
j
mij = mi,
(2)
∀j1, j2 s.t. s(j1)=s(j2) :
P
i mij1
cj1
=
P
i mij2
cj2
,
(3)
where Equation 2 ensures that the entire measure of
an input point gets transported onto the mesh C, and
Equation 3 ensures a uniform measure over each facet
of C.
An optimal transport plan is then deﬁned as a trans-
port plan π that minimizes the associated transport
cost:
cost(π) =
X
ij
mij∥pi −bj∥2.
Finding a transport plan minimizing the transport cost
results in a linear program with respect to the mij,
with equality (Eq. 2 and 3) and inequality constraints
(Eq. 1). Note that the number of bins, their positions,
as well as the square distances between input points and
bin centroids are all precomputed. In order to enforce
the uniformity constraint (Eq. 3) more sparsely, we also
introduce L additional variables li (one per simplex σi)
indicating the target measure density of the correspond-
ing simplex. The ﬁnal problem formulation is thus:
Minimize
P
ij mij∥pi −bj∥2
w.r.t. the variables mij and ls(j), and subject to:













∀i :
X
j
mij = mi
∀j :
X
i
mij = cj · ls(j)
∀i, j : mij ≥0, ls(j) ≥0
3.3 Local Relaxation
Solving directly for the formulation described above is
compute-intensive due to the number of variables and
constraints involved: it requires instantiating a dense
matrix (representing the constraints) of size (M × N +
L) × (M + N). For example, computing the optimal
transport cost between an input point set of 2, 100 sam-
ples and a simplicial complex containing 782 simplices
on which 7, 300 bins are placed involves solving for a
linear program of over 15 million variables and 9, 000


--- Page 7 ---

6
Julie Digne et al.
constraints. Alas, linear programming solvers do not
scale up well to such large numbers.
In order to improve scalability we propose an iter-
ative and local relaxation strategy instead, as summa-
rized in Algorithm 2. A subset of the global solution
space is explored through local LP solves over small
stencils, until a local minimum of the objective function
is reached. Note that we cannot guarantee convergence
of this local procedure to the global minimum; but the
minima reached in practice have consistently provided
satisfactory results in all of our tests.
Our procedure starts with a trivial transport plan
which maps each input point to its nearest vertex of the
simplicial complex C. Since no uniformity constraints
are imposed on vertices, this transport plan is valid,
yet obviously suboptimal in general. Subsequent local
optimizations will only decrease the global transport
cost or leave it unchanged, as local re-assignments are
made only if they generate smaller or equal cost after
optimization. The transport cost found through our
local stencil updates is thus an upper bound of the
global optimal transport cost. Our experiments showed,
unsurprisingly, that the convergence rate of this proce-
dure depends on the shape of local stencils used: the
larger the stencil, the faster the convergence—but with
the unfortunate side eﬀect that large stencils increase
the size of the corresponding linear program. We found
in practice that simply using the 1-ring of a chosen
simplex is a rather reliable choice. More precisely, the
local stencil N of a facet σ is deﬁned as all the facets
incident to σ, along with their vertices.
Armed with this scalable approximation of the trans-
port cost, we describe next how we put it to work for
surface reconstruction through simpliﬁcation.
4 Reconstruction through Simpliﬁcation
4.1 Initialization
We begin our reconstruction process by randomly pick-
ing a subset of the input points S and computing a 3D
Delaunay triangulation. We then construct a simplicial
pi
mij4
mij1
mij2
mij3
bj1
bj2
bj4
bj3
Fig. 4 Transport plan for a single input sample point pi.
Variable mij models the transport of the mass mi at an input
point pi to the jth bin of the facet.
Algorithm 2: Local stencil relaxation overview.
Input
: Simplicial complex C, point set S, threshold ε
Output: Locally optimal transport plan π = {mij}
for pi ∈S do
Transport pi to nearest vertex v ∈C;
new cost ←0;
repeat
for σj ∈C do
old cost ←new cost;
Build the stencil N of the facet σj;
Collect sample points and partial measures
{pi, emi} transporting onto this stencil;
Solve the linear program to ﬁnd the optimal
transport plan of (pi, emi) onto the bins of N;
Update transport plan π and cost new cost;
δ = new cost −old cost;
until δ ≤ε;
complex C from a subset of facets of this 3D triangu-
lation. To select this subset of facets, we perform two
steps: (1) we reuse the local stencil relaxation method
(Algorithm 2) to estimate a transport cost from all
the input points onto the facets and vertices of the 3D
triangulation; (2) we then build C with only the facets
containing non-zero transported measure. For step (1),
we use a stencil centered at each facet and containing
vertices and edges of the two tetrahedra adjacent to
the facet. For an inside facet of the triangulation, for
instance, this stencil contains 7 facets and 5 vertices.
Optimization is then performed by going over all sten-
cils of the triangulation. This stencil-based optimization
is repeated until the decrease in transport cost is below
a user-speciﬁed threshold (set to 10−5 in all our tests).
In practice, the global transport cost decreases rapidly,
and we need to go over all stencils only 10 times at
most. For step (2), we convert our data structure to a
simplicial complex for two main reasons: ﬁrst to allow
our reconstruction to have long and anisotropic sim-
plices; and more importantly, to remove the diﬃcult
issue identiﬁed in [18] of keeping the embedding of the
triangulation valid during decimation.
The initial Delaunay is built by taking a random
subset of the point set for eﬃciency. However, if the sub-
set is small enough we risk not having enough degrees
of freedom for representing the shape. Though we do
not have a theoretical guarantee for this subsampling,
we start in practice with only 10 to 20% of the samples
as it is usually above our target number of vertices and
suﬃcient to capture enough details. (Another option
could be to ﬁlter the initial Delaunay not based on the
transport but on the edge length, but this would result
in a much larger initial simplicial complex.)


--- Page 8 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
7
4.2 Decimation
From the initial simplicial complex C, we further sim-
plify the reconstruction through a greedy decimation
based on half-edge collapse operations. Note, however,
that a conventional decimation algorithm (e.g., [17,27]
and variants) can not be applied in our setup: the pres-
ence of outliers and noise renders typical error metrics
inadequate.
Our optimal transport framework provides a robust
alternative: we pick the next half-edge to collapse as the
one that induces the least increase in global transport
cost. To this end, we simulate the collapse of a can-
didate half-edge e and evaluate the induced change of
transport cost ∆. Since this cost change mostly aﬀects a
neighborhood Ωe of e, we can restrict the computation
of ∆only to Ωe. More speciﬁcally, setting Ωe to the
closure of simplices in the 1-ring of e, we ﬁrst gather
the set of samples pi transporting (partially or entirely)
on Ωe (Figure 5), adding up the already computed
transport cost of this set of (possibly partial) samples to
Ωe, simulate the collapse of e, and recompute the cost
of transporting the set of samples onto the resulting
simplices. The change of transport cost ∆is then set
to the diﬀerence of transport cost before and after the
simulated collapse of e. Once a half-edge is selected
and collapsed, we also update the transport plan of
the edges for which their 1-rings intersect the one-ring
of this collapsed edge. Finally, we increase scalability
by employing a multiple choice approach [48]: the next
half-edge to be collapsed is selected from only a small
set of randomly selected edges as recommended in [18],
instead of maintaining an compute-intensive exhaustive
priority queue. This decimation process is summarized
in Algorithm 3.
Fig. 5 Local stencil of an edge. For better depiction we
represent a manifold neighborhood of an edge (in red) and do
not depict the bins. Simplices in the local stencil are depicted
in blue. Point samples are depicted in green. We only solve
for the measures transported to the stencil (solid green lines)
and not for the measures transported outside of the stencil
(dash green lines).
Algorithm 3: Decimation algorithm.
Input
: Simplicial complex C, input point set S,
target number of vertices V
Output: Simplicial complex Cfinal with V vertices
for each edge e ∈C do
Simulate two half-edge collapse operators;
Push these half-edges to a priority queue P, sorted
by change of transport cost ∆.
repeat
Pop half-edge e∗out of P;
Collect set E of edges whose neighborhoods
intersect neighborhood of e∗;
Collapse e∗and update transport plan on the
neighborhood of e∗;
Update P by recomputing the change of cost ∆for
all edges in E.
until the simplicial complex has V vertices;
4.3 Vertex Relocation
So far our method based on half-edge collapses results
in an interpolating reconstruction, since vertices of the
ﬁnal complex can only be a subset of the input points.
This may lead to suboptimal results, even more so in
the presence of noise and outliers. We thus couple our
decimation with an optimization procedure in order
to relocate the vertices in the reconstructed simplicial
complex C. After the collapse of a half-edge e, the re-
maining vertex v of e is relocated by iterating two steps:
(1) for a given transport plan π, we move v toward
the position that best improves the optimal cost of π;
(2) then we update π around v accordingly. For the
ﬁrst step, we compute the locally optimal position of
v when the transport plan π (i.e., the values mij for
all i and j) is kept ﬁxed. To this end, we express the
position of each centroid of the facet bins in barycentric
coordinates within its containing triangle. Then ﬁnding
the optimal position of vertex v of triangle t = (v, v1, v2)
amounts to minimizing:
min
v
X
i
X
j
mij∥pi −αjv −βjv1 −γjv2∥2,
where αj, βj, γj are the barycentric coordinates of the
centroid of bin bj with respect to vertices (v, v1, v2).
The optimal position with respect to triangle t is:
v⋆(t) =
P
i
P
j mijαj(pi −βjv1 −γjv2)
P
i mijα2
j
.
Thus each triangle t adjacent to v yields an optimal
position v⋆(t). Furthermore, the vertex itself may have
input points assigned to its bin, so that we must add
the vertex contribution to its own relocation:
v⋆(v) =
P
i mijpi
P
i mij
.


--- Page 9 ---

8
Julie Digne et al.
with mij being the mass portion of sample pi assigned
to vertex bin bj of v. Thus each simplex (vertex or facet)
adjacent to vertex v contributes an optimal position for
v. The ﬁnal position v⋆is then chosen as an average
of optimal positions weighted by their corresponding
mass:
v⋆=
m(v) · v⋆(v) +
P
t adjacent to v
m(t) · v⋆(t)
m(v) +
P
t adjacent to v
m(t)
,
where m(t) is the total mass transported to simplex t
(corresponding to variable li in the general LP formu-
lation provided i is the index of simplex t). Vertex v
is ﬁnally moved at the midpoint between its current
position and the optimal position v⋆.
For the second step, we freeze the vertex locations and
update the transport map π by solving the local lin-
ear program (Algorithm 2). By alternating these two
steps, the vertices move to their locally optimal po-
sition, allowing for a better recovery of sharp features
and surface boundaries. Figure 6 depicts a simple vertex
relocation sequence in 2D for clarity.
Fig. 6 Vertex relocation. For visual clarity we choose a 2D
example with a single triangle and only facet bins. We ﬁrst
depict the input point set, here uniformly sampled on a
triangle, the initial simplicial complex composed of one facet,
and the facet bins and their capacities. For all subsequent
images we depict the transport plan throughout the vertex
relocation process with blue edges connecting the source point
samples and their target bin centroids.
4.4 Facet Filtering
When the decimation terminates, we could return as
our ﬁnal reconstructed mesh the subset of facets from
C that carry a non-zero measure. However, facets may
have non-zero measure due the presence of noise and
outliers; we thus found convenient to sort the facets
based on their measure density (i.e., the ratio of facet
measure to its area) and provide the user with an inter-
active slider to decide which threshold is most appropri-
ate. Figure 7 shows the reconstructed surface obtained
with diﬀerent ﬁltering thresholds.
Fig. 7 Facet ﬁltering. For a noisy input point set, the
simplicial
complex
returned
by
our
decimation
scheme
contains facets with small, but non-zero measure (top left).
The other images (top to bottom, left to right) show the
result of gradually increasing the threshold during the ﬁnal
ﬁltering of the facets. The best reconstruction in this example
is highlighted in a frame.
4.5 Experimental Results
We implemented our algorithm in C++ using CGAL’s
3D Delaunay triangulation [10] to initialize the recon-
struction, and our own data structure for simplicial
complexes. We used the Coin-OR Clp library [11] as our
linear program solver. Our implementation is partially
parallelized to accelerate computations, exploiting the
fact that all half-edge collapse simulations are indepen-
dent. The initialization and update of the priority queue
are, by far, the most costly operations, as each collapse
involves around 120 simulations on average. When us-
ing the exhaustive priority queue on a laptop with a
two-core processor, a point set containing 30, 000 points
is reconstructed in around 10 hours (initial and ﬁnal
simplicial complexes containing respectively 3, 000 and


--- Page 10 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
9
200 vertices). On a 8-core computation server, this com-
putation reduces to 2 hours (note that the computation
time reduction is not only due to parallelization but
also to a faster clock). However, when using a multiple-
choice approach with random sets of 40 collapses (as
we did in all results shown), the timings are three times
faster on average. The typical breakdown of computa-
tional time spent on each phase of the algorithm is as
follows: building the initial Delaunay mesh and ﬁltering
it takes around 5% of the total computation time; in the
remaining iterative process, 70% of the time is spent in
solving linear problems (needed for collapse simulation
and reassignment), 20% of the time in assembling the
LP systems, and the remaining 10% is spent on per-
forming the collapses. Throughout these computations,
memory consumption remains low; e.g., for the par-
ticular experiment mentioned above, the peak memory
usage was around 80Mb.
Robustness to noise. We tested our method on a
point set sampling a staircase shape with an increasing
amount of synthetic, uniform noise (Figure 8). Even in
the presence of signiﬁcant noise, the method tends to
recover the creases of the input shape well. Only for
noise magnitudes larger than 5% of the bounding box
size does our method fail: for such high noise levels,
spurious facets cannot be discarded by a simple thresh-
olding based on mass density. Our method can however
robustly handle pointsets from current point acquisition
devices, as they generally contain noise magnitudes be-
low this failure regime.
Fig. 8 Robustness to noise. We increase the amount of
synthetic noise from σ = 1% to σ = 2% and σ = 5%,
expressed in percentage of the longest edge length of the
bounding box. The reconstruction starts failing at σ = 5%.
Robustness to outliers. We also tested our method
on a point set that samples a cylinder (Figure 9). Re-
sults are excellent up to 15% of outliers, but our method
can fail when the amount of outliers exceeds 20%—
again, current acquisition devices and stereophotogram-
metric methods are usually good enough not to reach
this amount of outliers.
Feature preservation. Figure 10 depicts the fea-
ture preservation property of our approach on the blade
Fig. 9 Robustness to outliers. The reconstruction is eﬀective
even with 10% outliers (left; compare to outlier-free input in
Fig. 12) but fails from 20%. The outliers are added randomly
within a loose bounding box (120%) of the input point set.
model. Our approach performs well even on thin fea-
tures subtending small angles, for which implicit ap-
proaches (here, the noise-robust Poisson surface recon-
struction method of [24]) tend to smooth out features
and create spurious topological artifacts on low point
density regions.
Fig.
10 Reconstruction
of
the
blade
model
containing
30K
sample
points.
Top:
our
reconstruction.
Bottom:
the output of the Poisson reconstruction method (with
Delaunay
reﬁnement
used
for
contouring
the
resulting
implicit function), and closeup on a sharp crease subtending
a small angle, where the implicit approach fails.
On the cone model in Figure 11, all features (tip,
boundaries) are preserved and the simpliﬁcation is very
eﬀective. Similarly, on a cylinder model (Figure 12) the
boundaries are preserved and the simpliﬁcation leads
to anisotropic triangles with most edges aligned with
minimum curvature directions as expected. Figure 13
also illustrates boundary and sharp feature preserva-
tion, this time on a twisted bar.
Figure 14 illustrates the behavior of our approach
on two intersecting planar polygons. The algorithm be-
haves well down to 10 vertices, and the simplicial com-


--- Page 11 ---

10
Julie Digne et al.
Fig. 11 Reconstruction of a cone. Left: input point set.
Middle: input point set and ﬁnal simplicial complex with
(nearly uniform) facet densities shown. Right: ﬁnal complex.
Fig. 12 Reconstruction and simpliﬁcation of a cylinder. Left:
10K noisy sample points and reconstruction with 12 vertices
(facet density shown). Middle: transport plan between point
samples and bin centroids. Right: simplicial complex and
facet density.
Fig. 13 Reconstruction of a twisted bar. Sharp features are
well preserved.
plex maintains the initial topology during decimation.
Going down to 8 vertices (the expected minimum num-
ber of vertices) would require a richer set of topological
operators in order to disconnect the intersecting edge
before pursuing decimation; we did not pursue this par-
ticular extension.
Fig. 14 Reconstructing and simpliﬁcation of two intersecting
planar polygons until 10 vertices.
Figure 15 shows the performance of the method
on a LIDAR point cloud. Even with these noisy data,
our method recovers the features of the shapes and
produces a low complexity mesh.
Fig. 15 Reconstruction of an aerial LIDAR point cloud
capturing the rooftop of a house. Top: input point set,
middle: ﬁnal reconstruction, bottom: two other views. The
reconstruction yields a very simpliﬁed mesh despite the noise.
Point set courtesy of Qian-Yi Zhou and Ulrich Neumann.
Weaknesses. Given the eﬃciency of current linear
program solvers, results of our approach come at the
price of intensive computations, currently preventing its
use on large point sets. Also, there is currently nothing
in our formulation that favors 2-manifoldness, as the
main data structure is a simplicial complex initialized
by the facets of a 3D triangulation; this can lead, in
rare occasions, to invalid embedding as well as mul-
tiple facets covering the same area (see Figure 16).
The latter issue is more complex than just ensuring
a 2-manifold reconstruction, as complex features may
correspond to non-manifold shapes. One could deﬁne a
notion of “eﬀectiveness” per facet, but this would lead
to a non-linear objective function and require a richer
set of topological operators such as facet deletion.


--- Page 12 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
11
Fig.
16 Reconstruction
and
simpliﬁcation
of
a
scene
composed of two boxes. Left: 10K noisy sample points.
Right: reconstruction with 16 vertices. The level of anisotropy
matches our expectations but some facets of the boxes are
covered twice.
5 Feature Recovery
Another application of our proposed metric is to recover
sharp features and boundaries from the output of recon-
struction methods that are designed to produce smooth,
closed surfaces (e.g., Poisson reconstruction [24]). These
approaches are in general scalable and robust to noise,
but they round oﬀsharp features and ﬁll up holes, even
if a data ﬁtting term is added. We can remediate these
artifacts via vertex relocation and facet ﬁltering; this
feature recovery method is summed up in Algorithm 4.
Algorithm 4: Feature recovery.
Input
: Point set S, reconstructed mesh T .
Output: Feature-capturing mesh T
Compute initial assignment;
for all vertices of T do
Compute the relocation force;
Move the vertex in the direction of the force;
Update the transport plan around the vertex;
Filter out facets of T by thresholding mass densities.
The input of the algorithm is a surface triangle mesh
(the output of a smooth reconstruction algorithm) and
the original point set used for reconstruction. Bins are
ﬁrst sampled on the mesh. The initial assignment is per-
formed through relaxation as described in Section 3.3:
each sample is assigned to the nearest mesh vertex, and
local reassignments are iterated until a local minimum
for the transport cost is reached. Each mesh vertex is
then relocated as described in Section 4.3, by com-
puting the relocation direction, moving the point in
this direction, and updating the transport plan. One
should notice that this process depends on the mesh
vertices traversal order: the ﬁrst vertex is moved at
the midpoint between its current position and the com-
Fig. 17 Anchor. Noisy point set (top), Poisson reconstruction
(middle left), improved reconstruction (middle right) and
associated closeups.
puted optimal position, then the local transport plan
is updated, and then the next vertex is handled. The
traversal order could be randomized between relocation
iterations to avoid potential artifacts. However, all our
experiments were obtained using the same traversal or-
der with no visual bias due to this ﬁxed order. Figure 17
demonstrates how sharpness is recovered with this sim-
ple post-processing phase.
For open surfaces this method recovers boundaries
of the surface through the last ﬁltering step (section 4.4)
as can be seen on the church example (Figure 19 and
20). On the latter, the relocation seems incorrect at ﬁrst
glance on the bell tower, but the seemingly spurious tri-
angles created by the relocation procedure correspond
to actual geometry in the point set: these details of the
shape were lost by the Poisson reconstruction. On the
challenging synthetic point set used in Figure 21 the
vertex relocation recovers the sharpness of the features
as well. In terms of computational cost applying the
vertex relocation algorithm on the church mesh (23K
vertices, 232K input points) takes around 10 minutes.


--- Page 13 ---

12
Julie Digne et al.
Fig. 18 Blade. Poisson reconstruction (2 top rows) and
improved reconstruction (2 bottom rows). The input point
set is depicted with black dots on the global views and is not
depicted on the close-ups for clarity. Neither remeshing nor
edge ﬂips are applied: the spurious topological handles shown
in Figure 10 are not repaired, triangles are only pulled closer
toward the point set.
6 Conclusion
We introduced a surface reconstruction and simpliﬁca-
tion method which exhibits both robustness to noise
and outliers, as well as preservation of sharp features
and boundaries. Our approach is based on the decima-
tion of a simplicial complex guided by an optimal trans-
portation error metric between the reconstruction and
the initial point set. This error metric was also shown
Fig.
19 Church. Point set (left), Poisson reconstruction
(middle) and relocated mesh (right).
Fig.
20 Church. Point set (top), Poisson reconstruction
(bottom left) and its improvement via vertex relocation
(bottom right): ﬁltering combined with vertex relocation
allows recovery of the surface boundaries.
useful as a post-processing phase to recover features
from smooth reconstructed shapes.
The main drawback of our approach is its computa-
tional cost: despite our eﬀorts to introduce local relax-
ation, parallelization, and multiple-choice accelerations,
we cannot reconstruct large point sets in reasonable
time. The main strength of our approach lies in the
simplicity of its formulation: it is expressed directly on
the simplicial complex being reconstructed, departing
from common robust operators that require subsequent
contouring to obtain the ﬁnal reconstructed (but not
simpliﬁed) surface mesh. In addition, our formulation
can be trivially extended to allow for the reconstruction
of curves embedded in R3 by simply adding edge bins
to vertex and facet bins. Furthermore, our formulation
provides us with a transport plan, which can be used for
further geometry processing of the resulting simplicial
complex.


--- Page 14 ---

Feature-Preserving Surface Reconstruction and Simpliﬁcation from Defect-Laden Point Sets
13
Fig.
21 Sharp sphere. Left column: global view, right
column: close-up. From top to bottom: point set, smooth
reconstruction, and vertex relocation. Features are recovered
through vertex relocation.
As future work we wish to improve scalability. The
multi-scale approach of M´erigot [32] is certainly an in-
teresting direction but we believe it requires signiﬁcant
work to be truly practical.
Acknowledgments.This work was funded by the Euro-
pean Research Council (ERC Starting Grant “Robust
Geometry Processing”, Grant agreement 257474). We
also thank the National Science Foundation for partial
support through the CCF grant 1011944.
References
1. Adamson, A., Alexa, M.: Anisotropic point set surfaces.
In: Conference on Computer Graphics, Virtual Reality,
Visualisation and Interaction in Africa, p. 13 (2006)
2. All`egre, R., Chaine, R., Akkouche, S.: Convection-driven
dynamic surface reconstruction.
In: Shape Modeling
International, pp. 33–42. Cambridge, MA, USA (2005)
3. All`egre, R., Chaine, R., Akkouche, S.: A Dynamic Surface
Reconstruction Framework for Large Unstructured Point
Sets. In: IEEE/Eurographics Symposium on Point-Based
Graphics 2006, pp. 17–26 (2006)
4. Alliez, P., Cohen-Steiner, D., Tong, Y., Desbrun, M.:
Voronoi-based variational reconstruction of unoriented
point sets.
In: Eurographics Symposium on Geometry
Processing, pp. 39–48 (2007)
5. Amenta,
N.:
The
Crust
algorithm
for
3d
surface
reconstruction.
In:
Symposium
on
Computational
geometry, pp. 423–424 (1999)
6. Aurenhammer,
F.,
Hoﬀmann,
F.,
Aronov,
B.:
Minkowski-type
theorems
and
least-squares
clustering.
Algorithmica
20, 61–76 (1998).
URL
http://dx.doi.org/10.1007/PL00009187
7. Avron, H., Sharf, A., Greif, C., Cohen-Or, D.: ℓ1-sparse
reconstruction of sharp point set surfaces. ACM Trans.
on Graphics 29(5), 1–12 (2010)
8. Boissonnat,
J.D.,
Cazals,
F.:
Coarse-to-ﬁne
surface
simpliﬁcation with geometric guarantees.
Computer
Graphics Forum 20(3), 490–499 (2001)
9. Bonneel, N., van de Panne, M., Paris, S., Heidrich,
W.: Displacement interpolation using Lagrangian mass
transport. ACM Transactions on Graphics (SIGGRAPH
Asia) (2011)
10. CGAL, Computational Geometry Algorithms Library.
Http://www.cgal.org
11. CLP, coin-or linear program solver.
Http://www.coin-
or.org/Clp/
12. Daniels, J.I., Ha, L.K., Ochotta, T., Silva, C.T.: Robust
smooth
feature
extraction
from
point
clouds.
In:
IEEE International Conference on Shape Modeling and
Applications, pp. 123–136 (2007)
13. Digne, J., Morel, J.M., Souzani, C.M., Lartigue, C.: Scale
space meshing of raw data point sets. Computer Graphics
Forum 30(6), 1630–1642 (2011)
14. Dinh, H.Q., Turk, G., Slabaugh, G.: Reconstructing sur-
faces using anisotropic basis functions. In: International
Conference on Computer Vision, pp. 606–613 (2001)
15. Du, Q., Faber, V., Gunzburger, M.: Centroidal Voronoi
Tessellations: Applications and algorithms. SIAM Rev.
41(4), 637–676 (1999)
16. Fleishman, S., Cohen-Or, D., Silva, C.: Robust moving
least-squares ﬁtting with sharp features.
In: ACM
SIGGRAPH 2005 Papers, p. 552 (2005)
17. Garland, M., Heckbert, P.S.: Surface simpliﬁcation using
quadric error metrics. In: ACM SIGGRAPH, pp. 209–216
(1997)
18. de Goes, F., Cohen-Steiner, D., Alliez, P., Desbrun, M.:
An optimal transport approach to robust reconstruction
and simpliﬁcation of 2d shapes.
Computer Graphics
Forum 30(5), 1593–1602 (2011)
19. Gumhold, S., Wang, X., MacLeod, R.: Feature extraction
from point clouds. In: International Meshing Roundtable,
pp. 293–305 (2001)
20. Hornung, A., Kobbelt, L.: Robust reconstruction of
watertight 3D models from non-uniformly sampled point
clouds without normal information.
In: Eurographics
Symposium on Geometry Processing, pp. 41–50 (2006)
21. Huang, H., Li, D., Zhang, H., Ascher, U., Cohen-Or,
D.: Consolidation of unorganized point clouds for surface
reconstruction.
ACM Transactions on Graphics 28(5)
(2009)
22. Huesmann, M.: Optimal transport between random
measures. ArXiv e-prints (2012)
23. Jenke, P., Wand, M., Straßer, W.: Patch-graph re-
construction for piecewise smooth surfaces.
Vision,
modeling, and visualization 2008: proceedings p. 3 (2008)
24. Kazhdan, M., Bolitho, M., Hoppe, H.: Poisson surface
reconstruction.
In:
Eurographics
Symposium
on
Geometry Processing, SGP ’06, pp. 61–70 (2006)
25. Kolluri, R., Shewchuk, J.R., O’Brien, J.F.: Spectral
surface reconstruction from noisy point clouds.
In:
Eurographics Symposium on Geometry Processing, pp.
11–21 (2004)
26. Labatut, P., Pons, J.P., Keriven, R.: Robust and eﬃcient
surface reconstruction from range data.
Computer
Graphics Forum 28(8), 2275–2290 (2009)
27. Lindstrom, P., Turk, G.: Evaluation of memoryless
simpliﬁcation. IEEE Transactions on Visualization and
Computer Graphics 5(2), 98–115 (1999)


--- Page 15 ---

14
Julie Digne et al.
28. Lipman, Y., Cohen-Or, D., Levin, D.: Data-dependent
MLS for faithful surface approximation. In: Eurographics
Symposium on Geometry Processing, p. 67 (2007)
29. Lipman, Y., Cohen-Or, D., Levin, D., Tal-Ezer, H.:
Parameterization free projection for geometry recon-
struction.
ACM Transactions on Graphics 26(3), 22
(2007)
30. Lipman, Y., Daubechies, I.: Surface comparison with
mass transportation (2010). ArXiv preprint 0912.3488
31. McAsey, M., Mou, L.: Optimal locations and the mass
transport
problem.,
pp.
131–148.
Providence,
RI:
American Mathematical Society (1999)
32. M´erigot, Q.: A multiscale approach to optimal transport.
Computer Graphics Forum 30(5), 1583–1592 (2011)
33. M´erigot,
Q.,
Ovsjanikov,
M.,
Guibas,
L.:
Robust
Voronoi-based curvature and feature estimation. In: 2009
SIAM/ACM Joint Conference on Geometric and Physical
Modeling, pp. 1–12 (2009)
34. Ohtake, Y., Belyaev, A., Alexa, M., Turk, G., Seidel,
H.P.: Multi-level partition of unity implicits. In: ACM
SIGGRAPH, vol. 22(3), pp. 463–470 (2003)
35. Oztireli,
C.,
Guennebaud,
G.,
Gross,
M.:
Feature
preserving point set surfaces based on non-linear kernel
regression. In: Computer Graphics Forum, vol. 28(2), pp.
493–501 (2009)
36. Pang, X.F., Pang, M.Y.: An algorithm for extracting
geometric features from point cloud.
International
Conference
on
Information
Management,
Innovation
Management and Industrial Engineering 4, 78–83 (2009)
37. Pauly, M., Keiser, R., Gross, M.: Multi-scale feature
extraction
on
point-sampled
surfaces.
Computer
Graphics Forum 22(3), 281–289 (2003)
38. Peyr´e, G., Fadili, J., Rabin, J.: Wasserstein active
contours. Tech. rep., Preprint Hal-00593424 (2011). URL
http://hal.archives-ouvertes.fr/hal-00593424/
39. Rabin, J., Delon, J., Gousseau, Y.: Regularization of
transportation maps for color and contrast transfer. In:
IEEE International Conference on Image Processing, pp.
1933 –1936 (2010)
40. Rabin, J., Delon, J., Gousseau, Y.: Transportation
distances on the circle. J. Math. Imaging Vis. 41(1-2),
147–167 (2011)
41. Rabin, J., Peyr´e, G., Cohen, L.D.: Geodesic shape
retrieval via optimal mass transport.
In: European
Conference on Computer Vision: Part V, ECCV’10, pp.
771–784. Springer-Verlag, Berlin, Heidelberg (2010)
42. Rabin, J., Peyr´e, G., Delon, J., Bernot, M.: Wasserstein
barycenter and its application to texture mixing.
In:
A. Bruckstein, B. ter Haar Romeny, A. Bronstein,
M. Bronstein (eds.) Scale Space and Variational Methods
in Computer Vision, Lecture Notes in Computer Science,
vol. 6667, pp. 435–446. Springer Berlin / Heidelberg
(2012)
43. Reinhard, E., Ashikhmin, M., Gooch, B., Shirley, P.:
Color transfer between images. IEEE Comput. Graph.
Appl. 21(5), 34–41 (2001)
44. Rubner, Y., Tomasi, C., Guibas, L.J.: The earth mover’s
distance as a metric for image retrieval. Int. J. Comput.
Vision 40(2), 99–121 (2000)
45. Salman, N., Yvinec, M., M´erigot, Q.: Feature Preserving
Mesh Generation from 3D Point Clouds. In: Computer
Graphics Forum, vol. 29, pp. 1623–1632 (2010)
46. Villani, C.: Topics in Optimal Transportation. American
Mathematical Society (2010)
47. Walder, C., Chapelle, O., Sch¨olkopf, B.: Implicit surface
modelling as an eigenvalue problem.
In: Machine
Learning ICML 2005, pp. 936–939 (2005)
48. Wu, J., Kobbelt, L.: Fast mesh decimation by multiple-
choice techniques. In: Vision, Modeling, and Visualiza-
tion, pp. 241–248 (2002)
View publication stats
```

---

## Section 1.20: Framework to Vectorize Digital Artworks for Physical Fabrication based on Geometric Stylization Techniques

Source File: Framework to Vectorize Digital Artworks for Physical Fabrication based on Geometric Stylization Techniques.txt

### Paper Content:

```
# Framework to Vectorize Digital Artworks for Physical Fabrication based on Geometric Stylization Techniques.pdf
# Converted: 2025-07-19 12:45:28
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Framework to Vectorize Digital Artworks for Physical Fabrication based on Geometric Stylization Techniques.pdf
# Output: ../layer2_completion/txt/Framework to Vectorize Digital Artworks for Physical Fabrication based on Geometric Stylization Techniques.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/374448489
Framework to Vectorize Digital Artworks for Physical Fabrication based on
Geometric Stylization Techniques
Conference Paper · September 2023
CITATIONS
0
READS
556
1 author:
Lisa Ihde
Hasso Plattner Institute
10 PUBLICATIONS   11 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Lisa Ihde on 05 October 2023.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

cba
Gesellschaft für Informatik. (Hrsg.): SKILL Konferenz,
Lecture Notes in Informatics (LNI), Gesellschaft für Informatik, Bonn 2023 1
Framework to Vectorize Digital Artworks for Physical
Fabrication based on Geometric Stylization Techniques
Lisa Ihde1
Abstract: Imitation and reproduction of artistic styles using Image-based Artistic Rendering (IB-AR)
techniques such as oil painting or pencil hatching, became quite popular in recent years. The
development of graphics hardware and software laid the foundation for the development of techniques,
algorithms, and applications to eﬃciently apply such eﬀects to user-generated data visual media, such
as photos or videos. However, the high-quality physical reproduction of the resulting images, for
example, using canvas printing based on real pencils or brushes, introduces major challenges with
respect to precision and visual ﬁdelity. Speciﬁcally, reproduction devices such as pen plotters require
geometry represented as vector graphics, i.e., geometric primitives (points, lines, or polygons) as
input data format, which cannot be synthesized using IB-AR techniques in a straightforward approach.
The paper describes the analysis, design, and implementation of a framework for Geometry-based
Stylization Techniques (GSTs). GSTs denote image stylization techniques that generate geometric
primitives as resulting representations comprising an artistic style.
Keywords: Image Stylization; Vectorization; Image Processing; GST; Fabrication; Plotting; Line Art
1
Introduction
Imagine the remarkable potential of eﬀortlessly recreating the style of artistic masterpieces
akin to those painted by renowned artists (e. g., Wassily Kandinsky or Picasso). In recent years,
research has focused on imitating artistic techniques to generate such digital artworks using
Image-based Artistic Rendering (IB-AR) [Ky13, De13, SID17]. The resulting digital images
enjoy great popularity and are increasingly used in various domains and applications [Se16].
For example, the mobile application BeCasso2 enables the generation of user-created images
that evoke the essence of oil paintings, pencil hatching, or watercolor.
While the visual quality of these results closely resembles the original techniques, they lack
certain qualities during reproduction using non-digital media (e. g., using canvas printing).
For example, when printing with ink, the same properties are not obtained as when using a
brush with watercolors, or pencils. These visual diﬀerences in texture mean that plasticity,
for example, a three-dimensional eﬀect of the material, can hardly be achieved.
Therefore, devices such as pen plotters allow the use of a variety of pens, for example, pencils,
ball pens, or crayons. The problem is that IB-AR is deﬁned for artistic rendering primitives
1 Hasso Plattner Institute, Digital Engineering Faculty, University of Potsdam, Prof.-Dr.-Helmert-Str. 2-3, 14482
Potsdam, Germany, lisa.ihde@student.hpi.uni-potsdam.de
2 https://digitalmasterpieces.com/becasso/


--- Page 3 ---

2 Lisa Ihde
Fig. 1: Vectorized halftoning result of an input image showing gummy bears represented by 6,784
diﬀerent elements (’L’,’I’,’S’,’A’) generated from the presented framework. The vector graphic is
engraved and cut in poplar wood with a laser cutter. Therefore, the result is divided into background
(engraved) and foreground (cut). The wood artwork is complemented with a rainbow backlight.
(regions, strokes, stipples, or tiles) and not for geometric primitives [Ky13]. Vector graphics
are a description of a scene using geometric primitives such as points, lines, or polygons.
These vector graphics can be read by machines such as laser cutters [MB15], pen plotters,
programmable embroidery machines, or computerized numerical control (CNC) machines.
This paper describes the development of a software framework that enables the creation of
stylized images using geometric primitives. These results can then be produced in a new
way by devices such as pen plotters, or laser cutters (Fig. 1).
2
Related Work
Geometry-based Stylization Techniques (GSTs) [Ih22] denote image stylization techniques
that output a number of geometric primitives and their appearance attributes (e. g., color,
outline, or texture). GSTs oﬀer the possibility to represent geometric abstraction artists.
In addition, the combination of GSTs and traditional IB-AR techniques, can enable new
so-called Fused Stylization Techniques (FSTs).
Tracing is the process of generating a vector representation from a raster image input. The
existing approaches for the generation of vector graphics vary between pixel-based [AT04] to
line-based tracing [DK09, OG11, No13].There is only a little research about more complex


--- Page 4 ---

Framework to Vectorize Digital Artworks for Physical Fabrication based on GSTs 3
AutoTrace
Potrace
ImageTracer
VTracer
Example
Multi-color
yes
no
yes
yes
Performance
4870ms
(125 colors)
170ms
(2 colors)
1410ms
(125 colors)
3860ms
(125 colors)
Language
C
C
Java
Rust
License
GNU General
Public License
GNU General
Public License
Unlicense
MIT License
Released
1998
2001
2015
2020
Creator
Martin Weber
Peter Selinger
András Jankovics
Tsang Hao Fung
Cross-platform
yes
yes
yes
yes
Tab. 1: Comparison of open-source libraries for image tracing.
vectorization of IB-AR techniques. Glöckner et al. introduced an optimized vectorization of
GPU-based stylized images using intermediate data representations [Gl20]. The approach
enables interactive manipulation of parameters and supports various stylization pipelines.
There are machine learning-based vectorization methods[Si16, Ki18, Fa17, Re21], but these
are not suitable for tracing complex image stylization eﬀects. For example, Reddy et al.
focus on fonts and clipart rather than general user-generated images.
There are several tools and libraries for tracing images, but only a few are free and open-
source. The software Inkscape3 and web application SVGcode4 enable the generation
of vector graphics from a raster graphic. Both are based on the tracer library Potrace5.
Besides Potrace, Inkscape oﬀers other tracing methods by using the libraries AutoTrace6 and
libdepixelize [KL11]. The latter is designed to work with pixel-art images. The performance
of the approach depends on the size of the input image, which is why this library cannot be
applied to high-resolution user-generated images.
For comparison, two other libraries are explored in addition to Potrace and AutoTrace:
ImageTracer7 and VTracer8. The hardware used in the evaluation (Tab. 1) consists of an
3 https://inkscape.org/
4 https://svgco.de/
5 https://potrace.sourceforge.net/
6 https://autotrace.sourceforge.net/
7 https://github.com/jankovicsandras/imagetracerjava
8 https://www.visioncortex.org/vtracer/


--- Page 5 ---

4 Lisa Ihde
Intel(R) Core(TM) i7-8550U CPU, 16 GB RAM, and Intel(R) UHD Graphics 620. The
Windows 11 Home operating system was used to run and compile the libraries, and the
input image was taken from the benchmark dataset [MR17] with a resolution of 1024×685
pixels.
With regard to the approach of the paper and the support of colored input images, the Potrace
algorithm does not provide an option to generate a colored vector output. When comparing
the results of the other three libraries, it is noticeable that the AutoTrace library generates
gaps between the color areas. In addition, the runtime of this algorithm is the slowest.
ImageTracer has the best runtime and provides the best visual result because details such
as the clouds were recognized in contrast to VTracer. VTracer is the best of the presented
algorithms at detecting 125 colors and is the most recently released library from 2020. In
addition, cross-platform use is supported by all libraries.
3
Implementation
Generating a vector-based representation from a stylized image is a complex challenge. The
straight-forward approach to obtain a vector image from a raster image in general would be
to vectorize the image by using one of the introduced tracing algorithms. Unfortunately,
the tracer does not have any further information about the input image, nor does it have
a speciﬁc approach suitable for the use case. In addition, the input image is also stylized,
which then makes it even more diﬃcult for the tracer to detect the actual layers and
primitives. Therefore, the framework uses the rendering pass of an eﬀect to generate the
mentioned intermediate representations [Gl20] that facilitate and improve the quality of
vectorization. The framework fundamentally handles three data representations: (1) raster-
data representations, (2) vector-data representations, and (3) intermediate representations.
Additionally, a library is used introduced by Griese and designed for the execution of
IB-AR techniques based on Visual Computing Assets (VCAs)[Gr23]. In order to generate
vector-based representations for the VCAs, the library has to be extended to use a sampler
as input and generate an Scalable Vector Graphics (SVG) as output. The architecture of the
framework is composed of four components: Model, View, Controller, and Processing.
Model Component: The model includes selectable algorithms for shape abstraction,
segmentation, and shape colorization. It holds an instance of a VCA document [Gr23],
and other data structures (e. g., list of geometric primitives).
View Component: The view performs the visual representation of the components from
the model, for example, retrieving the associated parameter of the VCA document, or
displaying the selected color palette. Therefore, it provides the interaction interface
for the user to change settings (e. g., values of parameters) and pass inputs to the
model. The model responds to execution requests and updates the view accordingly.


--- Page 6 ---

Framework to Vectorize Digital Artworks for Physical Fabrication based on GSTs 5
Fig. 2: Overview of the graphical user interface with the diﬀerent interaction options. (A) Menu bar
with diﬀerent options to import and export raster- or vector-based graphic ﬁles. (B) Provides input data
as color, depth, normal, mask, and segments. (C) Select diﬀerent image segmentation techniques and
execute them. (D) Showing the diﬀerent groups consisting of shapes describing the scene structure
with the geometric primitives. (E) Tabs for diﬀerent scenes, for example, generating new input images,
or not combining with shapes in the current scene. (F) Tabs to switch between scenes. (G) Area
to display scene with geometric primitives. (H) Toolbox to select diﬀerent algorithms (e. g., Image
Texture Brush) to change the ﬁlling or outline of the selected geometric primitives. (I) Select canvas
texture applied to the background of the scene. (J) Select a color palette. (K) Select GST and execute
it. (L) Optional showing parameter for selected GST. (M) Select a VCA as FST and execute it. (N)
Showing associated parameter of the selected VCA.
Controller Component: The controller coordinates, for example, the active VCA document
to process VCA-based GSTs. The component also receives the user input (e. g.,
parameter changes) and forwards the information to the right place so that the
corresponding view can be updated.
Processing Component: The processing component provides access to the library interface.
It includes an adapter to encapsulate and wrap the access points.
The framework provides the essential Vectorizer class based on a ported version of the
ImageTracer library. The class includes some modiﬁcations to support diﬀerent color palettes
and to add a custom header at the beginning of the generated SVG ﬁle. Additionally, the
library is extended by ﬁve pass processor classes to process an instance of the corresponding
following pass classes.


--- Page 7 ---

6 Lisa Ihde
VectorizationPass: The class deﬁnes a pass to vectorize a sampler input.
SkeletonizePass: The class deﬁnes a pass to skeletonize lines in a sampler input.
VectorizeEdgePass: The class deﬁnes a pass to vectorize lines in a sampler input that
might have applied the SkeletonizePass ﬁrst.
VectorizationMergePass: The class deﬁnes a pass to add the SVG header, or merge SVGs
(e. g., merge the outputs of VectorizeEdgePass and VectorizationPass).
VectorElementPlacerPass: The class deﬁnes a pass to place vector graphic elements
deﬁned in a sampler input (e. g., used for half-toning [DI13]).
The graphical user interface (Fig. 2) is implemented using the Qt9 framework. The central
widget is a QTabWidget (E) for each scene. The QTabWidget consists of two tabs (F) to
display the GST and FST results. On the left and right are dock widgets to provide access to
the loaded input data (B), segmentation algorithm (C), scene structure (D), GST algorithm
(K), and FST algorithm (M). On the top is the menu bar as QMenuBar (A), for example, to
load new input data. On the bottom is a toolbar as QToolBox with diﬀerent views (H,I,J), for
example, to change the color of geometric primitives (QGraphicsItems).
4
Evaluation
For feasibility, the developed framework provides the implementation of several applications,
including hope style [CM09], halftoning [DI13], cartoon [WOG06]), pencil hatching [Pr01],
shape packing [CS03], but also the combination of GSTs and existing IB-AR techniques.
To implement each application for diﬀerent GSTs, concepts were developed on how the
quality of vectorization can be enhanced compared to automatic tracing, for example by
using cloning/instancing, grouping, detection of lines and line thickness, color gradients on
lines, blend-mode, and other approaches.
The generated vector graphics are physically reproduced. Therefore, diﬀerent ways of repro-
duction are identiﬁed and evaluated, for example, drawing, engraving, gluing, embossing,
cutting, or scratching. The devices used are a Trotec laser cutter Speedy 360 and a Silhouette
Curio plotter.
Vectorized Halftoning
For the physical production of vectorized halftoning, various properties of the vector graphic
can be set. For example, the selected shape of the halftone element, the size, the line
thickness, the ﬁll, and other settings should be taken into account. In Fig. 3 the result of
the vectorized halftone image consists of solid-ﬁlled circles. The device used will be the
9 https://www.qt.io/


--- Page 8 ---

Framework to Vectorize Digital Artworks for Physical Fabrication based on GSTs 7
(a) Yellow grid fabricated with orange Stabilo pen.
(b) Magenta grid fabricated with red Stabilo pen.
(c) Cyan grid fabricated with blue Stabilo pen.
(d) Key Grid fabricated with black Stabilo pen.
Fig. 3: Vectorized halftoning result with 31.720 halftone elements. It was fabricated with the Silhouette
Curio plotter using four Stabilo pens having the colors orange, red, blue, and black.
Silhouette plotter and thus the drawing area is approximately A5. Since a Stabilo pen has
a tip of 0.4mm, the smallest element will be at least that size. The plotter cannot be used
to draw all colors at once since there is no support for drawing with four pens at once.
Therefore, during the implementation, one color layer is drawn after another, layer by layer.
Drawing the 31.720 elements takes a total of almost 4 hours.
Fig. 1 shows how four diﬀerent shapes for the halftone element are displayed at once, which
have the four letters ’L’, ’I’, ’S’, and ’A’ deﬁned as a path. In addition, only one grid of the
four Cyan-Magenta-Yellow-Key (CMYK) layers is used to be fabricated. Additionally, the
segmented image distinguishes between the engraved (i. e., background) and cut area (i. e.,
gummy bears). The minimum size of a letter to choose for this device is about 5mm. This
gives a total area for the 6,784 elements on the wood panel of 32cm × 52cm. Then, for
example, an LED strip can be attached as a backlight and the artwork can be used as a wall
decoration.


--- Page 9 ---

8 Lisa Ihde
(a) Vectorized toon applied with glue roller pen of 0.7mm
tip width and glitter on paper.
(b) Vectorized toon result engraved in metal sheet with
Silhouette plotter and stippling tool of 0.1mm tip width.
Fig. 4: Physical reproduction of toon vectorization edges by applying glue roller pen with glitter, or
stippling tool.
Vectorized Toon
Two diﬀerent tools were used to create the vectorized toon eﬀect with the Silhouette plotter.
Fig. 4b shows the result with the stippling tool on a metal sheet. Therefore, the contact
pressure of the stippling tools needs to be chosen high and the minimum size of a polygon
should be at least the tip width (0.1mm). Fig. 4a draws the vectorized edges layer with a
glue roller pen by Sakura Quickie and then sprinkles glitter over it, which sticks to the glued
lines. The tip of the glue roller pen has a line width of 0.7mm and is thus almost twice as
wide as a Stabilo pen. Thus, the smallest detail and thus the minimum width of an edge has
to be set to 0.7mm.
Vectorized Hope Style
The vectorized hope style consists of diﬀerent color layers with diﬀerent ﬁllings (e. g., solid,
and texture). For physical reproduction, the vector graphic result is cut with the blade tool
along the edges of each polygon. Therefore, the result of the vectorized hope style was
printed on diﬀerent paper sheets. In this way, the exact color palette can be used instead of
letting the plotter draw everything with not matching colors. Then, each color layer was
cut out with the plotter and the blade tool along the edges. The cut pieces should have a
minimum size to recognize it as a polygon, and to use it in the next step. The minimum
size of a polygon was set to 3mm. After having all polygons cut out for each color layer,
they will be stacked on top of each other with adhesive pads. Fig. 5 shows the result, which
creates a 3D card eﬀect.


--- Page 10 ---

Framework to Vectorize Digital Artworks for Physical Fabrication based on GSTs 9
(a) Physical reproduction of hope style with adhesive
pads (3mm) to stack the layers.
(b) View on the physical reproduction of hope style
as stacked artwork from the front.
Fig. 5: Vectorized Hope Style result of a sunﬂower image with cut layers and stacked as 3D eﬀect
with adhesive pads.
Vectorized Pencil Hatching
For implementation, the vector graphic result of the vectorized pencil hatching should be
used to draw with a pencil on paper to achieve an artwork that seems to be drawn by hand.
The challenge of drawing with a pencil is that the tip will be shortened by using it on paper.
That behavior would inﬂuence the pressure strength and visibility of the drawn lines.
One solution could be to pause the plotter after a certain time, sharpen the pencil, and
reassemble it in the pen holder in a lower position. This procedure would be time-consuming
and uncertain to execute. Therefore, a pencil with a stronger tip is needed to avoid the
complicated process. There are pencils with a metal tip that are made for long-term usage
with not such high frequency to sharpen it based on an anodized aluminum barrel. This is
because the tip rubs against the paper, causing the alloyed metal molecules to break down
and remain on the paper surface. Fig. 6a shows the result of the implementation with such
an inkless metal pencil. The lines are more lighten than the usual pencil result, but the metal
pencil achieved the same coverage all over the paper sheet.
Alternatively, a Stabilo pen can be used, which traces the edge paths of the vector graphic
and will have the same distance to the paper sheet. The physical circumstances could still
inﬂuence the result. Fig. 6b shows at the bottom right corner that the color of the pen has
faded and therefore it was not drawn completely.


--- Page 11 ---

10 Lisa Ihde
(a) Plotted pencil hatching result using an inkless metal
pencil.
(b) Plotted pencil hatching result using a Stabilo pen.
Fig. 6: Comparison of vectorized pencil hatching as a plotted result implemented using inkless metal
pencil and Stabilo pen with a Silhouette plotter.
Segment-based Shape Packing
Fig. 7: Scan of shape packing result as scratch
artwork implemented with a Silhouette plotter.
Image segmentation is used to apply, for exam-
ple, diﬀerent shape types, or shape sizes on each
segment. In Fig. 7, the vector graphic result was
produced with the Silhouette plotter on scratch
paper. Therefore, the ﬁlling is set to transparent,
and the outlines are set to 0.5mm. The stipple
tool for scratching on scratch paper was used
again for the implementation making the rain-
bow layer visible. The minimum thickness of
the lines is determined by the stipple tool with
at least 0.1mm.
5
Conclusions
The applications implemented showcase the potential of the framework and its possible
physical reproduction, which helps to understand the framework’s versatility and applicability.
The framework provides interaction techniques for editing and modifying resulting vector-
based and raster-based representations.
The plotter tools used have a ﬁxed physical width that limits the production with the plotter.
Thus, increasing the canvas size, shape size, or outline width is necessary. Moreover, the
properties in the SVG representing the paths do not cover every desirable aspect. Therefore,


--- Page 12 ---

Framework to Vectorize Digital Artworks for Physical Fabrication based on GSTs 11
workarounds speciﬁc to each GST are required. For instance, for pencil hatching, the line
segments can be grouped by color to be individually selectable. This way, each color could
be drawn one after the other with the corresponding colored pencil. However, this approach
increases the reproduction time. More plotters from diﬀerent manufacturers should be
tested to deﬁne settings for the minimum size of geometric primitives, or the minimum
line thickness. Thus, physical production can then provide the best result. Furthermore,
materials such as brushes with acrylic or oil paints could be explored, as also strategies on
how to develop a suitable vector representation for them.
6
Acknowledgements
This paper is based on a master’s thesis written at the Computer Graphics Systems group
of Prof. Dr. Jürgen Döllner located at the Digital Engineering Faculty of the University
Potsdam and Hasso Plattner Institute. This work was supported by the Federal Ministry of
Education and Research (BMBF), Germany (mdViPro,01IS18092). Special thanks go to Dr.
Matthias Trapp, who acted as supervisor of the master’s thesis.
Bibliography
[AT04]
Antoniou, Vyron; Tsoulos, Lysandros: Converting raster images to XML and SVG. In:
SVG Open. 01 2004.
[CM09]
Cartwright, Lisa; Mandiberg, Stephen: Obama and Shepard Fairey: The Copy and Political
Iconography in the Age of the Demake. Journal of Visual Culture, 8:172–176, 11 2009.
[CS03]
Collins, Charles R.; Stephenson, Kenneth: A circle packing algorithm. Computational
Geometry, 25(3):233–256, 2003.
[De13]
Dev, Kapil: Mobile Expressive Renderings: The State of the Art. IEEE Computer Graphics
and Applications, 33(3):22–31, 2013.
[DI13]
Deussen, Oliver; Isenberg, Tobias: Halftoning and Stippling. In (Rosin, Paul; Collomosse,
John, eds): Image and Video-Based Artistic Stylisation, pp. 45–61. Springer London,
London, 2013.
[DK09]
Dhawan, Sanjeev; Kumar, Rakesh: Benchmarking: An Interactive Tool for Vectorization of
Raster Images. In: Proceedings of the 2Nd WSEAS. SENSIG’09/VIS’09/MATERIALS’09,
World Scientiﬁc and Engineering Academy and Society (WSEAS), Stevens Point, Wiscon-
sin, USA, pp. 112–126, 2009.
[Fa17]
Faraj, Noura; Xia, Gui-Song; Delon, Julie; Gousseau, Yann: A Generic Framework for the
Structured Abstraction of Images. In: Proceedings of the Symposium on Non-Photorealistic
Animation and Rendering. NPAR ’17, Association for Computing Machinery, New York,
NY, USA, 2017.
[Gl20]
Glöckner, D.-Amadeus J.; Ihde, Lisa; Döllner, Jürgen; Trapp, Matthias: Intermediate
Representations for Vectorization of Stylized Images. J.WSCG, 28(1-2):187–196, 2020.


--- Page 13 ---

12 Lisa Ihde
[Gr23]
Griese, Erik: , Analysis, Design and Implementation of an Interoperable Software Library
for Image and Video Processing, 3 2023.
[Ih22]
Ihde, Lisa; Semmo, Amir; Döllner, Jürgen; Trapp, Matthias: Design Space of Geometry-
based Image Abstraction Techniques with Vectorization Applications. volume 30, pp.
99–108, 2022.
[Ki18]
Kim, Byungsoo; Wang, Oliver; Öztireli, Cengiz; Gross, Markus: Semantic Segmentation for
Line Drawing Vectorization Using Neural Networks. 37(2):329 – 338, 5 2018. 39th Annual
Conference of the European-Association-for-Computer-Graphics (EUROGRAPHICS);
Conference Location: Delft, NETHERLANDS; Conference Date: APR 16-20, 2018.
[KL11]
Kopf, Johannes; Lischinski, Dani: Depixelizing Pixel Art. In: ACM SIGGRAPH 2011
Papers. SIGGRAPH ’11, Association for Computing Machinery, New York, NY, USA,
2011.
[Ky13]
Kyprianidis, Jan Eric; Collomosse, John; Wang, Tinghuai; Isenberg, Tobias: State of
the Ärt": A Taxonomy of Artistic Stylization Techniques for Images and Video. IEEE
Transactions on Visualization and Computer Graphics, 19(5):866–885, 2013.
[MB15]
Mueller, Stefanie; Baudisch, Patrick: Laser cutters. interactions, 22:72–74, 08 2015.
[MR17]
Mould, David; Rosin, Paul: Developing and applying a benchmark for evaluating image
stylization. Computers & Graphics, 67, 06 2017.
[No13]
Noris, Gioacchino; Hornung, Alexander; Sumner, Robert W.; Simmons, Maryann; Gross,
Markus: Topology-driven Vectorization of Clean Line Drawings. ACM Trans. Graph.,
32(1):4:1–4:11, February 2013.
[OG11]
Olsen, Sven; Gooch, Bruce: Image Simpliﬁcation and Vectorization. In: Proceedings of
the ACM SIGGRAPH/Eurographics Symposium on Non-Photorealistic Animation and
Rendering. NPAR ’11, ACM, New York, NY, USA, pp. 65–74, 2011.
[Pr01]
Praun, Emil; Hoppe, Hugues; Webb, Matthew; Finkelstein, Adam: Real-Time Hatching.
In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive
Techniques. SIGGRAPH ’01, Association for Computing Machinery, New York, NY,
USA, p. 581, 2001.
[Re21]
Reddy, Pradyumna; Gharbi, Michael; Lukac, Michal; Mitra, Niloy J.: , Im2Vec: Synthesiz-
ing Vector Graphics without Vector Supervision, 2021.
[Se16]
Semmo, Amir; Dürschmid, Tobias; Trapp, Matthias; Klingbeil, Mandy; Döllner, Jürgen;
Pasewaldt, Sebastian: Interactive Image Filtering with Multiple Levels-of-control on
Mobile Devices. In: Proceedings of ACM SIGGRAPH ASIA 2016 Mobile Graphics and
Interactive Applications (MGIA). ACM, New York, pp. 2:1–2:8, 2016.
[Si16]
Simo-Serra, Edgar; Iizuka, Satoshi; Sasaki, Kazuma; Ishikawa, Hiroshi: Learning to
Simplify: Fully Convolutional Networks for Rough Sketch Cleanup. ACM Trans. Graph.,
35(4), July 2016.
[SID17]
Semmo, Amir; Isenberg, Tobias; Döllner, Jürgen: Neural Style Transfer: A Paradigm Shift
for Image-based Artistic Rendering? In: Proceedings of Non-Photorealistic Animation
and Rendering (NPAR). ACM, New York, pp. 5:1–5:13, 2017.
[WOG06] Winnemöller, Holger; Olsen, Sven C.; Gooch, Bruce: Real-time Video Abstraction. ACM
Trans. Graph., 25(3):1221–1226, July 2006.
View publication stats
```

---

## Section 1.21: Graph based over-segmentation methods for 3d point clouds

Source File: Graph based over-segmentation methods for 3d point clouds.txt

### Paper Content:

```
# Graph based over-segmentation methods for 3d point clouds.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Graph based over-segmentation methods for 3d point clouds.pdf
# Output: ../layer2_completion/txt/Graph based over-segmentation methods for 3d point clouds.txt


--- Page 1 ---

Graph Based Over-Segmentation Methods for 3D Point
Clouds
Yizhak Ben-Shabat · Tamar Avraham · Michael Lindenbaum ·
Anath Fischer
Received: date / Accepted: date
Abstract Over-segmentation, or super-pixel generation,
is a common preliminary stage for many computer vi-
sion applications. New acquisition technologies enable
the capturing of 3D point clouds that contain color
and geometrical information. This 3D information in-
troduces a new conceptual change that can be utilized
to improve the results of over-segmentation, which uses
mainly color information, and to generate clusters of
points we call super-points. We consider a variety of
possible 3D extensions of the Local Variation (LV) graph
based over-segmentation algorithms, and compare them
thoroughly. We consider diﬀerent alternatives for con-
structing the connectivity graph, for assigning the edge
weights, and for deﬁning the merge criterion, which
must now account for the geometric information and
not only color. Following this evaluation, we derive a
Y. Ben-Shabat
Mechanical Engineering Department, Technion - Israel Insti-
tute of Technology, Haifa 32000, Israel.
Tel.: +972-4-829-2334
E-mail: sitzikbs@campus.technion.ac.il
T. Avraham
Computer Science Department, Technion - Israel Institute of
Technology, Haifa 32000, Israel.
Tel.: +972-4-829-4877
E-mail: tammya@cs.technion.ac.il
M. Lindenbaum
Computer Science Department, Technion - Israel Institute of
Technology, Haifa 32000, Israel.
Tel.: +972-4-829-4331
Fax.: +972-4-829-3900
E-mail: mic@cs.technion.ac.il
A. Fischer
Mechanical Engineering Department, Technion - Israel Insti-
tute of Technology, Haifa 32000, Israel.
Tel.: +972-4-829-3260
Fax.: +972-4-829-5711
E-mail: meranath@technion.ac.il
new generic algorithm for over-segmentation of 3D point
clouds. We call this new algorithm Point Cloud Local
Variation (PCLV). The advantages of the new over-
segmentation algorithm are demonstrated on both out-
door and cluttered indoor scenes. Performance analysis
of the proposed approach compared to state-of-the-art
2D and 3D over-segmentation algorithms shows signif-
icant improvement according to the common perfor-
mance measures.
Keywords 3D Point Cloud Over-segmentation · 3D
Point Cloud segmentation · Super-points · Grouping
1 Introduction
1.1 Overview
Image segmentation methods aim to divide the input
data according to object association. This inverse prob-
lem is often ill-posed. It is therefore common to relax
the requirement, and aim for an over-segmentation in
which the number of segments can be greater than the
number of objects in the scene. The goal is then to
get a rather small set of segments that do not cross ob-
ject boundaries; an over-segmentation algorithm is con-
sidered to perform well when the segment boundaries
overlap with the ground truth object boundaries while
also partially covering only a single object’s area. Over-
segmentation can be considered as a compact and in-
formative description of the scene, using a substantially
lower number of elements than the initial pixel repre-
sentation. It signiﬁcantly reduces the amount of data
that must be dealt with for the following image analy-
sis stages, without loss of information. The output ele-
ments of the over-segmentation process are commonly
called super-pixels. Super-pixels were shown to be very
arXiv:1702.04114v1  [cs.CV]  14 Feb 2017


--- Page 2 ---

2
Yizhak Ben-Shabat et al.
useful as inputs for higher-level vision tasks such as se-
mantic segmentation (Gould et al, 2008; Farabet et al,
2013), scene classiﬁcation (Juneja et al, 2013), 3D ge-
ometry inference (Hoiem, 2007), and tracking (Oron
et al, 2014; Wang et al, 2011).
In recent years, 3D point cloud representation of
geometric real-world data has gained popularity in the
ﬁelds of robotics, computer vision, and computer aided
design. This is mainly due to the emergence of low cost
3D sensing devices such as the Microsoft Kinect for in-
door scenes and the more expensive LiDAR for outdoor
scenes. Many point cloud based algorithms are being
developed for applications such as navigation, object
recognition, pose estimation, registration, and surface
reconstruction. One of the main challenges when work-
ing with 3D point clouds is the large number of points.
This challenge becomes even greater for real-time ap-
plications. One way to deal with the large amount of
data is to divide it into subsets and process them sepa-
rately. The data can be subdivided by a 3D regular grid,
but data-driven subdivision that relies on 3D properties
might be much more eﬀective.
Many over-segmentation methods have been pro-
posed for 2D images, while very limited research has
been done on adapting the over-segmentation approach
for 3D point clouds. We focus here on extensions of one
of the leading 2D over-segmentation algorithms, Lo-
cal Variation (LV) by Felzenszwalb and Huttenlocher
(1998, 2004). A few works incorporated 3D extensions
for it in diﬀerent applications. However, none of these
works compared the proposed extension to possible al-
ternatives. Here we propose, for each step of the original
algorithm, options for extending it to 3D point clouds,
and discuss the pros and cons of each. We thoroughly
test and compare each option and select the best com-
bination. We call this new algorithm Point Cloud Local
Variation (PCLV). Like the original algorithm, PCLV is
simple and fast yet powerful. The algorithm is generic
and can be applied on any data represented by a 3D
point cloud, regardless of its geometrical complexity or
the sensor used for its acquisition. We show that PCLV
performs better than state-of-the-art 2D methods and
better than all previously suggested 3D methods on an
extensive benchmark of indoor Kinect data. Addition-
ally, we demonstrate its performance on urban scenes
captured by an interferometry sensor. Our main contri-
butions are:
• A class of 3D over-segmentation algorithms, ob-
tained by applying the LV principles with diﬀerent
choices.
• The PCLV algorithm, obtained by optimizing over
all choices, which outperforms state-of-the-art 2D
and 3D methods.
• A thorough performance analysis of 3D point cloud
extensions of the LV algorithm.
• A performance analysis of existing 2D and 3D over-
segmentation algorithms on a large indoor dataset.
• Qualitative and promising results for new outdoor
scenes.
The paper is structured as follows. The rest of
this section overviews existing 2D and 3D over-
segmentation algorithms. Section 2 presents the 3D
point cloud over-segmentation algorithm analysis, and
its results are summarized in Section 3. The PCLV al-
gorithm is presented in Section 4. Experimental results
and comparison to state-of-the art algorithms are pre-
sented in Section 5 along with some implementation
details. Finally, Section 6 discusses the results and con-
cludes.
1.2 Related Work
In this section, we brieﬂy overview existing over-
segmentation methods for 2D images and 3D point
clouds.
1.2.1 2D Over-Segmentation Methods (Super-Pixel
Generation)
A short survey and comparison of existing 2D over-
segmentation methods can be found in Achanta et al
(2012). Classic algorithms include Graph-cut (Malik,
2000), Meanshift (Comaniciu and Meer, 2002), and Wa-
tershed (Meyer, 1994). Current state-of-the-art over-
segmentation methods include the Turbopixels algo-
rithm (TP) (Levinshtein et al, 2009), which evolves a
set of curves that converge to the segment borders us-
ing a level set approach, and the Simple Linear Iterative
Clustering (SLIC) algorithm (Radhakrishna et al, 2010;
Achanta et al, 2012), which is based on the k-means
clustering algorithm. One of the best performing yet
eﬃcient over-segmentation methods is the graph based
Local Variation (LV) algorithm (Felzenszwalb and Hut-
tenlocher, 1998, 2004). A connectivity graph is con-
structed by connecting each pixel to its 4 or 8 clos-
est neighbors. The square root of the squared sum of
diﬀerences in color space between connected pixels is
assigned as the weight on the graph edges. Then the al-
gorithm iteratively merges pixel regions by comparing
the dissimilarity between regions to the internal dissim-
ilarity of pixels within the region. The decision rule used
in the LV algorithm was recently explained, in a prob-
abilistic setting, as hypothesis testing from which the
Probabilistic Local Variation (pLV) algorithm (Baltaxe
et al, 2015) was derived. Other 2D variations of LV have


--- Page 3 ---

Graph Based Over-Segmentation Methods for 3D Point Clouds
3
been suggested (Fahad and Morris, 2006; Zhang and Al-
hajj, 2006), as well as video-based extensions (Grund-
mann et al, 2010). In this work we thoroughly analyze
the 3D extension of each LV algorithm stage and pro-
pose a new over-segmentation algorithm for 3D point
clouds.
1.2.2 3D Over-Segmentation Methods (Super-Point
Generation)
Similarly to 2D over-segmentation algorithms,which di-
vide an image into segments, 3D over-segmentation
methods divide a 3D point cloud into clusters, which we
call super-points. The desired properties of the super-
points are the same as for super-pixels: a super-point’s
boundaries should include the object boundaries and
overlap with only one object, while the total number
of super-points should be small. In contrast to over-
segmentation of 2D images, little work has been done
on over-segmentation of 3D point clouds.
The
Voxel
Cloud
Connectivity
Segmentation
(VCCS) algorithm (Papon et al, 2013) followed the 2D
SLIC (Achanta et al, 2012) algorithm and uses a vari-
ant of k-means clustering for the labeling of points.
First, the cluster seeding is done by partitioning the
3D space into a regular grid of voxels. Second, strict
spatial connectivity is enforced by the iterative cluster-
ing algorithm. The similarity between candidate points
is deﬁned by a distance function that takes into ac-
count the spatial distance, color and FPFH descriptors
(Rusu et al, 2009) between two points. This algorithm
has the advantage of producing regular and uniform
sized segments. However, it has ﬁve adjustable parame-
ters: the initial voxel grid resolution, the seed voxel grid
resolution, and three importance factors, one for each
of the color, distance, and geometrical components. A
main disadvantage of this algorithm is its dependence
on these parameters, which create a tradeoﬀbetween
accuracy and the running time of the algorithm.
The Boundary-enhanced Supervoxel Segmentation
(BESS) algorithm (Song et al, 2014) starts with a pre-
processing boundary detection stage and applies a clus-
tering process on a connectivity graph that excludes
these boundary points. The geometric features used in
the clustering step are the spatial coordinates concate-
nated with the angles, and angle distribution between
horizontal and vertical neighboring points. The main
drawback of this algorithm is its reliance on the point
order. It assumes that the input is point data from Li-
DAR with horizontal and vertical consecutive points.
This constraint is used both in the adjacency graph
construction and in the boundary detection stage, mak-
ing this algorithm inapplicable to general, unorganized
point clouds.
3D LV extensions were proposed in three papers.
Each extension used a diﬀerent graph construction
method, diﬀerent modalities to quantify the dissimilar-
ity between neighboring points, and a diﬀerent merge
criterion for the clustering of sub-graphs. It remains un-
clear how each of these diﬀerences aﬀected overall algo-
rithm performance. Strom et al (2010) used a sensor de-
pendent mesh for the connectivity graph, and weighted
the edges by both color dissimilarity and normal direc-
tion diﬀerences. Their merge criterion considered the
two dissimilarities separately. Schoenberg et al (2010)
combined these dissimilarities along with the Euclidean
distance into a single weight. Karpathy et al (2013)
omitted the color information and used an assumption
that concave regions are related to boundaries; their
weight included a penalty for the dissimilarity of angle
between normal vectors in these regions. These three
papers focus largely on the sensor and system setup.
In two of them, over-segmentation is not an end in it-
self but a tool for achieving other goals. The evaluation
process was either qualitative or in terms of the end
goal. Our focus here is on over-segmentation as an end
in itself. We present several 3D variations to this algo-
rithm and evaluate the performance of these variants
quantitatively and qualitatively.
2 Local Variation (LV) 3D Extensions
In this work we address the problem of achieving an ac-
curate over-segmentation result in 3D point clouds us-
ing a graph based approach. The input of the method
is a 3D point cloud representation of the boundary sur-
face of any general object or scene. The output is a di-
vision of the data into small segments. These segments
are essentially clusters of proximate points sharing ge-
ometric properties and appearance, which are diﬀerent
from points in surrounding clusters. The original LV
algorithm and all the 3D extensions discussed here are
constructed of the following steps:
1. Graph construction
2. Descriptor computation
3. Edge weight assignment
4. Sequential subgraph criteria based merging
In the ﬁrst step a graph is constructed from the in-
put 3D point cloud data. In the second step, for each
3D point, a descriptor is estimated. The descriptor is
a quantity, usually a vector of several elements, which
characterize the point and its local environment. Some
elements may be appearance related e.g. color, inten-
sity, or local texture, and some may relate to local ge-


--- Page 4 ---

4
Yizhak Ben-Shabat et al.
Fig. 1: Block diagram of LV variants for 3D over-
segmentation
ometrical properties of the point with respect to its
neighbors. The third step assigns one weight or more to
each graph edge. The weight is a dissimilarity measure
between two connected points. While the dissimilari-
ties arising from diﬀerent properties are often combined
into one scalar weight, using an “adding apples to or-
anges” approach, we use a diﬀerent approach to tackle
the challenge of combining these properties, which we
will refer to henceforth as modalities. The core of the
over-segmentation method is the ﬁnal merge stage. All
input points are initially super-points, which are itera-
tively merged to form larger super-points: the algorithm
sorts the graph edges in ascending order and then tra-
verses over the edges and applies a decision criterion to
determine whether the super-points connected by the
edge should be merged. Figure 1 summarizes the main
stages of the approach. Next, we detail the factors that
must taken into account when choosing how to extend
each of the stages for 3D point clouds.
2.1 Graph Construction
The construction of the connectivity graph G = (V, E)
is a crucial stage in the algorithm. It essentially deﬁnes
the space of possible clusters. If there is no path be-
tween two points in the connectivity graph, they will
never be in the same ﬁnal cluster. However, too many
graph edges decrease the algorithm’s eﬃciency. We con-
sider four alternative methods for constructing the con-
nectivity graph for a 3D point cloud:
1. Connecting every two points whose distance from
each other is less than a threshold R.
2. Connecting each point to its K-nearest neighbors.
3. Constructing a Delaunay triangulation and using it
as the connectivity graph.
4. Using a 2D image grid to construct a 4-connected or
8-connected graph, when such a projection is avail-
able (as originally constructed by the LV algorithm).
There are advantages and drawbacks to these diﬀerent
options.
Constructing a graph using neighbors within a given
radius R limits the length of graph edges, ﬁltering
out irrelevant connections. However, it also creates dis-
connected components for isolated points or points in
sparse regions, actually performing a segmentation de-
cision that was originally assigned to later stages in the
algorithm. In addition, the number of points encapsu-
lated in each sphere may vary drastically between dif-
ferent sections of the point cloud, creating unbalanced
graphs and a bias toward dense regions. Most impor-
tantly, a majority of real-world point clouds have inter-
nally varying point densities, which makes it hard to
adjust the R parameter.
The K-nearest neighbor graph overcomes some of
these drawbacks: it is less sensitive to varying point
cloud densities. In addition, the number of neighbors
for each point is predetermined, eliminating the bias
and creating a balanced graph. However, the K pa-
rameter still requires adjustment. This parameter has
a signiﬁcant impact on speed due to the required
K-nearest neighbor search. Furthermore, distant non-
related points may be connected in this graph.
The Delaunay mesh approach requires a triangula-
tion stage that is sensitive to noise as outlier points
may result in mesh structure that does not represent
the underlying geometry. Furthermore, similarly to the
K-nearest neighbor approach, triangulation may create
synthetic connections between distant points.
Using a 2D grid based graph requires a mapping
between the point cloud and an image. This mapping
is easily obtained when working with low cost sen-
sors such as the Microsoft Kinect, but is not always
available for other sensors. Once the image is given,
the connectivity graph is given as well, making the
graph construction stage highly eﬃcient in comparison
to the other graph construction methods. Furthermore,
although this method does not directly facilitate the ge-
ometry of all three dimensions, it still exploits a planar
geometric relationship between the sampled points in a
projection plane.
All the above options excluding the last may lead
to undesirable over-segmentation results, including dis-
persed segments that appear as small islands. These
are clusters that are disconnected in the 3D space. Fig-
ure 2 illustrates the cause for this problem. The blue


--- Page 5 ---

Graph Based Over-Segmentation Methods for 3D Point Clouds
5
Fig. 2: Dispersed segments. Blue and yellow nodes:
points in two diﬀerent super-points; black lines: graph
edges of an 8-connected image-based graph; red lines:
additional edges used by other graph construction
methods.
Fig. 3: Example of dispersed segments. (a) 3D point
cloud with RGB color overlay, (b) 3D point cloud with
resulting super-points colored in diﬀerent colors, and
(c) enlargement of a region with dispersed segments.
and yellow nodes represent points in two diﬀerent re-
sulting super-points. The black lines illustrate graph
edges of an 8-connected image-based graph, and the
red lines represent additional edges used by other graph
construction methods. The illustrated segmentation is
clearly impossible when using an 8-connected graph,
while it is possible for the other graph types. Indeed,
we observed such problematic results during our evalua-
tion (detailed in Section 3). See Figure 3 for an example
result using a “Radius R” based graph. Similar phe-
nomena were encountered for K-NN based graphs and
Delaunay based graphs. This issue can be addressed by
integrating a connected component analysis stage into
the algorithm. However, this limits the control over the
number of segments and requires a corresponding im-
age. This problem can be reduced by choosing lower
values of K and R.
The advantages and drawbacks
of each of the graph construction methods presented
above are summarized in Table 1.
Given the advantages and disadvantages above, it
remains diﬃcult to conclude which graph construction
method is preferable. Therefore, a quantitative com-
parison is necessary. This comparison is presented in
Section 3.2.1.
2.2 Descriptor Estimation
The LV algorithm, having been designed for 2D images,
uses only RGB color to characterize each pixel. When
the input is a 3D point cloud, we would like to exploit
the additional available dimension and to describe also
the local geometry of each point. However, it is not
easy to characterize the local geometry of a point in a
cloud and most studies that attempted to do so yielded
ambiguous results. Many of these studies were subse-
quently surveyed and evaluated in Guo et al (2015),
who concluded that some descriptors were better than
others for certain tasks while the FPFH (Rusu et al,
2009) descriptor performed well for all of the evaluated
tasks. We test here the eﬀectiveness of the FPFH de-
scriptor for the task of over-segmentation and compare
it to using the estimated normal at each point. We also
test the eﬀectiveness of using the absolute location of
each point. Along with color, that gives us 4 diﬀerent
point descriptors, and diﬀerent combinations thereof.
Each graph vertex (3D point) vi ∈V is therefore de-
scribed by one or more of the following properties:
1. RGB color (Ri, Gi, Bi)
2. Location (Xi, Yi, Zi)
3. FPFH descriptor vector (FPFHi, 1...33)
4. Estimated normal vector ((Nx)i, (Ny)i, (Nz)i)
Note that most 3D geometrical descriptors are esti-
mated using their local neighbors (even the “simple”
normal vector). As such, some“impurity” is introduced
into the algorithm: the similarity now relies on all of
the neighboring points rather than only on one.
Next we describe how the graph edge weights are de-
rived from these properties. A quantitative comparison
between properties and weight alternatives is presented
in Section 3.2.2.
2.3 Edge Weight Assignment
As discussed in Section 2.2, four diﬀerent “modalities”
were considered for describing each point. Respectively,
we deﬁne the dissimilarity between every two points
connected by an edge eij in the connectivity graph G
by:
1. The diﬀerence in color as the Euclidean distance in
the RGB space,
wc(eij) =
q
(Ri −Rj)2 + (Gi −Gj)2 + (Bi −Bj)2
√
3
.
2. The normalized Euclidian distance in the 3D space,


--- Page 6 ---

6
Yizhak Ben-Shabat et al.
Graph Construction
Method
Advantages
Drawbacks
Radius R
• Integrated edge length limitation
• R parameter adjustment
• Sensitive to non-uniform point density
• Dispersed segments
K nearest neighbors
• Balanced graph (non-biased to num-
ber of neighboring points).
• Insensitive to non-uniform point den-
sity
• K parameter adjustment
• Distant point connections
• Dispersed segments
Delaunay
• Insensitive to isolated points
• Parameter independent
• Sensitive to noise
• Distant point connections
• Dispersed segments
2D Image grid based
• Fast
• Requires a mapping from a point
cloud to a 2D grid.
• Does not directly incorporate 3D in-
formation.
Table 1: Pros and cons of the graph construction methods
wd(eij) =
q
(Xi −Xj)2 + (Yi −Yj)2 + (Zi −Zj)2 −dmin
dmax −dmin
where dmin, dmax are the minimum and maximum
distances within the given graph.
3. A measure of planarity using the angle between the
estimated normal vectors,
wn(eij) = 1 −ˆNi · ˆNj.
4. The histogram intersection (Barla et al, 2003) of the
FPFH descriptors (Rusu et al, 2009),
wF P F H(eij) = 1 −
33
X
l=1
min(FPFHi,l, FPFHj,l).
As we explain in the next section, the aforemen-
tioned dissimilarity measure can be combined into one
scalar, by for example, linear combination, and used as
the weight on the graph’s edge. Then, one criterion can
be used to decide whether to merge two super-points.
Another option is to assign a vector of weights to each
edge, associated with all, or some, of the dissimilarity
measures. In this case, the merge decision will use sev-
eral criteria.
The decision as to whether the weights should be
combined or used separately is left for the next stage.
Therefore, the output of the current algorithm stage is
a vector of weights ¯wij. As explained in Section 3.2, we
tested the contribution of each modality by combing
the weights in diﬀerent ways, as follows:
1. Color diﬀerences only, as in the original LV, ¯wij =
wc(eij)
2. Color diﬀerences and Euclidian distance,
¯wij
=
(wc(eij), wd(eij))
3. Color diﬀrences and normal diﬀerences,
¯wij
=
(wc(eij), wn(eij))
4. Euclidean distance and normal diﬀerences, ¯wij =
(wd(eij), wn(eij))
5. Color diﬀerences and FPFH descriptor diﬀerences,
¯wij = (wc(eij), wF P F H(eij))
6. Color diﬀerences, Euclidean distance and normal
diﬀerences, ¯wij = (wc(eij), wd(eij), wn(eij))
The following section addresses the question of how
to use these weight vectors when sequentially deciding
whether two super-points should be merged.
2.4 Merge Criterion
In the original LV algorithm (Felzenszwalb and Hutten-
locher, 2004, 1998), subgraph merging begins by sorting
the connectivity graph edges by the weight deﬁned only
by the color diﬀerences. Then, the connectivity graph
edges are traversed in ascending order and a merge
decision is made. Let eij be the graph edge that con-
nects the two sub-graphs Ci and Cj. Let w(e) be the
(scalar) weight on edge e, and let MST(Cx) be the min-
imum spanning tree of Cx. The merge criterion checks
whether


--- Page 7 ---

Graph Based Over-Segmentation Methods for 3D Point Clouds
7
w(eij) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) w(e) +
δ
|Cx|

.
Note that
δ
|Cx| is a segment-dependent adaptive
threshold function in which δ is a user controlled pa-
rameter that is related to the desired number of output
segments.
In our work, a few diﬀerent weights associated with
the diﬀerent modalities are considered for the merge
criterion. The merge criterion is the decision whether
or not to merge two sub-graphs based on a comparison
between the weight on the connecting graph edge and
an adaptive threshold value. Integrating the weights is a
non-trivial task, commonly described as the problem of
“adding apples to oranges”. Combining them into one
scalar requires each of the weights to be normalized
and factorized using parameters that require adjust-
ment. Furthermore, some information may be lost in
this process. For example, two distant points on diﬀer-
ent objects may have similar color and collinear normal
vectors; when combining these weights the Euclidean
distance’s inﬂuence is averaged with the other weights’
inﬂuence and the eﬀective weight may falsely imply that
both points belong to the same segment. On the other
hand, performing separate comparisons for each modal-
ity violates some of the algorithm’s original assump-
tions and guarantees. Normalization is also required in
this case in order to reduce the number of adjustable
parameters.
In this work we compare the following options for
using the weight information:
1. Linearly combining all modalities to a scalar weight
and using a single merge criterion,
w(eij) = kcwc(eij) + kdwd(eij) + knwn(eij),
w(eij) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) w(e) +
δ
|Cx|

.
2. Checking separate criteria for each modality and
merging only if all criteria are met. If, for instance,
we test the use of the weights associated with color,
normal, and Euclidean distance, then the criteria
that will be checked are:
wc(eij) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) wc(e) +
δ
|Cx|

and
wd(eij) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) wd(e) +
δ
|Cx|

and
wn(eij) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) wn(e) +
δ
|Cx|

.
As mentioned above, the number of segments is con-
trolled indirectly by adjusting the parameter that in-
tuitively reﬂects the initial tolerance of point dissimi-
larity within a segment. It is possible to use a diﬀerent
parameter for each of the modalities; we decided not
to do so in order not to introduce another parameter
adjustment and tuning challenge that would make it
much more diﬃcult to control the number of output
segments.
2.5 Over-segmentation Post-processing
In LV, as well as in all the algorithms tested here,
a post-processing stage merges all the small segments
with the segment closest to them in the graph. Small
segments are deﬁned as segments with less than 10%
of the number of points there would have been if the
3D point cloud had been subdivided uniformly (num-
ber of points divided by the desired number of seg-
ments). Note that for some applications the current
post-processing stage can be replaced by omitting small
segments.
3 Evaluation of LV 3D Variants
This section details the experimental results of the fam-
ily of point cloud over-segmentation algorithms that ex-
tended the 2D LV algorithm to 3D. We ﬁrst provide a
summary of the common evaluation metrics for over-
segmentation. Then, each algorithm stage is thoroughly
analyzed, using the original LV as the baseline. We then
select the best choice for each stage. Afterwards, we use
this analysis to derive the PCLV algorithm. We com-
pare its performance to that of other state-of-the-art
2D and 3D over-segmentation algorithms in Section 5.
The performance analysis was performed on the
NYU Depth V2 (Silberman et al, 2012) dataset, which
contains 1449 images of indoor scenes acquired by Mi-
crosoft Kinect. It includes RGB images aligned with
depth intensity images, human labeled ground truth
segmentation, camera parameters for 3D point cloud
reconstruction, and additional data. For this dataset
the normal vectors provided by Ladick´y et al (2014)
were used.
3.1 Evaluation Metrics
The dominant metrics used to test and compare
the performance of over-segmentation algorithms are
boundary recall and under-segmentation error (Neu-
bert and Protzel, 2012). We use the boundary recall


--- Page 8 ---

8
Yizhak Ben-Shabat et al.
deﬁnition from Martin et al (2001) and the under-
segmentation error deﬁnition from Silberman et al
(2012). Boundary recall quantiﬁes the fraction of the
ground truth boundaries that intersect with the algo-
rithm’s output boundaries. The under-segmentation er-
ror measures the area of incorrect segment overlaps.
In 2D over-segmentation studies these metrics were de-
ﬁned and used mainly for evaluating and comparing
algorithms using the Berkeley Segmentation Dataset
and Benchmark (Martin et al, 2001). Generally, such an
evaluation is only possible when an annotation of the
borders between objects is available. Border annotation
directly on a 3D point cloud is a diﬃcult task. For the
KITTI point-clouds dataset (Geiger et al, 2013), an-
notations are available for some of the objects (people
and vehicles), but full segmentation of scenes is not yet
provided. We follow Papon et al (2013) and project the
3D points to a 2D domain; we then compare the algo-
rithm’s results to the manually annotated segmentation
provided by the NYU dataset for that projection.
The evaluation metrics mentioned above will al-
ways prefer many small segments, and will provide the
best scores for the trivial over-segmentation, where each
point is considered as a separate segment. This is obvi-
ously an undesirable result. The objective is, of course,
to get the highest boundary recall and the lowest over-
segmentation error with a minimal number of segments.
Most over-segmentation algorithms allow some control
over the number of output segments. Therefore, each
metric is evaluated for each algorithm as a function of
the number of output segments. The formulation of the
measures is detailed below.
3.1.1 Boundary Recall
This metric evaluates the number of boundary pixels
correctly labeled by the algorithm versus the total num-
ber of boundary pixels. It is deﬁned to be the fraction
of ground truth (GT) edges that fall within a certain
distance d of at least one super-pixel boundary. Given
a ground truth boundary image T and the algorithm
output boundary image B, the number of true positive
(TP) pixels is computed by summing all boundary pix-
els in T for which exists a boundary pixel in B within
a range d. The number of false negative (FN) pixels
is computed by summing the boundary pixels in T for
which does not exist a boundary pixel in B within a
range d. We used d = 2. Finally, the boundary recall
(BR) is computed:
BR =
TP
TP + FN .
3.1.2 Under-segmentation Error
The under-segmentation error (UE) evaluation metric
penalizes the algorithm on segments that cross ground-
truth borders, by considering the “trespassed” areas as
errors. Let S be a ground-truth segment, let P be an
intersecting output segment, let Pin denote the inter-
section P ∩S, and let Pout denote P ∩¯S (the points
in P that do not intersect with S). Either Pin or Pout
has the smallest number of points, and, for each S and
P, the metric will integrate that number into the total
error.
UE = 1
N

X
S∈GT


X
P :P ∩S̸=∅
min (|Pin |, |Pout|)




where N is the number of segments in the GT image.
Note that this formulation distinguishes between inter-
nal and external clusters. An internal cluster is a cluster
with a majority of points within S. This formulation
prevents over-penalizing small “bleeding” overlaps.
3.2 Experimental Evaluation
In this section the diﬀerent extensions are analyzed with
respects to the evaluation metrics.
3.2.1 Graph Construction Analysis
The graph construction method may vary by applica-
tion. Here we compared the results obtained using the
four graph construction methods presented in Section
2.1 while maintaining all of the original LV stages. This
was done in order to isolate the eﬀect of the graph con-
struction choice. The results can be seen in Figure 4.
Surprisingly, integrating the third-dimension infor-
mation into the graph construction stage does not al-
ways improve the algorithm’s performance, as can be
seen in the results for the KNN graph and the ra-
dius based graph. Furthermore, for the Delaunay graph,
while the recall has improved, this improvement comes
with a signiﬁcant increase in the under-segmentation
error. Taking into account these results and the advan-
tages and disadvantages of each method, as summarized
in Table 1, we conclude that the best choice for this
stage would be to use the 8-connected graph construc-
tion method when a corresponding image is available
and to use a KNN graph when it is not.


--- Page 9 ---

Graph Based Over-Segmentation Methods for 3D Point Clouds
9
500
1000
1500
2000
Number of Segments
0.7
0.75
0.8
0.85
0.9
0.95
1
Recall
Boundary Recall
LV 8-connected Graph
LV 8KNN Graph
LV Delaunay Graph
LV R Graph
(a)
0
500
1000
1500
2000
2500
Number of Segments
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Under-segmentation Error
Under-Segmentation Error
LV
LV 8KNN Graph
LV Delaunay Graph
LV R Graph
(b)
Fig. 4: Graph construction comparison. (a) Boundary
recall; (b) Under-segmentation error.
3.2.2 Descriptors and Edge Weight Assignment
The modalities available for 3D point clouds may vary
according to the application. Here we compared the re-
sults of algorithms using each of the four modalities
deﬁned in Section 2.3. We isolated the eﬀect of descrip-
tor and weight choice by setting the graph construction
to be the same as in the LV algorithm (8-connected)
and applying the merge criterion separately on each
modality using a single adjustable parameter for the
adaptive threshold. Figure 5 depicts the boundary re-
call and under-segmentation error. It can be seen that
the recall is substantially lower and the error substan-
tially higher for the variant that does not use the color
information. Therefore, contrary to the conclusion of
Karpathy et al (2013), color clearly plays an impor-
tant role and each modality further contributes to per-
formance. Surprisingly, FPFH contributed less to per-
formance than did the angle between normal vectors.
This is because the FPFH estimation relies heavily on
the surrounding points; therefore, adjacent points will
500
1000
1500
2000
Number of Segments
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Recall
Boundary Recall
LV (wc)
(wc, wd)
(wc, wFPFH)
(wc, wn)
(wd, wn)
(wc, wd, wn)
(a)
0
500
1000
1500
2000
2500
Number of Segments
0.1
0.15
0.2
0.25
0.3
Under-segmentation Error
Under-Segmentation Error
LV (wc)
(wc, wd)
(wc, wFPFH)
(wc, wn)
(wd, wn)
(wc, wd, wn)
(b)
Fig. 5: Boundary recall (a) and under-segmentation er-
ror (b) for the 4 modalities: color (wc), distance (wd),
normal vectors (wn), FPFH feature (wF P F H), and their
combinations.
have similar FPFH values. Furthermore, it can be seen
that combining the color, normal, and Euclidian dis-
tance yielded the best over-segmentation performance.
An additional qualitative comparison between the
over-segmentation results is presented in Figure 6
(where the number of output segments is 500). At the
top right corner of the image, a shaded intersection be-
tween the wall and the ceiling can be seen; a zoomed-in
view of the over-segmentation results on this region is
also shown. The shading makes it impossible to ﬁnd
the location of this intersection using color information
alone because the color is approximately uniform. This
is evident in the results of the LV algorithm, where the
border follows the shade line rather than the under-
lying intersecting corner, while for the extension that
uses 3D information the corner boundary was correctly
located. This example emphasizes the contribution of
the normal vector to the over-segmentation process.


--- Page 10 ---

10
Yizhak Ben-Shabat et al.
LV (wc)
(wc, wd, wn)
Fig. 6: The advantage of using normal vectors. Top:
over-segmentation results; bottom: enlarged region of
interest; left: using color (wc); right: using a combina-
tion of color, distance, and normal vectors (wc, wd, wn).
3.2.3 Merge Criteria Selection and Further
Comparison to Previously Suggested LV Extensions
As discussed in Section 2.4, it is possible to linearly
combine the modality-based weights into one weight
and then use one merge criterion, or use multiple crite-
ria, one for each modality, all of which must be met if a
merge decision is to be made. We compared these two
options. Following the results reported in Section 3.2.2,
we used the modalities of color, normal and Euclidean
distance for both options. The results are reported in
Figure 7. The dark purple lines depict the performance
of the linear combination option, while the black lines
present the results of the multiple criteria option. The
latter option is more successful, while the linear com-
bination option performs slightly worse than the origi-
nal LV algorithm. This ﬁnding concurs with our notion
that combining the weights will average signiﬁcant dif-
ferences in color space. As discussed in Section 1.2.2,
point-cloud extensions for LV were suggested in Strom
et al (2010); Karpathy et al (2013), and (Schoenberg
et al, 2010). We also compared our approach to the
approaches suggested in these works. The approach of
Schoenberg et al (2010) is similar to the linear combi-
nation option discussed above.The method of Karpathy
et al (2013), which does not exploit the color informa-
tion, has lower recall and higher under-segmentation er-
ror than our results. Finally, the method of Strom et al
(2010), which yields a single data point on the evalu-
0
500
1000
1500
2000
Number of Segments
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Boundary Recall
Karpathy
LV
Schoenberg (Linear)
Strom
PCLV
(a)
0
500
1000
1500
2000
2500
Number of Segments
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Under-segmentation Error
Under-Segmentation Error
Karpathy
LV
 Schoenberg (Linear)
Strom
PCLV
(b)
Fig. 7: Single vs. multiple merge criteria comparison,
and comparison to previously suggested LV extensions.
(a) Boundary recall and (b) under-segmentation error.
ation graphs, has signiﬁcantly lower recall and higher
under-segmentation error than those of the other ap-
proaches. This may be attributed to the use of two
predeﬁned parameters for the color and normal vector
modalities, which may require optimization and adjust-
ment for every data type.
4 The Proposed Point Cloud Local Variation
Algorithm (PCLV)
Based on the analysis in the previous section, it is ev-
ident that the choices of an 8-connected graph, a de-
scriptor based on color, normal vectors, and Euclidean
distance, and multiple merge criteria yield better per-
formance than all the surveyed extensions. We denote
the algorithm that makes those choices PCLV (Point-
Cloud Local Variation). The PCLV algorithm is for-
mally summarized in Algorithm 1.


--- Page 11 ---

Graph Based Over-Segmentation Methods for 3D Point Clouds
11
Algorithm 1 Point Cloud Local Variation Algorithm
Input: 3D Point Cloud.
Output: Set of components C1, ..., Cn deﬁning the super-
points
1: Construct connectivity graph G = (V, E)
1. If an image mapping is available construct an 8-
connected graph.
2. Otherwise construct a K nearest neighbor graph.
2: Compute
descriptors
for
each
point
(X, Y, Z, R, G, B, Nx, Ny, Nz)
3: Compute and assign multi-value graph weights for each
edge (wc, wd, wn), e ∈E
4: Sort E by non-decreasing edge color weight (e1, e2, ..., em)
5: Initialize segmentation S0 with each vertex being a com-
ponent
6: for all q = 1, ..., m do
7:
eq = (vi, vj) ←edge with the qth smallest weight
8:
Cq−1
i
←component of Sq−1 containing vi
9:
Cq−1
j
←component of Sq−1 containing vj
10:
if












Cq−1
i
̸= Cq−1
j

and

wc(eq) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) wc(e) +
δ
|Cx|

and

wd(eq) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) wd(e) +
δ
|Cx|

and

wn(eq) ≤
min
x ∈{i,j}

max
e ∈MST (Cx) wn(e) +
δ
|Cx|












then
11:
Sq = Sq−1 ∪
n
Cq−1
i
∪Cq−1
j
o
\
n
Cq−1
i
, Cq−1
j
o
12:
else
13:
Sq = Sq−1
14:
end if
15: end for
16: Postprocessing: Merge all small segments to closest
neighbor.
5 PCLV Performance Analysis
We compared PCLV’s performance to that of 2D and
3D state-of-the-art over-segmentation algorithms (that
are not LV-based) using the NYU Depth V2 cluttered
indoor scenes dataset (Silberman et al, 2012). In addi-
tion, we demonstrate PCLV’s performance on outdoor
scenes.
5.1 Indoor Scene Over-Segmentation Performance
Evaluation
Figure 8 depicts the boundary recall and under-
segmentation error results for the following algorithms:
LV (Felzenszwalb and Huttenlocher, 1998, 2004), SLIC
(Achanta et al, 2012), pLV (Baltaxe et al, 2015), TP
(Levinshtein et al, 2009), VCCS (Papon et al, 2013) and
the proposed PCLV. It can be seen that PCLV, marked
in black, outperforms all the other algorithms in both
evaluation metrics. The result on the 2D algorithms
SLIC, LV, pLV and TP are consistent with previously
published results on the Berkeley dataset (Baltaxe et al,
2015). The publicly available implementation of the
VCCS algorithm in the Point Cloud Library (PCL)
(Rusu and Cousins, 2011) yields diﬀerent results than
the ones reported in the original paper (Papon et al,
2013). This diﬀerence can be attributed to discrepan-
cies between the paper and the publicly available imple-
mentation, such as the discrepancy in the geometrical
component: the implementation uses the angle between
normal vectors while in the paper the FPFH intersec-
tion kernel is used. In addition, the pixels with missing
depth information are not segmented in the PCL im-
plementation while in the paper a post-processing step
is used to integrate SLIC over-segmentation into these
regions. Furthermore, we applied the PCL implementa-
tion with the default values for several adjustable pa-
rameters. Therefore, for fairness, we present both the
PCL implementation results (in green) and the approx-
imated data points for the VCCS algorithm retrieved
from the original paper (Papon et al, 2013) (dashed or-
ange line). Note that this comparison may still be some-
what inaccurate with regard to the under-segmentation
error since in Papon et al (2013) a slightly diﬀerent def-
inition for it was used.
Figure 9 depicts some examples of the over-
segmentation results for the diﬀerent algorithms when
outputting 200 and 1200 segments. The LV, pLV and
PCLV algorithms segment the sink as a single unit (top
row, (c), (d), and (f)), while the while the VCCS and
SLIC algorithms subdivide it (top row, (b) and (e)). In
general, SLIC and VCCS produce regular and uniform
segments while LV, pLV and PCLV better follow the
GT boundaries. This introduces a compromise between
regularity and accuracy (Machairas et al, 2015; Veksler
et al, 2010). Note that Figure 9 (b) depicts the results
for the VCCS algorithm as presented in the original
paper.
5.2 Outdoor Scene Over-Segmentation Performance
Evaluation
We also experimented with diﬀerent outdoor 3D point
clouds provided by diﬀerent industrial companies. We
demonstrate here PCLV’s performance on a large point
cloud acquired by a data capturing vehicle mounted
with an interferometry sensor by Geosim, a 3D city
modeling technology company (Geosim, 2016). The
scanner is very accurate but the raw point clouds are
rather noisy and require some pre-processing. The pre-
processing stage included a basic color threshold ﬁlter


--- Page 12 ---

12
Yizhak Ben-Shabat et al.
500
1000
1500
2000
Number of Segments
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Boundary Recall
LV
pLV
SLIC
TurboPixels
VCCS
Approximate_VCCS
PCLV
(a)
0
500
1000
1500
2000
2500
Number of Segments
0.1
0.2
0.3
0.4
0.5
Under-segmentation Error
Under-Segmentation Error
LV
pLV
SLIC
TurboPixels
VCCS
Approximate_VCCS
PCLV
(b)
Fig. 8: Performance analysis of PCLV compared to 2D
and 3D state-of the-art over-segmentation algorithms.
(a) Boundary recall and (b) under-segmentation error
as a function of the number of segments.
to remove sensor-generated false points and denoising
(Rusu et al, 2008) to remove isolated noisy points. Note
that for this data the color information for each point is
an intensity value and not RGB. The ﬁnal input point
cloud included approximately 5.75M points. The PCLV
algorithm was applied using a K = 8 nearest neighbor
connectivity graph. Normal vectors were estimated us-
ing Hoppe et al (1992).
The input 3D point cloud after ﬁltering and denois-
ing is depicted in Figure 10 (a), and the PCLV algo-
rithm over-segmentation results are depicted in Fig-
ure 10, (b) which shows each segment in a distinct
color. Enlarged regions of the over-segmented 3D point
cloud are shown in Figure 10 (c),(d), and (e), where
points located on the same object are clustered together
as desired. An additional point of view of this over-
segmented cloud is depicted in Figure 11 (b), where
nicely segmented cars can be seen in the enlarged re-
gion.
6 Discussion and Conclusion
Super-pixels have become a common preliminary stage
in many high level vision related tasks which use 2D
images. We introduce a similar notion of super-points.
In 3D point clouds, the amount of data is usually much
larger than in 2D images, We considered a variety of ex-
tensions of the LV algorithm, which is known to be fast
and accurate on 2D images. We showed that the exten-
sion of LV into the 3D domain is not straightforward for
several reasons: (a) Unlike 2D images, 3D point clouds
are not structured on a grid, giving rise to the question
of how the connectivity graph should be deﬁned; (b)
3D point clouds contain not only color information but
also geometric information, giving rise to the question
of how to best exploit this additional information; and
(c), adapting the LV merge criterion is not trivial when
multiple modalities must be taken into account.
After a thorough analysis of these questions, we in-
troduced a new graph-based over-segmentation algo-
rithm for 3D point clouds. Our new Point Cloud Local
Variation (PCLV) algorithm is generic in the sense that
it is sensor independent and may be applied to any 3D
point cloud data.
An extensive empirical comparison of the PCLV al-
gorithm on cluttered indoor scenes from a large Kinect
dataset was conducted, along with a qualitative evalua-
tion of the algorithm on outdoor scenes. It was demon-
strated that PCLV is highly accurate and outperforms
all of the compared 2D and 3D over-segmentation meth-
ods. In future work the evaluation may be extended for
outdoor data. However, as far as we know, there is cur-
rently no benchmark with annotated segmentations for
point clouds of outdoor scenes. Therefore future work
may also focus on creating such a dataset or extend-
ing the annotations for an existing one, such as KITTI
(Geiger et al, 2013; Niemeyer et al, 2014).
The comparison between over-segmentation meth-
ods demonstrated a tradeoﬀbetween segment regular-
ity and accuracy. This has been previously addressed
in the literature (Veksler et al, 2010; Machairas et al,
2015). Regularity has some advantages, such as an ap-
proximately equal sized segments, sometimes desirable
for a speciﬁc application. Its main drawback, however,
is the lower correlation with the true segments’ bound-
aries. Furthermore, for the non-regular case, very small
segments may be attributed to noise and therefore omit-
ted. This further reduces the size of the output for any
following application. Future work may focus on im-
proving the regularity of the PCLV segments.
One of our interesting ﬁndings was that more com-
plicated 3D descriptors, such as FPFH, produced less
accurate results than those obtained using the simple


--- Page 13 ---

Graph Based Over-Segmentation Methods for 3D Point Clouds
13
Fig. 9: Results of diﬀerent over-segmentation algorithms. (a) human labeled ground truth, (b) VCCS, (c) LV, (d)
pLV, (e) SLIC, (f) PCLV.
Fig. 10: Outdoor 3D point cloud and PCLV over-
segmentation results. (a) The input point cloud after
ﬁltering and denoising, (b) PCLV over-segmentation
results for 3500 segments, (c)-(e) enlarged regions of
the over-segmented 3D point cloud, where the correctly
clustered points are shown for pedestrians (c), stop light
poles and sewer covers (d), and building facades and
signs (e).
normal vectors. We believe this is due not to the design
of these descriptors but to their being estimated from
the data; the more complicated they are, the more bias
is introduced into the estimation. Further investigation
of this issue will include attempts to integrate diﬀer-
Fig. 11: PCLV over-segmentation results. (a) The full
point cloud from a certain viewpoint, (b) the result-
ing segments for an enlarged region, showing correctly
segmented cars.
ent local geometric descriptors in the over-segmentation
process, with a focus on the way they are estimated.
Acknowledgements This work was supported by Magnet
Omek Consortium, Ministry of Industry and Trade, Israel.
References
Achanta R, Shaji A, Smith K, Lucchi A, Fua P,
S¨usstrunk S (2012) SLIC superpixels compared to
state-of-the-art superpixel methods. IEEE Transac-


--- Page 14 ---

14
Yizhak Ben-Shabat et al.
tions on Pattern Analysis and Machine Intelligence
34(11):2274–82
Baltaxe M, Meer P, Lindenbaum M (2015) Local Vari-
ation as a Statistical Hypothesis Test. International
Journal of Computer Vision 117(2):131–141, 1504.
06507
Barla A, Odone F, Verri A (2003) Histogram inter-
section kernel for image classiﬁcation. Proceedings
of the International Conference on Image Processing
2(1):513–516
Comaniciu D, Meer P (2002) Mean shift: a robust ap-
proach toward feature space analysis. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence
24(5):603–619
Fahad A, Morris T (2006) A Faster Graph-Based Seg-
mentation Algorithm with Statistical Region Merge.
In: International Symposium on Visual Computing,
Springer Berlin Heidelberg, pp 286–293
Farabet C, Couprie C, Najman L, LeCun Y (2013)
Learning Hierarchical Features for Scene Labeling.
IEEE Transactions on Pattern Analysis and Machine
Intelligence 35(8):1915–1929
Felzenszwalb P, Huttenlocher D (1998) Image segmen-
tation using local variation. Proceedings of the 1998
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition pp 98–104
Felzenszwalb PF, Huttenlocher DP (2004) Eﬃcient
Graph-Based
Image
Segmentation.
International
Journal of Computer Vision 59(2):167–181
Geiger A, Lenz P, Stiller C, Urtasun R (2013) Vi-
sion meets robotics: The KITTI dataset. The Inter-
national Journal of Robotics Research 32(11):1231–
1237
Geosim (2016) http://www.geosimcities.com
Gould S, Rodgers J, Cohen D, Elidan G, Koller D
(2008) Multi-class segmentation with relative loca-
tion prior. International Journal of Computer Vision
80(3):300–316
Grundmann M, Kwatra V, Han M, Essa I (2010) Ef-
ﬁcient hierarchical graph-based video segmentation.
In: Proceedings of the IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion, pp 2141–2148
Guo Y, Bennamoun M, Sohel F, Lu M, Wan J, Kwok
NM (2015) A comprehensive performance evaluation
of 3D local feature descriptors. International Journal
of Computer Vision 116(1):66–89
Hoiem D (2007) Recovering surface layout from an im-
age. International Journal of Computer Vision 75(1)
Hoppe H, DeRose T, Duchamp T, McDonald J, Stuet-
zle W (1992) Surface reconstruction from unorga-
nized points. ACM SIGGRAPH Computer Graphics
26(2):71–78
Juneja M, Vedaldi A, Jawahar CV, Zisserman A (2013)
Blocks that shout: Distinctive parts for scene classi-
ﬁcation. In: Proceedings of the IEEE Computer So-
ciety Conference on Computer Vision and Pattern
Recognition, pp 923–930
Karpathy A, Miller S, Fei-Fei L (2013) Object discov-
ery in 3D scenes via shape analysis. In: 2013 IEEE
International Conference on Robotics and Automa-
tion, pp 2088–2095
Ladick´y L, Zeisl B, Pollefeys M (2014) Discriminatively
trained dense surface normal estimation. In: Euro-
pean Conference on Computer Vision, Springer In-
ternational Publishing, Part 5, pp 468–484
Levinshtein A, Stere A, Kutulakos KN, Fleet DJ,
Dickinson SJ, Siddiqi K (2009) TurboPixels: fast
superpixels using geometric ﬂows. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence
31(12):2290–2297
Machairas V, Faessel M, C´ardenas-Pe˜na D, Chabardes
T, Walter T, Decenci`ere E (2015) Waterpixels. IEEE
Transactions on Image Processing 24(11):3707–3716
Malik J (2000) Normalized cuts and image segmen-
tation. IEEE Transactions on Pattern Analysis and
Machine Intelligence 22(8):888–905
Martin D, Fowlkes C, Tal D, Malik J (2001) A database
of human segmented natural images and its applica-
tion to evaluating segmentation algorithms and mea-
suring ecological statistics. In: Proceedings of the
Eighth IEEE International Conference on Computer
Vision., IEEE Comput. Soc, pp 416–423
Meyer F (1994) Topographic distance and watershed
lines. Signal Processing 38(1):113–125
Neubert P, Protzel P (2012) Superpixel benchmark and
comparison. In: Proc. Forum Bildverarbeitung
Niemeyer J, Rottensteiner F, Soergel U (2014) Con-
textual classiﬁcation of lidar data and building ob-
ject detection in urban areas. ISPRS Journal of Pho-
togrammetry and Remote Sensing 87:152–165
Oron S, Bar-Hillel A, Levi D, Avidan S (2014) Locally
orderless tracking. International Journal of Com-
puter Vision 111(2):213–228
Papon J, Abramov A, Schoeler M, Worgotter F (2013)
Voxel cloud connectivity segmentation-supervoxels
for point clouds. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pp 2027–2034
Radhakrishna A, Shaji A, Smith K, Lucchi A, Fua
P, Susstrunk S (2010) Slic superpixels. EPFL, Lau-
sanne, Switzerland, TechRep 149300
Rusu RB, Cousins S (2011) 3D is here: Point Cloud
Library (PCL). In: 2011 IEEE International Confer-
ence on Robotics and Automation, IEEE, pp 1–4


--- Page 15 ---

Graph Based Over-Segmentation Methods for 3D Point Clouds
15
Rusu RB, Marton ZC, Blodow N, Dolha M, Beetz M
(2008) Towards 3D Point cloud based object maps for
household environments. Robotics and Autonomous
Systems 56(11):927–941
Rusu RB, Blodow N, Beetz M (2009) Fast Point Fea-
ture Histograms (FPFH) for 3D registration. In: 2009
IEEE International Conference on Robotics and Au-
tomation, IEEE, pp 3212–3217
Schoenberg JR, Nathan A, Campbell M (2010) Segmen-
tation of dense range information in complex urban
scenes. In: 2010 IEEE/RSJ International Conference
on Intelligent Robots and Systems, IEEE, pp 2033–
2038
Silberman N, Hoiem D, Kohli P, Fergus R, Nathan Sil-
berman, Derek Hoiem PK, Rob Fergus (2012) Indoor
segmentation and support inference from RGBD im-
ages. In: European Conference on Computer Vi-
sion), Springer Berlin Heidelberg, Part 5, pp 746–760,
arXiv:1301.3572v2
Song S, Lee H, Jo S (2014) Boundary-enhanced super-
voxel segmentation for sparse outdoor LiDAR data.
Electronics Letters 50(25):1917–1919
Strom J, Richardson A, Olson E (2010) Graph-based
segmentation for colored 3D laser point clouds. In:
2010 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems, IEEE, pp 2131–2136
Veksler O, Boykov Y, Mehrani P (2010) Superpixels
and supervoxels in an energy optimization frame-
work. In: European Conference on Computer Vision,
Springer Berlin Heidelberg, Part 5, pp 211–224
Wang S, Lu H, Yang F, Yang MH (2011) Superpixel
tracking. International Conference on Computer Vi-
sion 2(3):1323–1330
Zhang M, Alhajj R (2006) Improving the graph-based
image segmentation method. In: Proceedings of the
International Conference on Tools with Artiﬁcial In-
telligence, IEEE, pp 617–624
```

---

## Section 1.22: Image Vectorization_ a Review

Source File: Image Vectorization_ a Review.txt

### Paper Content:

```
# Image Vectorization_ a Review.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Image Vectorization_ a Review.pdf
# Output: ../layer2_completion/txt/Image Vectorization_ a Review.txt


--- Page 1 ---

Image Vectorization: a Review
Maria Dziuba1, Ivan Jarsky1, Valeria Efimova1, and Andrey Filchenkov2
1 ITMO University {dziuba.maria,ivanjarsky,vefimova}@itmo.ru
2 GO AI LAB aaafil@gmail.com
Abstract. Nowadays, there are many diffusion and autoregressive mod-
els that show impressive results for generating images from text and other
input domains. However, these methods are not intended for ultra-high-
resolution image synthesis. Vector graphics are devoid of this disadvan-
tage, so the generation of images in this format looks very promising.
Instead of generating vector images directly, you can first synthesize a
raster image and then apply vectorization. Vectorization is the process
of converting a raster image into a similar vector image using primitive
shapes. Besides being similar, generated vector image is also required
to contain the minimum number of shapes for rendering. In this pa-
per, we focus specifically on machine learning-compatible vectorization
methods. We are considering Mang2Vec, Deep Vectorization of Technical
Drawings, DiffVG, and LIVE models. We also provide a brief overview
of existing online methods. We also recall other algorithmic methods,
Im2Vec and ClipGEN models, but they do not participate in the com-
parison, since there is no open implementation of these methods or their
official implementations do not work correctly. Our research shows that
despite the ability to directly specify the number and type of shapes,
existing machine learning methods work for a very long time and do not
accurately recreate the original image. We believe that there is no fast
universal automatic approach and human control is required for every
method.
Keywords: Vector graphics · Image vectorization · Computer vision.
1
Introduction
In computer graphics, two main approaches for image representation coexist.
While a bitmap image is a matrix of pixels, a vector image is a sequence of
shapes drawn with the canvas. Raster graphics are commonly used for complex
images containing a large number of visual details and complex color transitions.
Most often these are photos and photorealistic drawings. At the same time, vec-
tor images consist of figures, which typically have a constant one-color fill. This
results in the simplicity and abstractness of the resulting image. Therefore, the
primary domains of vector graphics are icons, logos, simple illustrations, and
fonts. Vector images are easily embedded in HTML markup and a crucial re-
quirement is the small size of the code describing them for fast data transfer
to the user and subsequent rapid rendering. The most popular vector format is
arXiv:2306.06441v1  [cs.CV]  10 Jun 2023


--- Page 2 ---

2
M. Dziuba et al.
SVG, which defines a vector image as a tag sequence using XML markup. Each
tag can use XML attributes to specify the shape color characteristics and trans-
formations. Using this markup, a renderer program draws an image consisting
of the specified figures. Thus, the task of vectorizing a bitmap image is similar
to the task of obtaining a sequence of shapes and their parameters that together
form the original raster image.
In 2014, generative adversarial models [9] became the first machine learning
algorithm for image synthesis. Since then, image synthesis has become an im-
portant part of digital art, data augmentation techniques, design, fashion, and
several other domains. Currently, the image generation task is solved with deep
generative models based on diffusion models [24,25] and autoregressive mod-
els [7,35].
Modern generative methods have recently achieved significant success in the
raster domain. However, despite these approaches being designed to generate
highly realistic images in different styles from the input text, the resulting images
do not have a high resolution; usually, it is less than 2048x2048 pixels. However,
it is not enough for logos, covers, and for printing. In these domains, vector
graphics are standard.
One of the ways to obtain such images is to use vector graphics instead of
raster graphics. Notably, little research is done in vector image generation [1,32,8,5,19,26,11,4].
The creation of vector images is still done by humans, but working with the vec-
tor image code is not an easy task. Therefore, the ability to generate such images
automatically with a minimal number of post-processing steps is very necessary.
A possible solution for obtaining a vector image is the vectorization of raster
images. It may also allow the enrichment of vector datasets that are required
for training vector image generation algorithms, but the size of which is still not
sufficient in comparison with bitmap image datasets.
The existing image vectorization methods can be divided into two categories:
algorithmic and machine learning-based methods. Algorithmic approaches have
recently been reviewed [31]. The authors classify the vectorization methods as
mesh-based and curve-based. Mesh-based methods split the image into non-
overlapping 2D patches and interpolate colors across them. The patch shape can
be triangular [38,18,10], rectangular [21,29,15], or even irregular, for example, in
the form of Bézigons, closed regions bounded by Bézier curves [30,16,34], and the
patch vertices or interior can store color and other attributes. Curve-based meth-
ods are based on diffusion curves, which are Bézier curves with colors defined on
their left and right sides. Color discontinuities can be modeled by diffusing the
colors from both sides of the curves to create the resulting image. For smooth
edges, the diffusion process might be followed by a blurring phase. There are dif-
ferent formulations of diffusion curves to work with: basic and harmonic [2,37,12],
and biharmonic [33] as well.
Although several vectorization methods exist, no decent comparison was
made to the best of our knowledge. In this paper, we focus on machine learning-
compatible image vectorization methods. We aim to classify and compare them
using different evaluation criteria. The main comparison criteria of the vector-


--- Page 3 ---

Image Vectorization: a Review
3
izing methods are: 1) similarity to the original bitmap; 2) the simplicity or
complexity of the resulting image including the number of shapes and their pa-
rameters; 3) the speed of generation; 4) versatility — the ability to generate a
fairly accurate copy of the input image without prior model training; 5) human
control to adjust hyperparameters.
The contributions of this paper are overview of machine learning-compatible
vectorization methods and comparison of their performance.
2
Machine Learning-compatible Methods
2.1
DiffVG
The work suggesting DiffVG [17] is grounding for machine learning-based vector
image generation methods as it implements a differentiable vector image ras-
terization function linking vector and raster domains. A raster image can be
vectorized with DiffVG by fitting a predefined number of randomly initialized
Bézier curves to the target image, see Fig. 1. Optimization can be performed
by minimizing the L2 loss between a raster and rasterized vector images or a
deep-learning-based perceptual loss [36].
Original
image
Step 1
Step 20
Step 50
Step 100
Step 200
Step 500
Fig. 1. Iterative vectorization using DiffVG method. Each generated image has 62
paths, the same number paths is used in the original vector image.
The authors also propose a simple variational autoencoder [13] for vectorizing
MNIST digits images [3]. The encoder convolves the input raster image into a
latent space, a vector image is synthesized from the embedding, then the input
and synthesized images are compared using the proposed rasterization function.
The resulting images are inaccurate because each character is represented by
several curves that look like the artist’s strokes. The results do not exactly
match the original images of such a simple dataset as MNIST.
One of this method features is also worth mentioning. The running time of
the rasterization algorithm significantly depends on the size of the output image.
However, output size reduction result in the loss of important image details. This
problem arises due to the nature of vector images. By analogy with raster images
size decrease, that leads to the loss of image details, the same effect occurs when
the number of paths is reduced in vector images.


--- Page 4 ---

4
M. Dziuba et al.
2.2
Im2Vec
The Im2Vec [23] paper offers a model for image vectorization and interpolation.
Its architecture is based on the variational auto-encoder (VAE) [14] proposed in
DiffVG. The model maps an input raster image into a latent space sequentially
generating a similar vector image. While training, to compare the generated
vector image with the input raster image, the vector image is rasterized using a
differentiable rasterizer.
In the paper, the authors propose a new way of generating closed shapes.
Initially, a circle is sampled for each shape, and then, based on the latent vectors
of the shapes generated with LSTM, the circle deforms.
Since the difference between the target and output images is significant at the
beginning of training, the authors suggest using multi-resolution loss. The paper
proposes to rasterize images in different resolutions, thus building a pyramid of
images, with the loss function for each layer being calculated. The total multi-
resolution loss optimized is:
EI∼D
L
X
l=1
∥pyrl(I) −Ol∥2,
where L is the number of pyramid levels, pyrl(I) is the l-th pyramid level,
Ol is output rasterized at the corresponding spatial resolution, and D is the
training dataset. Unfortunately, using the official implementation, the results
can hardly be reproduced. During the first 700 epochs of training on the emoji
dataset, which was collected and published by the authors, the generation result
has no changes and consists of a single point in the center of the image. We
have also noticed that the shapes colors are fixed in the implementation, i.e.
no separate color generation can be performed, so we have added a separate
LSTM model for predicting the colors of figures. However, resolving issues to
their implementation, the authors themselves point out that color prediction
causes instability during generation.
Thus, this work cannot be considered as a universal model for vectorizing any
image, because: 1) it must be pretrained on a large number of vector images,
which are hard to obtain; 2) the found shortcomings are likely to lead to learning
instability providing resulting images of poor quality.
2.3
LIVE
In paper [20], LIVE vectorizer was introduced. LIVE is a logical continuation of
the iterative method proposed in DiffVG. However, LIVE does not operate with
all shapes simultaneously from the first iteration. Instead, it gradually adds one
or more shapes to the canvas layer by layer and then performs an optimization
step.
Unlike DiffVG, LIVE operates only with closed shapes consisting of cubic
Bézier curves. There is an issue that some of them may become self-interacted


--- Page 5 ---

Image Vectorization: a Review
5
during optimization, which results in undesirable artifacts and incorrect topol-
ogy. Although additional paths could cover artifacts, it would complicate the
resulting SVG making it impossible to effectively investigate the underlying
topological data. The authors discovered that a self-intersecting path always
intersects the lines of its control points, and vice versa, assuming that all of
the Bézier curves are of the third order. Therefore, the authors introduce a new
loss function (Xing-loss) designed to solve this self-intersection problem. The
fundamental idea is to only optimize the case when the angle of the curve is
180°degrees. In other words, the authors urge the angle between the first and
last control points connections to be greater than 180°in a cubic Bézier path.
This loss acts as a regularizer on self-intersection and its formula is:
LXing = D1(ReLU(−D2))(1 −D1)(ReLU(D2))
where D1 is a characteristic of the angle between two segments of a cubic Bézier
path, and D2 = sin α — value of that angle.
To make each path responsible only for a single feature of the image, the
authors introduce Unsigned Distance guided Focal loss (UDF loss) as well; it
treats each pixel differently depending on how close it is to the shape contour.
According to intuition, the UDF loss amplifies differences near the contour and
suppresses differences in other areas — LIVE weighs an L2 reconstruction loss
by distance to the nearest path. By doing this, LIVE defends against the mean
color problem caused by MSE and keeps accurate color reconstruction:
LUDF = 1
3
w×h
X
i=1
d′
i
3
X
c=1
(Ii,c −ˆ
Ii,c)2,
where I is the target image, ˆI is the rendering, c indexes RGB channels in I, d′
i
is the unsigned distance between pixel i, and the nearest path boundary, and w,
h are width and height of the image.
LIVE produces relatively clean SVGs by initializing paths in stages, localized
to poorly reconstructed, high-loss regions. LIVE’s main advantage is its ability to
reconstruct an image with a user-defined amount of paths, significantly reducing
the SVG file size compared to other methods. However, it takes much time
to vectorize an image even on GPU, thus, this method is hardly applicable in
practice for complex images with a great optimal number of paths. See the
iterative process in Fig. 2.
2.4
ClipGen
The ClipGen paper [27] proposes a method based on deep learning for automati-
cally vectorizing the clipart of man-made objects. The suggested approach needs
a raster clipart image and relevant object category (for instance, airplanes). It
sequentially creates new layers, each formed by a new closed path that is filled
with a single color. All layers are combined to create a vector clipart that fits the
desired category to produce the resulting image. The suggested method is built


--- Page 6 ---

6
M. Dziuba et al.
Original
image
1 figure
(1 layer)
9 figures
(3 layers)
26 figures
(5 layers)
56 figures
(8 layers)
106 figures
(13 layers)
256 figures
(28 layers)
Fig. 2. Iterative vectorization using LIVE method.
on an iterative generative model that chooses whether to keep synthesizing new
layers and defines their geometry and appearance. For training their generative
model, they developed a joint loss function that includes shape similarity, sym-
metry, and local curve smoothness losses, as well as vector graphics rendering
accuracy loss for synthesizing a human-recognizable clipart. However, ClipGen
only works with a predefined number of categories, therefore, it cannot process
arbitrary images.
2.5
Mang2Vec
The authors of the Mang2Vec [28] paper suggest the first method for vectorizing
raster mangas by using Deep Reinforcement Learning. They develop an agent
that is trained to generate the best possible sequence of stroke lines while being
constrained to match the target manga visual features. The control parameters
for the strokes are then collected and converted to the vector format. They also
propose a reward to produce accurate strokes and a pruning method to avoid
errors and redundant strokes. Mang2Vec works only with black and white manga
and cannot be used with colored images.
Original
image
Init step 0
Step 1
Step 2
Step 3
Step 4
Step 5
Fig. 3. Vectorization using Mang2Vec method.
2.6
Deep Vectorization of Technical Drawings
The paper [6] proposes a technical line drawings vectorization method (DVoTD),
for example, for drawings of floor plans. The authors convert a technical raster
drawing, which is cleared of text, into a set of line segments and quadratic
Bézier curves that are specified by control points and width. They preprocess the


--- Page 7 ---

Image Vectorization: a Review
7
input image by eliminating noise, modifying contrast, and adding missing pixels.
Then, they divide the image into patches and calculate the starting primitive
parameters for each patch. To do this, each patch is encoded with a ResNet-based
feature extractor and decoded as feature embeddings of the primitives using a
sequence of transformer blocks. To train the network for primitive extraction,
the following loss function is proposed:
L(p, ˆp, θ, ˆθ) =
1
nprim
nprim
X
k=1
(Lcls(pk, ˆpk) + Lloc(θk, ˆθk),
where
Lcls(pk, ˆpk) = −ˆpk log pk −(1 −ˆpk) log(1 −pk),
Lloc(θk, ˆθk) = (1 −λ)∥θk −ˆθk∥1 + λ∥θk ˆθk∥2
2,
ˆp — the target confidence vector (is all ones, with zeros in the end that indicate
placeholder primitives, all target parameters ˆθk of which are set to zero).
The approximated primitives improve by aligning to the cleaned raster. The
improved predictions from all patches are combined.
3
Online Vectorization Methods
There are plenty of websites that can vectorize any raster image. Existing on-
line methods can be free to use (svgstorm.com, www.visioncortex.org/vtracer,
vectorization.eu) or proprietary (vectorizer.io).
Common options provided by these methods is the selection of vector graph-
ics output file format (SVG, EPS, PDF), color palette and the number of colors
used. Some services allow choosing the quality of image detail (Low, Medium,
High), the type of shapes used (Curve Fitting - pixel/polygon/curve), back-
ground removal and many other actions and parameters. These options affect
the processing speed, the visual result and the number of shapes. The generation
speed highly depends on the resolution of the input raster image and its details
complexity. On average, the processing time of one image is 10 seconds.
Even though they are easy to use, a lot of parameters should be fixed by
a user. It should be mentioned that the resulting image quality and its size
hardly depend on the input image quality and resolution. Low quality results
in producing images with a lot more paths that are actually needed leading to
noticeable artifacts.
One of the popular online services for vectorization is VTracer [22], which
provides many options for a user. According to its documentation, firstly the
method clusters the input image by hierarchical clustering and each of the output
clusters traces into vector. After converting pixels into staircase-like paths, the
method then simplifies the paths into polygons and in the end smoothens and
approximate them with a curve-fitter.
We have come across the following drawbacks of this service. Firstly, VTracer
does not work well with all image formats, for example, it produces a black


--- Page 8 ---

8
M. Dziuba et al.
background instead of a transparent one while processing PNGs and there are
no options to change this behaviour. Secondly, VTracer does not handle low-
quality images well, creating many unnecessary inaccurate shapes. In Fig. 4, we
show an example of the black background appearance for a PNG high-quality
image and the result for the same image converted to JPG having low quality.
Original
PNG image
PNG
vectorized
with black
background
(81 paths)
Original
JPG image
JPG
vectorized
(720 paths)
Fig. 4. VTracer vectorization and its issues with the black background color and inac-
curate vectorization of low-quality images.
4
Comparison
4.1
Comparison Criteria
To make a valuable comparison of vectorization methods, it is necessary to con-
sider that the visual appealingness and similarity of the resulting vector image
to the original raster image is not the only important criterion. The speed of
vectorization is also an important factor.
The main advantages of a vector image are its simplicity and a small number
of shapes used. Although a vector image can contain various shapes (circles,
rectangles, paths, etc.), vectorization methods tend to generate images using
only paths. Paths themselves consist of segments (Bèzier curves, straight lines,
etc.) and their number in each path should be low as well. This is necessary
both for simpler post-processing by designers and faster image transfer to the
user through the Internet with subsequent vector image rendering.
Thus, the main five criteria for evaluating vectorization methods are:
1. similarity to the original bitmap;
2. the simplicity or complexity of the resulting image including the number of
shapes and their parameters;
3. the speed of generation;
4. versatility — the ability to generate a fairly accurate copy of the input image
without prior model training;
5. human control to adjust hyperparameters.


--- Page 9 ---

Image Vectorization: a Review
9
Image De-
scription
Original
Mang2Vec
DVoTD
DiffVG
(closed)
DiffVG
(unclosed)
LIVE
Simple
Vector
Medium
Vector
Hard
Vector
Simple
Raster
Medium
Raster
Hard
Raster
Fig. 5. Qualitative comparisons of image vectorization results using different methods.
DiffVG closed stands for the DiffVG method with closed paths, unclosed - with unclosed
strokes. DiffVG and LIVE results for the 3 initially vector images have the paths
amount as in the original images, for the Simple raster image 32 paths were used, for
the Medium and Hard Raster – 1024 paths. The other methods have been run with
their default parameters.
However, taking into account all the criteria at the same time is challenging,
because the methods we consider have many different parameters that affect
all the criteria simultaneously. Typically, by changing one parameter, one can
achieve an increase in image processing speed but at the same time reduce the
quality of the resulting image.
4.2
Experiment Setup
We selected 6 images for comparison: 3 rasterized vector images and 3 bitmaps
of different complexity. The original target vector images before rasterization
had the following number of paths: dragon had 25, burger – 62, red landscape
– 100. Ideally, vectorization methods should create images consisting of an ap-


--- Page 10 ---

10
M. Dziuba et al.
proximately similar number of paths in a short period of time. At the same time,
it does not worth expecting vectorization methods to account for every image
detail on initially raster images, as an over-detailed vector image does not satisfy
the simplicity criterion — a smaller number of paths. It is also desirable that
simple monochrome patches should be decorated with a minimum number of
shapes and the image subject should not be lost.
We compare the following methods (with publicly available implementation):
Mang2Vec, Deep Vectorization of Technical Drawings (DVoTD), iterative Dif-
fVG and LIVE methods, and online methods. The following models are not
included in the comparison: 1) Im2Vec [23], because we could not confirm in
practice the results described in the paper, and it also requires additional serious
pre-training for processing relatively diverse images; 2) ClipGEN [27], because
the model is limited to the set of predefined classes and there no its implementa-
tion is publicly available; 3) VAE and GAN introduced in DiffVG [17], because
they also require additional pre-training. Also, even on such a simple dataset as
MNIST, we found their results are not satisfactory enough; 4) algorithmic meth-
ods, since we found no implementations publicly available. Fig. 5 contains the
original images and their vectorized versions using different methods reviewed
in our paper.
Our experiments have proven that the Mang2Vec and DVoTD models are not
versatile, since they are capable of processing only black-and-white images. At
first glance, Mang2Vec vectorizes the image well, but its significant drawback is
the use of a very large number of shapes: for instance, the “burger” image on the
last 5th iteration had 3065 paths and the “dragon” image on the 20th iteration
16600 had paths. Also, the method adds many <clippath> and <circle> tags,
which seems useless. The method uses image splitting into patches and performs
a separate vectorization of each patch, which is acceptable when processing de-
tailed manga images. However, a large monochrome space becomes divided into
a large number of shapes, which is unacceptable. In the Mang2Vec method, you
can specify a different number of iterations, but with a small number of them,
the patches boundaries, into which the division is performed, become clearly
visible. Since Mang2Vec automatically resizes image to 4096x4096 resolution its
working time is constant and is 157 seconds.
Deep Vectorization of Technical Drawings (DVoTD) was meant to be able
to use either quadratic Bézier curves or straight lines. We managed to run the
method for curves, but the implementation of the second method (straight lines)
is imperfect and the code is likely to contain some issues that lead to a crash
during execution, which we could not fix. The method struggles to fill in contours
with a solid color, as it was originally made to generate black stroke lines. We
ran the method with default parameters and, for instance, the “dragon” image
had 132 paths. The running time of the 270x480 image was 142 seconds, 373x405
– 179 seconds, 582x496 – 273 seconds.
Vectorization by the DiffVG iterative method can be done in two ways: gen-
erating images consisting of curves and of closed shapes. The approach is simple
and quite effective, but it produces many artifacts with shapes that the method


--- Page 11 ---

Image Vectorization: a Review
11
apparently attempts to hide, but it fails to succeed. In addition, the reconstruc-
tion of absolutely exact visual copies of the original vector images cannot be
obtained even using a large number of paths (1024 shapes).
Different renderers convert a vector image to a bitmap in different ways.
For example, images generated by DiffVG and LIVE will look inaccurate and
careless, when they are rendered by the InkScape. This behavior occurs due
to the fact that these images contain curves protruding beyond the edges of
the viewBox attribute, and InkScape displays them instead of cropping them.
At the same time, other renderers, for example, in Google Chrome browser,
process images correctly without extra curves that remain beyond the viewBox.
However, these curves are still a problem, since they are superfluous and they
create additional artifacts in the image code and add an extra size to it.
The LIVE method iteratively adds a layer consisting of one or more shapes
specified by user to the image and optimizes the resulting image. In addition,
number of image processing iterations after applying each layer should be spec-
ified manually. LIVE sets the number of iterations to 500 by default, but we
noticed that after about 200 iterations, the image almost does not change, so
we set this value in our experiments. In the optimization process, LIVE uses the
DiffVG rasterizer to convert the current vector image into a raster image and
compare it with the target image. Rasterization is performed by default at the
same resolution as the target image, but for large resolutions it is computation-
ally time-consuming. For example, for a resolution of 1080x1920, processing the
first layer in 200 iterations on the Nvidia RTX 3090 Ti GPU took 212 seconds,
then by 10th layer, processing of one layer reached 244 seconds. Finally, pro-
cessing of 28 layers has took almost 2 hours. Therefore, we decided to pre-scale
the raster images so that their maximum side does not exceed 512 pixels. At
the same time, it is worth noting that this approach carries the risk of losing
details in the image. With this approach, the processing of the first layer took 16
seconds, but by the 10th layer, the processing time of one layer was 40 seconds.
The total processing time of 46 layers took about 32 minutes. When limiting the
maximum image side to 256 pixels, the processing time of the first layer was 6
seconds, and totally 32 layers were processed in 22 minutes. It should also be
known that the processing time is also affected by the number of shapes in each
layer. In our experiments we used less than 7 shapes in each layer only the first 5
layers, after that we used 7, 10, 20 or 30 shapes in each layer gradually increasing
the number.
LIVE creates the most accurate images among the ML methods. However,
the most significant disadvantage of this method is a very long image processing
time, which depends on the number of layer additions, the number of iterations
of processing each layer, the dimensions of the image for which intermediate
rasterization is performed.
It is worth noting that DiffVG also has problems with long intermediate
rasterization, however, due to the smaller total number of iterations, this is less
noticeable. The results of DiffVG and LIVE operation times are shown in more
detail in the Tab. 1 and Tab. 2.


--- Page 12 ---

12
M. Dziuba et al.
Image Resolution Total paths Time (sec)
512x512
16
47
405x373
62
48
496x582
25
56
512x512
32
56
512x288
256
143
512x381
256
186
512x288
512
216
512x381
512
280
1920x1080
32
296
1920x1080
64
319
1920x1080
100
349
512x288
1024
389
1920x1080
256
435
512x381
1024
446
1920x1432
256
582
1920x1080
512
583
1920x1432
512
770
1920x1080
1024
837
1920x1432
1024
1099
Table 1. The running time of the DiffVG iterative algorithm at different startup
parameters on NVidia RTX 3090Ti GPU. Total iterations number is 500. There is no
serious speed difference between methods with closed and unclosed paths.
Image Resolution
Layers schema
Total layers Total paths Time (sec)
256x256
4x1
4
4
33
256x256
8x1
8
8
77
256x218
1,2,3,4,5x3
7
25
78
256x256
16x1
16
16
158
235x256
1,3,5x2,7x7
11
62
162
144x256
1,3,4,5,7,10x8
13
100
169
256x218
25x1
25
25
278
256x256
32x1
32
32
423
235x256
62x1
62
62
945
144x256
100x1
100
100
1231
471x512
1,3,5,7,10x24
28
256
1448
190x256
1,3,5,7,8,10x2,20x4,30x30
41
1024
2332
256x218
1,3,5,7,8,10x2,20x4,30x30
41
1024
2490
288x512
1,3,5,7,8,10x2,20x4,30x30
41
1024
3197
471x512
1,3,5,7,8,10x2,20x4,30x30
41
1024
3891
1080x1920
1,3,5,7,10x24
28
256
7150
Table 2. The running time of the LIVE algorithm with different startup parameters
on NVidia RTX 3090Ti GPU. Each layer is processed for 200 iterations.


--- Page 13 ---

Image Vectorization: a Review
13
The requirement of DiffVG and LIVE models of directly controlling the num-
ber and type of applied shapes on the one hand is their advantage, but on the
other hand, since there are no good models for determining the required number
of shapes, automatic vectorization of a large set of various raster images becomes
almost impossible.
The online methods we found show the best quality of image vectorization.
However, this is achieved by using a large number of shapes, the number of which
can only be controlled indirectly by specifying the number of available colors.
The results are presented in Tab. 3.
Method
Approach Type
Code Versatility
Speed
Number of figures
DiffVG
ML: Iterative
+
+
Low
User-defined
Im2Vec
ML: VAE
+
-
Pretrain needed
User-defined
LIVE
ML: Iterative
+
+
Very low
User-defined
ClipGEN
ML: Iterative+DL
-
-
Pretrain needed
User-defined
Mang2Vec
ML: RL
+
-
Medium
Many
DVoTD
ML: DL
+
-
Medium
Many
VTracer
Algorithmic, online
+
+
Very High
Medium
Table 3. Classification and comparison of vectorization methods. ’ML’ means machine
learning, ’DL’ means deep learning, and ’RL’ means reinforcement learning.
However, none of the considered methods could recreate exact copies using
such a number of figures.
5
Conclusion
In this work we have shown that current image vectorization methods are diffi-
cult to use in practice. Online methods without manual hyperparameter tuning
create images containing a large number of paths, which increases the amount
of used memory, and design refinement becomes a time-consuming task. All the
existing machine learning-compatible methods also require human control and
adjustment of method iterations number, output vector image parameters, etc.
The Im2Vec model is not capable of storing and generating complex images and
is not a universal vectorizer that could create a vector analog for any input im-
age. The LIVE method is the only universal model that allows you to control
the number of drawn shapes, however, due to the use of an iterative approach,
generating a single image takes a huge amount of time.
According to our measurements, DiffVG is the fastest among ML methods
without much quality losses. However, a large number of paths are required for
high-quality results. At the same time, LIVE is able to get no worse quality
using fewer shapes. However, the main problem with the LIVE method is its
extra-long running time.


--- Page 14 ---

14
M. Dziuba et al.
Perhaps, generally, it is best to use online methods, but they do not allow
you to adjust the number of applied shapes.
To summarize, vectorization methods are connected with a tradeoff between
image quality, path number, segment number, closed or not paths are, number
of iterations, and running time.
References
1. Carlier, A., Danelljan, M., Alahi, A., Timofte, R.: Deepsvg: A hierarchical gen-
erative network for vector graphics animation. CoRR abs/2007.11301 (2020),
https://arxiv.org/abs/2007.11301
2. Dai, W., Luo, T., Shen, J.: Automatic image vectorization using superpixels and
random walkers. 2013 6th International Congress on Image and Signal Processing
(CISP) 2, 922–926 (2013)
3. Deng, L.: The mnist database of handwritten digit images for machine learning
research. IEEE Signal Processing Magazine 29(6), 141–142 (2012)
4. Efimova, V., Chebykin, A., Jarsky, I., Prosvirnin, E., Filchenkov, A.: Neural style
transfer for vector graphics (2023)
5. Efimova, V., Jarsky, I., Bizyaev, I., Filchenkov, A.: Conditional vector graphics
generation for music cover images. arXiv preprint arXiv:2205.07301 (2022)
6. Egiazarian, V., Voynov, O., Artemov, A., Volkhonskiy, D., Safin, A., Taktasheva,
M., Zorin, D., Burnaev, E.: Deep vectorization of technical drawings. In: Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XIII 16. pp. 582–598. Springer (2020)
7. Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image
synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 12873–12883 (2021)
8. Frans, K., Soros, L.B., Witkowski, O.: Clipdraw: Exploring text-to-drawing syn-
thesis through language-image encoders. CoRR abs/2106.14843 (2021), https:
//arxiv.org/abs/2106.14843
9. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.: Generative adversarial networks. Communications of the
ACM 63(11), 139–144 (2020)
10. Hettinga, G.J., Echevarria, J., Kosinka, J.: Efficient image vectorisation using mesh
colours. The Eurographics Association (2021)
11. Jain, A., Xie, A., Abbeel, P.: Vectorfusion: Text-to-svg by abstracting pixel-based
diffusion models. arXiv preprint arXiv:2211.11319 (2022)
12. Jeschke, S., Cline, D., Wonka, P.: Estimating color and texture parameters for
vector graphics. In: Computer Graphics Forum. vol. 30, pp. 523–532. Wiley Online
Library (2011)
13. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013)
14. Kingma, D.P., Welling, M.: An introduction to variational autoencoders. arXiv
preprint arXiv:1906.02691 (2019)
15. Lai, Y.K., Hu, S.M., Martin, R.R.: Automatic and topology-preserving gradient
mesh generation for image vectorization. ACM Transactions on Graphics (TOG)
28(3), 1–8 (2009)
16. Lecot, G., Lévy, B.: Ardeco: automatic region detection and conversion. In: Euro-
graphics Symposium on Rendering (2006)


--- Page 15 ---

Image Vectorization: a Review
15
17. Li, T.M., Lukáč, M., Michaël, G., Ragan-Kelley, J.: Differentiable vector graphics
rasterization for editing and learning. ACM Trans. Graph. (Proc. SIGGRAPH
Asia) 39(6), 193:1–193:15 (2020)
18. Liao, Z., Hoppe, H., Forsyth, D., Yu, Y.: A subdivision-based representation for
vector image editing. IEEE transactions on visualization and computer graphics
18(11), 1858–1867 (2012)
19. Lopes, R.G., Ha, D., Eck, D., Shlens, J.: A learned representation for scalable vector
graphics. CoRR abs/1904.02632 (2019), http://arxiv.org/abs/1904.02632
20. Ma, X., Zhou, Y., Xu, X., Sun, B., Filev, V., Orlov, N., Fu, Y., Shi, H.: Towards
layer-wise image vectorization. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 16314–16323 (2022)
21. Price, B., Barrett, W.: Object-based vectorization for interactive image editing.
The Visual Computer 22, 661–670 (2006)
22. Pun, S., Tsang, C.: Vtracer (2020)
23. Reddy, P., Gharbi, M., Lukac, M., Mitra, N.J.: Im2vec: Synthesizing vector graph-
ics without vector supervision. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 7342–7351 (2021)
24. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 10684–10695 (2022)
25. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022)
26. Schaldenbrand, P., Liu, Z., Oh, J.: Styleclipdraw: Coupling content and style in
text-to-drawing translation. arXiv preprint arXiv:2202.12362 (2022)
27. Shen, I.C., Chen, B.Y.: Clipgen: A deep generative model for clipart vectorization
and synthesis. IEEE Transactions on Visualization and Computer Graphics 28(12),
4211–4224 (2021)
28. Su, H., Niu, J., Liu, X., Cui, J., Wan, J.: Vectorization of raster manga by deep
reinforcement learning. arXiv preprint arXiv:2110.04830 (2021)
29. Sun, J., Liang, L., Wen, F., Shum, H.Y.: Image vectorization using optimized
gradient meshes. ACM Transactions on Graphics (TOG) 26(3), 11–es (2007)
30. Swaminarayan, S., Prasad, L.: Rapid automated polygonal image decomposition.
35th IEEE Applied Imagery and Pattern Recognition Workshop (AIPR’06) pp.
28–28 (2006)
31. Tian, X., Günther, T.: A survey of smooth vector graphics: Recent advances in
representation, creation, rasterization and image vectorization. IEEE Transactions
on Visualization and Computer Graphics (2022)
32. Wang, Y., Lian, Z.: Deepvecfont: synthesizing high-quality vector fonts via dual-
modality learning. ACM Transactions on Graphics (TOG) 40(6), 1–15 (2021)
33. Xie, G., Sun, X., Tong, X., Nowrouzezahrai, D.: Hierarchical diffusion curves for
accurate automatic image vectorization. ACM Transactions on Graphics (TOG)
33(6), 1–11 (2014)
34. Yang, M., Chao, H., Zhang, C., Guo, J., Yuan, L., Sun, J.: Effective clipart im-
age vectorization through direct optimization of bezigons. IEEE Transactions on
Visualization and Computer Graphics 22, 1063–1075 (2016)
35. Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku,
A., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich
text-to-image generation. arXiv preprint arXiv:2206.10789 (2022)


--- Page 16 ---

16
M. Dziuba et al.
36. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 586–595 (2018)
37. Zhao, S., Durand, F., Zheng, C.: Inverse diffusion curves using shape optimization.
IEEE transactions on visualization and computer graphics 24(7), 2153–2166 (2017)
38. Zhou, H., Zheng, J., Wei, L.: Representing images using curvilinear feature driven
subdivision surfaces. IEEE transactions on image processing 23(8), 3268–3280
(2014)
```

---

## Section 1.23: Improving_NURBS_Surface_Sharp_Feature_Representati

Source File: Improving_NURBS_Surface_Sharp_Feature_Representati.txt

### Paper Content:

```
# Improving_NURBS_Surface_Sharp_Feature_Representati.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Improving_NURBS_Surface_Sharp_Feature_Representati.pdf
# Output: ../layer2_completion/txt/Improving_NURBS_Surface_Sharp_Feature_Representati.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/228339084
Improving NURBS Surface Sharp Feature Representation
Article  in  International Journal of Computational Intelligence Research (IJCIR) · January 2007
DOI: 10.5019/j.ijcir.2007.97
CITATIONS
10
READS
305
3 authors, including:
Nallig Leal
Universidad Autónoma del Caribe
27 PUBLICATIONS   360 CITATIONS   
SEE PROFILE
Oscar Ortega
University of Antioquia
16 PUBLICATIONS   64 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Nallig Leal on 03 June 2014.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

International Journal of Computational Intelligence Research. 
ISSN 0973-1873 Vol.3, No.2 (2007), pp. 131-138 
© Research India Publications http://www.ijcir.info 
 
 
 
Improving NURBS Surface Sharp Feature 
Representation 
  
Nallig Eduardo Leal1, Oscar Ortega Lobo1 and John William Branch2 
 
1Systems Engineering Department, University of Antioquia, 
Medellín, Ant. Cl 67 #53-108, Colombia 
E-mail: nallig_eduardo@yahoo.com.mx, oortega@udea.edu.co 
 
2Systems School, National University, 
Medellín, Ant. Cl 59A #63-20, Colombia 
E-mail: jwbranch@unalmed.edu.co 
 
 
Abstract—Surface fitting is one of the most important stage of 
the 3D reconstruction process, since in this stage the 
computational model of the real object is obtained. NURBS is a 
surface fitting method widely used that has become a standard 
in CAD/CAM system due to its stability, flexibility and local 
modification properties. Despite of the advantages of fitting with 
NURBS, it is still necessary improve the representation of sharp 
features like high curvatures, edges and corners with this fitting 
method. Find a correct parameterization of the NURBS can 
contribute to improve the representation of sharp features, even 
though the manipulation of the NURBS parameters imply deal 
with non linear problems when fitting. In this paper, a new 
method 
for 
improving 
NURBS 
surface 
sharp 
feature 
representation is presented. The method first subdivides the 
fitting data in clusters, by using SOM. Then, in each cluster uses 
an evolutionary strategy to obtain the weights of the NURBS so 
that the error fitting is minimized and the representation of 
sharp features is improved. 
 
Keywords—Evolutionary Strategies, NURBS, SOM, Surface 
optimization.  
I. Introduction 
The 3D reconstruction is a process by which objects are 
reproduced in the computer memory, keeping its physical 
characteristics (dimensions, volume and shape). The 3D 
reconstruction is a difficult task that covers, in general terms, 
five stages: data acquisition, registration, integration, 
segmentation and surface fitting [1]. The approach presented 
in this paper deals with the surface fitting stage, in which the 
computational model of the object is obtained. 
NURBS is one of the most employed surface fitting 
models, provided that is a standard representation of curves 
and surfaces [2] and is widely supported by modern standards 
like OpenGL and IGES, which are used for graphics and 
geometric data exchange [3]. In addition, the NURBS surface 
model has stability, flexibility, local modification properties 
and is robust to the noise. Yet, the NURBS surface model has 
some disadvantages: the input data points should be mapped 
on a regular grid structure and the representation of sharp 
features of the 3D object surfaces, due to the smooth 
properties of the model, is poor. This paper focuses on the 
second one. 
When fitting parametric surfaces, like NURBS, the choice 
of the parameters values make a great difference in the shape 
of the resultant surface [4]. The NURBS surface fitting 
depends on the values of knots, weights and control points. 
Control points describe the geometry that the NURBS surface 
must follow, but the weights determine the local degree of 
influence that a control point has, i.e. the weights determine 
the local shape whereas control points determine the global 
shape. 
NURBS surfaces can be constructed using deterministic 
algorithms, Piegl and Tiller [2] presented a method for 
constructing NURBS surfaces in this way, even though some 
method for constructing NURBS deterministically assign 
homogeneous weighting factors, therefore the resultant 
surface locally do not follow close enough the geometry 
described by the control points. 
A way for optimizing the NURBS surface fitting is 
manipulating the values of the parameter surface, but this 
implies to deal with non linear problems during the fitting 
process. The manipulation of NURBS parameters can be 
done by means of deterministic method, e.g. assigning the 
weighting factors proportional to the length of the control 
points polygon spans [5], but such way of NURBS 
construction can affect the local shape of the surface. 
When optimizing NURBS surfaces described by large 
amount of control points, the optimization space can be too 
irregular, therefore deterministic methods can easily fall in 
both local minimal or do not reach the convergence point. 
Furthermore, the deterministic methods require accomplish 
differentiability conditions. In 2004 Cuartas [6] presented a 
deterministic method for optimizing NURBS surfaces by 


--- Page 3 ---

132 
 
Nallig Eduardo Leal et al
 
 
 
manipulating 
the 
knot 
vectors 
using 
the 
Levenberg-Marquardt algorithm. The method has both 
convergence problems and initial value problems. 
In cases where deterministic method can not be applied 
successfully, heuristic methods like evolutionary algorithms 
are alternatives that can be considered, due to they do not 
have the restrictions of the deterministic and have advantages 
like multiples starting and searching points in a given space, 
just require to know the objective function and do not require 
to know its derivatives nor additional information, 
Evolutionary algorithms use probabilistic transition rules 
instead of deterministic rules and work with coded parameters 
instead of real parameters. In 2000 Weinert and Mehnen [7] 
used an evolutionary algorithm to optimize NURBS surfaces, 
obtaining 
good 
results; 
though 
the 
method 
was 
computationally expensive. 
Several approaches have used evolutionary algorithms for 
improving the surface fitting techniques; such approaches 
have produced promising results [7], [8], [9], even though in 
real value optimization, the approaches that use evolutionary 
strategies have a better computational performance. 
In this paper a new method for improving sharp feature 
representation of NURBS surfaces is presented. The method 
first subdivides the fitting data in clusters where perform a 
local optimization, by using SOM. Then, for each cluster, 
uses an evolutionary strategy to obtain the weights of the 
control points, without change the location of the points, for 
reducing the fitting error and improving sharp feature 
representation, keeping the global and local surface geometry 
of the real object. 
The remainder of this paper is organized as follows. In 
section 2, we present related work dealing with surface 
optimization, emphasizing in NURBS. In section 3, the 
fundamentals of NURBS surfaces are presented. In section 4, 
we present a short review of SOM. In section 5, we present the 
fundamentals of evolutionary strategies. In section 6, the 
stages of our method are explained. In section 7, the results of 
our method are provided. In section 8, conclusions and future 
work are discussed. 
II. Related work 
Provided that NURBS is a standard representation of curves 
and surfaces [2] and is one of the most used surface fitting 
methods, it is important try to solve the drawbacks the model 
has. It is also important to know what techniques have been 
employed for both solving the drawbacks other surface fitting 
techniques have, and how those techniques have been 
employed for optimize them, in order to realize a direct 
adaptation for NURBS surface optimization. 
Fujiwara and Sawai [9] employ genetic algorithms to 
improve the approximation of human faces using triangular 
meshes. The idea is to approximate an original surface, the 
closest as possible, finding the best location of the points that 
define the 3D triangular mesh. Two algorithms were utilized 
to generate the meshes. The first one, perform a selection and 
mating of the points population in a only iteration. The second 
one, is a genetic algorithm which coded triangular meshes like 
individuals. The method is not able to approximate surfaces 
like toroids. 
Cuartas [6] presents a method for manipulating the knot 
vectors of a NURBS surface in order to optimize the surface 
fitting. The manipulation of the knot vectors is made using the 
Levenberg-Marquardt algorithm. The method is expensive 
computationally, only can be applied on organized range data, 
has stability problems and is dependents of parameters. 
Weinert and Mehnen [7] use an evolutionary strategy to 
optimize the approximation of NURBS surfaces to set of 
organized 3D data. The method manipulates the inner control 
points, the knot vectors and the weights of the NURBS 
surface in order to obtain the parameters values that reduce 
fitting error. The simultaneous manipulation of the control 
points, knots and weights turn the optimization problem more 
complicate and can difficult the convergence of the solution. 
Furthermore, the manipulation of the control points can 
distort the geometry of the original surface and the reduction 
of the surface fitting error by manipulating the knots, is not as 
significant as the reduction by manipulating the weights.  
III. NURBS 
NURBS stand for Non Uniform Rational B-Spline, are 
parametric tensor product defined according to the following 
expression 
 
∑∑
∑∑
=
=
=
=
=
n
i
m
j
j
i
q
j
p
i
n
i
m
j
j
i
j
i
q
j
p
i
w
v
N
u
N
P
w
v
N
u
N
v
u
S
0
0
,
,
,
0
0
,
,
,
,
)
(
)
(
)
(
)
(
)
,
(
 
 
(1) 
 
where 
j
i
w , are the weights, 
j
iP, are the control points and 
)
(
),
(
,
,
v
N
u
N
q
j
p
i
are the B-Spline basis functions or order p  
and q  respectively, defined over the non periodic node 
support 
{
}
r
u
u
u
S
,...,
0
=
 and 
{
}
r
v
v
v
S
,...,
0
=
 [10], which 
can be calculated in a recursive way by the Cox and De Boor 
formula [11], [12] according to (2)  
 



≤
≤
=
+
caso
 
otro
en 
     
0
 
if
      
1
)
(
1
0,
i
i
i
u
u
u
u
N
                   
1
1
1
,1
1
1
,
,
)
(
)
(
)
(
)
(
)
(
+
+
+
−
+
−
+
+
+
−
−
+
−
−
=
i
p
i
p
i
p
i
i
p
i
p
i
i
p
i
u
u
u
N
u
u
u
u
u
N
u
u
u
N
 
(2) 
 
A NURBS surface is completely determined by its control 
points 
j
iP, , i.e. the surface change in a predictable way 
according to control points movement. This is known as the 
local support property and allows the surface be affected, only 


--- Page 4 ---

Improving NURBS Surface Sharp Feature Representation 
133 
locally, by the movement of a control point. The main 
difficulty when fitting NURBS surfaces is obtain a suitable 
parameterization and choice automatically the number of 
control points and their positions to define the surface 
topology. 
The weighting 
j
i
w ,  factors of NURBS surfaces play an 
important role in the fitting process, since these factors 
determine how much a control point influence locally the 
shape of the surface. When the weighting factors of NURBS 
surfaces are assigned in a homogeneous way and their values 
are ones, the NURBS model is reduced to a particular case 
known as B-Spline surfaces, which are limited in the 
representation of free form and conic surfaces. If we want 
approximate close enough set of data, which represent free 
form surfaces using NURBS, it is necessary to manipulate the 
NURBS parameters, but how it was previously mentioned, 
such manipulation implies deal with no linear problems 
during the fitting process. Furthermore, negatives values or 
zeros in the weighting factors can degenerate the construction 
of the surface. Fig. 1 illustrates the importance of the 
weighting factors in the NURBS model. The circles represent 
the control points and the line represents a NURBS curve. It is 
notable how the weighting factors affect the local geometry of 
the surface. 
 
 
 
 
(a) 
(b) 
 
 
(c) 
(d) 
Figure 1.  Weighting factors effect. (a) Point 3 with weighting 
factor equal to 0 (b) Point 3 with weighting factor equal to 0.5 
(c) Point 3 with weighting factor equal to 1 (d) Point 3 with 
weighting factor equal to 2. 
 
 
When fitting data points using NURBS, it is attempt to 
minimize: 
 
( )
( )
( )
( )
∑
∑∑
∑∑
=
=
=
=
=








−
=
np
l
n
i
m
j
j
i
q
j
p
i
n
i
m
j
j
i
j
i
q
j
p
i
l
w
v
N
u
N
P
w
v
N
u
N
z
1
2
0
0
,
,
,
0
0
,
,
,
,
δ
 
 
(3) 
where np  is the number of control points. If the number of 
knots an their positions are fixed, the set of weighting factors 
is known and only the control points {
}
{
}
R
P
m
j
n
i
j
i
∈
=
=
1
1
,
are 
considered during the optimization of (3), we have a linear 
problem of least squares. But if the knots or the weights are 
unknown, it will be necessary to solve a non linear problem. 
In many applications the knots location is not necessary, 
therefore knots values are obtained using some heuristic 
techniques. 
IV. SOM 
Artificial neural networks generally consist of simple 
computational elements called neurons which are highly 
interconnected to each other. The neurons are arranged in 
layers that interconnect to other layers. A neuron can receive 
connections from neurons of other layers and even from 
neurons of the same layer that it belong. 
SOM (Self Organizing Map, also known as Kohonen 
network) is a one single layer neural network, denoted  by 
{
}
m
n
n
n
,...,
,
2
1
 where, in its basic configuration, neurons are 
arranged two-dimensionally, and have some neighboring 
neurons, like shows Fig. 2. 
 
 
 
 
Figure 2.  Two-dimensional SOM 
 
The input patterns to the network is a set of n-dimensional 
vectors denoted by {
}
nx
x
x
,...,
,
2
1
. Each neuron of the 
network has a weight vector 
i
w  which has the same 
dimensionality the input patterns have. Basically, SOM is an 
unsupervised neural network, even though there are several 
supervised variants known as Learning Vector Quantization 
(LVQ) [13]. The unsupervised SOM is employ for clustering 
and has the following learning procedure [14]: 
 
1. 
Initialize randomly the weights 
)
,...,
2,1
(
m
i
 
wi
=
 
of the neurons. Let the training time 
1
=
t
. 
2. 
Present new input values {
}
nx
x
x
,...,
,
2
1
 
3. 
Compute the Euclidean distance of all outputs nodes 
to the input point: 
(
)
∑
=
=
−
=
n
j
j
i
j
i
m
i
       
w
x
d
1
2
,
,...,
1
,
 
(4) 


--- Page 5 ---

134 
 
Nallig Eduardo Leal et al
 
 
 
4. 
Find the winner neuron 
in  as the neuron with 
minimum Euclidean distance to the current input 
parameter 
5. 
Compute the size of the neighborhood 
( )t
N
 of the 
winner neuron 
6. 
Update the weights of the neurons in the 
neighborhood according to (5) 
 
(
)
( )
( ) ( )
( )
(
)
t
w
t
x
t
t
w
t
w
i
i
i
−
+
=
+
η
1
 
(5) 
where ( )t
η
 is a gain term decreasing in time. 
7. 
Let 
1
+
= t
t
. Repeat 2-7 until the network is trained. 
V. Evolutionary Strategies 
Evolutionary Strategies (ES) was developed in 1964 by 
Rechenberg and Schwefel in the University of Berlin, as a 
experimental optimization technique [15]. ES tray to pretend, 
in contrast with Genetic Algorithms, the effects of the genetic 
procedures in the phenotype. ES belong to a kind of 
probabilistic numerical optimization algorithms, which 
include Evolutionary Programming (EP), Genetic Algorithms 
(GA) and Genetic Programming (GP), which are known as 
Evolutionary Algorithms. 
The first variant of a ES, called (
)
ES
−
+1
1
, works based 
on two only individuals, a parent and a descendent per 
generation. The descendent is created by applying variations, 
called mutations, binomially distributed (with mean equal to 
zero and variance 
2
σ
) to the parent. The descendent can be 
the parent in the next generation, if is better than the parent, in 
the contrary case, the parent will be the survivor for the next 
generation. 
(
)
ES
−
+1
1
 was replaced for the (
)
ES
−
+ λ
µ
and 
(
)
ES
−
λ
µ,
variants, with 
1
>
µ
 parents and 
1
>
λ
 
descendent per generation. In these new variants, was 
introduced the recombination concept, in order to create 
individuals as the cross of the parent attributes. After mutation 
and the individuals evaluation, the descendents replace the 
parents if are better than they. Depending on the selection 
type, µ  new individuals are selected only from the 
descendent population (
)
ES
−
λ
µ,
, or µ  new individuals 
are selected from the parents and the descendent 
(
)
ES
−
+ λ
µ
. Beside the mutation and the recombination, 
(
)
ES
−
+ λ
µ
and (
)
ES
−
λ
µ,
control the size of the 
mutation step by an auto-adaptation process that learn, the 
mutation step size and optionally the covariance, during the 
evolutionary searching process [15]. ES use three main 
operators for changing the population whereas a stop criterion 
is not reached. 
Recombination: produce new individuals crossing the 
information contained in the parents. Depending on the 
individual variable representation some algorithms can be 
applied for recombination purpose: discrete recombination, 
local intermediate recombination, global intermediate 
recombination, point crossover and n-point crossover. The 
recombination operator allows the exploration of the 
searching space. 
Mutation: After recombination, the descendent are 
changed with a probability ,p  by introducing small variation 
known as mutations. Mutation allows introducing new 
possible solutions and the exploitation near to a given solution 
(individual). Mutation follows the scheme given in (6) and (7) 
Selection: chose the best individuals of the population 
according to a fitting criterion. 
 
(
))1,0
(
)
1,0
(
0
'
i
i N
N
i
i
e
⋅
+
⋅
=
τ
τ
σ
σ
 
(6) 
 
)1,0
(
'
'
i
i
i
i
N
x
x
⋅
+
=
σ
 
(7) 
 
where 
)1,0
(
N
 is a random variable normally distributed with 
mean 0 and variance 1, 
i
τ
τ
,
0
are constant that control the 
mutation step. 
VI. Proposed method 
Let  
{
}
n
p
p
p
P
,...,
,
2
1
=
 be a set of 3D points sampled from a 
real 
object, 
which 
has 
rectangular 
topology, 
and 
{
}
m
s
s
s
S
,...,
,
2
1
=
 be 
a 
NURBS 
surface 
that 
approximates P , our problem consist of minimizing de 
approximation error given by (8). 
 
δ
<
=
S
P
d
S
E
,
)
(
 
(8) 
 
where 
S
P
d
,  is the total distance between P  and the NURBS 
approximation surface S . The parameter δ  is a given user 
error tolerance. It is attempted obtain the configuration of S  
so that (8) is true. 
Since the influence of the NURBS surface control points is 
only local, the sampled points P  will be divided in clusters 
where will carry on a local optimization process, which 
reduces the computational cost of the proposed method. 
The optimization process starts with a clustering of the set 
of points 
,
P  such clustering will be achieved by a SOM. The 
objective of the SOM is to find homogeneous regions where 
run the optimization process without distort the local shape of 
the surface. The points of P  will be presented to the SOM as 
the training patterns. It is hoped at last of the training the SOM 
have found the homogeneous regions where run the 
optimization process, like shows Fig. 3. 
 
 


--- Page 6 ---

Improving NURBS Surface Sharp Feature Representation 
135 
 
 
Figure 3.  Clusters found by SOM. Synthetic dataset sampled 
from the function 
2
2
y
x
z
+
=
 
 
Once 
clustered 
,
P
 an 
evolutionary 
strategy 
(
)
ES
−
+ λ
µ
 will optimize the local fitting of the NURBS 
in each cluster. The evolutionary strategy configuration is as 
follow: 
Individuals: the individuals of the strategy are conformed 
by the weights of the cluster points and the mutation steps σ , 
like shows Fig. 4. 
 
 
1
w  
2
w  
… 
n
w  
1
σ  
2
σ  
… 
n
σ  
 
Figure 4.  Individual of the strategy 
 
where 
iw  are the control point weights and 
i
σ  are the 
mutation step sizes. 
Mutation operator: uncorrelated mutation with n  
mutation step sizes 
'
σ
s is applied to the individuals, 
according to (6) and (7).  
Recombination operator: the recombination operator is 
different for object variable 
iw  than parameters
i
σ . A global 
intermediary recombination is applied to object variables, 
according to (9), whereas a local intermediary recombination 
is applied to mutation step sizes
i
σ , according to (10). 
∑
=
=
ρ
ρ
1
,
1
'
k
i
k
i
b
b
 
(9) 
 
(
)
i
k
i
i
k
i
i
b
u
b
u
b
,
,
2
1
1
'
−
+
=
 
(10) 
where i  is the allele of the individual, 
ib  is the value of the 
allele, ρ  is the size of the recombination pool and µ  is a 
random number uniformly distributed in [0, 1]. 
Selection operator: the best individuals according to the 
aptitude function given in (8). In order to perform a fast 
compute of the distance between the points P  and the 
NURBS surface S , the points of S are store in a kd-tree 
structure, so that the searching process for finding the nearest 
points between P and S  is 
( )
n
log
order. 
The following algorithm summarizes the optimization 
process: 
Perform a clustering of P  by using SOM 
For each cluster do 
 
Set individual size = cluster size 
 
Set population size = µ  
 
Initialize randomly the population 
 
Evaluate the population in the aptitude function (8) 
 
While the stop criterion δ do not reached do 
 
 
For i = 1 to 
9.0
⋅
λ
 do 
 
 
 
(
)
(
)
µ
,
1
rand
i
Population
mut
Ind
=
 
 
 
End for 
 
 
For i = 1 to 
1.0
⋅
λ
 do 
 
 
 
(
)
(
)
µ
,
1
rand
i
Population
rec
Ind
=
 
 
 
End for 
 
 
select
Population =
(
)
λ
µ +
from
 
 
End while 
End for 
VII. Experimental results 
The proposed method was evaluated in scanned data and 
synthetic data. Three different models are used to illustrate 
global and local aspects of the NURBS optimization process. 
The Angel model obtained from Ohio State University and 
two synthetic models. The method was implemented in a 
1.4GHZ Pentium M with 512MB of RAM. 
The surface optimization process takes 3 minutes for 
processing 15K points. After 10 generations average, the 
evolutionary strategy reached the minimum, i.e. the distance 
between P  and the optimized NURBS surface S  reached 
an average of 14% less that the distance between P  and the 
non optimized NURBS surface. Fig. 5 shows the process 
results after applying the method to a set of synthetic data. 
Fig. 6 shows the process results after applying the method to a 
set of scanned data. The profiles presented in Figs. 6a – 6c 
show the complexity of the surface. Fig. 7 shows the 
improvement of the sharp features obtained with our method. 
In Fig. 7b the NURBS surface points follow the control points 
(red circles) closer than in Fig. 7a, which improve sharp 
feature representation. Fig. 7d and Fig. 7f show sharp features 
which in Figs 7c and 7e, respectively, look smoothed. Table I 
presents some statistics of our method results.  
The obtained results show the effectiveness of the proposed 
method. During the tests were used (
)
ES
−
+ λ
µ
and 
(
)
ES
−
λ
µ,
 evolutionary strategies, but the perform of the 
(
)
ES
−
+ λ
µ
 was superior in terms of error reduction. This 
behavior is because (
)
ES
−
+ λ
µ
 incorporate information 
from parents and children to next generation, which maintain  


--- Page 7 ---

136 
 
Nallig Eduardo Leal et al
 
 
 
 
 (a)                   (b) 
 
                                                           (c)                                                                             (d) 
Figure 5.  Method stage after applying in synthetic data. (a) Original dataset (b) Dataset after clustering process (c) Non 
optimized NURBS surface (d) Optimized NURBS surface. 
 
 
(a)                (b)                 (c)  
 
                            
 
    (d)                         (e) 
Figure 6.  Method stage after applying in scanned data. (a) Left profile of the original dataset (b) Frontal profile of the original 
dataset (c) Right profile of the original dataset (d) Non optimized NURBS surface (d) Optimized NURBS surface 


--- Page 8 ---

Improving NURBS Surface Sharp Feature Representation 
137 
       
 
(a)                                                                                (b) 
 
             
 
(c) 
                                                                                                    (d) 
 
             
 
(e)                                                                                     (f) 
 
Figure 7.  Sharp features preservation. (a) (c) (d) Non optimized NURBS surface segment (b) (d) (f) Optimized NURBS 
surface segment, sharp features improvements 
 
the best individuals during the evolutionary process, even 
though in this way the process can be fall in local minimal. In 
contrast with (
)
ES
−
λ
µ,
 which can forget and so exclude 
the information of the best individuals. In terms of execution 
time, (
)
ES
−
λ
µ,
 had a negligible advantage over 
(
)
ES
−
+ λ
µ
. Only results of the (
)
ES
−
+ λ
µ
 are 
provided. During the evolutionary process, the weighting 
factors are restricted to the [0,1] interval. If as a results of a 
mutation, a recombination or a simple initialization, the 
weighting factors are outer to this interval, its value is set to 
zero, or set to one, according to the case.  
 
 
To verify that the shape of the original point cloud were not 
distorted, two metrics were defined. i) The relative error 
bdl
E
 
between the diagonal length of the bounding box of P and 
the diagonal length of the bounding box of S . ii) The 
normalized modeling error 
avg
E
, according to (11), given in 
[16]. In our tests, the relative error was 0.031% and the 
modeling error was 0.01 
                  
N
d
D
N
i
i
avg
∑
−
=
=
1
0
 
                  
L
D
E
avg
avg =
 
(11) 


--- Page 9 ---

138 
 
Nallig Eduardo Leal et al
 
 
 
where 
id  and N  denote the signed distance from the data 
ix  and the number of the total data, respectively. L  is the 
bounding box length. 
VIII. Conclusions 
A new simple method for improving NURBS surface sharp 
feature representation was presented. It was shown both our 
method applicability in regular and irregular surfaces and the 
effectiveness of the method maintaining sharp features. The 
clustering stage reduces the computational cost in both 
memory and processing, due to the local influence of the 
control points of the NURBS surface. 
Our method could be used for optimizing approaches that 
use NURBS patches. In that approaches, it would not be 
necessary the clustering process, since the optimization 
process would be carry on for each NURBS patch. 
A way for improving our method is establish automatically 
the number of cluster where run the optimization process.  
 
Table I. Statistics of our method results 
Processed images 
30 
Average points per image 
15K 
Average points per cluster  
854 
Tests per image 
12 
Time for clustering 
5 seconds 
Time for optimizing 
3 minutes 
Average generations per test  
10 
µ  
5 
λ  
35 
Distance reduction 
14% 
Relative error 
0.031% 
Modeling error 
0.01 
 
References 
[1] A. Myers, “Introductory literature review surface 
reconstruction from three dimensional range data,” The 
University of Adelaidey, Ciudad, Estado, Tech. Rep. 
1999.  
[2] L. 
Piegl 
and 
W. 
Tiller, 
The 
NURBS 
Book. 
Springer-Verlag, 1995. 
[3] M. Ristic, “Efficient fitting of Non-Uniform Rational 
B-Spline surfaces using non-organized 3D data,”  
SPIE’S. vol. 11, pp. 2-6 No. 1, December 2000. 
[4] H. Jung and K. Kim, “A New Parameterization Method 
for NURBS Surface Interpolation,” In The International 
Journal of Advanced Manufacturing Technology, 2000. 
[5] D. F. Rogers, An Introduction to NURBS. 2000. 
[6] E. Cuartas, “Optimización de la representación con 
superficies NURBS de imágenes de rango mediante el 
algoritmo de Levemberg-Marquardt”. EITI Universidad 
Nacional de Colombia sede Medellín. 2004. 
[7] K. Weinert and J. Mehnen, “Discrete NURBS-Surface 
Approximation using an Evolutionary Strategy,” Tech. 
Rep. Department of Machining Technology, University 
of Dortmund, Germany. 2000. 
[8] K. Weinert, T. Surmann and J. Mehnen, "Evolutionary 
Surface Reconstruction Using CSG-NURBS Hybrids," 
Tech. Rep. Department of Machining Technology, 
University of Dortmund, Germany. 2000. 
[9] Y. Fujiwara and H. Sawai, "Evolutionary Computation 
Applied to Mesh Optimization of a 3-D Facial Image," In 
IEEE Transactions on Evolutionary Computation, Vol. 3, 
No. 2, July 1999. 
[10] J. Cordero and J. Parejo, “Curvas y Superficies para 
Modelado Geométrico”, 1st  ed, Alfa Omega, España, 
2002. 
[11] M.Cox, “The Numerical Evaluation of B-Splines,” In J. 
Ins. Mathematics and Applications, Vol. 10, pp. 
134-139, 1972. 
[12] C. De Boor, “On Calculating With B-Splines,” In J. 
Approximation Theory, Vol. 6, No. 1, July, pp. 50-62. 
1972 
[13] R. M. Hristev, The ANN Book. GNU public license,1998 
[14] M. Hoffmann, “Local update of B-spline surfaces by 
Kohonen neural network” 
[15] T. Bäck, and H. Schwefel, Genetic Algorithms in 
Engineering and Computer Science. John Wiley & Sons 
Ltd. 1995. 
[16] I. Park, I. Yun and S. Lee, “Constructing NURBS 
Surface Model from Scattered and Unorganized Range 
Data,” Second International Conference on 3-D Digital 
Imaging and Modeling. 
 
View publication stats
```

---

## Section 1.24: Learning Surface Reconstruction from Point Clouds in the Wild

Source File: Learning Surface Reconstruction from Point Clouds in the Wild.txt

### Paper Content:

```
# Learning Surface Reconstruction from Point Clouds in the Wild.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Learning Surface Reconstruction from Point Clouds in the Wild.pdf
# Output: ../layer2_completion/txt/Learning Surface Reconstruction from Point Clouds in the Wild.txt


--- Page 1 ---

HAL Id: tel-03968622
https://hal.science/tel-03968622v2
Submitted on 28 Mar 2023
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Learning Surface Reconstruction from Point Clouds in
the Wild
Raphael Sulzer
To cite this version:
Raphael Sulzer. Learning Surface Reconstruction from Point Clouds in the Wild. Machine Learning
[cs.LG]. Université Gustave Eiffel, 2022. English. ￿NNT : 2022UEFL2042￿. ￿tel-03968622v2￿


--- Page 2 ---

 
Learning Surface Reconstruction
from Point Clouds in the Wild
Th`ese de doctorat de l’Universit´e Gustave Eiffel
´Ecole doctorale n° 532, Math´ematiques, Science, et Technologie de l’Information et de la Communication (MSTIC)
Sp´ecialit´e de doctorat : Signal, Image, et Automatique
Unit´e de recherche : Laboratoire des Sciences et Technologies de l’Information Geographique (LASTIG), IGN.
Th`ese pr´esent´ee et soutenue `a l’Universit´e Gustave Eiffel,
le 17/10/2022, par
Raphael SULZER
Composition du Jury
Pierre ALLIEZ
Pr´esident, Rapporteur
Directeur de recherche, TITANE, INRIA, France
Julie DIGNE
Rapportrice
Charg´ee de recherche, LIRIS, CNRS, France
Michael WIMMER
Examinateur
Professeur, Faculty of Informatics, TU WIEN, Autriche
Renaud MARLET
Co-Directeur de th`ese
Directeur de recherche, LIGM, Ecole des Ponts, France
Bruno VALLET
Directeur de th`ese
Directeur de recherche, LASTIG, IGN-ENSG, France
Lo¨ıc LANDRIEU
Encadrant
Charg´e de recherche, LASTIG, IGN-ENSG, France


--- Page 3 ---

©2022 – Raphael Sulzer
all rights reserved.
www.raphaelsulzer.de


--- Page 4 ---

Learning Surface Reconstruction
from Point Clouds in the Wild
Abstract
Modern 3D acquisition technology unlocks the possibility to represent the world under
the form of 3D point clouds. However, point clouds are usually not suﬃcient to model
complex physical processes. Instead, a variety of applications in science and engineering
require a representation of objects or scenes under the form of a continuous surface.
In this thesis, we consider the problem of surface reconstruction from point clouds
using supervised deep learning techniques.
In particular, we are interested in point
clouds in the wild, i.e. generated from measurements outside of the laboratory, either
directly with 3D scanners or indirectly through multi-view stereo. Such point clouds
often depict large scenes with multiple diﬀerent objects and clutter, and include defects
such as noise, outliers, non-uniform sampling or missing data. These characteristics
complicate the reconstruction of a topologically and geometrically accurate surface from
point clouds in the wild.
After having been successfully deployed on many related computer vision tasks, su-
pervised deep learning has recently been used to address the surface reconstruction
problem. Deep surface reconstruction (DSR) can learn point cloud defects or surface
patterns from a given training set, and use the learned knowledge during reconstruction.
However, current DSR methods exhibit two main limitations. First, supervised deep
learning often requires a lot of data to train. However, point clouds in the wild typi-
cally depict complex objects or scenes, making it costly, ambiguous, or intractable to
gather true surfaces. Second, existing DSR algorithms are often too computation- and
memory-intensive to process millions of points. In this thesis, we address both issues by
introducing novel supervised deep learning methods to handle large-scale point clouds
with real-world characteristics while only training on small synthetic datasets.
The thesis includes three main contributions. First, we survey and benchmark several
surface reconstruction methods, including learning and traditional approaches proposed
over the last three decades. To make the problem tractable and produce geometrically
and topologically accurate results even under challenging conditions, non-learning meth-
ods often rely on priors on the input point cloud or output surface. In contrast, DSR
algorithms learn these priors directly from the training set of point clouds and corre-
sponding true surfaces. We benchmark diﬀerent methods on the task of reconstructing
iii


--- Page 5 ---

objects from synthetically scanned defect-laden point clouds. Our ﬁndings show that
DSR methods are able to reconstruct accurate and complete surfaces from point clouds
with moderate defects, given that similar defects are also present during training. How-
ever, the reconstruction quality from point clouds with unseen defect type is often worse
compared to non-learning methods. Traditional methods, on the other hand, show a
high robustness to defects, even with constant parametrization for diﬀerent inputs.
Another shortcoming of most learning based methods is the fact that they ignore
sensor poses and only operate on point locations. Sensor visibility holds meaningful
information regarding space occupancy and surface orientation. We present two sim-
ple strategies to augment point clouds with visibility information, which can directly
be integrated with various DSR architectures with minimal adaptation. Our proposed
modiﬁcations consistently improve the accuracy of generated surfaces, as well as the
capability of the networks to generalize to unseen domains. We also release syntheti-
cally scanned versions of popular shape datasets to encourage the development of DSR
algorithms capable of using visibility information.
Lastly, we introduce Delaunay-Graph Neural Networks (DGNNs), a novel learning-
based visibility-aware surface reconstruction method for large-scale point clouds in the
wild. DGNN relies on a 3D Delaunay tetrahedralisation of the input point cloud, whose
cells are classiﬁed as inside or outside the surface by a graph neural network and an
energy model solvable with a graph cut. The graph neural network makes use of both
local geometric attributes and line-of-sight visibility information to learn a visibility
model from a small amount of synthetic training data while generalizing to real-life
acquisitions.
iv


--- Page 6 ---

Reconstruction de Surfaces `a partir de Nuages de Points
par Apprentissage Profond
R´esum´e
Les technologies d’acquisition 3D r´ecentes permettent de repr´esenter le monde sous
la forme de nuages de points 3D. Cependant, ces nuages de points ne sont g´en´eralement
pas suﬃsants pour mod´eliser des processus physiques complexes.
Au contraire, de
nombreuses applications en sciences et en ing´enierie n´ecessitent une repr´esentation sous
la forme d’une surface continue.
Dans cette th`ese, nous consid´erons le probl`eme de reconstruction de surface `a partir
de nuages de points par apprentissage profond supervis´e.
En particulier, nous nous
int´eressons `a la reconstruction de surface `a partir de nuages de points r´eels, c’est-`a-dire
g´en´er´es `a partir de mesures eﬀectu´ees sur le terrain: soit directement avec des scan-
ners 3D, soit indirectement par photogrammetrie. Ces nuages repr´esentent souvent de
grandes sc`enes contenant de multiples objets de formes diverses. Ces nuages peuvent
aussi inclure des d´efauts tels que du bruit d’acquisition, des valeurs aberrantes, un
´echantillonnage non uniforme ou des donn´ees manquantes, ce qui complique la recon-
struction d’une surface topologiquement et g´eom´etriquement pr´ecise.
Apr`es avoir ´et´e utilis´e avec succ`es pour de nombreuses tˆaches de vision par ordi-
nateur, l’app-rentissage profond supervis´e a r´ecemment ´et´e appliqu´e au probl`eme de
reconstruction de surface. Cependant, les m´ethodes courantes souﬀrent encore de deux
principales limitations. Tout d’abord, l’apprentissage profond supervis´e n´ecessite sou-
vent un grand nombre de donn´ees annot´ees. Les nuages de points r´eels d´ecrivent des
objets ou des sc`enes complexes, ce qui rend la collecte de surfaces r´eelles coˆuteuse, am-
bigu¨e ou math´ematiquement diﬃcile. Deuxi`emement, les algorithmes d’apprentissage
existants sont souvent trop gourmands en calcul et en m´emoire pour traiter des millions
de points simultan´ement. Nous abordons ces deux probl`emes en introduisant de nou-
velles m´ethodes d’apprentissage profond supervis´e pour traiter des nuages de points `a
grande ´echelle avec des caract´eristiques du monde r´eel tout en ´etant entraˆın´ees sur de
petits ensembles de donn´ees synth´etiques.
Cette th`ese comprend trois contributions principales. Tout d’abord, nous passons en
revue et ´evaluons plusieurs m´ethodes de reconstruction de surface `a partir de nuages de
points. En plus des m´ethodes d’apprentissage, nous ´evaluons certaines des approches
traditionelles propos´ees au cours des trois derni`eres d´ecennies. Pour rendre le probl`eme
iii


--- Page 7 ---

tractable et produire des r´esultats g´eom´etriquement et topologiquement pr´ecis mˆeme
dans des conditions diﬃciles, les m´ethodes sans apprentissage reposent souvent sur des
hypoth`eses sur la structure des nuages de points en entr´ees ou des surfaces reconstruites.
En revanche, les algorithmes de reconstruction de surfaces par apprentissage profond
(DSR) apprennent ces hypoth`eses directement `a partir d’un ensemble d’entraˆınement de
nuages de points et des surfaces r´eelles leur correspondant. Nous ´evaluons les m´ethodes
d’apprentissage et traditionnellles pour la tˆache de reconstruction d’objets `a partir de
nuages de points avec d´efauts scann´es synth´etiquement. Nos r´esultats montrent que
les m´ethodes DSR sont capables de reconstruire des surfaces pr´ecises et compl`etes `a
partir de nuages de points pr´esentant un degr´e mod´er´e de d´efauts att´enu´es, `a condition
que ces d´efauts soient pr´esents pendant l’entraˆınement. Cependant, la qualit´e de la
reconstruction pour les nuages de points avec d´efauts non presents dans l’ensemble
d’entraˆınement est souvent moins bonne que celle des m´ethodes sans apprentissage. Les
m´ethodes sans apprentissage, en revanche, sont d’une grande robustesse aux d´efauts,
mˆeme avec une param´etrisation constante pour diﬀ´erentes entr´ees.
Un autre d´efaut de la plupart des m´ethodes DSR est le fait qu’elles ignorent la pose
des capteurs et n’op`erent que sur la position des points. La visibilit´e des capteurs con-
tient pourtant des informations importantes sur l’occupation de l’espace et l’orientation
de la surface. Nous pr´esentons deux fa¸cons simples d’enrichir les nuages de points avec
des informations de visibilit´e, qui peuvent ˆetre directement exploit´ees par des r´eseaux de
reconstruction de surface en ne n´ec´essitant qu’une adaptation minimale. Nous montrons
que les modiﬁcations propos´ees am´eliorent syst´ematiquement la pr´ecision des surfaces
g´en´er´ees ainsi que la capacit´e des r´eseaux `a g´en´eraliser `a des nouveaux domaines. Nous
publions ´egalement les versions scann´ees synth´etiquement de base de donn´ees de formes
3D largement utilis´ees, aﬁn d’encourager le d´eveloppement d’algorithmes DSR capables
d’utiliser les informations de visibilit´e. Enﬁn, nous pr´esentons une nouvelle m´ethode
de reconstruction de surface bas´ee sur l’apprentissage et tenant compte de la visibilit´e
pour les nuages de points r´eels `a grande ´echelle. Notre m´ethode repose sur une tri-
angulation 3D de Delaunay (3DT) dont les cellules sont class´ees comme int´erieur ou
ext´erieur de la surface recherch´ee par un r´eseau de convolution sur graphe (GNN) et
un mod`ele ´energ´etique r´esolvable avec une coupe de graphe. Le GNN utilise `a la fois
des attributs g´eom´etriques locaux et des informations de visibilit´e pour apprendre un
mod`ele de visibilit´e `a partir d’une petite quantit´e de donn´ees de formes synth´etiques
tout en g´en´eralisant aux acquisitions r´eelles.
iv


--- Page 8 ---

Contents
1
Introduction
1
1.1
Surface reconstruction from 3D point clouds in the wild . . . . . . . . .
5
1.2
Problem statement and objectives . . . . . . . . . . . . . . . . . . . . .
10
1.3
Reading guide and contributions . . . . . . . . . . . . . . . . . . . . . .
11
2
A Survey and Benchmark of Automatic Surface Reconstruc-
tion from Point Clouds
14
2.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2
Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.3
Surface deﬁnition, representations, properties and reconstruction . . . .
17
2.4
Survey
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.5
Benchmark setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.6
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3
Deep Surface Reconstruction from Point Clouds with Visi-
bility Information
53
3.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.2
Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.3
Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.4
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
3.5
Limitations and perspectives . . . . . . . . . . . . . . . . . . . . . . . .
74
3.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4
Scalable Surface Reconstruction with Delaunay-Graph Neu-
ral Networks
75
4.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.2
Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.3
Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.4
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.5
Limitations and perspectives . . . . . . . . . . . . . . . . . . . . . . . .
97
v


--- Page 9 ---

4.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
5
Conclusion
108
5.1
Summary and conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .
109
5.2
Outlook and future work
. . . . . . . . . . . . . . . . . . . . . . . . . .
110
References
123
vi


--- Page 10 ---

List of Figures
1.1
3D urban analysis pipeline
. . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Diﬃculties in surface reconstruction from point clouds . . . . . . . . . .
3
1.3
Point clouds in the wild and reconstructed surfaces
. . . . . . . . . . .
4
1.4
Point cloud defects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.5
Point clouds with visibility information . . . . . . . . . . . . . . . . . .
8
1.6
Learning and non-learning based surface reconstruction . . . . . . . . .
9
2.1
Approximating and interpolating surfaces from point clouds
. . . . . .
18
2.2
Synthetic and real point clouds . . . . . . . . . . . . . . . . . . . . . . .
31
2.3
Synthetic scanning procedure . . . . . . . . . . . . . . . . . . . . . . . .
32
2.4
Ground truth shapes of the benchmark datasets . . . . . . . . . . . . .
33
2.5
Learning-based reconstructions . . . . . . . . . . . . . . . . . . . . . . .
43
2.6
Optimization-based experiments . . . . . . . . . . . . . . . . . . . . . .
46
2.7
Learning- and optimization-based reconstructions
. . . . . . . . . . . .
48
2.8
Learning- and optimization-based reconstructions of real-world point
clouds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.1
Surface reconstruction with visibility information . . . . . . . . . . . . .
55
3.2
Visibility-augmented point cloud . . . . . . . . . . . . . . . . . . . . . .
57
3.3
Object-level reconstruction on ModelNet10 I . . . . . . . . . . . . . . .
63
3.4
Object-level reconstruction on ModelNet10 II . . . . . . . . . . . . . . .
64
3.5
Scene-level reconstruction on Synthetic Rooms . . . . . . . . . . . . . .
66
3.6
Out-of-domain object-level reconstruction on ShapeNet . . . . . . . . .
68
3.7
Cut of out-of-domain object-level reconstruction on ShapeNet
. . . . .
69
3.8
Out-of-domain object-level reconstruction from real-world scans
. . . .
71
3.9
Cut of out-of-domain object reconstruction of Ignatius . . . . . . . . .
72
3.10 Out-of-domain scene-level reconstruction on SceneNet and ScanNet
. .
73
4.1
Scene-level reconstruction on ETH3D . . . . . . . . . . . . . . . . . . .
77
4.2
DGNN pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.3
DGNN visibility features
. . . . . . . . . . . . . . . . . . . . . . . . . .
80
vii


--- Page 11 ---

4.4
Graph neural network scheme
. . . . . . . . . . . . . . . . . . . . . . .
84
4.5
Qualitative results on Berger et al.’s anchor
. . . . . . . . . . . . . . .
91
4.6
Qualitative results on Berger et al.’s daratech . . . . . . . . . . . . . . .
92
4.7
Qualitative results on Berger et al.’s dancing children . . . . . . . . . .
99
4.8
Qualitative results on Berger et al.’s gargoyle . . . . . . . . . . . . . . .
100
4.9
Qualitative results on Berger et al.’s lord quasimoto . . . . . . . . . . .
101
4.10 Comparison of DGNN and ConvONet on ETH3D reconstruction . . . .
102
4.11 Numerical results on ETH3D . . . . . . . . . . . . . . . . . . . . . . . .
103
4.12 Comparison of DGNN and Jancosek et al. on ETH3D reconstruction
.
104
4.13 Failure case on ETH3D . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.14 Indoor ETH3D reconstruction
. . . . . . . . . . . . . . . . . . . . . . .
106
4.15 Outdoor ETH3D reconstruction
. . . . . . . . . . . . . . . . . . . . . .
107
viii


--- Page 12 ---

List of Tables
2.1
Overview of surface- and volume-based surface reconstruction methods
22
2.2
Scanning conﬁgurations for Berger et al. benchmark . . . . . . . . . . .
30
2.3
Benchmark setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.4
Detailed benchmark setup . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.5
Numerical results for learning-based reconstructions . . . . . . . . . . .
42
2.6
Numerical results for optimization-based surface reconstruction . . . . .
45
2.7
Numerical results for learning- and optimization-based reconstructions .
47
2.8
Runtimes for learning-based reconstruction . . . . . . . . . . . . . . . .
50
2.9
Summary of benchmark results . . . . . . . . . . . . . . . . . . . . . . .
51
3.1
Ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.2
Numerical results for object-level reconstruction . . . . . . . . . . . . .
62
3.3
Numerical results for scene-level reconstruction . . . . . . . . . . . . . .
65
3.4
Numerical results for out-of-domain object-level reconstruction . . . . .
67
3.5
Runtimes for object-level reconstruction with visibility information . . .
70
4.1
DGNN Ablation study
. . . . . . . . . . . . . . . . . . . . . . . . . . .
89
4.2
Numerical results for Berger et al. benchmark
. . . . . . . . . . . . . .
93
4.3
Numerical results for ETH3D reconstructions . . . . . . . . . . . . . . .
94
4.4
Numerical results for ETH3D reconstruction per scene . . . . . . . . . .
95
4.5
Runtimes and memory footprint . . . . . . . . . . . . . . . . . . . . . .
97
ix


--- Page 13 ---

Glossary
2D
two-dimensional
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3D
three-dimensional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
3DT 3D Delaunay tetrahedralisation . . . . . . . . . . . . . . . . . . . . . . . . .
21
PSR Poisson Surface Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . .
25
SPSR Screened Poisson Surface Reconstruction . . . . . . . . . . . . . . . . . . .
26
IER intrinsic-extrinsic ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
IGR Implicit Geometric Regularisation
. . . . . . . . . . . . . . . . . . . . . . .
29
LIG
Local Implicit Grids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
P2M Point2Mesh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
SAP Shape As Points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
P2S
Points2Surf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
ONet Occupancy Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
ConvONet Convolutional Occupancy Networks
. . . . . . . . . . . . . . . . . .
27
DGNN Delaunay-Graph Neural Network
. . . . . . . . . . . . . . . . . . . . . .
25
POCO Point Convolution for Surface Reconstruction . . . . . . . . . . . . . . . .
28
IoU
intersection over Union
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
CD
Chamfer distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
NC
normal consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
PCA principal component analysis . . . . . . . . . . . . . . . . . . . . . . . . . .
25
MLP multilayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
BCE binary cross entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
MSE mean square error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
CNN convolutional neural network . . . . . . . . . . . . . . . . . . . . . . . . . .
24
GNN graph neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
x


--- Page 14 ---

MVS multi-view stereo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
SfM structure from motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
LiDAR Light Detection and Ranging
. . . . . . . . . . . . . . . . . . . . . . . .
1
NeRF neural radiance ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
SDF signed distance function . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
OF
occupancy function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
FCN fully connected network . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
DSR deep surface reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
TFT triangle-from-tetrahedra . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
xi


--- Page 15 ---

Acknowledgments
I thank my supervisors Lo¨ıc, Bruno and Renaud. They provided a great amount of
inspiration and experience during my PhD and were good team mates :). Thank you!
I thank the whole LaSTIG lab for welcoming me with open arms: Alexandre, Ali,
Amin, Anatol, Arno, Cl´ement, Damien, Evelyn, Ewelina, Lˆamˆan, Laurent, Mathieu,
Mehdi, Mohamed, Nathan, Oussama, Paul, Qasem, St´ephane, Teng, Romain, Yanis,
Yilin and especially Vivien and other great friends I met in the past 3,5 years in Paris:
Danijela, Adrien, Sol`ene, Diwan, Ma¨elle, Malo, Iris, Mathilde, Negin, Juliette and Sasha.
Special thanks also go to Arkose Montreuil, la Forˆet de Fontainebleau et le Bois de
Vincennes for being excellent places to climb and relax.
xii


--- Page 16 ---

1
Introduction
Modern three-dimensional (3D) acquisition technology, such as Light Detection and
Ranging (LiDAR) or multi-view stereo (MVS) brought the ability to record the world
under the form of 3D point clouds. However, point clouds are usually not suﬃcient
to model complex physical processes. Instead, a variety of applications in science and
engineering require a representation of objects or scenes under the form of a continu-
ous surface. For example, in medicine, the continuous surface of an organ allows for
diagnosing and monitoring malformations [111]. In robotics, the surface of a robots en-
vironment enables path planning and collision detection [24]. In architecture, building
and civil engineering, the surface of a city permits the computation of light, heat or noise
propagation [14]; and in environmental engineering, a continuous surface of a terrain
permits to model wind or ﬂoods [73] (see Figure 1.1). Digitally and continuously mod-
eling an existing surface involves converting physical measurements into mathematical
and digital models [1]. This process includes surface reconstruction from point clouds,
one of the key scientiﬁc challenges in digital geometry processing.
Surface reconstruction from point clouds addresses the problem of producing a con-
tinuous representation of a surface of which discrete point samples have been acquired.
However, discrete point samples usually do not cover the entire surface geometry and
do not contain topological information about the recorded surface (cf. Figure 1.2a and
1.2b). Theoretically, there are inﬁnitely many surfaces that can pass through, or near
the point samples. This means there is no unique solution to surface reconstruction
from point clouds, making it an ill-posed problem. If no prior information is given, a
reconstruction algorithm can only approximate the real surface between sampled points.
1


--- Page 17 ---

(a) Data acquisition
(b) Surface reconstruction
(c) Analysis
Figure 1.1: 3D urban analysis pipeline: 3D point clouds of urban environments are often
acquired by aerial laser scanning or aerial imagery (a). The acquired point cloud can be
used to generate a digital surface model using a surface reconstruction algorithm (b) —
the central topic of this thesis. The surface model can then be used for advanced urban
analysis, such as a numerical wind simulation (c). Here, this pipeline is exempliﬁed on a
part of the city of Strasbourg, France around the Mus´ee zoologique.
2


--- Page 18 ---

(a) Unknown Topology
(b) Unknown Geometry
(c) Acquisition Defects
Figure 1.2: Diﬃculties in surface reconstruction from point clouds: In each plot, we
show the real surface
, point samples
, and possible reconstructions
. The correct
topology and geometry of the real surface are not known from the point samples (a,b). The
point samples may also include acquisition defects such as noise (c). The goal of any surface
reconstruction algorithm is ﬁnding a good approximation of the real surface, in terms of its
geometry and topology. Learning based surface reconstruction can learn shape patterns or
sampling errors such as the one exempliﬁed here, and use the learned knowledge during
reconstruction for a better approximation.
Therefore, the goal of surface reconstruction from point clouds is to ﬁnd a good approx-
imation of the real surface, in terms of its geometry and topology.
Another challenge in surface reconstruction from point clouds lies in the fact that the
sampling of a real surface is often not error free (cf. Figure 1.2c). Low quality sensors
or diﬃcult acquisition conditions can result in point cloud defects, which complicate the
reconstruction of a good surface approximation. For these reasons, surface reconstruc-
tion from point clouds has been a long standing problem in digital geometry processing.
In this thesis, we revisit the problem of surface reconstruction from point clouds with
modern learning-based techniques. We aim to develop deep learning architectures that
can learn point cloud defects or surface patterns from a given training set, and use
the learned knowledge during reconstruction for a better surface approximation. We
thereby focus on surface reconstruction from point clouds in the wild. Point clouds in
the wild are recorded in an uncontrolled environment outside of the laboratory. They
can depict dynamic environments with multi-scale surfaces, ranging from single objects
to entire countries and include defects such as noise, outliers, nonuniform sampling or
missing data.
In this ﬁrst chapter, we brieﬂy introduce the problem of surface reconstruction from
3D point clouds in the wild, the concept of visibility information, and existing deep
learning architectures for surface reconstruction. We then discuss the objectives and
limitations of this work, as well as its contributions. Finally, we provide a reading guide
for the rest of this document.
3


--- Page 19 ---

(a) Aerial LiDAR Point Cloud
(b) Terrestrial MVS Point Cloud
(c) Aerial LiDAR Surface Reconstruction
(d) Terrestrial MVS Surface Reconstruction
Figure 1.3: Point clouds in the wild and reconstructed surfaces: Real world point
clouds acquired under challenging conditions often have defects such as noise, outliers, non-
uniform sampling or missing data (a, b). Surfaces reconstructed from this data often inherit
some of these defects (c, d). In (a), we show an aerial LiDAR point cloud. Due to the
angle of incident rays there are very few points on the fa¸cades of the buildings. As a result,
the inferred surface in (c) is missing details such as windows on the fa¸cades. In (b) we
show an MVS point cloud from terrestrial images. The complex shapes and low textured
surfaces lead to noise, outliers and missing data in the point cloud, which, in turn, leads to
erroneous and noisy surface parts in (d). The input point clouds are from Strasbourg3D1
(a) and ETH3D [99] (b). The reconstructions are generated with the traditional Screened
Poisson Surface Reconstruction [65] (c) and the method proposed in Vu et al. [114] (d).
1https://3d.strasbourg.eu/
4


--- Page 20 ---

(a) Shape
(b) No Defect
(c) Nonuniform
(d) Noise
(e) Outliers
(f) Missing Data (g) Misaligned
(h) No Defect
(i) Nonuniform
(j) Noise
(k) Outliers
(l) Missing Data (m) Misaligned
Figure 1.4: Point cloud defects [9]: We show a 2D shape
and samples
. The
samplings include diﬀerent forms of defects which aﬀect the reconstructed surface
.
1.1
Surface reconstruction from 3D point clouds in the wild
The most commonly used methods for acquiring point clouds outside of the laboratory
are active LiDAR and passive MVS (cf. Figure 1.3). LiDAR sensors or cameras for
an MVS acquisition are used stationary or mounted on moving platforms such as cars,
airplanes or satellites. The sensors itself range from low quality sensors, e.g. built into
smartphones, to high quality sensors, mounted on dedicated surveying platforms. The
diverse set of acquisition subjects and techniques leads to a variety of diﬀerent properties
and imperfections for point clouds in the wild. Most traditional approaches for surface
reconstruction rely on handcrafted or data driven priors on the input point cloud or
output surface to overcome acquisition defects. This approach allows to reconstruct
surfaces suﬃcient for visualisation and a variety of other applications. However, when
input point clouds include heavy defects, or when highly accurate surfaces are required,
current surface reconstruction methods may not suﬃce [13].
5


--- Page 21 ---

1.1.1
Point cloud properties and imperfections
Throughout this document we consider P ∈R3×P a 3D point cloud deﬁned by the
absolute point positions in space, where P is the number of points p in the cloud.
Point cloud defects can be classiﬁed into the ﬁve diﬀerent groups illustrated in Fig-
ure 1.4. We brieﬂy discuss the ﬁve groups here and refer the reader to the survey of
Berger et al. [9] for a more complete discussion.
Deficient and nonuniform sampling density.
An important relation for surface
reconstruction from point clouds can be established between the local feature size of a
surface S ⊂R3 and the density of P. The local feature size LFS(x) is deﬁned as the
minimal distance between a point x ∈S and the medial axis of S [2]. A point cloud
with point samples p ∈P is called an ϵ-sampling of S, if every point x has a point
p in Euclidean distance at most ϵLFS(x) [28]. Some traditional surface reconstruction
algorithms guarantee topological validity and geometric convergence between the recon-
struction and S, given that the input point cloud is noise free and has a small ϵ-value
[2, 15, 28]. However, the local feature size can in general not be computed without hav-
ing access to S. Thus, an ϵ-sampling can not be guaranteed for point clouds in the wild.
Uniformly sampling the surface with a high density leads to large redundant amounts of
data, while uniformly sampling with a low density can lead to loss of information. One
option for point cloud acquisition can thus be to deﬁne the minimal feature size that
should be recovered and drive the acquisition process accordingly [1]. However, point
clouds in the wild often non-uniformly sample the underlying surface, e.g. as a result
of varying distance and orientation between surface and sensor [9]. Such a nonuniform
sampling can pose problems for certain surface reconstruction methods, e.g. for deﬁning
ﬁxed distances to determine local neighborhoods [9], or for partitioning the point cloud
with regular grids.
Noise.
LiDAR points can include noise in the form of oﬀsets from the real surface,
distributed along the ray coming from the sensor [96]. Points that are randomly dis-
tributed near the surface are common noise for MVS acquisitions. The noise is a result
of mismatches during dense point cloud reconstruction or erroneous approximations of
the camera position and orientation. This happens in particular for low texture sur-
faces, or in highly variable outdoor environments which include vegetation and clutter.
The level of noise can also vary within one MVS point cloud. Estimating a model rep-
resenting such kind of noise is often not possible [114]. Noisy MVS point clouds are
thus challenging for surface reconstruction algorithms, especially if the level of noise is
close to the local feature size. The challenge for surface reconstruction algorithms is to
preserve small features in the surface while discarding or smoothing noise.
6


--- Page 22 ---

Outliers.
Outliers are points randomly distributed in the acquisition space, not close
to the surface. In MVS point clouds, single outlier points or groups of outliers are often
the result of large mismatches or erroneous orientation between images. Furthermore,
they occur in LiDAR acquisitions due to reﬂecting objects and glass [99]. Structured
outliers pose a signiﬁcant problem for surface reconstruction algorithms, as they are
hard to ﬁlter and can produce ghost structures in the surface (see Figure 1.4k).
Missing data.
The main reason for missing data in point clouds are (self-)occlusions.
This happens when an object or surface part lies between subject and sensor, i.e. the
subject is occluded.
Even for surfaces with little complexity this is very common.
Missing data is one of the main challenges for surface reconstruction algorithms, because
it requires to reconstruct entire surface parts without local input information.
Misaligned scans.
Large-scale scenes often require to split the acquisition into sev-
eral smaller parts, e.g. for changing the sensor position during multiple stationary acqui-
sitions. Subsequently, the scans have to be aligned to produce one point cloud depicting
the entire scene. If the position and orientation between the scans is unknown, it has
to be inferred using an alignment technique called registration [57]. This process may
however not be error free and lead to misaligned scans.
In general, LiDAR points clouds are accurate but their density varies with the acqui-
sition distance and angle. Furthermore, occlusions are more common for some LiDAR
sensors, due to a limited ﬁeld of view. MVS point clouds, acquired in the wild, are
usually less accurate, but provide denser information, with less missing data. One way
to overcome some of the sensor limitations is to combine diﬀerent acquisitions [25].
However, this can be more costly and time consuming.
1.1.2
Visibility information
Both LiDAR and image-based point cloud acquisitions allow to directly measure or
recover the position of the sensors relative to P. We consider C ∈R3×C the positions
of a set of sensors, where C is the number of sensors c. C can very from a few (for
stationary acquisitions) to hundreds of diﬀerent positions (for moving sensors).
Given a point p and its corresponding sensor position c, we call the half open segment
between a point and its sensor the line-of-sight cp (cf. Figure 1.5). It is half open,
because it includes c but not p.
Up to uncertainties in the acquisition, cp must lie
outside the scanned subject, because it is the path of reﬂected light from surface point
to sensor. We call this additional information visibility information.
While visibility information is key for point cloud reconstruction techniques such as
structure from motion (SfM) and MVS, it is only explored by a small subset of surface
7


--- Page 23 ---

(a) LiDAR Point Cloud
(b) Image Point Cloud
(c) LiDAR Point Cloud With Visibility
(d) Image Point Cloud With Visibility
Figure 1.5: Point clouds with visibility information: Terrestrial point clouds (a, b) from
the terrace scene of ETH3D [99]. We visualize some of the sensor positions
and lines-
of-sight
(c, d). This so-called visibility information can help surface reconstruction
algorithms in identifying free space and in correctly orienting the surface towards the sensor.
8


--- Page 24 ---

(a) Point Cloud
(b) SPSR [65]
(c) Labatut et al. [71]
(d) Ground Truth Surface
(e) SAP [91]
(f) DGNN [109]
Figure 1.6:
Learning and non-learning based surface reconstruction: We show a
(synthetic) point cloud (a), its corresponding ground truth surface (d) and corresponding
non-learning (b,c) and learning based (e,f) surface reconstructions.
The learning based
methods are related to the non-learning methods, however, they have the ability to incor-
porate learned knowledge from a training set during reconstruction. In this case, the two
learning methods were trained on a large shape database including similar (airplane) models
as the one shown here. The training point clouds had the same density and level of noise
as the one shown here, which makes the learning methods more robust to these defects.
reconstruction algorithms. However, as shown by one of the most widely used surface
reconstruction algorithms [71], incorporating visibility information into the surface re-
construction problem can be beneﬁcial by providing a prior that lowers the solution
space, and helps in correctly orienting the surface.
1.1.3
Deep Learning for Surface Reconstruction
In recent years the deep learning community started to address the surface reconstruc-
tion problem [49, 79, 82]. Most deep surface reconstruction (DSR) approaches use su-
9


--- Page 25 ---

pervised learning to learn point cloud characteristics or shape patterns directly from a
training set. However, supervised deep learning often requires large amounts of training
data. To this end, DSR methods train on large synthetic shape dataset with continuous
true surfaces, sampled to generate point clouds. For mimicking point cloud defects, e.g.
noise is added to the point clouds following a simple Gaussian noise model. However,
such trained models may not be useful to reconstruct unseen shapes from point clouds in
the wild [86]. On the other hand, directly training DSR methods with point clouds in the
wild is often not feasible, as gathering and modeling true surfaces for such point clouds
is either too complex, costly or mathematically intractable. Furthermore, a variety of
methods work on regular 3D grids, i.e. voxels, which are processed with large fully
connected network (FCN) architectures. As a result, existing architectures are often
too compute- and memory-intensive to process large-scale point clouds. Lastly, existing
learning-based DSR algorithms only operate on point locations and ignore visibility in-
formation. Neural radiance ﬁelds (NeRFs) and other diﬀerentiable volumetric rendering
techniques [84, 86, 87] use visibility information to reconstruct a surface directly from
images. However, they often require a slow optimization process and leverage little or
no shape priors.
1.2
Problem statement and objectives
In this thesis, we aim to overcome some of the main limitations of existing traditional
and learning based surface reconstruction methods, namely:
• Traditional methods can not incorporate learned priors, e.g. to ﬁll large parts of
missing data or to ﬁlter and smoothen defects suﬃciently
• Learning methods do not scale to large point clouds and their generalization
capability to unseen point cloud defects and shapes has not yet been studied
systematically
• Learning methods do not yet incorporate visibility information
We investigate the requirements of deep learning based architectures for surface re-
construction from point clouds in the wild. We aim to develop supervised deep learning
architectures that can handle large-scale point clouds, while being trained on small syn-
thetic datasets. The methods should be robust to common defects for point clouds in
the wild and produce topologically valid and geometrically accurate surfaces. We con-
sider point clouds with visibility information and aim to develop algorithms that can
beneﬁt by using this additional information.
10


--- Page 26 ---

1.2.1
Scope and limitations
A major advantage of deep learning methods is that they can be trained end-to-end for
a speciﬁc task without the necessity of intermediate representations. For this reason
we focus on surface reconstruction methods that take a defect-laden point cloud as
input and export a continuous surface. We neither discuss pre-processing algorithms,
such as point cleaning [96] or resampling methods [80], nor post-processing algorithms,
such as mesh repair, surface smoothing, or sharp feature recovery. Ideally, we would
like to directly export surfaces which can be visualised and are valid input for surface
analysis algorithms. To this end, we focus on the reconstruction of a watertight, non-
self intersecting surface, a useful requirement for a variety of downstream engineering
applications. This entails that the discussed and developed algorithms aim to ﬁll all
wholes in the point set and provide a watertight reconstruction, even if there is no
guarantee.
LiDAR and image based sensors are the two main sensors for acquiring point clouds in
the wild. We study both, unprocessed LiDAR and MVS point clouds. However, albeit
the end-to-end paradigm, we do not investigate methods that can directly use images
in an end-to-end fashion to reconstruct surfaces. We rely on dedicated pipelines for this
step. This has the beneﬁt that our developed architectures can handle both LiDAR and
image based point clouds without modiﬁcation, and can even input a combination of
both.
During the course of this thesis, a variety of surface reconstruction methods based
on deep neural networks have emerged.
We call all of these methods deep surface
reconstruction. However, not all deep surface reconstruction methods incorporate shape
priors learned from a training set. While we also discuss such non-learning (optimization
based) methods in Chapter 2, they are not the main focus of our work.
Recent research also suggest to use end-to-end learning for directly performing surface
analysis, such as ﬂuid dynamics from scanned point clouds. This approach removes the
need to explicitly compute a continuous surface as an intermediate representation. While
an interesting ﬁeld of research, it can lead to more computation. Each new analysis
requires a new optimization from a raw point cloud instead of a richer information
surface. Additionally, such methods do not allow surface visualisation. We do not cover
these approaches.
1.3
Reading guide and contributions
We assume that the reader of this thesis is familiar with sensors and methods to acquire
3D point clouds such as LiDAR and MVS [72] and with basic principles of polygon
mesh processing [18], computer vision [105] and deep learning [47]. In the digital PDF
version of this document all abbreviations and acronyms are linked to the Glossary. In
11


--- Page 27 ---

turn, the Glossary links to the ﬁrst occurrence of the abbreviation or acronym, where
it is spelled out, explained and, if applicable, a citation is given.
In Chapter 2, we formally introduce the problem of surface reconstruction from point
clouds.
This chapter can been seen as an introduction for readers unfamiliar with
the problem of surface reconstruction from point clouds and popular methods that
address it. We summarise traditional test-of-time as well as recent learning-based surface
reconstruction methods. We benchmark all methods on the same datasets to assess the
inﬂuence of learning in surface reconstruction.
In Chapter 3, we augment existing deep surface reconstruction methods to enable
the processing of point clouds with visibility information. We show that visibility in-
formation consistently improves the accuracy of generated surfaces as well as the gen-
eralization capability of the networks to unseen domains. The chapter is published in
Sulzer et al. [108].
In Chapter 4, we present a novel deep surface reconstruction method that can pro-
cess large-scale point clouds in the wild, and reconstruct surfaces including objects not
present during training. The chapter is published in Sulzer et al. [109].
In Chapter 5, we summarise and conclude the thesis and provide an outlook into
future work.
1.3.1
Open Source Software and Datasets
Several open source software packages published on GitHub are also a result of this
thesis and provide means for reproducing the presented results.
mesh-tools
mesh-tools1 is a collection of tools for reconstructing and processing meshes. The pro-
grams are mainly written using CGAL2 and are based on several scientiﬁc publications.
Included is, e.g. our implementation of the surface reconstruction algorithm published
by Labatut et al. [71], which is extensively used in this thesis; and methods for surface
normal estimation and orientation.
dsr-benchmark
dsr-benchmark3 includes a collection of datasets and an evaluation pipeline for surface
reconstruction algorithms introduced in Chapter 2.
1https://github.com/raphaelsulzer/mesh-tools
2https://www.cgal.org/
3https://github.com/raphaelsulzer/dsr-benchmark
12


--- Page 28 ---

dsrv-data
dsrv-data4 is a collection of several deep surface reconstruction algorithms modiﬁed to
be able to process point clouds with visibility information, as introduced in Chapter 3.
dgnn
dgnn5 is a surface mesh reconstruction algorithm presented in Chapter 4 mainly written
using pytorch6 and pytorch-geometric7.
4https://github.com/raphaelsulzer/dsrv-data
5https://github.com/raphaelsulzer/dgnn
6https://pytorch.org/
7https://pytorch-geometric.readthedocs.io/en/latest/
13


--- Page 29 ---

2
A Survey and Benchmark of
Automatic Surface Reconstruction
from Point Clouds
We survey and benchmark traditional and novel learning-based algorithms that address
the problem of surface reconstruction from point clouds. Surface reconstruction from
point clouds is particularly challenging when applied to real-world acquisitions, due to
noise, outliers, non-uniform sampling and missing data. Traditionally, diﬀerent hand-
crafted priors of the input points or the output surface have been proposed to make the
problem more tractable. However, hyperparameter tuning for adjusting priors to diﬀer-
ent acquisition defects can be a tedious task. To this end, the deep learning community
has recently addressed the surface reconstruction problem. In contrast to traditional
approaches, deep surface reconstruction methods can learn priors directly from a train-
ing set of point clouds and corresponding true surfaces. In our survey, we detail how
diﬀerent handcrafted and learned priors aﬀect the robustness of methods to defect-laden
input and their capability to generate geometric and topologically accurate reconstruc-
tions.
In our benchmark, we evaluate the reconstructions of several traditional and
learning-based methods on the same grounds. We show that learning-based methods
can generalize to unseen shape categories, but their training and test sets must share
the same point cloud characteristics. We also provide the code and data to compete in
our benchmark and to further stimulate learning-based surface reconstruction.
14


--- Page 30 ---

2.1
Introduction
Surface reconstruction from point clouds is a key step between acquisition and analysis
of surface models and is a long-standing problem in digital geometry processing. In this
chapter, we survey and benchmark several traditional and learning-based methods that
address the problem of surface reconstruction from point clouds.
Traditionally, surface reconstruction methods made the problem more tractable by
using handcrafted priors, imposed on the input, such as point density, level of noise or
outliers, and on the output, such as smoothness, topological properties or the shape
category. In contrast, recent methods introduced by the deep learning community can
learn point cloud defects or shape patterns directly from training data and therefore
promise to reconstruct more accurate surfaces without the need for manual parameter
tuning. However, so far DSR methods have mostly been applied on datasets with a small
number of diﬀerent object categories. Such datasets are not representative for real-world
applications, where algorithms have to reconstruct surfaces containing a large variety
of shapes unseen during training.
Furthermore, DSR methods are often applied on uniformly sampled point clouds.
Likewise, such point clouds are not representative for real-world acquisitions, as they do
not model non-uniformity or missing data stemming e.g. from occlusions, or transparent
and low texture areas.
The ability to reconstruct shapes, either from unseen shape
classes or from point clouds with unseen defects is rarely studied in a systematic manner
for DSR methods.
To this end, we propose several experiments to benchmark algorithms for surface
reconstruction from point clouds. We make use of a variety of publicly available shape
datasets with object surfaces of diﬀerent complexities. The objects are represented by
a true surface S, which is a boundary-free 2-manifold, i.e. each point on the surface
has a neighborhood that is homeomorphic to an open subset of the Euclidean plane.
We synthetically scan the objects to produce point clouds with real characteristics.
Having access to the true surfaces allows us to measure the geometric and topological
reconstruction quality of the benchmarked methods.
We also verify our ﬁndings on
real-world point clouds.
We compare novel learning-based algorithms to traditional test-of-time methods to
speciﬁcally study the inﬂuence of learned priors incorporated into the surface recon-
struction process. We thereby pay special attention to the generalization capability of
methods to unseen domains. Our main contributions are as follows:
• We review methods for surface reconstruction from point clouds from over three
decades up to recent learning-based methods. We contrast popular test-of-time
with novel DSR methods.
• We benchmark traditional and learning-based methods on the same ground across
15


--- Page 31 ---

several experiments, using openly available shape datasets and point clouds gen-
erated with synthetic and real scanning.
2.2
Related work
2.2.1
Surveys
There exists only few works that survey the broad ﬁeld of surface reconstruction from
point clouds [9, 17, 28, 120], most of them predating the advance of learning-based
surface reconstruction [9, 17, 28]. Surface reconstruction methods are often grouped
into interpolating or approximating methods [18]. Interpolating methods “connect” all
points of the input point cloud, or a subset thereof, usually by linearly interpolating
between pairs of points. Approximating methods often deﬁne one or several smooth
functions approximating the point cloud globally or locally. See Figure 2.1 for an il-
lustration.
Berger et al. [9] and Cazals & Giesen [28] provide detailed reviews for
approximating and interpolating surface reconstruction methods, respectively.
To the best of our knowledge, only one survey includes learning-based methods [120].
However, this survey predates important developments for learning-based methods, such
as the incorporation of local information [21, 31, 45, 62, 92, 95, 109]. In this work,
we review both interpolating and approximating methods and focus on novel ideas
in learning-based surface reconstruction. While many reconstruction methods can be
distinguished by the prior assumptions they impose [9], we argue that a variety of
successful methods combine diﬀerent priors. This makes grouping by priors diﬃcult.
We thus organize methods into two groups: surface-based and volume-based approaches.
This breakdown closely relates to the two main classes of mathematical representations
of a surface: parametric and implicit.
2.2.2
Benchmarks
To date, benchmarks for surface reconstruction from point clouds are rare.
Many
methods use custom datasets to evaluate their approach, usually generated by uni-
formly sampling point clouds from ground truth shapes of existing shape collections
[21, 31, 62, 89, 92, 95]. However, the characteristics of the sampled point clouds often
diﬀer across publications, which hampers the ability to fairly compare the results of
diﬀerent works. Furthermore, the point clouds often lack common defects of real ac-
quisitions, such as missing data or outliers. One notable exception is the benchmark
of Berger et al. [8]. The authors develop a synthetic range scanning procedure to pro-
duce scans with realistic artifacts, such as noise, non-uniformity and misaligned scans
and create point clouds from shapes with non-trivial topology and details of various
feature sizes.
While providing interesting results, the benchmark predates learning-
based surface reconstruction and only considers traditional approximating methods. In
16


--- Page 32 ---

the benchmarks proposed in this paper, we reuse their synthetic range scanning pro-
cedure and their ﬁve test shapes, as they provide realistic and challenging input for
both learning-based and traditional algorithms. We also implement our own synthetic
scanning procedure for MVS-like point clouds. We use the synthetic scanning to scan
existing large shape datasets to create training datasets with true surfaces and point
clouds with realistic characteristics.
A problem related to surface reconstruction is the generation of point clouds from
2D information such as overlapping images. There exists a variety of benchmarks us-
ing data captured in a laboratory environment [61, 100] or in the wild [68, 99, 107].
These benchmarks often use a low quality image acquisitions as reconstruction input.
Simultaneously, a higher quality acquisition, e.g. from LiDAR scans, serves as reference.
One problem with this approach is that, even for high quality acquisition techniques,
it is diﬃcult to produce complete ground truth point clouds. This issue is sometimes
addressed by decreasing the ground truth domain to speciﬁc evaluation areas, in which
reliable information is available either from recorded points or sightlines between points
and sensors [61, 99]. However, in contrast to true surfaces, reference point clouds, do not
allow to calculate topologic metrics such as the number of components or diﬀerential
metrics such as surface normals. Furthermore, most learning-based methods require
closed reference surfaces instead of reference point clouds for training.
2.3
Surface definition, representations, properties and reconstruc-
tion
In this section, we ﬁrst provide a deﬁnition of a surface and its mathematical and digital
representations. We then discuss important surface properties. Finally, we establish the
connection between mathematical surface representations, and the grouping of surface
reconstruction algorithms used in our survey.
2.3.1
Definition
A surface can be deﬁned as an orientable, continuous 2-manifold in R3, with or without
boundaries [18, 54, 88]. These properties are important for surface visualisation and
processing, and we will discuss them further down. Mathematically, there are two main
classes of surface representations: parametric and implicit.
2.3.2
Representations
Parametric surfaces are deﬁned by a function f : Ω7→S that maps a parameter domain
Ω∈R2 to the surface S = f(Ω) ∈R3. However, for complex surfaces it is not feasible to
ﬁnd a single function that can parameterise S. Therefore, the parameter domain Ωis
usually split into sub-regions for which an individual function is deﬁned [18]. The most
17


--- Page 33 ---

(a) Open Interpolating
(b) Open Interpolating
(c) Closed Interpolating
(d) Closed Interpolating
(e) Open Approximating
(f) Open Approximating
(g) Closed Approximating (h) Closed Approximating
Figure 2.1: Approximating and interpolating surfaces from point clouds: A surface
generated from point samples can either interpolate (top row) or approximate (bottom row)
the samples. Theoretically, there exist an inﬁnite number of surfaces with diﬀerent geometry
and topology that can pass through, or near, the samples. We show eight diﬀerent surfaces
reconstructed from the same point cloud
in (a) - (h). The point cloud can be
seen as a sampling of a part of a real surface. All reconstructed surfaces are watertight,
as they are either closed and boundary-free, or their only boundary is the intersection with
the domain boundary
. The surface in (d) is non-manifold in the center-vertex. All other
surfaces are manifold. Except for (h), all surfaces are comprised of only one component.
In contrast to the point cloud depicted here, in our benchmark, we mainly consider point
clouds sampled from closed surfaces.
18


--- Page 34 ---

common way is to segment Ωinto triangles, which are planar by deﬁnition. A set of
triangles approximating S can be eﬃciently stored and processed as a triangle surface
mesh M = (V, E, F), with triangle facets F, edges E and vertices V.
Implicit surfaces are deﬁned by the level-set c of a scalar valued function F : R3 7→R:
Sc = {x ∈R3 | F(x) = c}.
(2.1)
The most common choice of the implicit function F is either a signed distance or an
occupancy function. A signed distance function (SDF) gives the distance from a 3D
point x in space to the surface; with points in the interior signed a negative value, and
points on the exterior signed a positive value. An indicator or occupancy function (OF)
usually has a value of 1 inside the surface and 0 outside. The c-level-set of F then yields
the surface S, where c = 0 in the case of a signed distance function and c = 0.5 in the
case of an occupancy function. Similar to the parametric case, the implicit function
domain is often split into sub-regions, such as voxels, octree-nodes or tetrahedra, and
constant functions are deﬁned in each sub-region.
2.3.3
Properties
The reconstructed surface Sr should be close in terms of geometry and topology to the
real surface S from which the point cloud P is sampled. To facilitate subsequent geomet-
ric operations on Sr, such as sampling or deforming the surface, a mesh reconstruction
M is also desirable. Sr and M, respectively, should have the following properties (see
Figure 2.1 for illustrations):
• Watertight: A geometric surface is closed if it is boundary-free. A mesh M is
closed—or boundary-free—if no edge is incident to exactly one facet. However,
a reconstructed surface of a real scene necessarily has a border deﬁned e.g. by
the limit of the scan coverage.
One may still reconstruct a closed surface by
intersecting it with the boundary of the domain in which f or F is deﬁned: e.g.
the convex hull or bounding box of P.
However, this procedure may not be
desirable, as it can hinder simple geometric analysis such as the calculation of
surface area. Instead, we deﬁne a surface as watertight if it is boundary free,
except for a possible intersection with the domain boundary.
• Manifold: We consider real and geometric surfaces to be 2-manifolds, i.e. each
point on the surface has a neighborhood that is homeomorphic to an open subset
of the Euclidean plane. A mesh M is manifold if it is edge- and vertex-manifold,
and intersection-free.
– Edge-manifold: For each edge E, the set of facets F sharing this edge form
a topological (half-)disk. This means that no edge can be incident to more
than two facets.
19


--- Page 35 ---

– Vertex-manifold: For each vertex V, the set of facets sharing this vertex form
a topological (half-)disk. This means that facets with a common vertex form
an open or closed fan, i.e. there are no dangling facets.
• Intersection-free: M is intersection free if all pairs of facets not sharing an edge
or vertex do not intersect.
• Orientable: M is orientable if one can deﬁne a consistent continuous orientation
of each facet. This means that the order of the vertices of all facets is either
clockwise or counter-clockwise and a common edge of two adjacent facets has
opposite orders on the two sides.
The watertight property is useful for simulations such as ﬂuid dynamics. Manifold-
ness and orientability are often required for mesh storing and processing, in particular
because they are a prerequisite for the widely-used half-edge data structure [66, 78].
Furthermore, intersection-free and orientable surfaces lead to a well-deﬁned notion of
inside and outside, which is important for mesh visualization and a variety of geometric
opertations.
2.3.4
Reconstruction
Surface reconstruction from point clouds is the process of constructing a continuous
surface of which discrete point samples have been acquired. In our survey, we group
methods for surface reconstruction from point clouds into two groups: surface- and
volume-based. Surface-based reconstruction methods consists in ﬁnding (a set of) pa-
rameterised surfaces Sr that approximate the point cloud P, either in the form of
triangles or larger two-dimensional (2D) patches, or by deforming parameterised enclos-
ing envelops such as meshed spheres. The main challenge for surface-based methods
using a single function f is that the topology of Ωhas to be equivalent to the topology
of S, which is usually unknown. The main challenge for surface-based methods with
individual functions for sub-regions of S, on the other hand, is to guarantee a consis-
tent transition between each region. Hence, these methods often struggle to produce an
intersection-free, manifold and watertight surface.
Volume-based methods, on the other hand, segment a subset of R3 into interior (in-
side) and exterior (outside) subspaces. The surface is implicitly deﬁned as the interface
between the two subspaces. Most, but not all algorithms in this class formulate the
problem as ﬁnding an implicit function. Surfaces from volume-based methods are guar-
anteed to be watertight and intersection-free, but not necessarily manifold [28].
While surface-based methods can directly yield a mesh, e.g. by triangulating Ω,
volume-based methods usually require an additional processing step. If the implicit
ﬁeld is discretized with tetrahedra, one can simply use a process which is sometimes
called triangle-from-tetrahedra (TFT). TFT builds a triangle mesh from all triangles
20


--- Page 36 ---

that are adjacent to one inside- and one outside-tetrahedra. Another option is the algo-
rithm of Boissonnat and Oudot [16] that iteratively samples F along lines from inside to
outside to ﬁnd points that lie on S and builds a triangle mesh from these points. One of
the most popular methods for mesh extraction from an implicit ﬁeld is Marching Cubes
[76], which (i) discretizes the implicit function into voxels, (ii) constructs triangles in-
side each voxel that have at least one inside and one outside vertex and (iii) extracts
a triangulation as the union of all triangles. Recently, mesh extraction has also been
addressed by the deep learning community. Neural meshing [113] speciﬁcally addresses
the case where an implicit function is represented by a neural network, and aims to
extract meshes with fewer triangles compared to Marching Cubes from such a function.
In both, surface- and volume-based groups, there are methods that come with theo-
retical guarantees about the topology and geometry of the reconstruction in the absence
of noise and when the point sampling is dense enough [28]. However, in this paper, we
are mostly interested in the robustness of methods to defect-laden input point clouds
from 3D scanning.
2.4
Survey
In this section, we review important surface- and volume-based surface reconstruction
methods and discuss their robustness against diﬀerent point cloud defects.
We also
show that learning-based approaches are often related to more traditional methods.
2.4.1
Surface-based reconstruction
Interpolating approaches
Advancing-front techniques.
Most traditional surface-based approaches linearly
interpolate between the point samples P, or a subset thereof. This can be done eﬃciently
by triangulating triplets of points which respect the empty ball property i.e. no other
point lies within their circumsphere.
Triangulating all triplets of P that have this
property leads to the 3D Delaunay tetrahedralisation (3DT) of P. The Ball Pivoting
algorithm [11] is a greedy approach to ﬁnd local triplets of points that form a triangle
which is part of the surface. The ﬁrst step is to (i) deﬁne a ball with constant radius,
related to the density of P and to (ii) select a seed triplet of points. The ball must
touch all three points and have no other point in its interior. The points then form the
ﬁrst surface triangle. Then, (iii) the ball pivots around an edge of the triangle until it
touches a new point, forming a new surface triangle. Once all possible edges have been
processed the algorithm starts with a (iv) new seed triangle until all points of P have
been considered. The algorithm has later been reﬁned to be more robust to non-uniform
sampling [44, 93]. The Ball Pivoting algorithm and its related variations are often called
21


--- Page 37 ---

Table 2.1: Overview of surface- and volume-based surface reconstruction methods:
We show an overview of surface- and volume-based surface reconstruction methods, both
non-learning and learning-based, together with their input requirements (normals, sensor
pose) and output type (triangle mesh or implicit ﬁeld). Attributes denoted in brackets are
optional. Methods with a local receptive ﬁeld divide the point cloud into smaller sub-regions
and deﬁne individual functions or surface patches for each sub-region. Methods with a global
receptive ﬁeld consider the entire point cloud at once. Methods denoted with both combine
local and global receptive ﬁelds. We test methods in bold in our benchmark.
Method
learning
normals
sensor pose
receptive ﬁeld
output
Surface-based
BPA
[11]
local
triangle mesh
Sharf et al.
[101]
both
triangle mesh
AtlasNet
[49]
✓
local
triangle mesh
IER
[74]
✓
both
triangle mesh
PointTriNet
[102]
✓
local
triangle mesh
DSE
[95]
✓
local
triangle mesh
P2M
[52]
both
triangle mesh
Volume-based
SPSR
[65]
✓
both
implicit ﬁeld
Labatut et al. [71]
✓
global
triangle mesh
ONet
[79]
✓
global
implicit ﬁeld
DeepSDF
[89]
✓
global
implicit ﬁeld
IM-Net
[35]
✓
global
implicit ﬁeld
ConvONet
[92]
✓
both
implicit ﬁeld
IGR
[48] (✓)
(✓)
global
implicit ﬁeld
LIG
[62]
✓
✓
local
implicit ﬁeld
DGNN
[109]
✓
✓
both
triangle mesh
SAP
[91]
✓
both
implicit ﬁeld
P2S
[45]
✓
both
implicit ﬁeld
SAP
[91] (✓)
both
implicit ﬁeld
POCO
[21]
✓
(✓)
local
implicit ﬁeld
22


--- Page 38 ---

advancing-front techniques. Their main drawback is that they are not robust to point
cloud defects such as noise or point clouds with large missing parts.
Selection-based
Similar to advancing-front techniques, the idea to iteratively build
the triangulation from initial candidate triangles has also been explored in learning-
based methods [74, 102]. PointTriNet [102] (i) starts with an initial set of seed triangles
from a k-nearest neighbor graph of P. Then, (ii) a ﬁrst network takes in neighboring
points and triangles of each seed triangle, and estimates its probability to be part of
the surface. (iii) Triangles with high probability are selected to be part of the ﬁnal
surface and (iv) a second network proposes new candidate triangles constructed from
two points of already selected surface triangles and neighboring points. The proposed
new candidates are, again, processed by the ﬁrst network and the algorithm continues for
n user-deﬁned iterations. The loss function is based on Chamfer distance between input
points and the reconstructed surface, which allows the method to be trained without the
need for ground truth meshes. IER-meshing [74] also (i) starts with a large set of seed
triangles from a k-nearest neighbor graph. It then deﬁnes a so-called intrinsic-extrinsic
ratio (IER), as the quotient of geodesic and Euclidean distance between points of a
triangle. (ii) This ratio is estimated by an multilayer perceptron (MLP) from learned
point features per triangle and supervised with IER’s from a ground truth mesh. (iii)
Only triangles with an IER close to 1 (i.e. Euclidean distance ≈geodesic distance) are
considered to be part of the surface and (iv) selected based on handcrafted heuristics.
Both aforementioned methods have shown to be robust against small amounts of noise
in the input point cloud. However, their reconstructed surfaces are neither manifold nor
watertight.
Tangent plane and other projection methods
Another class of surface-based
interpolating approaches are tangent plane methods. This class includes the algorithm
of Boissonnat [15], which is according to Cazals and Giesen [28] probably the ﬁrst
algorithm to address the surface reconstruction problem. The basic idea is to (i) ﬁnd a
tangent plane for each sample point, (ii) project the points local neighborhood on the
tangent plane, (iii) construct 2D Delaunay triangulations of the projected points and
(iv) merge the local reconstructions. A shortcoming of such an approach is that tangent
planes are diﬃcult to use in areas with high curvature or thin structures [95]. To this
end, the idea of using local 2D Delaunay triangulations of projected points has been
reﬁned in a recent learning-based approach [95]. Instead of tangent planes, DSE-meshing
[95] uses logarithmic maps, local surface parametrizations around a point p, based on
geodesics emanating from p. This method (i) classiﬁes geodesic neighbors of each point
in P from a set of k-nearest neighbors. Then, (ii) an MLP approximates a logarithmic
map parametrization to gain a 2D embedding of the geodesic neighbors. Lastly, (iii)
neighboring logarithmic maps are mutually aligned and triangulated. This step allows
23


--- Page 39 ---

the method to reconstruct surfaces with fewer non-manifold edges, compared to methods
that process triangles independently. However, the surface is still not watertight and
the method has not been tested for reconstruction from noisy point clouds.
Patch-fitting
Patch-ﬁtting methods are related to tangent plane approaches. Instead of interpolating
the initial point set, a new triangulation patch is formed. AtlasNet [49] is based on this
idea and was one of the ﬁrst learning-based surface reconstruction methods. Small 2D
triangulated patches are transformed to ﬁt P based on transformations predicted by
an MLP. Similar to interpolating approaches, this method cannot guarantee to ﬁll all
gaps between patches, which results in a non-watertight and potentially self-intersecting
surface.
Surface deformation
One of the only classes of surface-based approaches that can guarantee a watertight
surface are deformation-based methods. Sharf et al. [101] introduced a method that (i)
iteratively expands an intial mesh contained within the input point cloud along the face
normal directions, and (ii) moves the mesh vertices to ﬁt the input point cloud using
moving least squares. The method is shown to be robust against missing data, but
requires careful parameter tuning to be robust against noise or outliers. Point2Mesh
(P2M) [52] is also based on the aforementioned idea, but avoids the need for tuning
parameters by hand.
The method takes as input a convex hull or a low resolution
Poisson reconstruction [65] of P, and shrink-wraps this initial surface to best ﬁt the point
cloud. The process is guided by multiple local convolutional neural networks (CNNs)
that share weights. The idea is that the weight sharing between the CNNs acts as a prior
that identiﬁes symmetric features in the shape while being able to ignore unsystematic,
random defects in the point cloud. One problem with this approach is that the topology
of the initial surface stays constant during reconstruction. If the correct topology of the
surface is not known, it cannot be recovered. For example, if the sought surface has
holes, they cannot be reconstructed from a convex hull initialisation.
This poses a
limitation for reconstructing arbitrary objects in the wild.
2.4.2
Volume-based reconstruction
Interpolating approaches
Volume-based interpolating approaches commonly start by constructing a 3DT of P.
In R3 a Delaunay triangulation (or tetrahedralization) subdivides the convex hull of P
with tetrahedra. The 3DT is created in such a way that no point of P is contained in the
24


--- Page 40 ---

circumspheres of any tetrahedra. For well distributed point clouds it can be constructed
in O(n log n) [3]. The Delaunay triangulation does not directly generate the surface, as
it connects points in any direction. However, if the sampling P of S is dense enough a
subcomplex of the 3DT is guaranteed to include a surface Sr closely approximating the
geometry and topology of S [28]. One of the simplest ways to recover this subcomplex
from a 3DT is to (i) prune all tetrahedra with circumspheres larger than a user speciﬁed
constant radius α and then (ii) keeping only the boundary triangles. This leads to a
so-called α-shape [10]. Similar to the Ball Pivoting algorithm the radius of the ball
(here α) depends on the point density. For error free and dense samplings, alpha-shapes
and some other interpolation methods [2, 15, 28] provide provable guarantees that the
reconstructed surface is topologically correct [28]. Another way to recover a surface
from a 3DT is inside-outside labelling [26, 53, 59, 60, 69, 71, 85, 104, 109, 109, 114, 122].
Here, all tetrahedra of a 3DT of P are (i) labelled as either inside or outside with
respect to Sr, and (ii) the surface is deﬁned as the interface between tetrahedra with
diﬀerent labels. This guarantees to produce intersecting-free and watertight surfaces.
The inside-outside labelling is usually implemented through a global energy minimized
with graph-cuts. Inside-outside potentials are computed using visibility information and
spatial regularization is achieved through surface smoothness or low area priors in the
energy. This approach has been shown to be robust against most kinds of acquisition
defects of moderate levels [59, 71, 114] and is capable of reconstructing (very) large
scale scenes [85]. Delaunay-Graph Neural Network (DGNN) [109] is a learning-based
method that replaces the handcrafted potentials in the aforementioned energy with a
graph neural network (GNN). The GNN takes local geometric attributes and visibility
information as input and operates locally on small subgraphs of the 3DT. The locality
makes the method scale to large scenes. The method of Luo et al. [77] proceeds similarly,
but without the use of visibility information and a global energy formulation. Instead,
the GNN processes the 3DT of entire objects at once, which can hamper scalability.
Implicit functions
Arguably the largest class of surface reconstruction algorithms represent the surface
with an implicit function (cf. Equation 2.1). One of the ﬁrst methods that used implicit
functions for surface reconstruction was presented in Hoppe et al. [54]. Hoppe et al. (i)
calculate tangent planes at each input point of P, using principal component analysis
(PCA) of the local neighborhood. They then (ii) approximate an SDF by mapping
an arbitrary point x ∈R3 to its signed distance to the closest tangent plane. (iii) The
surface is deﬁned as the 0-level-set of the SDF. The local tangent plane estimation makes
the process sensitive to low density sampling and noise, and computationally expensive.
Poisson surface reconstruction.
The most popular approach for surface recon-
struction based on implicit functions is Poisson Surface Reconstruction (PSR) [63]. The
25


--- Page 41 ---

idea is that the Laplacian of an indicator function χ, whose c-level-set approximates the
unknown surface S, should equate the divergence of a vector ﬁeld ⃗N associated with P:
∆χ = ∇· ⃗N .
(2.2)
The vector ﬁeld ⃗N is deﬁned by the oriented normals of P. To deﬁne χ the algorithm
(i) builds an octree on P and (ii) sets up a system of hierarchical functions, locally
supported in each octree node, and (iii) globally solved by using a sparse linear system,
which makes the method time and memory eﬃcient. Dirichlet conditions can be imposed
on the bounding box of the surface with χ = 0 to ensure that the surface is closed.
The approach is known to inherently produce smooth surfaces, but also over-smooth
the surface in parts.
The later introduced Screened Poisson Surface Reconstruction
(SPSR) [65] can reconstruct much sharper surfaces by constraining Equation 2.2 to
pass through P. Additionally, it introduces the choice of Neumann boundary conditions
which allows the surface to intersect the boundary of the domain in which F is deﬁned.
This is useful for open scene reconstruction. Recently the method has been revisited
again, to impose Dirichlet constraints on a tight envelope around P, enabling better
reconstructions in areas of missing data [64]. Poisson surface reconstruction produces
watertight meshes and has shown to be robust against almost all kinds of acquisition
defects of moderate levels. However, all Poisson-based approaches require well oriented
normals as input, which can pose a signiﬁcant limitation in practice.
Neural implicit functions
The most common approach to surface reconstruction
with deep networks is to model F in Equation 2.1 with a neural network. This was
ﬁrst done in the pioneering works of Mescheder et al. [79], Park et al. [89], and Chen &
Zhang [35].
In the case of Occupancy Networks (ONet) [79], F is modelled with a simple FCN
architecture. The network takes as input a point cloud P and one or several test points x
and outputs the occupancy of the test points in relation to the surface from which P was
sampled. The conditioning on the input point cloud slightly changes the formulation of
Equation 2.1 to:
S = {x ∈R3 | Fθ(x, P) = c} .
(2.3)
To estimate the network weights θ, the network is trained with batches B of K objects
using a simple binary cross entropy (BCE) loss:
LB (θ) = 1
|B|
|B|
X
i=1
K
X
j=1
BCE (Fθ (xij, Pi) , oij) ,
(2.4)
26


--- Page 42 ---

where oij is the ground truth occupancy of test point xij. To compute the ground
truth occupancy oij, the training objects have to be available in the form of watertight
surfaces. A common approach is to use large shape collections, such as ShapeNet [32]
for training. Similar ideas have been introduced in IM-Net [35] and DeepSDF [89] to
model an occupancy or signed distance function with a neural network.
Instead of
an encoder-decoder architecture as in ONet, the authors of DeepSDF [89] introduce an
auto-decoder which is trained to ﬁnd a shape code z that best explains an objects shape.
This slightly changes Equation 2.3 and Equation 2.4, where the point cloud input P
is replaced by a shape code z in the form of a 256-dimensional vector. The DeepSDF
architecture then allows to reconstruct a complete signed distance ﬁeld (and thus the
shape), given a shape code z. However, to ﬁnd the shape code for a speciﬁc shape during
inference, at least a few ground truth signed distance values are necessary. This can
be a signiﬁcant limitation in practice. A common downside of the ﬁrst DSR networks
based on neural implicit ﬁelds is their simple fully connected network architecture. This
architecture does not allow the incorporation of local point cloud information [92] and
often leads to oversmoothing or inaccuracies of the inferred surface.
To this end, occupancy networks have later been reﬁned by prepending 2D or 3D
U-Nets [39, 97] before the fully connected occupancy network, to better incorporate
local information. The idea is to (i) extract point features from local neighborhoods
and (ii) aggregate these features in 2D or 3D grid cells. The U-Nets are then used
to (iii) integrate local and global information using multiple down- and upsamplings.
(iv) Finally, the fully connected ONet is used to compute test point occupancies. The
approach is called Convolutional Occupancy Networks (ConvONet) [92]. Just as for
the fully connected architectures, the network can be trained with test points x with
known occupancy values o. In the same work, the authors also introduce an overlapping
sliding-window approach in which a single trained ConvONet can be used to reconstruct
entire indoor scenes. However, this approach requires to carefully scale the scene, such
that the sliding window captures parts of the scene with comparable surface features
during training and inference.
Furthermore, for large-scale scenes, a sliding-window
approach can be very time-consuming.
Local Implicit Grids (LIG) and DeepLS [31] also split input point clouds into over-
lapping subregions, and treat each subregion separately. The methods infer local shape
codes z for parts of objects or scenes. These local shape codes have the additional ben-
eﬁt that they can represent parts from several diﬀerent object classes. For example, a
ﬂat part-surface may belong to a table top or to a TV screen. This makes the methods
less prone to overﬁt on speciﬁc shape categories used during training. However, the
methods are largely based on IM-Net and DeepSDF. This means they also require a
sort of ground truth test point during inference to optimize for the shape codes. Addi-
tionally, similar to the sliding window method of ConvONet, the region size (i.e. part
size) has to be tuned.
Using the same encoder architecture as ConvONet, Shape As Points (SAP) [91] intro-
27


--- Page 43 ---

duces the combination of neural implicit ﬁelds with a diﬀerentiable Poisson solver. The
method estimates (i) oriented normals as well as k point oﬀsets for each input point, to
correct and densify the point cloud P. (ii) The resulting point cloud of size k|P| is fed
to a diﬀerentiable Poisson solver [65] that computes an indicator grid, i.e. ˆχ evaluated
on all nodes of a regular voxel grid. (iii) This indicator grid is supervised with a ground
truth indicator grid χ. The ground truth indicator grid is created prior to training, from
a Poisson reconstruction of a dense and error free point cloud, sampled from a ground
truth mesh. A simple mean square error (MSE) loss is used for training the network:
L = |ˆχ −χ|2
(2.5)
The entire pipeline is diﬀerentiable which allows to update point oﬀsets, oriented
normals and the network parameters during training (with batches of shapes). During
inference, the computed indicator grid can simply be converted to a mesh using march-
ing cubes. In contrast to the original Poisson Surface Reconstruction, SAP allows to
incorporate learned priors and does not need P to be equipped with oriented normals.
In general, all of the methods based on voxel grids in this paragraph require the
size of the initial voxels to be constant during training, because the resolution of the
convolution layers depends on the voxel grid. This poses problems for training on point
clouds with diﬀerent densities. A dense voxel grid can be memory intensive and long to
train, while a coarse voxel grid can oversmooth the input and lead to loss of information.
Another way to combine local and global information, that avoids the use of grids
was introduced in Points2Surf (P2S). P2S uses both a local test point neighborhood
sampling, and a global point cloud sampling which are both processed using MLPs and
combined to predicted a signed distance for the test point.
The k-nearest neighbor
sampling makes this method less sensitive to point density, at the cost of increasing
computational complexity, since the local neighborhood sampling has to be performed
for each test point during inference.
Point Convolution for Surface Reconstruction (POCO) only relies on local neighbor-
hoods and computes a latent vector per point using a point convolution backbone. The
occupancy of a test point x is then predicted using attention-based weighing of neigh-
boring latent vectors. This approach can focus the parameters of the learned implicit
function to be used close to the surface. However, it also requires neighborhood sam-
pling during inference. Similar to most other DSR methods, POCO is trained on object
point clouds with a ﬁxed number of points for easy mini-batching. However, to make the
method more robust to point clouds with higher density during inference, the authors
use a procedure called test-time augmentation. During inference, the latent vectors of
each input point p are computed several times, from diﬀerent local subsamples and then
averaged.
Another approach to use neural implicit surface representations is to ”train” (or op-
timize) the weights of a deep neural network per shape [48, 91]. The idea is to leverage
28


--- Page 44 ---

inherent symmetries of deep neural networks to act as priors in the reconstruction pro-
cess, similar to the surface deformation based Point2Mesh discussed above. To this end,
Gropp et al. [48] designed a simple fully connected network representing a signed dis-
tance function. To encourage the reconstruction of a smooth 0-level-set, given an input
point cloud P, they design a loss function which (i) should vanish on P and (ii) which
gradients ∆PF should be of unit 2-norm and similar to the normals of P. The method
is called Implicit Geometric Regularisation (IGR). SAP also has an optimization-based
variant where (i) the indicator grid, computed with the diﬀerential Poisson solver from
the input point cloud P is used to compute a mesh. (ii) The mesh is then sampled,
which allows to calculate a Chamfer loss between the sampled and input point cloud
and, again, update the network weights, point oﬀsets and oriented normals. (iii) This
process is repeated until a user deﬁned stopping criterion. The optimization-based vari-
ants of SAP and IGR can be trained per shape, without the need for ground truth
meshes for supervision. However, in this optimization-based setting, they cannot learn
and incorporate shape priors from a training set.
An upside of all DSR methods based on neural implicit representations is that they
can store an implicit function, potentially conditioned on a point cloud, in the weights
of a neural network. Especially DSR architectures that are entirely grid-less can directly
relate their degrees of freedom to represent the surface. This can be more ﬂexible com-
pared to voxel, octree, or tetrahedral representations. Being a relatively new discovery,
the full potential of neural network-based surface representations has probably yet to
be explored.
2.5
Benchmark setup
In this section, we describe our set up of a series of experiments for benchmarking several
surface reconstruction algorithms discussed in the previous section. We ﬁrst describe
how we generate realistic point clouds by using synthetic range and MVS scanning
procedures. We then describe the datasets we used and several experiments to evaluate
the performance of reconstruction methods.
Finally, we provide an overview of the
competing methods.
Synthetic scanning for point cloud generation
In an ideal setting, we would
evaluate methods on real point cloud acquisitions together with their true surfaces.
However, generating true surfaces of real objects requires error free and dense input
point clouds or substantial manual intervention. Therefore, such a dataset is diﬃcult to
produce. MVS benchmarks [61, 68, 99, 100, 107] commonly use image acquisitions for
the reconstruction input and a highly complete and precise acquisition, e.g. from multi-
ple stationary LiDAR scans as reference. We make use of such datasets for evaluation.
Using such a dataset for training surface reconstruction networks requires reconstructing
29


--- Page 45 ---

Table 2.2: Scanning conﬁgurations for Berger et al. benchmark: We show the ﬁve
diﬀerent scanner conﬁgurations used in our modiﬁed version of the Berger et al.’s scanning
procedure. We use the resulting scans to evaluate object-level reconstruction with varying
point-cloud defects and for training data generation. For the low resolution (LR) scans the
scanning process results in 1000 to 3000 points per shape, and for the high resolution (HR),
the scanning process yields around 10 000 to 30 000 points.
Low res. (LR)
High res. (HR)
HR + noise (HRN)
HR + outliers (HRO)
HR + noise + outliers (HRNO)
Camera resolution x, y
50, 50
100, 100
100, 100
100, 100
100, 100
Scanner positions
5
10
10
10
10
Min/max range
70/300
70/300
70/300
70/300
70/300
Additive noise
0
0
0.5
0
0.5
Outliers (%)
0
0
0
0.1
0.1
a watertight surface from the high-quality acquisition. However, even with high-quality
acquisitions, parts of the object or scene may be missing due to occlusions, for example.
These issues ultimately lead to inconsistencies in the ground truth and make this source
of data unreliable to train DSR networks. Additionally, existing datasets of point cloud
acquisitions and reliable ground truth surface information only consist of a handful of
objects or scenes. Instead, training and evaluation of learning-based surface reconstruc-
tion is often done on point clouds sampled from synthetic surfaces stemming from large
shape collections. However, such point clouds are not representative for real-world ac-
quisitions, as they do not model non-uniformity or missing data stemming e.g. from
occlusions, or transparent and low texture areas. To this end, we resort to synthetic
scanning to produce point clouds from synthetic surfaces in our benchmark. In contrast
to directly sampling the surfaces, synthetic scanning can produce point clouds with re-
alistic defects, such as anisotropy and missing data from (self-)occlusion, see Figure 2.2.
At the same time, the synthetic surfaces provide reliable information for training and
evaluation.
Synthetic range scanning
We use the range scanning procedure from the surface
reconstruction benchmark of Berger et al. [8]. To this end, we modiﬁed their provided
code to export the camera positions of the scanning process along with the point cloud.
We also add outliers to the produced point clouds by uniformly sampling the bounding
box of the object.
The scanning procedure produces uniform, evenly spaced point
clouds. We choose ﬁve diﬀerent scanner settings to scan each test shape: (i) a low
resolution setting replicates point clouds obtained from long range scanning and (ii) a
high resolution setting produces point clouds with close to no defects. Three further
settings produce high resolution point clouds with challenging defects such as (iii) noise,
(iv) outliers or (v) noise and outlier defects combined. See Table 2.2 for details. Because
Berger et al.’s provided code pipeline is too time and memory extensive, we cannot
generate a dataset suﬃciently large for training DSR methods. Thus, we only use this
dataset for testing. We refer the reader to the original benchmark paper [8] for further
30


--- Page 46 ---

(a) High Quality Mesh
(b) MVS
(c) Range scan
(d) Uniform sampling
(e) Synthetic MVS
(f) Synthetic range scan
Figure 2.2: Synthetic and real point clouds: Surface reconstruction methods are often
tested on uniform surface samplings (d). Instead, we test methods on synthetic MVS (e)
and synthetic range scans (f). In contrast to uniform surface sampling, synthetic scanning
can produce realistic point cloud defects, such as missing data from occlusion, often present
in real scans (b,c).
31


--- Page 47 ---

(a) Synthetic scanning setup
(b) Synthetic MVS
(c) Synthetic range scanning
Figure 2.3: Synthetic scanning procedure: We randomly place sensors on bounding
spheres with multiple radii around the object (a). To produce MVS like point clouds, we
consider rays aiming at uniformly sampled points on the circumsphere of the object (b).
This produces non-uniform point clouds with missing data similar to real MVS point clouds.
For synthetic range scanning, we use Berger et al.’s [8] pipeline, which considers ray targets
arranged on a uniform grid aiming at the object (c). This produces uniform point clouds
with missing data similar to real range scanning point clouds.
details about the scanning pipeline.
Synthetic MVS
To mimic MVS acquisitions, we synthetically scan objects by plac-
ing virtual sensors on two bounding spheres around an object and shooting rays to
the circumsphere of the object. Sensor positions (ray origin) and ray target points are
uniformly sampled on the surface of the spheres. A 3D point is then given as the inter-
section of the ray and the objects surface. Our goal is not to mimic an MVS pipeline
but rather produce point clouds with similar characteristics. We depict our scanning
procedure in Figure 2.3. We produce two diﬀerent scans with our approach: (i) sparse
point clouds with 3, 000 points per object and Gaussian noise on the point position
with zero mean and standard deviation 0.005 as in [92], and (ii) dense point clouds with
10, 000 points per object of which 10% are outliers and Gaussian noise on the point
position with zero mean and standard deviation 0.005. For both versions we scan from
10 diﬀerent sensor positions.
2.5.1
Datasets
We consider a variety of datasets to evaluate the versatility and precision of diﬀerent
reconstruction methods. We use closed surfaces from ShapeNet, ModelNet and Berger
et al., as they are widely available.
ShapeNet and ModelNet are suﬃciently big to
train surface reconstruction networks.
Most learning-based methods require reliable
32


--- Page 48 ---

(a) Berger et al.
(b) ShapeNet
(c) ModelNet
Figure 2.4: Ground truth shapes of the benchmark datasets: We show an example
shape of each class of ModelNet in (c) and of ShapeNet in (b) and the ﬁve shapes of Berger
et al. in (a).
33


--- Page 49 ---

inside/outside querying of the models for training. To this end, we make the models
watertight using ManifoldPlus [56]. Note that we also use the train sets to tune the
parameters of learning-free methods. The watertight surfaces of the test sets allow for
a reliable quantitative evaluation of the reconstructions. For qualitative evaluation, we
also test on real scans [61, 68, 100] which further allows us to evaluate the reconstruction
of open surfaces.
All surfaces are scaled to be contained inside the unit cube.
In
the following we give additional details for each dataset used in our benchmark. See
Figure 2.4 for example shapes.
ShapeNet
As is common practice in related studies, we use Choy et al.’s [38] 13 class
subset of ShapeNet as well as its train/val/test split. We generate point clouds with
3, 000 and 10, 000 points using our synthetic MVS-like scanning.
ModelNet10
We use ModelNet10 shapes as a second object shape dataset.
Its
shapes are less complex than ShapeNet’s, with more ﬂat surfaces and fewer details.
Additionally, the number of training shapes is smaller (4k vs 30k objects). We use the
full train set and the test sets for the 6 out of 10 classes which are not represented in
ShapeNet. We generate point clouds with 3, 000 points with our synthetic MVS-like
scanning.
Berger et al.
We select ﬁve shapes from the benchmark of Berger et al..
These
shapes include challenging characteristics such as details of various sizes or a non-trivial
topology, which makes them more diﬃcult to reconstruct than ModelNet shapes. We
generate point clouds between 3, 000 and 10, 000 points using our synthetic MVS and
range scanning procedures.
Real MVS and range scans
We select a range scan from Tanks and Temples [68],
and two MVS point clouds from DTU [61] and from Middlebury [100]. We subsample
these point clouds to 50, 000 points.
2.5.2
Experimental Setup
We show a summary of our experimental setup on Table 2.3 and Table 2.4. In the
following, we provide details for each experiment.
In-distribution (E1)
First, we train and evaluate methods on ShapeNet using all
13 categories and sparse point clouds with 3, 000 points and Gaussian noise with zero
mean and standard deviation 0.005. With this experiment, we evaluate the capacity of
learning methods to complete missing data of sparse point clouds and eliminate noise.
34


--- Page 50 ---

Table 2.3: Benchmark setup: In E1 to E4, we train surface reconstruction methods on
noisy point clouds of ShapeNet objects. In E1, we test on ShapeNet. In E2, we test on
ShapeNet, but with denser point clouds and 10% outliers. In E3, we test on the simpler
ModelNet objects. In E4, we test on the Berger et al. shapes. In E5, we train the methods
on the simpler ModelNet dataset and test on ShapeNet. In E6, we test optimization-based
methods on synthetic range scans of the Berger et al. dataset. In E7 and E8, we directly
compare learning- and optimization-based methods on synthetic and real point clouds.
Experiment
Training set
Test set
1
In-distribution
ShapeNet (synthetic MVS)
ShapeNet (synthetic MVS)
2
Out-of-distribution
unseen point cloud characteristics
ShapeNet (synthetic MVS)
ShapeNet (synthetic MVS)
3
Out-of-distribution
unseen shape categories,
less complex
ShapeNet (synthetic MVS)
ModelNet (synthetic MVS)
4
Out-of-distribution
unseen shape categories,
similar complexity
ShapeNet (synthetic MVS)
Berger et al. (synthetic MVS)
5
Out-of-distribution
unseen shape categories,
more complex
ModelNet (synthetic MVS)
ShapeNet (synthetic MVS)
6
Optimization
–
–
Berger et al. (synthetic range scan)
7
Out-of-distribution vs. optimization
unseen shape categories vs. optimization
ShapeNet (synthetic MVS)
Berger et al. (synthetic MVS)
8
Out-of-distribution vs. optimization
unseen point cloud characteristics and
shape categories vs. optimization
–
ShapeNet (synthetic MVS)
Real MVS & range scan
35


--- Page 51 ---

Table 2.4: Detailed benchmark setup: We present the four diﬀerent experiments of
our benchmark for surface reconstruction algorithms. For E1 to E4, we train methods to
reconstruct ShapeNet objects from noisy point clouds. In E1, we test on the ShapeNet test
set. In E2, we test on ShapeNet, but reconstruct from denser point clouds with noise and
outliers. In E3, we test on ModelNet with the same sampling as in E1. In E4, we test on ﬁve
Berger et al. shapes with the same sampling as in E1. Finally, in E5, we train the methods
on ModelNet and test on ShapeNet, with the same sampling as in E1.
Training set
Test set
Experiment
Name
# shapes
complexity
# points
σ noise
% outliers
Name
# shapes
complexity
# points
σ noise
% outliers
1
ShapeNet
30, 661
⋆⋆
3, 000
0.005
0
ShapeNet
1, 300
⋆⋆
3, 000
0.005
0
2
ShapeNet
30, 661
⋆⋆
3, 000
0.005
0
ShapeNet
1, 300
⋆⋆
10k
0.005
10
3
ShapeNet
30, 661
⋆⋆
3, 000
0.005
0
ModelNet
506
⋆
3, 000
0.005
0
4
ShapeNet
30, 661
⋆⋆
3, 000
0.005
0
Berger et al.
5
⋆⋆
3, 000
0.005
0
5
ModelNet
3, 979
⋆
3, 000
0.005
0
ShapeNet
1, 300
⋆⋆
3, 000
0.005
0
6
–
Berger et al.
5
⋆⋆
see Table 2.2
7
see Section 2.5.2
Berger et al.
5
⋆⋆
3, 000
0.005
0
8
ShapeNet
30, 661
⋆⋆
3, 000
0.005
0
Real
3
⋆⋆⋆
10, 000
variable
variable
Out-of-distribution (unseen point cloud characteristics) (E2)
We evaluate
the models trained in E1 on test shapes scanned with a diﬀerent setting than the train
shapes. We use dense point clouds with 10, 000 points of which 10% are outliers. We
add the same noise as in E1. Here, we investigate whether learning methods are able
to generalize to diﬀerent point cloud characteristics.
Out-of-distribution (unseen shape categories, less complex) (E3)
We eval-
uate the models trained in E1 on shapes from unseen categories but with the same point
cloud characteristics. We use six categories of ModelNet which are not present in the
ShapeNet training set. In this experiment, we investigate whether learning methods
generalize to unseen categories.
Out-of-distribution (unseen shape categories, similar complexity) (E4)
This experiment is similar to E3, but the test set is comprised of ﬁve shapes from Berger
et al. which do not correspond to ShapeNet’s categories, but have similar complexity.
Out-of-distribution (unseen shape categories, more complex (E5)
This
experiment is similar to E3 and E4, but we retrain all methods on the simpler shapes
from ModelNet10. Here, we assess whether learning methods can generalize from simple
shapes to more complex ones, a diﬃcult out-of-distribution setting.
Optimization (E6)
We evaluate several recently developed optimization-based meth-
ods, and two traditional test-of-time optimization-based methods. We use the Berger
et al. dataset for this experiment.
36


--- Page 52 ---

Out-of-category vs. optimization (E7)
We compare learning- and optimization-
based methods on the same dataset. For this we run optimization-based methods on
MVS scans of the Berger et al. shapes and compare the results to experiment E4.
Out-of-distribution vs. optimization (E8)
Finally, we compare learning- and
optimization-based methods on real MVS and range scanning point clouds. For learning-
based methods we use the models from E1.
2.5.3
Surface reconstruction methods
We brieﬂy describe the optimization- and learning-based methods that we will bench-
mark below. For a more complete description of these methods and their related con-
cepts we refer the reader to our survey in Section 2.4. Note that while some of the
optimization-based methods are based on deep networks, and we call them DSR meth-
ods, they do not learn shape priors from a training set.
Instead, the networks are
“trained” (or optimized) for each new point cloud to reconstruct a surface and rely on
novel regularization techniques to increase their robustness to noise, outliers and miss-
ing data. Conversely, while some traditional methods are not based on a deep network
architecture, we tune their (hyper)parameters on the training set by using a grid search
over diﬀerent parameter combinations. When we need to extract a surface from an
implicit ﬁeld, we use marching cubes [76] with a resolution of 1283.
Optimization-based methods
IGR [48]
Implicit Geometric Regularisation (IGR) is a DSR method, operating di-
rectly on the point cloud using a simple fully connected network architecture that esti-
mates an indicator function from point positions and normals. We optimize the network
weights for 100, 000 iterations for each scan/shape.
LIG [62]
Local Implicit Grids (LIG) trains an autoencoder to encode crops of a signed
distance function gained from ground truth shapes. For inference, only the decoder
part of the autoencoder is retained. Then, crops of the input point cloud with oriented
normals are augmented with 10 new points along each normal, representing ground
truth signed distance information. An initial latent vector is then decoded to produce
an SDF and iteratively optimized so that the augmented point cloud crop best matches
the SDF. A post-processing removes falsely-enclosed volumes. As code for training is
unavailable, we only use the optimization part, with a pretrained model on ShapeNet
(without noise). We use the sensor position to orient jet-estimated normals [29].
37


--- Page 53 ---

P2M [52]
Point2Mesh (P2M) is an optimization-based method which iteratively moves
vertices of an initial mesh to ﬁt a point cloud.
SAP [91]
Shape As Points (SAP) has a supervised learning- and an optimization-
based variant. In the learning variant, the method estimates the oriented normals as
well as k point oﬀsets for each input point, to adjust and densify the point cloud. The
resulting point cloud of size k | P | is then used by a diﬀerentiable Poisson solver
[65] to compute an indicator grid, which is supervised with a ground truth indicator
grid computed prior to training. The entire pipeline is diﬀerentiable which allows for
updating point oﬀsets, oriented normals and the network parameters.
SPSR [65]
Screened Poisson Surface Reconstruction (SPSR) is a classic non learning-
based method which approximates the surface as a level-set of an implicit function
estimated from point positions and normal information. We use the sensor position to
orient jet-estimated normals [29]. We chose an octree of depth 10 and Dirichlet boundary
condition. We also use the provided surface trimming tool for post-processing, but could
not ﬁnd parameters that consistently improve the reconstructed surface.
Labatut et al. [71]
Labatut et al. is a graph-cut-based method for range scans that
makes use of visibility information. Because there is no oﬃcial implementation of the
algorithm, we reimplemented it ourselves. To compare with optimization-based meth-
ods, we use the parametrization suggested by the authors: point weights αvis = 32 and
σ = 0.01; regularization strength λ = 5.
Learning-based methods
ConvONet [92]
Convolutional Occupancy Networks (ConvONet) is a DSR method
that ﬁrst extracts point features and averages them on cells of three 2D grids, or one
3D grid (variant). 2D or 3D grid convolutions then create features capturing the local
geometry. Last, the occupancy of a query-point is estimated with a fully connected
network from interpolated features stored on each node of the 2D or 3D grid.
SAP [91]
In the optimization variant, the method starts as the learning-based variant
described above. Then, the estimated indicator grid is used to compute a mesh and
points are sampled on the mesh to calculate a Chamfer loss between the mesh and input
point cloud.
DGNN [109]
This method uses a graph neural network to estimate the occupancy
of Delaunay cells in a point cloud tetrahedralization from cell geometry and visibility
features. A graph-cut-based optimization then reinforces global consistency.
38


--- Page 54 ---

POCO [21]
Point Convolution for Surface Reconstruction (POCO) extracts point
features using point cloud convolution [19], then estimates the occupancy of a query
point with a learning-based interpolation from nearest neighbors.
SPRS [65]
See method description above. For the learning-based experiments, we
perform a grid search over octree depth d = {6, 8, 10, 12} and boundary conditions b =
{dirichlet, neumann, free}. We use the parametrization with the best mean volumetric
IoU for reconstructions of the training set.
Labatut et al. [71]
See method description above. For the learning-based experi-
ments, we perform a grid search over regularization strength λ = {1.5, 2.5, 5, 10}, and
point weights α = {16, 32, 48} and σ = {0.001, 0.01, 0.1, 1}. We use the parametrization
with the best mean volumetric IoU for reconstructions of the training set.
2.5.4
Evaluation metrics
We want the reconstructed surface Sr to be as close as possible to the real (or ground
truth) surface S in terms of geometry and topology. To measure this “closeness” we use
several metrics.
Geometric metrics
We evaluate the geometric quality of reconstructions with the volumetric intersection
over Union (IoU), symmetric Chamfer distance (CD) and normal consistency (NC).
Volumetric IoU
In the following, let Sg and Sr be the set of all points that are
inside or on the ground truth and reconstructed surface, respectively. The volumetric
IoU is deﬁned as:
IoU (Sg, Sr) =|Sg ∩Sr|
|Sg ∪Sr| .
We approximate volumetric IoU by randomly sampling 100, 000 points in the union of
the bounding boxes of Sg and Sr.
Chamfer distance
To compute the Chamfer distance and normal consistency, we
sample a set of points Pg and Pr on the facets of the ground truth mesh and the
reconstructed mesh, respectively, with |Pg| = |Pr| = 100, 000. We approximate the
39


--- Page 55 ---

symmetric Chamfer distance between Sg and Sr as follows:
CD(Sg, Sr) =
1
2|Pg|
X
x∈Pg
min
y∈Pr ||x −y||2
+
1
2|Pr|
X
y∈Pr
min
x∈Pg ||y −x||2 .
Normal consistency
Let n(x) be the unit normal of a point x. We set this normal
to be the normal of the facet from which x was sampled. Let ⟨·,·⟩the Euclidean scalar
product in R3. Normal consistency is deﬁned as:
NC(Sg, Sr) =
1
2|Pg|
X
x∈Pg
*
n(x), n
 
argmin
y∈Pr
||x −y||2
!+
+
1
2|Pr|
X
y∈Pr

n(y), n

argmin
x∈Pg ||y −x||2

.
Topological metrics
We evaluate the topological quality of reconstructions through the number of compo-
nents, the number of non-manifold edges and the number of boundary edges.
Number of components
If not stated otherwise, the ground truth surfaces of our
datasets have exactly one component. In consequence, the reconstructed surfaces should
also have one component.
Number of boundary edges
The surfaces of all ground truth objects in our datasets
are closed. We verify this by measuring the number of boundary edges of the recon-
structed meshed surface which should be zero. Note that if boundary edges only appear
on the intersection of the reconstruction with its bounding box we still classify the
reconstruction as watertight, according to the deﬁnition in Section 2.3.3.
Number of non-manifold edges
The surfaces of all ground truth objects in our
datasets are 2-manifolds. We verify this by measuring the number of non-manifold edges
of the reconstructed meshed surface which should be zero.
Runtimes
To evaluate the scalability of methods, we measure the average time it takes to recon-
struct a surface of ShapeNet from 3,000 points.
40


--- Page 56 ---

2.6
Experiments
2.6.1
Learning-based surface reconstruction from synthetic MVS
point clouds (E1 - E5)
We examine the precision and versatility of novel supervised-learning methods and two
traditional methods for which training sets were used for tuning parameters. All eval-
uated methods perform well when reconstructing shapes from known categories and
known point cloud characteristics (E1). The learning-based methods show a signiﬁ-
cantly superior performance of at least 5% over SPSR and Labatut et al. (see Table 2.5).
The methods based on neural implicit ﬁelds (POCO, SAP and ConvONet) produce vi-
sually and quantitatively the best reconstructions (see Figure 2.5, ﬁrst column). DGNN
does not perform as well as most other learning methods in this experiment. The sparse
point clouds used in this experiment do not contain point samples on all details. How-
ever, due to the interpolating nature of DGNN surface details cannot be reconstructed
without input points.
In E2, domain shifts results in worse performance, both quantitatively and qualita-
tively for all methods except SPSR. SPSR shows robustness against outliers and beneﬁts
from the higher point density. Most learning methods do not produce satisfying results
(see Figure 2.5, second column). The reconstruction of SAP is too smooth and lacks de-
tails, but does not show as severe defects as the reconstructions of other learning-based
methods. Labatut et al. suﬀers from the low regularization weight tuned for the out-
lier free point clouds and could beneﬁt from higher regularization to remove erroneous
ﬂoating components from outliers.
When reconstructing out-of-category ModelNet shapes (E3), the neural implicit ﬁeld
methods exhibit visually the best reconstructions. SAP and POCO produce quanti-
tatively the best reconstructions (see Table 2.5).
The interpolating method DGNN
performs better than ConvONet.
In E4, we reconstruct shapes from Berger et al. which have similar complexity than
the shapes from ShapeNet used for training. The only learning methods able to leverage
information from the common point cloud characteristics to improve the test results are
DGNN and POCO.
In E5, most methods overﬁt the simpler ModelNet shapes when retrained and used
to reconstruct the more complex ShapeNet shapes. Even SPSR slightly suﬀers from
tuning parameters on ModelNet. The best reconstructions on ModelNet are achieved
with an octree depth of d = 8 (instead of d = 10 on ShapeNet) leading to worse results
on ShapeNet: 77.1 vIoU in E1 vs. 74.6 vIoU in E5. The parameter tuning of Labatut
et al. stays unchanged. DGNN is the only method that does not overﬁt on ModelNet
and yields the best results, both quantitatively and qualitatively. In fact, it performs
as well as when trained on ShapeNet directly.
ConvONet is only able to outperform traditional methods when the training and
41


--- Page 57 ---

Table 2.5: Numerical results for learning-based reconstructions (E1 to E5): SPSR
[65] is the only method that reconstructs surfaces with a high volumetric intersection over
Union (IoU) and a low Chamfer distance (CD) in each experiment. Therefore, its recon-
structions have the highest mean volumetric IoU and the lowest mean CD. However, SPSR
also reconstructs the least compact surfaces on average (i.e. surfaces with the highest num-
ber of components).
Labatut et al. [71] reconstructs the most compact surfaces.
The
reconstructions of DGNN [109] have the highest mean volumetric IoU of the tested learning
methods. The reconstructions of SAP [91] have the lowest mean CD of the tested learning
methods and the highest normal consistency. ConvONet and SPSR are the only methods
that reconstruct surfaces without boundary and non-manifold edges.
Volumetric IoU (%) [↑]
Normal consistency (%) [↑]
Method
E1
E2
E3
E4
E5
Mean
E1
E2
E3
E4
E5
Mean
ConvONet2D
[92]
85
47.3
79.3
65.1
68.3
69
92.7
76.4
90
78
87.8
85
ConvONet3D
[92]
84.8
15.1
83.6
76.4
51
62.2
93
71.8
93.1
87.2
82.5
85.5
SAP
[91]
88.7
59.8
89.2
78.3
54.9
74.2
93.5
86.7
94.1
89
87.1
90.1
DGNN
[109]
84.5
38.1
87
82.9
84.4
75.4
85.4
68.8
88.5
85.2
85.5
82.7
POCO
[21]
89.5
8.74
90.6
83.9
40.9
62.7
93.6
75.6
94.2
89.5
82.9
87.1
SPSR
[65]
77.1
80.7
80.7
77.6
74.6
78.1
87.7
83.2
89.1
86.3
88
86.9
Labatut et al.
[71]
80.3
60.4
83.9
79.4
80.3
76.9
81
73
84.6
80.8
81
80.1
Chamfer distance (per-point ave. %)
[↓]
Number of components
[↓]
Method
E1
E2
E3
E4
E5
Mean
E1
E2
E3
E4
E5
Mean
ConvONet2D
[92]
0.553
7.51
0.997
1.43
0.979
2.29
1.6
34.8
2.55
3.6
3.2
9.16
ConvONet3D
[92]
0.546
10.9
0.76
0.887
2.44
3.1
1.37
13.6
1.6
2.6
1.5
4.13
SAP
[91]
0.437
2.09
0.547
0.734
0.924
0.946
2.71
86
3.45
5.6
10.5
21.7
DGNN
[109]
0.549
2.54
0.635
0.586
0.55
0.973
1.31
16.1
1.13
1
1.31
4.16
POCO
[21]
0.416
10.5
0.516
0.579
1.32
2.67
2.32
178
2.82
2
16.3
40.2
SPSR
[65]
0.801
0.659
0.873
0.786
0.886
0.801
9.26
185
11.1
8
3.24
43.3
Labatut et al.
[71]
0.665
6.97
0.747
0.671
0.665
1.94
1.22
9.02
1.05
1
1.22
2.7
Number of boundary edges
[↓]
Number of non-manifold edges [↓]
Method
E1
E2
E3
E4
E5
Mean
E1
E2
E3
E4
E5
Mean
ConvONet2D
[92]
0
0
0
0
0
0
0
0
0
0
0
0
ConvONet3D
[92]
0
0
0
0
0
0
0
0
0
0
0
0
SAP
[91]
0
0.00923
0
0
8.44
1.69
0
0
0
0
0
0
DGNN
[109]
0
0
0
0
0
0
1.35
2.24
0.646
0.4
1.69
1.26
POCO
[21]
0
121
0
0
41.7
32.5
0
0.00154
0
0
0
0.000308
SPSR
[65]
0
0
0
0
0
0
0
0
0
0
0
0
Labatut et al.
[71]
0
0
0
0
0
0
9.35
28.5
8.47
9.6
9.35
13.1
42


--- Page 58 ---

Input
CONet2D [92]
CONet3D [92]
SAP [91]
DGNN [109]
POCO [21]
SPSR [65]
Labatut et al. [71]
Ground truth
In-distribution (E1)Out-of-distribution (E2)Out-of-category (E3)Out-of-category (E4)Out-of-category (E5)
Figure 2.5: Learning-based reconstructions (E1 to E5): DGNN [109], SAP [91] and
SPSR [65] provide visually the best reconstructions without prevalent defects.
43


--- Page 59 ---

test sets share the same point cloud characteristics and shape categories. SAP pro-
duces much better reconstructions and is the learning-based method with the highest
robustness against outliers. It is also the only method explicitly predicting normals. As
a result SAP reconstructs surfaces with the highest mean normal consistency over all
experiments. The local learning and global regularisation approach of DGNN produces
competitive results in all experiments, except for the outlier setting of E2. DGNN is the
learning-based method producing surfaces with highest mean IoU over all experiments.
The local attention-based learning mechanism of POCO leads to the best results when
the task does not involve reconstruction from unseen domains. It provides the most
faithful reconstructions in three experiments in which point cloud characteristics are
identical in train and test set (E1, E3, E4). However, POCO is heavily aﬀected by
outliers (E2), which can be explained by its purely local approach. POCO also tends
to overﬁt on simple training shapes (E5). The reconstructions of POCO, as well as the
ones of SAP contain boundary edges only in areas where the reconstructions intersect
the bounding box i.e. they are still watertight. SPSR proves robust to various defects
and shape characteristics, providing fair results, with the highest mean IoU and Cham-
fer distance across the board. However, its reconstructions are the least compact, i.e.
they have the highest number of components. Labatut et al.’s parametrization proves
slightly less robust, as the method is aﬀected by outliers. Its mean IoU is higher than
that of any learning method, and its reconstructions are the most compact surfaces
with an average number of components of 2.7. However, it is also the only method that
produces a signiﬁcant amount of non-manifold edges.
2.6.2
Optimization-based surface reconstruction from synthetic
range scanning point clouds (E6)
This experiment evaluates the precision and versatility of non-learning methods. The
benchmarked approaches consist in neural network based methods optimizing a function
to ﬁt an input point cloud and rely on novel regularization techniques to increase their
robustness to noise, outliers and missing data. Furthermore, we benchmark the two
traditional methods SPSR and Labatut et al. with standard parameter settings. We
reconstruct surfaces of Berger et al. from synthetic range scanning point clouds with
various diﬀerent defects. We show numerical results in Table 2.6 and visualisations in
Figure 2.6. Almost all reconstructions provided by the two traditional methods are
much more truthful than the DSR methods, with a mean volumetric IoU almost 10
points higher across all point cloud defects. IGR does visually not provide a good result
on the exemplary shape, especially on thin surface parts. Quantitatively, the method
provides the best reconstruction for the neural networks based methods in the absence
of outliers, and even the best overall reconstruction for the noisy high resolution scans.
LIG does not provide good reconstructions for any of the settings. This can be explained
by its pretrained model on defect-free uniform high density point clouds. Furthermore,
44


--- Page 60 ---

Table 2.6: Numerical results for optimization-based surface reconstruction (E6):
Optimization-based reconstruction of the Berger et al. shapes from synthetic range scans.
LR is a low resolution scan, HR a high resolution scan, HRN a high resolution scan with
noise, HRO a high resolution scan with outliers, and HRNO a high resolution scan with noise
and outliers. The methods are optimized per shape and per scan using standard settings as
mentioned in the corresponding publications.
Volumetric IoU (%) [↑]
Normal consistency (%) [↑]
Method
LR
HR
HRN
HRO
HRNO
Mean
LR
HR
HRN
HRO
HRNO
Mean
IGR
[48]
80.8
92.5
83.6
63.7
62.7
76.7
88
96.3
83.9
77.8
71.5
83.5
LIG
[62]
46.9
50.3
63.9
66
63.8
58.2
88.7
92.2
89
77
75.2
84.4
P2M
[52]
75.2
83.3
75.5
71.3
67.8
74.6
86.3
92.2
88.1
84.5
82.1
86.6
SAP
[91]
75.6
89.1
72.4
55.3
34.9
65.4
83.4
94.8
61.6
74.5
55.3
73.9
SPSR
[65]
77.7
90.2
82.8
90.3
82.1
84.6
88.1
96
88.1
96.2
85.8
90.9
Labatut et al. [71]
81.3
93.4
80.1
93.4
79.1
85.5
87.6
96
66.3
94.9
66.5
82.3
Chamfer distance (per-point ave. %)
[↓]
Number of components
[↓]
Method
LR
HR
HRN
HRO
HRNO
Mean
LR
HR
HRN
HRO
HRNO
Mean
IGR
[48]
0.674
0.322
0.554
7.96
7.72
3.45
6.8
1.2
35.2
44
97.4
36.9
LIG
[62]
0.745
0.581
0.781
7.89
7.8
3.56
1
1
1
1.6
1
1.12
P2M
[52]
0.817
0.473
0.729
1.53
2.13
1.13
1.2
1
1.2
1.4
1.6
1.28
SAP
[91]
0.852
0.32
0.701
3.99
3.93
1.96
73.2
85.6
937
1.8e+03
1.96e+03
971
SPSR
[65]
0.794
0.369
0.572
0.362
0.607
0.541
1.2
1.6
3.6
3.8
20.2
6.08
Labatut et al. [71]
0.635
0.314
0.608
0.339
0.641
0.507
1
1
1.2
1.2
1
1.08
Number of boundary edges
[↓]
Number of non-manifold edges [↓]
Method
LR
HR
HRN
HRO
HRNO
Mean
LR
HR
HRN
HRO
HRNO
Mean
IGR
[48]
0
0
0
0
0
0
0
0.8
0.8
5.2
4.2
2.2
LIG
[62]
69
42.8
17.2
0
0
25.8
0
0
0
0
0
0
P2M
[52]
0
0
0
0
0
0
0
0
0
0
0
0
SAP
[91]
0
0
0
0
449
89.8
0
0
0
0
0
0
SPSR
[65]
0
0
0
0
0
0
0
0
0
0
0
0
Labatut et al. [71]
0
0
0
0
0
0
1
5.8
24.4
3.8
22
11.4
45


--- Page 61 ---

Input
IGR
LIG
P2M
SAP
SPSR
Labatut et al.
Ground truth
LR
HR
HRN
HRO
HRNO
Figure 2.6: Optimization-based experiments: In each column we show the results of
diﬀerent methods of one of the ﬁve learning-based experiments.
46


--- Page 62 ---

Table 2.7: Numerical results for learning- and optimization-based reconstructions
(E7): Learning- and optimization-based reconstruction of the Berger et al. test shapes from
synthetic MVS scans. The learning methods were trained on synthetic MVS scans from
ShapeNet. Optimization-based methods are optimized per scan using standard settings.
BE stands for boundary edges and NME for non-manifold edges. Only the learning-based
methods POCO and DGNN reconstruct surfaces with a higher mean volumetric intersection
over Union and lower Chamfer distance than all optimization-based methods.
Method
Vol. IoU
[↑]
Normal consist.
[↑]
Chamfer dist.
[↓]
Components
[↓]
BE
[↓]
NME [↓]
Learning
ConvONet2D
[92]
65.1
78
1.43
3.6
0
0
ConvONet3D
[92]
76.4
87.2
0.887
2.6
0
0
SAP
[91]
78.3
89
0.734
5.6
0
0
DGNN
[109]
82.9
85.2
0.586
1
0
0.4
POCO
[21]
83.9
89.5
0.579
2
0
0
Optimization
IGR
[48]
78.3
83.8
0.775
15.4
0
0.4
LIG
[62]
45.7
86.6
0.831
1
65.6
0
P2M
[52]
74.5
85
0.768
2
0
0
SAP
[91]
71.9
77
0.811
133
0
0
SPSR
[65]
77.6
86.4
0.785
8
0
0
Labatut et al. [71]
79.4
80.8
0.671
1
0
9.6
its post-processing makes the reconstructions non-watertight. P2M provides geometri-
cally fair reconstructions and the topologically best reconstructions with a low number of
components, and watertight and manifold surfaces for all reconstructions. SAP provides
fair reconstructions in the absence of outliers. None of the neural network based meth-
ods is robust against outliers. As in the learning-based experiments, SPSR generates
high quality reconstructions for all input defects, and achieves the best mean normal
consistency. Labatut et al. achieves the best mean IoU and mean Chamfer distance
while providing the reconstructions with the lowest number of components. However,
the reconstructions of Labatut et al. are the only ones with a signiﬁcant number of
non-manifold edges.
2.6.3
Learning- and optimization-based surface reconstruction from
synthetic MVS point clouds (E7)
To directly compare learning- and optimization-based reconstructions on the same
dataset, we also reconstruct the Berger et al. shapes from synthetic MVS scans (cf.
E4) with the optimization-based methods. Thus, for learning-based methods, we use
the models trained on synthetic MVS scans from ShapeNet (cf. E4) and we optimize
non-learning methods per shape using standard settings. We show the numerical results
in Table 2.7 and visualisations in Figure 2.6. The learning-based methods DGNN and
POCO beneﬁt from the training on point clouds with the same characteristics as in the
47


--- Page 63 ---

Learning
(a) CONet2D
(b) CONet3D
(c) SAP
(d) DGNN
(e) POCO
(f) Input
Optimization
(g) IGR
(h) LIG
(i) P2M
(j) SAP
(k) SPSR
(l) Lab. et al.
(m) GT
Figure 2.7: Learning- and optimization-based reconstructions: We show reconstruc-
tions of dancing children from Berger et al. from a low resolution input point cloud with
noise (f) and the ground truth shape (m). The methods used in the ﬁrst row (a - e) were
trained on ShapeNet, on point clouds with the same sampling as the one used here. In
the second row (g - k) we show the reconstructions for the same shape (from the same
sampling) from optimization-based methods.
test set and reconstruct more truthful surfaces than the optimization-based methods.
Similar to E6, Labatut et al. produces the best results among the optimization-based
methods.
2.6.4
Learning- and optimization-based surface reconstruction from
real point clouds (E8)
Finally, we reconstruct surfaces from real MVS and range scanning point clouds. Again,
for learning-based methods, we use the models trained on synthetic MVS scans from
ShapeNet (cf. E4) and we optimize non-learning methods per point cloud. We show
the reconstructions in Figure 2.8. The MVS point cloud from Middlebury (Figure 2.8a)
is contaminated with a large amount of varying noise. SAP is the only learning method
which reconstructs a smooth surface without missing details (Figure 2.8d). However, it
suﬀers from small amounts of topological noise in the form of holes. The optimization-
based method P2M provides a visually good reconstruction with few defects (Fig-
ure 2.8i). In Figures 2.8m and 2.8y, optimization-based methods handle the additional
domain shift to an open scene better compared to learning-based methods. The two
traditional methods SPSR and Labatut et al. provide the visually best results on aver-
age.
This experiment also shows that our ﬁndings on synthetic point clouds coincide with
those on real-world point clouds, validating our experimental setup.
48


--- Page 64 ---

Learning
(a) Input
(b) CONet2D
(c) CONet3D
(d) SAP
(e) POCO
(f) DGNN
Optimization
(g) IGR
(h) LIG
(i) P2M
(j) SAP
(k) SPSR
(l)
Labatut et al.
Learning
(m) Input
(n) CONet2D
(o) CONet3D
(p) SAP
(q) POCO
(r) DGNN
Optimization
(s) IGR
(t) LIG
(u) P2M
(v) SAP
(w) SPSR
(x)
Labatut et al.
Learning
(y) Input
(z) CONet2D (aa) CONet3D
(ab) SAP
(ac) POCO
(ad) DGNN
Optimization
(ae) IGR
(af) LIG
(ag) P2M
(ah) SAP
(ai) SPSR
(aj)
Labatut et al.
Figure 2.8: Learning- and optimization-based reconstructions of real-world point
clouds (E8):
Learning methods are trained on synthetic MVS scans from ShapeNet.
Optimization-based methods are optimized per shape using standard settings.
The tra-
ditional methods SPSR [65] and Labatut et al. [71] provide visually the best reconstructions
with noticeable defects only the noisy Temple Ring MVS point cloud ((k) and (l)).
49


--- Page 65 ---

Table 2.8: Runtimes for learning-based reconstruction: Times (in seconds) for recon-
structing one object from a point cloud of 3,000 points averaged over the ShapeNet test
set. GC stand for Graph-cut; SE stands for surface extraction, such as marching cubes or
triangle-from-tetrahedron. Note that diﬀerent variants and implementations of marching
cubes are used by diﬀerent methods, which also inﬂuences the runtimes.
Model
Feature extraction
Decoding/GC
SE
Total
ConvONet2D
[92]
0.016
0.32
0.17
0.51
ConvONet3D
[92]
0.008
0.21
0.17
0.40
SAP
[91]
0.022
0.017
0.047
0.088
DGNN
[109]
0.11
0.28
0.01
0.39
POCO
[21]
0.088
13.72
0.33
15.74
P2S
[45]
69.06
11.51
80.57
SPSR
[65]
1.25
Labatut et al.
[71]
0.1
0.07
0.01
0.18
2.6.5
Runtimes
On Table 2.8, we report detailed runtimes for the methods tested in the learning-based
experiments. SAP is the fastest of all reconstruction methods. DGNN also shows fast
runtimes, while POCO is slow, due to its extensive use of neighborhood sampling. We
also compare runtimes of P2S. We were not able to include this method in experiments
E1 to E5 due to its long runtime for training and inference.
2.6.6
Summary and analysis
In the right circumstances, learning-based methods can produce highly detailed surfaces
while remaining robust to noise and missing data. However, this requires training on
large sets (30k shapes in our experiments) of suﬃciently complex surfaces and associ-
ated point clouds. Even if the tested learning methods can generalize to unseen shape
categories to some extent, the training and test sets must share the same point cloud
characteristics. This suggests that these methods mainly learn priors related to the
acquisition characteristics of the input point clouds, and less on the shapes themselves.
However, learning-based methods do not produce satisfying results when the training
shapes are too simple, or when the point clouds include unknown defects, such as outliers
(seeTable 2.9). Mixing traditional and learning-based methods, as in SAP or DGNN,
results in higher robustness to domain shifts and leads to short reconstruction times.
Except for IGR, the tested novel optimization-based methods are not robust to acqui-
sition defects and they rarely provide better results compared to the two traditional
methods SPSR and Labatut et al..
50


--- Page 66 ---

Table 2.9: Summary of benchmark results: Each method is rated with one to three stars
per attribute, determined by the qualitative and quantitative results of our benchmark.
Robustness to out-of-distribution
Mesh quality
Runtime
Learning
noise
outliers
density
category
watertightness
manifoldness
compactness
w/o training
ConvONet2D
[92]
*
*
*
*
***
***
*
***
ConvONet3D
[92]
*
*
*
*
***
***
***
***
SAP
[91]
**
**
**
**
***
***
*
***
DGNN
[109]
*
*
*
***
***
**
***
***
POCO
[21]
**
*
*
**
***
***
*
**
Robustness to
Mesh quality
Runtime
Optimization
noise
outliers
low density
category
watertightness
manifoldness
compactness
w/ training
IGR
[48]
***
**
**
***
***
**
*
*
LIG
[62]
*
*
*
***
**
***
***
*
P2M
[52]
*
**
*
***
***
***
***
*
SAP
[91]
*
*
*
***
***
***
*
*
SPSR
[65]
***
***
***
***
***
***
*
***
Labatut et al.
[71]
***
***
***
***
***
*
***
***
2.7
Conclusion
Surface reconstruction from point clouds is a well studied subject in the ﬁeld of digital
geometry processing. However, constant developments in acquisition techniques and
novel ideas for surface reconstruction and analysis bring forward new challenges. In this
paper, we survey the ﬁeld of surface reconstruction from point clouds and benchmark
several related methods. We revisit traditional test-of-time approaches for surface re-
construction and detail how they inspired novel approaches. We evaluate traditional and
novel optimization and learning-based methods on various tasks and datasets. We show
that novel optimization-based methods are not as robust against defects as traditional
methods. For in-distribution point clouds with characteristics similar to the ones of the
training set, learning methods provide more accurate reconstructions than traditional
approaches. However, real-world scenes often include a multitude of diﬀerent and highly
complex objects, and their acquisitions may contain a variety of defects. Most learning
methods require shapes of similar complexity in training and test sets and they are not
robust to out-of-distribution acquisition defects. These limitations of learning-based
methods hinder the reconstruction of point clouds in the wild. Generating or ﬁnding
adequate training data that includes a large variety of complex shapes scanned with
realistic defects is a diﬃcult task. Future work in learning-based surface reconstruction
should focus on training on point clouds with realistic acquisition defects, e.g. from
common sensors and acquisition settings, or on increasing the methods’ robustness to
unseen defects.
We will address the limited generalization capability of current DSR methods in the
next chapter, where we show that training learning-based methods on point clouds with
visibility information substantially increases their generalization capability to unseen
51


--- Page 67 ---

domains. We show that the added visibility information allows to reconstruct complex
objects and scenes from real-world scans even when the methods are trained on simple
training shapes.
In Chapter 4, we also show how DGNN can be trained on point clouds with multiple
defects and densities at the same time. This allows to produce superior results compared
to traditional test-of-time methods, even when reconstructing complex outdoor scenes
from highly defect-laden MVS point clouds.
52


--- Page 68 ---

“What you see is what you get!”
Geraldine Jones
3
Deep Surface Reconstruction from Point
Clouds with Visibility Information
Most current neural networks for reconstructing surfaces from point clouds ignore sensor
poses and only operate on point locations. Sensor visibility, however, holds meaningful
information regarding space occupancy and surface orientation.
In this chapter, we
present two simple ways to augment raw point clouds with visibility information, so it
can directly be leveraged by surface reconstruction networks with minimal adaptation.
Our proposed modiﬁcations consistently improve the accuracy of generated surfaces as
well as the generalization capability of the networks to unseen domains.
53


--- Page 69 ---

3.1
Introduction
The problem of reconstructing a watertight surface from a point cloud has recently
been addressed by a variety of deep learning based methods. Compared to traditional
approaches, deep surface reconstruction (DSR) can learn shape priors [79, 89] and lever-
age shape similarities [52] to complete missing parts [41], ﬁlter outliers, or smoothen
noise in defect-laden point clouds. DSR methods, however, often derive priors from
training datasets with few shape classes, generalizing poorly to unseen categories or
datasets. Learning more local priors improves consistency across diﬀerent objects or
scenes [62, 109] but may result in higher sensitivity to noise or other defects. Besides,
lack of global context complicates surface orientation.
For real world point clouds, usually acquired via active or passive methods such as
LiDAR scanning or MVS, the sensor position can be known and used to relate each
observed point with a line-of-sight. Such visibility information can then help to orient
surface normals [98] or predict occupancy [59, 71, 114]. While visibility is key for MVS,
it has largely been ignored by DSR methods. In fact, sensor positions are usually not
given in reconstruction benchmarks from point clouds.
To remedy this, (i) we consider real data containing visibility information and, (ii)
for synthetic shape benchmarks, we use a virtual scanning procedure to pair 3D points
with the position of their sensor. We then, show that many DSR methods can easily be
adapted to beneﬁt from visibility information (cf. Figure 3.1). Our main contributions
are as follows:
• We propose two simple ways to add visibility information to 3D point clouds, and
we detail how to adapt DSR methods to utilize them, with very little changes.
• Using synthetic and real data, at object and scene level, we show for a wide
range of state-of-the-art DSR methods that models leveraging visibility recon-
struct higher-quality surfaces and are more robust to domain shifts.
3.2
Related work
Many traditional surface reconstruction methods use visibility information [25, 26, 59,
60, 71, 114, 122]. They are usually based on a 3DT, which is intersected with lines-of-
sight to attribute visibility features to Delaunay cells. While such methods can scale to
billions of points [27] and are robust to moderate levels of noise and outliers, they do
not incorporate learned shape priors.
In contrast, recent DSR methods have shown to produce more accurate surfaces than
traditional approaches for shape categories and point cloud defects encountered during
training. Many DSR methods use an implicit surface representation, either based on
occupancy [79], or on the distance to the surface, whether it is signed [31, 35, 48, 83, 89]
54


--- Page 70 ---

(a) Reconstruction using only the points position.
(b) Reconstruction with visibility augmented point cloud.
Figure 3.1: Surface reconstruction with visibility information: We augment each 3D
point
with a sightline unit vector
pointing towards the sensor observing it. Additionally,
two auxiliary points are placed before
and after
the observed point along the sightline.
This allows DSR networks, with very little modiﬁcation, to reconstruct a signiﬁcantly more
accurate surface.
55


--- Page 71 ---

or unsigned [4, 5, 36, 121]. To integrate local information, diﬀerent forms of convolutions
are used, either on regular grids [37, 41, 62, 92, 110], directly on points [21, 112] or via
an MLP instead [45]. Other methods rather use an explicit surface representation such
as a mesh, which is deformed [52] or whose elements are classiﬁed [102, 109].
A key issue is to get a sense of surface point orientation, to choose between recon-
structing a thin volume (two main opposite orientations) or a thicker one (one main
orientation at void-matter interface). Some methods dismiss the orientation issue by
requiring oriented normals as input [62, 65, 112, 117], albeit producing such normals is
a challenging task in itself [70, 81, 98]. We show that oriented normals can be advanta-
geously replaced by visibility information.
Only few deep-learning methods make use of visibility information, typically from
multiple views with camera pose information. RayNet [90] aggregates features from
pixels of diﬀerent views that intersect in the same voxel, but it outputs a dense point
cloud, not a watertight surface mesh. Neural radiance ﬁelds [84, 86, 87] somehow also
model the free space between a point and its sensor. However, they often require a slow
optimization process and leverage little or no shape priors. We argue that DGNN [109]
(Chapter 4), that classiﬁes Delaunay cells with a graph neural network, currently is
the only general DSR method from point clouds with visibility. However, DGNN relies
on handcrafted visibility features, while in this chapter, we propose to augment the
input point clouds without crafting explicit priors. For point clouds for which visibility
information is not available, Vis2Mesh [106] shows that rendering virtual views can
signiﬁcantly improve the reconstruction quality of a traditional method.
3.3
Method
We consider a 3D point cloud P where each point p ∈P has some coordinates xp ∈R3
and knows the position cp ∈R3 of a sensor observing it. Instead of only using the raw
point coordinates (xp)p∈P as the input (Ip)p∈P of a DSR network, we propose two simple
ways to augment point cloud P with visibility information, and adapt DSR methods
accordingly.
3.3.1
Sightline Vector (SV)
For each p ∈P, we deﬁne a unit vector vp pointing from the observation xp to the
sensor cp:
vp = (cp −xp)/∥cp −xp∥.
(3.1)
This contains useful information for surface orientation. We normalize the vector as the
distance to the sensor is not as relevant as the viewing angle.
56


--- Page 72 ---

real point
before-point
after-point
line-of-sight
vector
sensor
position
real surface
Figure 3.2: Visibility-augmented point cloud: Each observed point is associated to a
sightline unit vector pointing towards its sensor. Two new points before and after each point
are added. They help to disambiguate occupancy.
3.3.2
Auxiliary Points (AP)
To help the network predict empty and full space immediately in front of and behind
the observed surface, we consider two auxiliary points to each point p: a before-point
pb and an after-point pa, located along the sightline on each side of p:
xpb = xp + dvp,
(3.2)
xpa = xp −dvp,
(3.3)
where d is a characteristic distance in the point cloud P, e.g., the average distance
from a point to its nearest neighbor. By construction, pb is likely outside the scanned
object or scene (modulo sensing noise and outliers), and pa, likely inside (modulo object
thickness too).
3.3.3
Visibility-Augmented Point Cloud
We use sightline vectors and auxiliary points to add visibility information to an input
point cloud, separately or together.
(SV) To use sightline information only, we simply concatenate the sightline vector chan-
nelwise to the point coordinates to form the network input: Ip = (xp ⊕vp) ∈R6.
(AP) To use auxiliary points only, we add before-points pb and after-points pa to P,
with tags t ∈R2 concatenated to point coordinates to identify the point type, i.e.,
57


--- Page 73 ---

Iq = (xq ⊕tq) ∈R5 with q ∈{p, pb, pa}, where tp = [0 0] (observed point), tpb = [1 0]
(before-point), or tpa = [0 1] (after-point).
(SV+AP) When combining both kinds of visibility information, before-points pb and
after-points pa are given the same sightline vector as their reference point, i.e., vpb =
vpa = vp, and we take as input Ip = (xp ⊕vp ⊕tp) ∈R8.
While holding a similar kind of information, no augmentation can be reduced to the
other one. SVs alone are not enough to place APs without knowing d, and APs alone,
as they are not associated to their observed point in P, cannot determine SVs (cf.
Figure 3.2). This is also conﬁrmed empirically in Section 3.4.4.
3.3.4
Modifying an Existing Architecture
We can adapt most DSR networks to handle visibility-augmented point clouds with only
few modiﬁcations:
• We change the input size (number of channels) of the ﬁrst layer of the network
(generally an encoder), increasing it by 2, 3 or 5, depending on the augmentation
respectively (AP, SV, SV+AP).
• We directly add auxiliary points to the point cloud, thus tripling the number of
input points. For methods based on neighboring point sampling, we add auxiliary
points after sampling for more eﬃciency.
The batch size may need to be adjusted to ﬁt a larger point cloud in memory, but the
rest of the network stays unchanged. Its size is mostly unaltered (e.g., +0.005% for
ConvONet [92]).
3.4
Experiments
To assess our proposal, we ﬁrst describe how we generate synthetic point clouds with
visibility information, and the real-world datasets we use. Then, we detail our simple
adaptation of six diﬀerent DSR baseline networks to leverage our visibility information,
compare the quality of the reconstructed surfaces and analyze the generalization capa-
bility of the networks trained on point clouds with and without visibility information.
3.4.1
Datasets
We consider a variety of object and scene datasets, both synthetic and real, to show the
versatility of our approach. For generating point clouds from artiﬁcial shapes we use
our synthetic MVS scanning procedure described in Section 2.5.
58


--- Page 74 ---

ModelNet10.
We use the oﬃcial train/test splits of all 10 object classes of Mod-
elNet10 [118] and hold out 10% of the train set for validation. We make the models
watertight using ManifoldPlus [56]. We then scan the models from 10 diﬀerent sensor
positions to produce 3, 000 points per object and add Gaussian noise with zero mean
and standard deviation 0.005 as in [92].
ShapeNet.
We study the generalizability of models trained on ModelNet10 by testing
on 1000 shapes per class from the ShapeNet [32] test set of Choy et al. [38] (9 out of 13
classes are not represented in ModelNet10). We use the watertight models provided by
the authors of Occupancy Networks [79] and scan the models using the same scanning
procedure as for ModelNet10. We apply a transformation to the models (and scans)
to match their orientation to the orientation of the ModelNet10 objects (except for
networks marked with † in Table IV, which were trained with the original orientation).
Synthetic Room.
We use the train/val/test splits of Synthetic Rooms and the pro-
vided watertight scenes [92]. For scanning, we only place sensors in the upper hemi-
spheres. We scan 10 000 points and add Gaussian noise with zero mean and standard
deviation 0.005 as in [92].
SceneNet.
We test on a few synthetic scenes of SceneNet [51] using the given virtual
scans, voxel-decimated to 1 cm.
ScanNet.
We test on a few real scenes of ScanNet [40] using the provided real RGB-D
scans, voxel-decimated to 2 cm.
Tanks and Temples.
We use the real LiDAR point cloud of the Ignatius statue from
the Tanks and Temples dataset [68] downsampled to 10, 000 points.
Middlebury.
We use an MVS point cloud of the TempleRing from Middlebury [100],
made with OpenMVS [30] and downsampled to 10, 000 points.
DTU.
We use an MVS point cloud of scan1 from DTU [61], made with OpenMVS
[30] and downsampled to 10, 000 points.
3.4.2
Evaluation Metrics
We use the geometric evaluation metrics presented in Section 2.5.4: volumetric inter-
section over union (IoU), mean Chamfer distance ×100 (CD) and normal consistency
(NC).
59


--- Page 75 ---

3.4.3
DSR Baselines
ConvONet [92].
This method ﬁrst extracts point features and projects them on
three 2D grids, or one 3D grid (variant).
2D or 3D grid convolutions then create
features capturing local occupancy. Last, the occupancy of a query-point is estimated
after interpolating grid features. We consider the 3 × 642 2D-plane encoder and the 643
3D-volume variant. To adapt them, we change the input size of the point encoder’s ﬁrst
layer.
Points2Surf [45].
This method predicts both the occupancy of a query point and its
unsigned distance to the surface. It uses both a local query-point neighborhood sampling
and a global point-cloud sampling. We use the best-performing variant (uniform global
sampling, no spatial transformer). To adapt it, we increase the input size of the ﬁrst
layer of both the local and global encoders, and when a point is sampled, locally or
globally, we add its two auxiliary points on the ﬂy.
Shape As Points [91].
For each input point, the method estimates its normal as well
as k point oﬀsets that are used to correct and densify the point cloud. The resulting
point cloud of size k|P| is then fed to a diﬀerentiable Poisson solver [65]. To adapt the
method, we change the input size of the ﬁrst layer of the encoder, and of the normal
and oﬀset decoders as they also input the point cloud. We directly add auxiliary points
as input, whose normal and oﬀsets will thus be computed too.
Local Implicit Grids (LIG) [62].
This method trains an auto-encoder from dense
point cloud patches. For inference, a given sparse patch with oriented normals is ﬁrst
augmented, close to our idea, with 10 new points along each normal; then reconstruction
uses latent vectors minimizing a decoder-based training loss, and a post-processing
removes falsely-enclosed volumes. As training code is unavailable, we use the model
pretrained on ShapeNet (without noise). For oriented normals, we use Jets [29] oriented
with a minimum spanning tree [98], as in [121]. To exploit visibility, we replace normals
with sightline vectors; we do not add (more) auxiliary points.
POCO [21].
This method extracts point features using point cloud convolution [19],
then estimates the occupancy of a query point with a learning-based interpolation on
nearest neighbors. To adapt it, we increase the input size of the ﬁrst layer and add
auxiliary points on the ﬂy only in the ﬁrst layer.
Delaunay-Graph Neural Network (DGNN) [109].
This method, introduced in
Chapter 4, uses a graph neural network to estimate the occupancy of Delaunay cells in
a point cloud tetrahedralization. A graph-cut-based optimization then reinforces global
60


--- Page 76 ---

Table 3.1: Ablation study: The vanilla model of ConvONet [92] trained and tested on
ModelNet10 with diﬀerent ways to add visibility or normal information.
Model
SV
AP
IoU ↑
ConvONet-2D (3 × 642) [92]
0.853
+ sightline vectors (SV) only
✓
0.871
+ auxiliairy points (AP) only
✓
0.881
+ both SV and AP
✓
✓
0.889
+ sensor position
cp
0.870
+ unnormalized SV
cp −xp
0.870
+ estim. normals / estim. orientation
Jets [29] / MST [98]
0.853
+ estim. normals / sensor orientation
Jets [29] / sensor-base [98]
0.868
+ true normals
GT normals
0.879
consistency. The method, which already uses visibility, outperforms other traditional
reconstruction methods that use visibility information. As it already exploits visibility,
we do not alter it, but use it as baseline for comparison.
Hyperparameters.
For all methods, unless otherwise stated, training and evaluation
are unchanged; we keep the value of the hyperparameters used in the original papers.
When marching cubes [76] are needed for surface extraction, we use a grid resolution of
1283.
3.4.4
Ablation Study
To validate our design, we compare in Table 3.1 various ways to add visibility informa-
tion to the vanilla model of ConvONet.
Independently, SVs and APs signiﬁcantly improve performance (+1.8 and +2.8 IoU
pts). A reason why APs are more proﬁtable could be that the network is tailored for
points, not points with sightline features. While SVs and APs capture a similar kind of
information, they are, however, complementary: combining them is even more beneﬁcial
(+3.5 IoU pts). Our general interpretation is that SVs help to decide whether a locally
“thin” point cloud is to be considered as a noisy scan of a single surface, or as a (less
noisy) scan on both sides of a thin surface. They thus have an impact on local shape
topology, which can bring a notable gain. Auxiliary points convey similar information,
but also contribute more directly to reﬁne the surface position. Replacing SVs by the
sensor position or by the unnormalized point-sensor vector gives essentially the same
performance than our unit vector. This can be explained by the fact that our scanning
procedure does not introduce signiﬁcant variation in terms of distance to the sensor.
61


--- Page 77 ---

Table 3.2: Object-Level Reconstruction: DSR methods trained and tested on Mod-
elNet10, with and without sightline vectors (SV) or auxiliary points (AP). † Trained on
ShapeNet.
Model
SV
AP
IoU ↑
CD ↓
NC ↑
ConvONet-2D [92]
0.853
0.618
0.934
ConvONet-2D [92]
✓
0.871
0.557
0.936
ConvONet-2D [92]
✓
✓
0.889
0.508
0.944
ConvONet-3D [92]
0.885
0.493
0.949
ConvONet-3D [92]
✓
0.911
0.424
0.956
ConvONet-3D [92]
✓
✓
0.923
0.393
0.959
Points2Surf [45]
0.842
0.590
0.890
Points2Surf [45]
✓
0.859
0.544
0.896
Points2Surf [45]
✓
✓
0.856
0.548
0.897
Shape As Points [91]
0.903
0.438
0.948
Shape As Points [91]
✓
0.907
0.430
0.950
Shape As Points [91]
✓
✓
0.914
0.410
0.954
POCO [21]
0.907
0.422
0.945
POCO [21]
✓
0.915
0.408
0.950
POCO [21]
✓
✓
0.917
0.406
0.950
† LIG [62]
–
0.974
0.849
† LIG [62]
✓
–
0.880
0.882
DGNN [109]
✓
0.866
0.543
0.884
Yet, for real world acquistions, with a larger range of sensor distances, normalizing the
SV ensures more stability.
Adding SVs outperforms estimated normals [29] with estimated orientation [98], and
even estimated normals with sensor-based orientation. While using ground-truth nor-
mals is slightly more beneﬁcial than SVs, combining SV+AP yields the best overall
performance, which highlights the richness of our visibility information.
We also experiment with adding more than two auxiliary points: (i) at distance 0.5d
or 2d, (ii) at the midpoint between sensor and point, or (iii) as grazing points, estimated
by densely sampling the sightlines with auxiliary points and keeping the ones close to
an input point. None of these strategies brought signiﬁcant improvements over simply
adding two points at distance d on both sides of the observed point.
62


--- Page 78 ---

Bare
Augmented
Bare
Augmented
Bare
Augmented
Bare
Augmented
Input
ConvONet-2D [92]
Points2Surf [45]
Shape As Points [91]
POCO [21]
Ground Truth
Figure 3.3: Object-level reconstruction on ModelNet10 I: Reconstructed shapes from
the ModelNet10 test set using four diﬀerent DSR methods trained on ModelNet10. Top
rows of each object use the bare point cloud as input, and bottom rows use the point cloud
augmented with visibility information.
63


--- Page 79 ---

Bare
Augmented
Bare
Augmented
Bare
Augmented
Bare
Augmented
Input
ConvONet-2D [92]
Points2Surf [45]
Shape As Points [91]
POCO [21]
Ground Truth
Figure 3.4: Object-level reconstruction on ModelNet10 II: Reconstructed shapes from
the ModelNet10 test set using four diﬀerent DSR methods trained on ModelNet10. Top
rows of each object use the bare point cloud as input, and bottom rows use the point cloud
augmented with visibility information.
64


--- Page 80 ---

Table 3.3: Numerical results for scene-level reconstruction: ConvONet trained and
tested in sliding-window mode on Synthetic Rooms.
Model
SV
AP
IoU ↑
CD ↓
NC ↑
ConvONet-3D [92]
0.805
0.598
0.906
ConvONet-3D [92]
✓
✓
0.832
0.569
0.911
3.4.5
Object-Level Reconstruction
Table 3.2 reports the performance on ModelNet10 of various models, with and without
sightline vectors or auxiliary points.
ConvONet, both planar and volumetric variants, gain about +3 IoU pts with visibility
information.
The resulting surface is more accurate, especially in concave parts, as
illustrated in Figure 3.3 and Figure 3.4.
Points2Surf improves with sightline vectors, but auxiliary points do not improve fur-
ther: the sensor vectors are enough to resolve ambiguities for the occupancy estimation,
but distance estimation does not further beneﬁt from auxiliary points.
Shape As Points beneﬁts from sightline vectors, although not as much as other meth-
ods, probably because the model also estimates normals which provide a similar infor-
mation as visibility. Still, adding auxiliary points further gains +0.6 IoU pts, yielding
more complete and smoother surfaces.
POCO similarly beneﬁts +1 IoU pts from sightline vectors but not much from the
further addition of auxiliary points. While sightline vectors help for surface orientation,
POCO is already accurate enough for APs to bring little reﬁnement.
LIG produces poor results, likely because the only available model is trained on
ShapeNet, with uniform sampling, little or no noise, and because oriented normals are
only estimated. We cannot report IoU because LIG’s post-processing creates holes in
some objects. Yet, replacing the estimated normals by sightline vectors improves the
predicted surface.
DGNN, which already exploits visibility and outperforms ConvONet-2D and Points2-
Surf, is outdistanced on this dataset by methods that use our augmented point clouds.
3.4.6
Scene-Level Reconstruction
To study the impact of visibility information at scene level, we train and test ConvONet
on Synthetic Rooms, in sliding-window mode [92]. We report quantitative results in
Table 3.3 and qualitative results in Figure 3.5. The model gains almost +3 IoU pts with
visibility information, showing that beneﬁts scale to scenes, not just to objects.
65


--- Page 81 ---

Bare
Augmented
ConvONet [92]
ConvONet [92] (+SV+AP)
Ground Truth
Figure 3.5: Scene-Level Reconstruction on Synthetic Rooms: Reconstructed scenes
of the Synthetic Rooms dataset using ConvONet [92] in sliding-window mode, with and
without visibility information.
66


--- Page 82 ---

Table 3.4:
Out-of-Domain Object-Level Reconstruction: DSR methods trained on
ModelNet10 and tested on ShapeNet, with and without sightline vectors (SV) or auxiliary
points (AP). † Trained on ShapeNet.
Model
SV
AP
IoU ↑
CD ↓
NC ↑
† ConvONet-2D [92]
0.852
0.560
0.929
ConvONet-2D [92]
0.685
0.979
0.878
ConvONet-2D [92]
✓
0.667
1.042
0.833
ConvONet-2D [92]
✓
✓
0.780
0.847
0.883
ConvONet-3D [92]
0.628
0.972
0.885
ConvONet-3D [92]
✓
0.759
0.724
0.905
ConvONet-3D [92]
✓
✓
0.823
0.685
0.912
Points2Surf [45]
0.807
0.561
0.876
Points2Surf [45]
✓
0.836
0.516
0.886
Points2Surf [45]
✓
✓
0.833
0.522
0.887
† Shape As Points [91]
0.838
0.577
0.923
Shape As Points [91]
0.556
0.923
0.870
Shape As Points [91]
✓
0.749
0.843
0.881
Shape As Points [91]
✓
✓
0.809
0.641
0.915
POCO [21]
0.391
1.119
0.839
POCO [21]
✓
0.832
0.618
0.901
POCO [21]
✓
✓
0.815
0.635
0.887
DGNN [109]
✓
0.844
0.549
0.854
3.4.7
Generalization to New Domains
ShapeNet.
To evaluate the impact of added visibility on the generalization capability
of DSR methods, we train on ModelNet10 and test on ShapeNet (Table 3.4).
We observe that ConvONet, Shape As Points and POCO trained with visibility in-
formation generalize much better on the new objects and classes, with a gain up to
+44 IoU pts.
For comparison, we also show the scores of oﬃcial models trained on
ShapeNet, although trained on uniformly sampled points rather than virtual scans,
which explains the drop of performance compared to the numbers in the papers [91, 92].
Points2Surf also improves by up to +3 IoU pts with added sightline vectors, but not
further with APs.
We show the results of object-level reconstruction on ShapeNet in Figure 3.6 and
Figure 3.7. All methods also visually beneﬁt from added visibility information. In par-
ticular, ConvONet produces very accurate and complete surfaces of the unseen shape
classes. The reason for the largely improved volumetric IoU when using visibility infor-
67


--- Page 83 ---

Bare
Augmented
Bare
Augmented
Bare
Augmented
Input
ConvONet-3D [92]
Points2Surf [45]
Shape As Points [91]
POCO [21]
Ground Truth
Figure 3.6: Out-of-domain object-level reconstruction on ShapeNet: Reconstructed
shapes from the ShapeNet test set using four diﬀerent DSR methods trained on ModelNet10.
Top rows of each object use the bare point cloud as input, and bottom rows use the
point cloud augmented with visibility information. The last two rows show a cut of the
reconstructions that are shown on the two other rows immediately above.
68


--- Page 84 ---

Bare
Augmented
Bare
Augmented
Input
ConvONet-3D [92]
Points2Surf [45]
Shape As Points [91]
POCO [21]
Ground Truth
Figure 3.7: Cut of out-of-domain object-level reconstruction on ShapeNet: Recon-
structed shapes from the ShapeNet test set using four diﬀerent DSR methods trained on
ModelNet10. Top rows of each object use the bare point cloud as input, and bottom rows
use the point cloud augmented with visibility information. The last two rows show a cut of
the reconstructions that are shown on the two other rows immediately above.
69


--- Page 85 ---

Table 3.5: Runtimes for object-level reconstruction with visibility information: Aver-
age times (in seconds) for reconstructing one object from a point cloud of 3 000 points with
and without sightline vectors (SV) or auxiliary points (AP). MC is marching cubes. Times
are averaged over the ModelNet10 test set.
Model
SV
AP
Encoding
Decoding
MC
Total
ConvONet-2D [92]
0.016
0.32
0.17
0.51
ConvONet-2D [92]
✓
0.016
0.34
0.17
0.54
ConvONet-2D [92]
✓
✓
0.016
0.33
0.17
0.52
Points2Surf [45]
69.06
11.51
80.57
Points2Surf [45]
✓
71.92
11.35
83.27
Points2Surf [45]
✓
✓
173.2
11.41
184.7
Shape As Points [91]
0.022
0.017
0.047
0.088
Shape As Points [91]
✓
0.023
0.017
0.046
0.086
Shape As Points [91]
✓
✓
0.024
0.041
0.047
0.114
POCO [21]
0.088
13.72
0.33
15.74
POCO [21]
✓
0.091
13.68
0.33
15.66
POCO [21]
✓
✓
0.093
13.70
0.33
15.67
mation is illustrated in Figure 3.7 and Figure 3.9. For out-of-domain reconstructions,
the baseline methods often predict hollow shapes, i.e., empty space enclosed inside an
object. This leads to backfaces behind the real surface and a poor volumetric IoU. On
the contrary, our models, trained on visibility-augmented point clouds, learn to distin-
guish between empty and full space more reliably and do not produce such artifacts.
Real-World Data.
The increased generalization capability of the models is also
validated when reconstructing surfaces from real-world scans obtained with LiDAR or
MVS. In Figures 3.10 and 3.8, we show that networks using visibility information can
reconstruct more accurate and more complete surfaces.
The results in Figure 3.10
and in the last row of Figure 3.8 represent open scenes, while all methods used for
reconstruction were only trained on the closed ModelNet10 objects. Methods using our
augmented point clouds with visibility information cope much better with this additional
domain shift.
3.4.8
Runtimes
On Table 3.5, we report detailed runtimes for the tested methods, with and without
visibility information.
Adding sightline vectors does not signiﬁcantly increase the runtime for any of the
tested methods. The eﬀect of auxiliary points depends on the method. For ConvONet,
70


--- Page 86 ---

Bare
Augmented
Bare
Augmented
Bare
Augmented
Input
ConvONet-3D [92]
Points2Surf [45]
Shape As Points [91]
POCO [21]
HD Scan
Figure 3.8: Out-of-domain object-level reconstruction from real-world scans: Recon-
structed shapes from a LiDAR point cloud (top, Ignatius from Tanks And Temples [68]) and
MVS point clouds (middle, TempleRing from Middlebury [100], bottom scan1 from DTU
[61]) using four diﬀerent DSR methods trained on ModelNet10. Top rows of each object
use the bare point cloud as input, and bottom rows use the point cloud augmented with
visibility information. HD Scan is a high-density point cloud.
71


--- Page 87 ---

ConvONet-3D [92]
Points2Surf [45]
Shape As Points [91]
POCO [21]
Bare
Augmented
Figure 3.9: Cut of out-of-domain object reconstruction of Ignatius: A cut (along
the green curve) of the reconstructed surface of Ignatius. The reconstructions from the
bare point cloud (top row) include empty space enclosed inside the object with backfaces,
leading to a poor volumetric IoU. The reconstructions from the point cloud augmented with
visibility information (bottom row) include only one surface, close to the input points.
72


--- Page 88 ---

SceneNet
ScanNet
Input
Bare
Augmented
Figure 3.10: Out-of-domain scene-Level reconstruction on SceneNet and ScanNet:
POCO [21] trained on ModelNet10, with and without visibility information, is run on scenes
from SceneNet (synthetic RGB-D scan) and ScanNet (real RGB-D scan).
most of the processing time is spent computing grid features. The encoding of 3d points
is performed by a small PointNet network [94], whose runtime is only a small fraction
of the total time. As a consequence, adding APs does not incur signiﬁcant changes
in computation time (< +2%). In contrast, Points2Surf uses a large point encoding
network and is 2.2 times slower with APs. Shape As Points is 1.3 times slower due to
the fact that we decode 3 times as many points as the baseline method. POCO is also
essentially unaﬀected (< +2%) by the addition of auxiliary points as they only impact
the ﬁrst (small) layer of the point-convolution backbone.
73


--- Page 89 ---

3.5
Limitations and perspectives
The position of auxiliary points depends on parameter d, which is the average distance,
across the whole scene, from a point to its nearest neighbor. To better handle point
density variations, it could be set locally rather than globally. Besides, as this posi-
tioning is also sensitive to sampling noise, d could also be directly adjusted after noise
estimation.
Our current approach only associates each point with a single sensor, while MVS
points typically have several. A more eﬃcient and versatile approach than simply du-
plicating sightlines is still an open issue.
Last, we resort to virtual scans because current 3D reconstruction benchmarks do
not provide sensor positions. While we show that using our augmented point clouds
allows common architectures to successfully generalize from virtual to real scenes, our
training set may fail to replicate some challenging conﬁgurations encountered using
actual sensors.
3.6
Conclusion
The sensor poses are often ignored in point cloud processing, even though available
with most acquisition technologies. For acquisitions with stationary sensors, only 3C
4 byte ﬂoats and P 2 byte unsigned integers are necessary to store sensor poses. For
mobile acquisitions, where the sensor position varies for each point p, 3P ﬂoats are
necessary. Image-based pipelines usually already contain sensor poses within the image
orientation. We present two straightforward ways to exploit sensor positions in order to
augment point clouds with visibility information. Our experiments show that various
DSR methods can be adapted with minimal eﬀort to exploit these visibility-augmented
point clouds, resulting in improved accuracy and completeness of reconstructed surfaces,
as well as a substantial increase in generalization capability.
74


--- Page 90 ---

“Bigger is always better!”
Adam Savage
4
Scalable Surface Reconstruction with
Delaunay-Graph Neural Networks
In this chapter, we introduce a novel learning-based, visibility-aware, surface recon-
struction method for large-scale, defect-laden point clouds.
Our approach can cope
with the scale and variety of point cloud defects encountered in real-life Multi-View
Stereo (MVS) acquisitions.
Our method relies on a 3D Delaunay tetrahedralisation
whose cells are classiﬁed as inside or outside the surface by a graph neural network and
an energy model solvable with a graph cut. We name this approach Delaunay-Graph
Neural Network (DGNN). Our model, making use of both local geometric attributes
and line-of-sight visibility information, is able to learn a visibility model from a small
amount of synthetic training data and generalizes to real-life acquisitions.
75


--- Page 91 ---

4.1
Introduction
One of the most successful approaches for surface reconstruction from large point clouds
in the wild is to (i) tessellate the convex hull of the point cloud using a 3DT, (ii) label the
resulting cells as inside or outside, and (iii) extract the surface as the interface between
cells with diﬀerent labels [59, 71, 114]. This guarantees to produce non-self-intersecting
and watertight surfaces, a useful requirement for downstream engineering applications.
A remaining problem is that the methods typically rely on an energy formulation with
handcrafted unary and binary potentials. Tuning the balance between data ﬁdelity and
regularity in these methods tends to be diﬃcult due to the high variability in nature
and amplitude of the defects of real-life point clouds.
In the following chapter, we present a novel method for reconstructing watertight
surfaces from large point clouds based on a 3DT whose cells are associated with a graph-
adjacency structure, local geometric attributes, and visibility information derived from
camera positions (see Figure 4.2). We then train a GNN to associate each cell with a
probability of being inside or outside the reconstructed surface. In order to obtain a
spatially regular cell labelling, these probabilities are incorporated into a global energy
model that can be solved with a graph cut. This scheme directly predicts a spatially
regular labeling, which leads to a smoother surface. Furthermore, graph-cut solving
algorithms can easily scale to large point clouds, as opposed to other learning-based
surface reconstruction methods, which tend to be limited to objects, or operate with
sliding windows, as remarked by [92].
To the best of our knowledge, our method is the ﬁrst deep learning-based mesh re-
construction algorithm able to take visibility information into account. This property
is valuable, especially in areas lacking suﬃciently dense input points.
It is also the
ﬁrst deep learning surface reconstruction method using a memory-eﬃcient GNN im-
plementation built on a 3DT. We argue that combining the scalability of traditional
computational geometry algorithms with the adaptability of modern deep learning ap-
proaches paves the way to learning-based large-scale 3D information processing. We
validate our approach by showing that, even when trained on a small synthetic dataset,
our method is able to generalize to large-scale, real-life, and complex 3D scenes and reach
state-of-the-art performance on an open-access MVS dataset [99] (see Figure 4.1).
4.2
Related work
4.2.1
Graph Cut-Based Surface Reconstruction
Surface reconstruction based on inside-outside labelling and volumetric segmentation is
traditionally formulated as a graph-cut optimization problem [25, 26, 59, 60, 114, 122].
The 3D space is discretized into the cells of a 3DT of captured points [71] or the cells of
an arrangement of detected planes [33]. A graph (T , E) is formed for which the vertices
76


--- Page 92 ---

(a) Image from courtyard
(b) Input MVS point cloud
(c) Our reconstruction
(d) Screened Poisson [65]
(e) Vu et al. [114]
(f) Jancosek et al. [60]
(g) Ours
Figure 4.1: Scene-level reconstruction on ETH3D:
Reconstruction of the courtyard
scene of the ETH3D benchmark [99]. Top: a set of images, among which (a), is transformed
into a dense MVS point cloud pictured in (b), from which our method reconstructs a mesh,
displayed in (c) after texturation [115]. Bottom: untextured mesh reconstructions obtained
by SPSR in (d), the algorithms of Vu et al. [114] in (e) and Jancosek et al. [60] in (f),
and our proposed reconstruction in (g). Our method provides at the same time a higher
accuracy (e.g., wall pattern in the background, that is reconstructed more truthfully) and a
higher completeness (e.g., the back rest of the front chair).
77


--- Page 93 ---

are the cells T of the complex, and the edges in E connect cells with a common facet.
Each cell t ∈T is to be assigned with an occupancy label lt in {0, 1}, where 0 means
outside and 1 means inside. For this, each cell t is attributed a unary potential Ut
expressing a likelihood of being inside or outside the scanned object. Additionally, each
facet interfacing two adjacent cells s and t is attributed a binary potential Bs,t, which
takes low values when the facet is likely to be part of a regular reconstructed surface
and higher values otherwise. The label assignment of cells is performed by minimizing
an energy in the following form:
E(l) =
X
t∈T
Ut(lt) + λ
X
(s,t)∈E
Bs,t(ls, lt) ,
(4.1)
where λ ≥0 is the regularization strength. This energy E is globally minimized by
computing a minimum cut in an appropriate ﬂow graph or using a linear programming
approach [20].
The unary potentials commonly depend on visibility criteria, such as: (i) cells with
sensors are always outside, (ii) cells traversed by lines of sight (virtual lines between a
sensor and an observed point) are likely outside, or (iii) cells behind a point are likely
inside. These visibility models are not robust to the acquisition noise and outliers of
real-life point clouds, so the unary potentials can be adjusted to the local point density
[59, 60, 114, 122], or by using other modalities [25].
Binary potentials are used to force neighbouring cells crossed by the same line of
sight to have the same labeling. Additionally, they can incorporate low-area [25, 26] or
other shape-based priors [59, 71, 114]. Instead of hand-tuning the visibility model, we
propose to learn it by training a neural network to produce unary potentials from local
visibility and local geometric information.
4.2.2
Deep Learning-Based Surface Reconstruction
Recently, deep learning-based models have been proposed for reconstructing surfaces
from point clouds or other modalities, operating on a discrete mesh or with continuous
functions. We brieﬂy discuss these approaches below, and refer the reader to the survey
in Section 2.4 for a more recent and complete discussion of related works.
Surface-based approaches rely on transforming a discretized 2D surface, such as 2D
patches or spheres [49, 74, 102, 119], meshes [42, 46, 52], charts [116], or learned prim-
itives [43], in order to best ﬁt an input point cloud.
While such methods can lead
to impressive visual results, they either cannot guarantee that the output mesh is wa-
tertight and intersection-free, or are limited to simple topologies and low resolution.
Additionally, they are typically memory intensive, which prevents them from scaling to
large scenes.
Volume-based approaches learn a continuous mapping from the input space R3 either
to R, deﬁning the signed distance to the surface [4, 5, 31, 48, 89], or directly to an
78


--- Page 94 ---

(a)
Delaunay trian-
gulation and ray cast-
ing.
(b) Local and con-
textual learning with
gnn.
(c) Global optimiza-
tion with graph cuts.
(d)
Surface
recon-
struction.
Figure 4.2: DGNN pipeline: (a) The input point cloud is triangulated, and visibility
information is derived from lines of sight
and from camera positions
. (b) A graph
neural network uses this local and contextual visibility information to predict an occupancy
score for each tetrahedron. (c) A global energy derived from the network’s output ﬁnds a
minimal cut
in an adapted ﬂow graph. (d) The reconstructed surface
is deﬁned as
the interface between cells with diﬀerent (inside and outside) labels.
occupancy value {0, 1} [79, 82, 92]. The network training can be either unsupervised
[75], aided by geometric regularization [48], or supervised by ground-truth surface in-
formation [79, 82, 92]. Some continuous methods [79, 89, 92] predict the occupancy or
signed distance conditionally to a latent shape representation, and thus learn a dataset-
speciﬁc shape distribution. This can lead to diﬃculty in generalizing to shapes from
unseen classes. Even though volume-based approaches deﬁne a surface in continuous
space with implicit functions, they often rely on a discretization of 3D space to learn
these functions [79, 82, 92]. Recent works propose to scale these methods to larger
scenes using an octree structure [82] or a sliding window strategy [92].
While our method also relies on a discretization of space, our 3DT is directly com-
puted from the input point cloud and is thus adaptive to the local resolution. Our
method guarantees to produce watertight surfaces, can operate at large scale, and gen-
eralizes to unseen shapes and scenes.
4.3
Method
We explain here how to construct a 3DT augmented with expressive but lightweight
visibility features that are leveraged by a memory-eﬃcient GNN and used in a global
energy formulation to extract the target surface.
79


--- Page 95 ---

t3
t2
t1
t0
t4
t5
p
c
len(cp, t1)
len(cp, t0)
cp →
cp
cp ∈Lf
t0
cp ∈Lv
t1
real
cp →∈Rv
t2
cp →∈Rf
t3
surface
Figure 4.3: DGNN visibility features: A line-of-sight cp between a camera c and a visible
3D point p also deﬁnes a ray cp →. The line-of-sight cp traverses the two outside tetrahedra
t0 and t1, while the ray cp →traverses the two inside tetrahedra t2 and t3. Neither cp, nor
cp →traverse t4 or t5; they thus do not contribute to their visibility information.
4.3.1
Visibility-Augmented 3D Tetrahedralization
We consider P ∈R3×P a 3D point cloud deﬁned by the absolute point positions in space,
where P is the number of points p in the cloud. Furthermore, we consider C ∈R3×C
the absolute positions of a set of sensors used to capture these points, where C is the
number of sensors c. We ﬁrst construct a 3DT tessellating the convex hull of P into
a ﬁnite set of tetrahedra T . Each tetrahedron t is characterized by its four vertices
Vt ∈R3×4 and four facets Ft ∈N3×4. At the boundary of the convex hull, each facet
is incident to an inﬁnite cell whose fourth vertex is at inﬁnity. This ensures that each
facet of the 3DT is incident to exactly two tetrahedra.
Let L ⊂C × P be the lines-of-sight from cameras c of C to points p of P seen from c. A
line-of-sight cp ∈L is an oriented segment from c to p. These deﬁnitions are illustrated
in Figure 4.3. In the case of MVS point clouds, a single point can be seen from multiple
cameras. Similarly, we call cp →∈R ⊂C × P the ray extending line-of-sight cp from
the seen point p to inﬁnity. To simplify the computation of visibility information, we
truncate the ray traversal after the second tetrahedron. For instance, in Figure 4.3,
cp →does not go beyond t3.
80


--- Page 96 ---

4.3.2
Feature Extraction
The occupancy, or insideness, of a tetrahedron w.r.t. the target surface can be inferred
by combining geometrical and visibility information. Indeed, a tetrahedron t traversed
by a line-of-sight cp is see-through, and most likely lies outside the surface. Conversely,
if a tetrahedron is traversed by a ray cp →and no line-of-sight, it may lie inside the
surface, especially if close to p.
However, visibility-based information is not suﬃcient to retrieve a perfect labelling
of tetrahedra. First, there is no connection between the discretization of the space by
the 3DT and the distribution of lines-of-sight. There may be a signiﬁcant number of
tetrahedra not traversed by any line-of-sight nor any ray, depending on the geometry
of the acquisition. Second, noise and outliers — stemming from MVS for example —
can result in inaccurate and unreliable visibility information. Thus, we propose to use a
GNN to propagate and smooth visibility-based information, as well as other contextual
information, to all tetrahedra in the 3DT of an object or a scene.
Tetrahedron features.
While one could argue for directly learning features from
tetrahedra and camera positions in an end-to-end fashion, this resulted in our exper-
iments in a signiﬁcant computational overhead and a very diﬃcult geometric task for
a neural network to learn. Instead, we propose to derive computationally light, yet
expressive, handcrafted features encoding the local geometry and visibility information
of tetrahedra.
For a tetrahedron t ∈T , we denote by Lv
t the set of lines-of-sight that traverse t and
that end at one of its vertices v ∈Vt, and by Lf
t the set of lines-of-sight that intersect t
through its facets and do not end at one of its vertices:
Lv
t = {(c, p)∈L | (cp) ∩t ̸= ∅, p ∈Vt}
(4.2)
Lf
t = {(c, p)∈L | (cp) ∩t ̸= ∅, p /∈Vt}.
(4.3)
Likewise, we denote Rv
t and Rf
t the equivalent sets for rays in R.
These sets are informative for determining the occupancy of a tetrahedron. Indeed,
a tetrahedron t for which Rv
t is nonempty indicates that it is directly behind an element
of the surface, hinting at a higher probability of insideness. A nonempty Rf
t indicates
that t was hidden by a surface, hinting at a possible insideness. Indeed, since the hit
occurred before t, this carries less conﬁdence as it could be due to an occlusion or a thin
structure.
Conversely, a tetrahedron t with nonempty Lf
t indicates that it is traversed by a
line-of-sight, indicating a high probability of outsideness. A nonempty Lv
t also indicates
that t is traversed by a line-of-sight, but since the hit is on one of the corners of the
tetrahedron, this prediction is likely to be aﬀected by acquisition noise, and hence has
a lower conﬁdence.
81


--- Page 97 ---

To characterize the inﬂuence of lines-of-sights and rays with respect to a given tetra-
hedron t, we deﬁne two measures: count(t) and dist(t). count(t) ∈N4 corresponds to
the number of each type of lines or rays intersecting with t:
count(t) =
h
|Lv
t |, |Lf
t|, |Rv
t |, |Rf
t|
i
.
(4.4)
Then, to measure the proximity between t and the impact point p of a traversing
line-of-sight cp, we deﬁne len(cp, t) as the distance between p and the exit point of cp
in t seen as from p. As represented in Figure 4.3, this corresponds to the length of the
longest segment between p and the portion of cp intersecting t:
len(cp, t) =
max
y∈(cp)∩t∥p −y∥.
(4.5)
When (cp) ∩t is empty, len(cp, t) is set to zero. We deﬁne len(cp →, t) in the same
manner for rays. Finally, dist(t) characterizes the proximity of tetrahedron t with the
observed points p of its intersecting lines-of-sight and rays:
dist(t) =
"
min
cp∈Lv
t
len(cp, t), min
cp∈Lf
t
len(cp, t),
min
cp →∈Rv
t
len(cp →, t),
min
cp →∈Rv
t
len(cp →, t)

.
(4.6)
We complement the 8 visibility features deﬁned by count(t) and dist(t) with 4 mor-
phological features: the volume of t, the length of its shortest and longest edges, and
the radius of its circumsphere. This leads to a set of 12 handcrafted features ft for
each tetrahedron t ∈T , that we normalize (zero mean and unit standard deviation)
independently.
It is important to note that none of the aforementioned features can be computed in
a meaningful way for inﬁnite cells of the 3DT. We simply set all feature values to zero,
which can be interpreted as a padding strategy.
4.3.3
Contextual Learning
We learn contextual information with a GNN using the propagation scheme GraphSAGE
of Hamilton et al. [50] with a depth of K (see Figure 4.4). This scheme can be performed
independently for each tetrahedron, allowing us to perform inference on large graphs
with limited memory requirements.
We denote by G = (T , E) the undirected graph whose edges E ⊂T 2 link cells that are
adjacent, i.e., share a facet. We consider one tetrahedron t in T , and compute hop(t, K)
its K-hop neighborhood in G, i.e., the set of nodes s of T which can be linked to t using
82


--- Page 98 ---

at most K edges.We leverage the local context of a tetrahedron t with a message-passing
scheme over its local neighborhood in G. We ﬁrst initialize the features of all nodes s in
the subgraph hop(t, K) with the handcrafted features deﬁned in Section 4.3.2: x0
s = fs.
We then apply the following update rule in two nested loops over k = 0, . . . , K −1 and
for all s ∈hop(t, K −1):
xk+1
s
= σ

norm

W (k)

xk
s

 mean
u∈N(s)

xk
u

,
(4.7)
with N(s) the one-hop neighborhood of node s, σ an activation layer, norm a normal-
ization layer, and [· || ·] the concatenation operator. {W (k)}K−1
k=0 is a set of K learned
matrices, each operating only at the k-th iteration. After K iterations, a multilayer
perceptron (MLP) maps the embedding xK
t to a vector of dimension 2 representing the
inside/outside scores for tetrahedron t:
(it, ot) = MLP(xK
t ) .
(4.8)
The main advantage of this simple scheme is that it can be performed node-wise from
the K-hop neighborhood of each node and run the update scheme locally. Memory
requirements only depend on K, i.e., subgraph extraction, and not on the size of the
full graph G. This allows us to scale inference to large graphs. Likewise, training can
be done by sampling subgraphs of depth at least K, and does not require to load large
graphs in memory.
4.3.4
Loss Function
For defect-laden point clouds aﬀected by noise, the ground-truth surface is generally not
exactly aligned with the faces of the 3DT created from the input points. Consequently,
tetrahedra intersecting the true surface can be only partially inside or outside, and
cannot be attributed a pure 0 or 1 occupancy label. Instead, we deﬁne the ground-
truth insideness/outsideness ig ∈[0, 1]|T | as the proportion of each cell’s volume lying
inside of a ground-truth closed object; ig can take any value between 0 and 1.
We convert the tetrahedra’s predicted inside/outside scores to an occupancy using
the sotfmax fonction:
ˆıt =
exp(it)
exp(it) + exp(ot)
(4.9)
We deﬁne the ﬁdelity loss for each t as the Kullback-Leibler divergence of the true
occupancy ig
t and the softmaxed predicted occupancy ˆıt:
KLt (ˆıt) = ig
t log (ˆıt) + (1 −ig
t ) log (1 −ˆıt) + q ,
(4.10)
83


--- Page 99 ---

i/o
12
128
128
2
Figure 4.4: Graph neural network scheme: Illustration of our GNN update for tetrahe-
dron
. Information is pooled at diﬀerent hops k (here K = 2) into an increasingly rich
descriptor. A linear layer based on the last cell embedding assigns an inside/outside score
to the central tetrahedron. Note that each prediction can be performed independently for
each tetrahedron by considering only the K-hop subgraph.
84


--- Page 100 ---

with q a quantity that does not depend on ˆıt, and can thus be ignored while training
the network. We deﬁne the total loss as the average of all tetrahedra’s ﬁdelity weighted
by their volume Vt:
L (ˆı) =
1
P
t∈T Vt
X
t∈T
Vt KLt (ˆıt) .
(4.11)
4.3.5
Global Formulation
Deﬁning the target surface directly from the inside/outside scores predicted by the
network can result in a jagged surface due to non-consistent labelling of neighboring
tetrahedra. To achieve a smooth label assignment, even in areas with heavy noise, we
use the inside/outside scores it, ot to deﬁne the unary potentials in the formulation of
Equation 4.1:
U(lt) = it [lt = 0] + ot [lt = 1] ,
(4.12)
with [x = y] the Iverson bracket, equal to 1 if x = y and 0 otherwise. We also add a
constant factor αvis to ot for tetrahedra containing a camera, indicating that they must
lie outside the surface.
We deﬁne the binary potentials introduced in Equation 4.1 with a surface quality
term that allows us to reconstruct a smooth surface and to eﬃciently remove isolated
or non-manifold components in the ﬁnal surface mesh. We use the same surface quality
term
Bs,t(is, it) = 1(is ̸= it) βs,t
(4.13)
as Labatut et al. [71] for a facet interfacing the tetrahedra s and t. Considering the
intersection of the circumspheres of s and t with the facet, with angles φ and ψ, then
βs,t is deﬁned as:
βs,t = 1 −min{cos(φ), cos(ψ)} .
(4.14)
The energy E(l) in Equation 4.1 with unary and binary potentials as deﬁned above
can be minimized eﬃciently by constructing a ﬂow graph and using a min-cut solver
[22].
4.3.6
Surface Extraction and Cleaning
We can deﬁne the target surface by considering the labeling of T obtained by minimizing
E(l). The reconstructed surface is composed of all triangles whose adjacent tetrahedra
have diﬀerent labels. Triangles are oriented towards the outside tetrahedra.
For open
85


--- Page 101 ---

scene reconstruction, we optionally apply a standard mesh cleaning procedure, imple-
mented in OpenMVS [30], by removing spurious and spike faces (whose edges are too
long). This is especially useful for outdoor scenes containing areas with very little input
data, such as far-away background or sky, and for which outliers can result in isolated
components with low-quality surfaces. In our experiments, all competing methods, at
least visually, beneﬁt from this classic postprocessing for open scene reconstruction.
4.4
Experiments
In this section, we present the results of two sets of numerical experiments to show
the performance of our reconstruction method for both objects and large-scale scenes.
In both settings, our method is only trained on a small synthetic dataset, and yet
outperforms state-of-the-art learning and non learning-based methods, highlighting its
high generalization capability. A third experiment, where we trained DGNN on large
object shape databases was presented in Section 2.6.
4.4.1
Evaluation Setting
Training Set.
We train our network on a small subset of 10 shapes for each of the 13
classes of the ShapeNet subset from [38]. We found this small number to be suﬃcient for
our network to learn diverse local shape conﬁgurations. We produce watertight meshes
of these models using the method of Huang et al. [55]. We then synthetically scan the
models with diﬀerent degrees of outliers and noise and build corresponding 3DTs.
To obtain the ground-truth occupancy, we sample 100 points in each tetrahedron
and determine the percentage of these sampled points lying inside their corresponding
ground-truth models. In total, we train our network on around 10M tetrahedra.
Hyper-Parameters.
We train our model by extracting batches of 128 subgraphs
of depth K = 4 centered around random tetrahedra of our training set. We param-
eterize our model with K = 4 linear layers of width 64, 128, 256 and 256 for each
hop respectively. After each linear layer, we apply batch normalization [58] and ReLU
non-linearities. The ﬁnal cell embeddings are mapped to inside/outside scores using an
MLP 256 →64 →2. We train the network with the Adam optimizer [67] with an initial
learning rate of 10−4 which we decrease by a factor of 10 every 10 epochs. For the
graph-cut optimization, we set the camera bias term αvis to 100 and the regularization
strength to λ = 1.
We use the same hyper-parameters for all dataset variants, in particular for all set-
tings of the scanning procedure of Berger et al.’s benchmark [8]. While we could choose
parameters that better ﬁt speciﬁc noise and outlier levels, we argue that a single real-life
86


--- Page 102 ---

scene can present multiple defect conﬁgurations simultaneously. Consequently, recon-
struction algorithms should be versatile enough to handle diﬀerent noise and outlier
ratios with a single parameterization.
Competing Methods.
We compare our model with other mesh reconstruction meth-
ods that have available (or re-implementable) code, and the ability to scale to large
scenes with several million points:
• ConvONet [92] is a deep model, like ours, but that does not take visibility into
account.
We use the model (with multi-plane decoder) pretrained on the en-
tirety of ShapeNet for the object-level reconstruction. Among all the available
pretrained models, this one gave the best performance. We use the volume de-
coder model pretrained on the synthetic indoor scene dataset [92] for scene-level
reconstruction, where we set the voxel size to 4 cm.
• IGR [48] is a deep model which we optimize for 30, 000 iterations on each object
using the oﬃcial implementation.
• SPSR [65] is a classic non-learning-based method which approximates the sur-
face as a level-set of an implicit function estimated from normal and point in-
formation. We chose an octree of depth 10 and Dirichlet boundary condition for
object-level reconstruction and 15 and Neumann boundary condition for scene-
level reconstruction.
We also experimented with post-processing the Screened
Poisson reconstruction with the included surface trimming tool, but could not
ﬁnd consistent trimming parameters that improve the mean values of Poisson
presented in Table 4.2 and Table 4.3.
• Labatut et al. [71] is a graph-cut-based method for range scans that makes use
of visibility information. We use our own implementation of the algorithm and
use the parametrization suggested by the authors (αvis = 32 and λ = 5) and with
σ set according to an estimation of the scan noise.
• Vu et al. [114] is an extension of Labatut et al. [71] to MVS data. We use its
OpenMVS [30] implementation with min-point-distance = 0.0, free-space-support
= 0 and default parameters for all other settings.
• Jancosek et al. [60] also exploits visibility in a graph-cut formulation, with
special attention to weakly-supported surfaces. We use the OpenMVS [30] im-
plementation with min-point-distance = 0.0, free-space-support = 1 and default
parameters for all other settings.
For scene reconstruction, we compare all methods without mesh cleaning and with
default clean (Section 4.3.6) options in OpenMVS.
87


--- Page 103 ---

Evaluation Metrics.
We use the evaluation metrics presented in Section 2.5.4 for
object-level reconstruction.
For scene-level reconstruction, we measure intrinsic mesh properties, such as the
number of components and the surface area. We also extrinsicly evaluate the mesh
reconstruction methods at a given precision τ. To this end, we uniformly sample random
points from the reconstructed meshes. We then evaluate (i) accuracy, (ii) completeness,
and (iii) F1-score of the reconstructions. Accuracy is deﬁned as the fraction of sampled
points on the reconstructed mesh within distance τ to any point in the ground truth.
Completeness is deﬁned as the fraction of ground-truth points for which there exists a
point sampled on the reconstructed mesh within a distance τ. The harmonic mean of
accuracy (precision) P(τ) and completeness (recall) R(τ) is the F1-Score F(τ) deﬁned
as:
F(τ) = 2P(τ)R(τ)
P(τ) + R(τ).
(4.15)
We use the ETH3D Evaluation Program [99] to compute these values from the ground-
truth LiDAR scans and samplings of the meshed surfaces. In the original benchmark, the
authors evaluate MVS reconstructions with threshold τ as low as 1 cm. Generating such
mesh samplings implies sampling over 300 million points for some scenes. To accelerate
this procedure, we only sample 900 points per m2 on the reconstructed meshes. This
allows us to compute accuracy and completeness with a threshold of 5 cm and up.
The protocol accounts for incomplete ground truth by segmenting the evaluation space
into occupied, free and unobserved regions and sampled (reconstruction) points in the
unobserved space are later ignored.
4.4.2
Design Choices and Ablation Study
In this section, we evaluate the eﬀect of several of our design choices on the performance
of our algorithm.
Direct Prediction.
We assessed the impact of the graph cut step by evaluating
the quality of the surface obtained using only the unary terms: tetrahedrons with an
insideness over 0.5 are predicted as inside, and the others outside. This leads to very
fragmented reconstructed surfaces (over 10 times more components), especially in the
background of the scenes. Given that our objective is to produce compact watertight
surfaces, we chose to use a regularization, here with a global energy minimization.
Learning Binary Weights.
We designed an GNN able to predict binary weights
in the energy model along the unaries. However, this lead to more fragmented surfaces
and overall lower performance. The diﬃculties of learning the potentials of an energy
88


--- Page 104 ---

Table 4.1: Ablation Study. Mean IoU of diﬀerent design choices of our method. We train
all models on a subset consisting of 1100 shapes of ModelNet10 and test on ModelNet10
and ShapeNet.
Model
ModelNet10
ShapeNet
Vanilla model
86.1
84.1
Feature ablation
No geometric
-1.4
-1.3
No visibility
-4.5
-6.4
No vertex
-0.2
-0.2
No facet
-0.3
-0.4
Model ablation
No edge convolution
-0.3
-0.5
No volume weighting
+0.4
0.0
No graph-cut
+0.1
-0.3
Receptive ﬁeld
3-hop
-0.3
-0.3
4-hop
0.0
0.0
5-hop
+0.1
+0.1
model with a neural network are expected, as neural networks operate locally and in
continuous space, while graph cuts operate globally and in discrete space. In fact, we
can interpret our GNN prediction as the marginal posterior inside/outside probability
of each tetrahedron, while the graph cut provides an inside/outside labeling of maxi-
mum posterior likelihood (MAP) in a ﬁtting Potts model [23]. These two tasks being
conceptually diﬀerent, we were not able to successfully learn our surface reconstruction
in an end-to-end fashion and leave this endeavor for future work.
Using Facet Features With Edge Conditioned Convolutions.
We tried re-
placing our GNN scheme with the Dynamic Edge Conditioned Convolution of Simonovsky
et al. [103] for its ability to leverage facet features derived from both ray and tetrahe-
drons. This resulted in a marginal increase in performance (under 1% decrease of the
Chamfer distance) at the cost of an increase in computational and memory requirements.
For the sake of simplicity and with scalability in mind we keep the simple GraphSAGE
scheme.
Relevance of Geometric Features.
We tried training a model using only visi-
bility features and no tetrahedron-level geometric features. In doing this ablation, we
lose between 10% of F1-Score on ETH3D. This demonstrates that visibility informa-
89


--- Page 105 ---

tion should be combined with geometric information, which is not typically done in
traditional approaches.
4.4.3
Object-Level Reconstruction
Experimental Setting.
To evaluate the adaptability of our method to a wide range
of acquisition settings, we use the surface reconstruction benchmark of Berger et al. [8].
It includes ﬁve diﬀerent shapes with challenging characteristics such as a non-trivial
topology or details of various feature sizes. The provided benchmark software allows to
model a variety of range scanning settings to produce shape acquisitions with diﬀerent
defect conﬁgurations. We apply to each shape diﬀerent settings such as varying resolu-
tion, noise level, and outlier ratio, meant to reproduce the variety of defects encountered
in real-life scans. See Section 2.5 for details about the scanning procedure.
Results.
The results are presented in Table 4.2 and illustrated Figures 4.5 to 4.9.
We observe that the competing learning-based methods have a hard time with this
dataset. ConvONet [92] does not generalize well from the simple models of ShapeNet
to the more challenging objects evaluated here. As for IGR [48], it works well in the
absence of noise and outliers but produces heavy artifacts on defect-laden point clouds.
In contrast, our method is able to generalize to the new unseen shapes and signiﬁcantly
outperforms ConvONet and IGR. Our method also outperforms the state-of-the-art
and highly specialized algorithm of Labatut et al. [71], showing that the graph neural
network is able to learn a powerful visibility model with a higher accuracy than methods
based only on handcrafted features.
4.4.4
Large-Scale Scene Reconstruction
Experimental Setting.
To evaluate the ability of our method to scale to entire
scenes, we experiment with the high-resolution MVS benchmark ETH3D [99].
This
benchmark is originally designed to evaluate MVS algorithms (point cloud reconstruc-
tion) under challenging real-life conditions. Ground-truth point clouds and camera poses
are openly available for a training set including 7 indoor and 6 outdoor scenes. The
ground truth consists of LiDAR scans post-processed to only contain reliable points.
While we cannot train our network on this dataset due to the lack of closed surfaces
in the ground truth, we can evaluate the quality of the output of our algorithm after
sampling points on the reconstructed surface. To this end, we generate dense point
clouds from downsampled images (3100 × 2050 pixels) of the 13 training scenes using
a patch-based MVS algorithm [6] implemented in OpenMVS [30]. The point clouds
and associated camera poses are used as inputs for all mesh reconstruction methods
evaluated in Section 4.4.1. Additionally, as input for the Screened Poisson and IGR
90


--- Page 106 ---

Figure 4.5: Qualitative results on Berger et al.’s anchor: We show the input point
clouds in column 1. ConvONet [92] (column 2) does not generalize well to the unseen new
shape. IGR [48] (column 3) works well at high resolution but fails in the other cases. The
Screened Poisson [65] algorithm (column 4) does not reconstruct the sharp features well,
but is robust against outliers, even close to the surface. The reconstructions of Labatut
et al. [71] (column 5) and ours (column 6) are visually similar for the easier high resolution
case. Our method performs slightly better on the low resolution, and noise cases.
91


--- Page 107 ---

Figure 4.6: Qualitative results on Berger et al.’s daratech: We show the input point
clouds in column 1. ConvONet [92] (column 2) does not generalize well to the unseen new
shape. As with other shapes, IGR [48] (column 3) works well at high resolution but generates
artefacts or fails in other settings. The Screened Poisson [65] algorithm (column 4) does not
reconstruct the sharp features well, but is robust against outliers, even close to the surface.
In the low resolution setting, our algorithm is incomplete where Labatut creates unwanted
surface parts.
92


--- Page 108 ---

Table 4.2: Numerical results for Berger et al. benchmark: Object-level reconstruction
with various point cloud settings: low resolution (LR), high resolution (HR), high resolution
with added noise (HRN), high resolution with added outliers (HRO), high resolution with
noise and outliers (HRNO). We measure the symmetric Chamfer distance to the ground truth
(per-point average for objects of size 75 as done in Berger et al.’s benchmark [8]), volumetric
IoU (%), number of components (ground-truth meshes all have only one component) and
number of non-manifold edges (none in the ground truth). All metrics are averaged over the
5 shapes of the benchmark dataset. We compare IGR [48] “optimized” for 30, 000 iterations
on each of the 5 variants (LR, HR, HRN, HRO, HRNO) of the 5 shapes, screened Poisson
reconstruction [65] with an octree of depth 10, Labatut et al. [71] with αvis = 32, λ = 5
and σ set according to an estimation of the scan noise, and ConvONet [92] and our method
trained on the ShapeNet subset from [38].
Chamfer distance (per-point ave. %)
[↓]
Volumetric IoU (%) [↑]
Method
LR
HR
HRN
HRO
HRNO
Mean
LR
HR
HRN
HRO
HRNO
Mean
ConvONet
[92]
1.90
1.80
2.31
2.91
3.73
2.53
67.8
71.3
62.9
61.4
57.3
64.1
IGR
[48]
1.03
0.44
0.80
11.87
11.50
5.13
80.4
93.0
84.2
27.5
27.8
62.6
Poisson
[65]
1.09
0.48
0.80
0.46
0.86
0.74
79.1
91.9
84.3
91.9
83.3
86.1
Labatut et al. [71]
0.89
0.42
0.89
0.46
0.95
0.72
81.9
94.5
80.9
94.3
80.6
86.4
Ours
0.88
0.41
0.77
0.41
0.78
0.65
82.0
95.6
84.7
95.3
84.7
88.5
Number of components
[↓]
Number of non-manifold edges [↓]
Method
LR
HR
HRN
HRO
HRNO
Mean
LR
HR
HRN
HRO
HRNO
Mean
ConvONet
[92]
3.2
2.0
6.0
12.8
14.0
7.6
0.0
0.0
0.0
0.0
0.0
0.0
IGR
[48]
2.2
2.2
43.0
43.0
101.2
38.3
0.0
0.0
0.0
0.0
0.0
0.0
Poisson
[65]
1.4
1.2
5.2
3.0
28.0
7.8
0.0
0.0
0.0
0.0
0.0
0.0
Labatut et al. [71]
1.0
1.0
2.6
1.2
4.0
2.0
0.8
0.6
18.4
0.4
14.4
6.9
Ours
1.2
1.0
1.0
1.0
1.2
1.1
0.0
2.0
0.2
1.2
0.0
0.7
93


--- Page 109 ---

Table 4.3: Numerical results for ETH3D reconstructions: We report the following
extrinsic and intrinsic metrics on the ETH3D dataset: F1-Score at 5 cm (F1), number of
connected components (CC), and surface area of the mesh in square meters ×10−2 (Area).
Numbers in parentheses are from the meshes before the cleaning step. Note that the F1-
Score is calculated from the mean values of accuracy and completeness over all scenes, in
contrast to Table 4.4 where it is the mean F1-Score over all scenes.
Method
F1
CC
Area
Poisson [65]
66.8 (67.2)
83 (23131)
82 (116)
Vu et al. [114]
70.6 (70.8)
17 (560)
17 (125)
Jan. et al. [60]
70.0 (67.7)
14 (667)
14 (78)
Ours
73.1 (72.1)
23 (253)
11 (24)
algorithm, we estimate surface normals using Jets [29] and consistently orient them
towards the sensor.
We also assess the scalability and generalization capability of ConvONet on real world
outdoor scenes. We use the volume decoder model pretrained on the synthetic indoor
scene dataset [92] operating on a sliding window.
To avoid prohibitively expensive
computations caused by far away outliers, we manually crop most of the scenes to
limit the bounding volume. It is important to note that our method requires no such
preprocessing. However, even in this prepared setting, the resulting surfaces were of
signiﬁcantly lower quality than all other methods. This can be explained by the fact
that, even if the ConvONet model was trained on collections of ShapeNet models, the
distribution of objects in this training set is very diﬀerent from the real-life scenes of
ETH3D. Our model being purely local, does not suﬀer from this lack of generalizability
(see Figure 4.10 for a qualitative comparision of ConvONet and our method). A time
and memory comparison between ConvONet, Vu et al. and our method is given in
Table 4.5. As for IGR, the size of its network prevents us from reconstructing ETH3D
scenes. Consequently, in the following, we exclude ConvONet and IGR from evaluations
on ETH3D.
Results.
In Figure 4.11, we present the accuracy-completeness curve for τ = 5, 10, 20 and 50 cm,
illustrating the varying trade-oﬀs between completeness and accuracy for the diﬀerent
methods. For instance, at τ = 50 cm, all methods have an F1-score of around 95%. In
comparison, our method provides a lower completeness but a higher accuracy; neverthe-
less, it results in a better overall F-score. The higher accuracy provided by our method
is illustrated in Figure 3.1 and Figure 4.12, where ﬁne details that are hard to recon-
struct are better preserved. In Table 4.3, we report intrinsic mesh quality measures at
τ = 5 cm for diﬀerent methods. We improve the F-score by 2.5 points, while producing
a surface up to 35% more compact.
In Table 4.4, we show the F1-Score at τ = 5 cm of all 13 scenes of the ETH3D dataset
94


--- Page 110 ---

Table 4.4: Numerical results for ETH3D reconstruction per scene: Per scene and
mean F1-Score of all scenes of the train dataset of ETH3D [99] for uncleaned and cleaned
mesh reconstructions at distance τ = 5 cm. The best (highest) values per scene are in
bold. We perform better than all competing methods on 8 scenes out of 13. On average,
our method performs between 2 and 5% better than the competing methods, and improve
the F1-score for 8 out of 13 scenes. The mesh cleaning only improves the F1-score of the
reconstruction of Jancosek et al. [60].
F1-score - uncleaned mesh
F1-score - cleaned mesh
scene
Poisson
Vu et al.
Jan. et al.
Ours
Poisson
Vu et al.
Jan. et al.
Ours
kicker
0.75
0.79
0.75
0.76
0.75
0.81
0.78
0.78
pipes
0.77
0.79
0.77
0.76
0.77
0.78
0.77
0.75
delivery area
0.69
0.70
0.66
0.71
0.69
0.70
0.68
0.71
meadow
0.45
0.52
0.51
0.58
0.40
0.50
0.50
0.60
oﬃce
0.60
0.65
0.59
0.59
0.60
0.64
0.62
0.58
playground
0.61
0.70
0.63
0.70
0.60
0.69
0.66
0.73
terrains
0.73
0.78
0.76
0.75
0.74
0.78
0.77
0.76
terrace
0.79
0.76
0.74
0.83
0.79
0.79
0.78
0.85
relief
0.72
0.67
0.64
0.80
0.73
0.69
0.67
0.80
relief 2
0.70
0.68
0.67
0.79
0.71
0.70
0.70
0.78
electro
0.65
0.64
0.60
0.68
0.65
0.65
0.64
0.69
courtyard
0.76
0.75
0.72
0.77
0.75
0.75
0.74
0.77
facade
0.50
0.52
0.50
0.53
0.51
0.55
0.54
0.50
mean
0.67
0.69
0.66
0.71
0.67
0.69
0.68
0.71
95


--- Page 111 ---

for both uncleaned and cleaned mesh reconstructions. Our method produces the best
reconstruction scores for 9 out of 13 scenes. Mesh cleaning did not signiﬁcantly alter
the scores as it resulted in less complete but more accurate reconstructions.
It is important to note that the ETH3D stereo benchmark is typically used to evaluate
the quality of point clouds produced by dense MVS methods. In contrast, the approaches
we evaluate concern the reconstruction of compact and watertight meshes. It is a much
harder task.
Watertightness in particular, requires special attention regarding holes
and close parallel surfaces, while algorithms producing point clouds may ignore such
considerations. Consequently, the comparison of the methods we evaluate with other
entries of the ETH3D benchmark is not valid.
We would like to stress that, while our model is learning-based, its training set [32]
is very diﬀerent from the one used to evaluate the performance [99]: we train our
method on few artiﬁcial, simple and closed objects, while we evaluate on complex real-
life scenes. Furthermore, our network does not optimize towards the main evaluation
metrics. Instead, we optimize towards a high volumetric IoU of outside and inside cells.
This implies that our model, while being simple, can learn a relevant visibility model
that is able to generalize to data of unseen nature.
4.4.5
Speed and memory.
In Table 4.5, we report the speed and GPU memory requirements of the diﬀerent com-
peting methods for reconstructing the meadow scene from ETH3D. Our approach com-
pares favorably to ConvONet for all space-time trade-oﬀs, on top of improved recon-
struction metrics. Note that this is mainly due to the adaptability of our method to
the highly varying point cloud density of the scene. Our method only has to process a
small number of large tetrahedron in empty space, while the sliding window approach of
ConvONet still has to perform time-consuming decoding in such regions (cf. Table 2.8).
While the added GNN inference step of our method results in a slower overall predic-
tion compared to Vu et al. [114], we argue that the added accuracy justiﬁes the extra
processing time. Our method can process the entirety of the ETH benchmark in under
25 minutes.
Besides, thanks to our eﬃcient modiﬁed GraphSAGE scheme, the unary potentials
can be computed purely locally; global prediction agreement is achieved by the graph
cut. We can control precisely the memory usage by choosing the number of tetrahedra
to process at a time, each one using around 10 MB of memory. This memory usage
can be further improved with a memory-sharing scheme between nodes, allowing us
to process up to 400, 000 tetrahedra simultaneously with 8 GB of VRAM, which is the
same amount of memory necessary for ConvONet to process a single sliding window.
96


--- Page 112 ---

Table 4.5: Runtimes and memory footprint: We report, for the reconstruction of the
meadow scene (ETH3D), the computation time for point/tetrahedron features (Feat), tetra-
hedralization (3DT), network inference (Inference), graph cut (GC), and marching cubes
(MC). Batch size is given in number of subgraphs / sliding windows. Our model alone ﬁlls
470 MB of VRAM, while ConvONet ﬁlls 540 MB.
Batch size Feat. 3DT
Inference
GC / MC Total
Vu et al. [114]
-
13 s
4 s
-
-
14 s
31 s
Ours
400k
14 s
4 s
24 s
7.9 GB
16 s
58 s
Ours
1
14 s
4 s
75 s
0.5 GB
16 s
109 s
ConvONet [92]
1
5 s
-
145 s
7.9 GB
14 s
164 s
4.5
Limitations and perspectives
While neural implicit ﬁelds can also be used in conjunction with image encoders, al-
lowing for a direct end-to-end image-to-surface reconstruction, our approach only works
with point clouds as input.
Besides, as common in Delaunay-based methods, our reconstructed surface is bound
to go through the triangles of the Delaunay tetrahedralization. This can limit precision
when the acquisition is noisy, and prevent us from reconstructing details below the sam-
pling resolution. Future work could address this issue by incorporating mesh reﬁnement
strategies such as vertex displacement into the learning architecture.
As for learning-based methods in general, our approach requires the training and test
datasets to have comparable distributions. However, since the inference is purely local,
we do not need both datasets to contain similar objects. Yet the characteristics of the
acquisition must be similar in terms of accuracy and density.
We also do not fully exploit the potential of deep learning by providing the network
with handcrafted features in the ﬁrst GNN layer. One way to replace the handcrafted
visibility features would be to directly introduce lines-of-sight as nodes in the graph,
and connect these nodes to the cells which are traversed by the lines-of-sight.
The
handcrafted geometric features could be replaced by PointNet features, similar to the
approach of ConvONet [92].
Initial experiments also show that DGNN can be used without subsequent graph-
cut optimization by using a regularization loss at subgraph level. Depending on the
acquisition defects, we achieve results comparable to the ones with subsequent graph-
cut.
Our current architecture relies on local subgraphs with a small receptive ﬁeld. While
this enables fast runtimes for training and inference, it limits the feature size our learning
algorithm can capture. Possible research directions involve using bigger receptive ﬁelds
and incorporating pooling and up-sampling operations into the graph neural network.
This could help the learning algorithm to better process information at diﬀerent scales
97


--- Page 113 ---

and make the triangulation more adaptive to diﬀerent features sizes.
4.6
Conclusion
We propose a scalable surface reconstruction algorithm based on graph neural networks
and graph-cut optimization. Our method, trained from a small artiﬁcial dataset, is able
to rival with state-of-the-art methods for large-scale reconstruction on real-life scans.
Thanks to the locality of the prediction of the unary potentials associated with tetra-
hedra, our method can perform inference on large clouds with millions of tetrahedra.
Our approach demonstrates that it is possible to integrate deep-learning with computa-
tional geometry techniques to successfully tackle the hard problem of watertight surface
reconstruction at large scale.
98


--- Page 114 ---

Figure 4.7: Qualitative results on Berger et al.’s dancing children: We show the input
point clouds with diﬀerent levels of noise and outliers to emulate challenging MVS settings
in column 1. Note that in contrast to ConvONet [92], our method generalizes much better
to unseen objects, is highly resilient to outliers, and does not produce the ﬂoating artifacts
of the IGR [48] and Labatut [71] algorithms. The Screened Poisson reconstruction [65] is
visually similar to ours, but occasionally produces unwanted surface parts.
99


--- Page 115 ---

Figure 4.8: Qualitative results on Berger et al.’s gargoyle: We show the input point
clouds in column 1. ConvONet [92] (column 2) does not generalize well to the unseen
new shape. IGR [48] (column 3) generates many surface components from outliers. The
Screened Poisson [65] algorithm (column 4) does not reconstruct the sharp features well,
but is robust against outliers, even close to the surface. The reconstructions of Labatut
et al. [71] (column 5) and ours (column 6) are visually similar for the easier high resolution
case. While both methods are very robust against outliers, our method performs slightly
better on the low resolution, outlier and noise cases.
100


--- Page 116 ---

Figure 4.9: Qualitative results on Berger et al.’s lord quasimoto: We show the input
point clouds in column 1.
ConvONet [92] (column 2) does not generalize well to the
unseen new shape.
IGR [48] (column 3) is not able to ﬁlter outliers in the scan.
The
Screened Poisson [65] algorithm (column 4) does not reconstruct the sharp features well.
The reconstructions of Labatut et al. [71] (column 5) and ours (column 6) are visually similar
for the defect-free cases. Both methods produce small artifacts in the high resolution case:
between the book and nose for Labatut et al. [71] and between the book and left foot for
ours. Both methods are very robust against outliers.
101


--- Page 117 ---

(a) Dense MVS input.
(b) Ours.
(c) ConvONet [92].
Figure 4.10: Comparison of DGNN and ConvONet on ETH3D reconstruction: Re-
construction of the pipes scene of the ETH3D benchmark [99]. We show the dense MVS
point cloud in (a), the mesh reconstructions obtained by ConvONet [92] in (c) and our
proposed reconstruction in (b). Similar to object-level reconstruction, ConvONet does not
generalize well to the unseen new shapes in this scene. Our learning algorithm, operating
purely locally, is able to reconstruct the pipes and ﬁll all holes in the point cloud acquistion.
102


--- Page 118 ---

65
70
80
90
100
55
60
70
80
90
100
5cm
50cm
Completeness
Accuracy
Ours
Vu et al.
Jancosek et al.
Poisson
cleaned surface
Figure 4.11: Numerical results on ETH3D: Each point corresponds to the accuracy and
completeness at a given error threshold, respectively at 5, 10, 20 and 50 cm. Dashed lines
represent the performance of meshes cleaned by post-processing. Our method produces
meshes with a higher accuracy but a lower completeness.
103


--- Page 119 ---

(a) Dense MVS input.
(b) Our textured mesh.
(c) Details Image.
(d) Jancosek et al. [60].
(e) Ours.
Figure 4.12: Comparison of DGNN and Jancosek et al. on ETH3D reconstruction:
Our mesh reconstruction method takes as input a dense MVS point cloud (a) and produces a
mesh (b), simultaneously preserving ﬁne details and completing missing parts (here textured
with [115]). We represent: in (c), a cropped image of a detail from the terrace scene of
the ETH3D benchmark [99]; in (d), the reconstruction by Jancosek and et al. [60]; and in
(e), our reconstruction. Notice the missing staircase and spurious vertical pattern on the
concrete wall in (d). In contrast, our method (e) reconstructs part of the staircase as well
as the ﬁne-grained wall textures.
104


--- Page 120 ---

Figure 4.13: Failure case on ETH3D: Reconstruction of the delivery area scene of the
ETH3D benchmark [99]. We show the ground truth that is used for evaluation in (a). A
set of images, such as the one represented in (b), is transformed into a dense MVS point
cloud (c), from which a mesh can be reconstructed and textured [115], as shown in (d) with
our proposed mesh reconstruction. We show the untextured mesh reconstructions obtained
by the screened Poisson algorithm in (e,i), the algorithms of Vu et al. [114] in (f,j) and of
Jancosek et al. [60] in (g,k), and ﬁnally our proposed reconstruction in (h,l). Our method
does not close the wall on the right, but performs slightly better on reconstructing the no-
parking sign. Yet, considering the whole scene, the holes we create do not cover a larger
area than other methods.
105


--- Page 121 ---

Figure 4.14: Indoor ETH3D reconstruction: Reconstruction of the kicker scene of the
ETH3D benchmark [99]. We show the ground truth that is used for evaluation in (a). A
set of images, such as the one represented in (b), is transformed into a dense MVS point
cloud (c), from which a mesh can be reconstructed and textured [115], as shown in (d) with
our proposed mesh reconstruction. We show the untextured mesh reconstructions obtained
by the screened Poisson algorithm in (e,i), the algorithms of Vu et al. [114] in (f,j) and of
Jancosek et al. [60] in (g,k), and ﬁnally our proposed reconstruction in (h,l). All methods
struggle to reconstruct the table and the chairs, that have little data support.
106


--- Page 122 ---

Figure 4.15: Outdoor ETH3D reconstruction: Reconstruction of the meadow scene of
the ETH3D benchmark [99]. We show the ground truth that is used for evaluation in (a).
A set of images, such as the one represented in (b), is transformed into a dense MVS point
cloud (c), from which a mesh can be reconstructed and textured [115], as shown in (d) with
our proposed mesh reconstruction. We show the untextured mesh reconstructions obtained
by the screened Poisson algorithm in (e,i), the algorithms of Vu et al. [114] in (f,j) and
of Jancosek et al. [60] in (g,k), and ﬁnally our proposed reconstruction in (h,l). Trees and
outliers in the sky lead to a large number of isolated components in all mesh reconstructions.
Most of these small components can be removed with the heurestic mesh cleaning step that
we apply as post-processing.
107


--- Page 123 ---

“There is no real ending. It’s just the place where
you stop the story.”
Frank Herbert
5
Conclusion
In this last chapter, we summarise the main ﬁndings of this thesis and give an outlook
on future work.
108


--- Page 124 ---

5.1
Summary and conclusion
Learning-based surface reconstruction.
In this thesis, we revisited the prob-
lem of surface reconstruction from point clouds with modern learning-based techniques.
We focused on watertight surface reconstruction from defect-laden point clouds. We
showed that learning-based methods are well suited for learning point cloud character-
istics and defects from a training set of point clouds and corresponding true surfaces.
This allows for reconstructing more accurate surfaces compared to traditional meth-
ods, provided that the input point clouds exhibit similar defects in train and test sets.
However, we also showed that most learning based methods do not generalize to point
clouds with unseen defect types. We thus advocate the use of learning-based surface re-
construction when the nature and characteristics of the defects are consistent between
train and test set. Methods that mostly rely on local information do not need both
datasets to contain similar objects, but the training dataset has to be suﬃciently large
and include diverse shapes. We further advice the use of DGNN, when the training set
is small, and when short runtimes during training and inference are important. If no
adequate training data is available, traditional methods are still a good choice, as they
are robust to diﬀerent densities and acquisition defects.
Visibility information.
We introduced an easy to implement, yet powerful method
to incorporate visibility information into deep surface reconstruction networks. This
method consistently improves the accuracy of reconstructed surfaces and the capabil-
ity of DSR networks to generalize to unseen domains. Visibility information helps to
correctly orient the reconstructed surface and provides valuable information for space
occupancy. Most sensors for point cloud acquisition can provide visibility information
in the form of sensor poses. Sensor poses can be stored with little memory cost. We
believe that the beneﬁts of visibility information are worth this additional cost and
visibility information should further be used for deep surface reconstruction.
Combining local learning with global optimization.
We also showed that the
combination of locally learned occupancy priors in combination with a global optimiza-
tion can outperform traditional methods for the task of large-scale surface reconstruction
from point clouds in the wild. The locality of our learning algorithm has several bene-
ﬁts. First, we require only a small amount of training data, which is a useful property
for methods that require expensive 3D supervision. This is due to the fact that local
shape patterns can be shared across diﬀerent shape classes. Second, local learning al-
lows to better control the memory requirements of the learning algorithm, as it removes
the need to embed a global representation of the surface. Global optimization, imple-
mented in our DGNN architecture with a graph-cut, helps to ﬁlter artifacts resulting
from heavy noise and outliers.
109


--- Page 125 ---

3D Delaunay tetrahedralisation.
We also showed that a 3DT can be used in
combination with graph convolutions. The 3DT provides a data structure which is easy
to construct and well suited for discretising the domain of non-uniform point clouds.
The 3DT makes DGNN adaptive to the density of the input point clouds. This allows for
training on point clouds with diﬀerent densities, and for using all input points during
inference. At the same time, the 3DT removes the need for expensive neighborhood
searches by using the inherent graph structure to deﬁne local neighborhoods.
Point clouds in the wild.
As our reconstruction of complex real-world scenes
show, surfaces reconstructed from point clouds in the wild still exhibit a variety of
defects. One key issue which we consider unresolved is the size of the receptive ﬁeld of
the learning algorithm. Point clouds in the wild include defects and shapes in a large
variety of scales. This requires providing shape priors in a variety of scales, e.g. for
closing holes in large regions of missing data, while reconstructing ﬁne details in noisy
regions at the same time. More complex approaches with graphs of several scales or
graph convolution schemes with pooling and up-sampling operations may be an answer
to this multi-scale issue.
5.2
Outlook and future work
Training on real data.
Most surface reconstruction networks can be trained on real
data with the intent to learn acquisition- or sensor-speciﬁc point cloud priors. However,
the availability of real-world point clouds with corresponding true surfaces is sparse. We
have experimented with training on real data and ground truth surfaces reconstructed
from high quality acquisitions, but could so far not achieve good results. This area needs
further investigation. Possible research directions involve learning without a ground
truth surface, or simulating or measuring data for suﬃciently large training sets for
certain types of acquisitions and sensors.
Surface generalization.
Surface reconstruction networks could also be trained
with simpliﬁed ground truth surfaces, such as polygon meshes with large planar facets
[7]. Most deep surface reconstruction networks, including DGNN, could be trained with
such surfaces without any modiﬁcation. Large datasets of point clouds and simpliﬁed
models are for example available through city wide LiDAR or MVS aquisitions and hand-
engineered 3D city models. This could allow for directly learning simpliﬁed models from
point clouds end-to-end. A similar approach has already been implemented by using
neural implicit ﬁelds to segment precomputed polyhedral volumes [34].
Visibility information.
Given the high impact of visibility information in our ex-
periments, we also suggest to explore the potential outside of surface reconstruction, for
110


--- Page 126 ---

other task involving point clouds such as semantic segmentation. Recent developments
around NeRFs and diﬀerentiable volumetric rendering also heavily rely on visibility in-
formation. They use a similar strategy to ours for incorporating visibility information
with sampled points along an extended ray [87]. However, their optimization process
is currently slow due to dense evaluation of the neural implicit ﬁeld along the ray [91].
Future work could involve incorporating 3D priors with such methods or combining
them with LiDAR point clouds in a joint optimization.
Joint reconstruction and semantisation.
Incorporating semantic information
directly in the reconstruction process could be beneﬁcial. A semantic aware reconstruc-
tion algorithm could (i) identify regions with known object categories, (ii) use strong
shape priors in such regions, and (iii) rely on geometric priors in semantically unknown
regions.
Surface requirement for real-world applications.
Future investigations should
also focus on assessing the impact of geometric and topological surface errors on certain
types of analysis and applications. This has e.g. be done for 3D city models [12]. In
this way, surface reconstruction algorithms could focus on the most important surface
properties for a given task at hand, such as sharp feature recovery or outlier removal.
Furthermore, the quality of the triangulation itself may also be important. For ex-
ample, applying ﬁnite element methods directly to a mesh output, requires a mesh with
well-constructed triangles without small angles. While DGNN already provides a good
triangulation by relying on a 3DT, one could directly optimize for a mesh output with
such qualities.
Deep surface analysis.
Recent research also suggests to use end-to-end learning
for directly performing surface analysis, such as ﬂuid dynamics from scanned point
clouds. This approach removes the need to explicitly compute a continuous surface as
an intermediate representation. While an interesting ﬁeld of research, it can lead to
more computation. Each new analysis requires a new optimization from a raw point
cloud instead of a richer information surface. Additionally, such methods do not allow
surface visualisation.
Delaunay triangulations and graph neural networks.
We advice to do fur-
ther research on the combination of graph neural networks and Delaunay triangulations.
Graph neural networks are a powerful class of learning algorithms. Delaunay triangula-
tions contain rich adjacency information while being fast to compute. They are adaptive
to the data at hand and applicable to a variety of geometric problems. The combination
of GNNs and Delaunay triangualtions could be useful for a variety of diﬀerent appli-
111


--- Page 127 ---

cations in geometry processing such as mesh reﬁnement or simpliﬁcation, or even for
tasks such as semantic segmentation.
112


--- Page 128 ---

Bibliography
[1] Alliez, P. (2017).
Surface Reconstruction.
Symposium on Geometry Process-
ing 2017 Graduate School Lecture. http://school.geometryprocessing.org/
summerschool-2017/slides/Alliez_SurfaceReconstruction_SGP.pdf. 1, 6
[2] Amenta, N., Bern, M., & Eppstein, D. (1998). The crust and the β-skeleton: Com-
binatorial curve reconstruction. Graphical models and image processing, 60(2),
125–135. 6, 25
[3] Attali, D., Boissonnat, J.-D., & Lieutier, A. (2003). Complexity of the delau-
nay triangulation of points on surfaces the smooth case. In Proceedings of the
nineteenth annual symposium on Computational Geometry (pp. 201–210). 25
[4] Atzmon, M. & Lipman, Y. (2020). SAL: Sign agnostic learning of shapes from
raw data. In Conference on Computer Vision and Pattern Recognition (CVPR).
56, 78
[5] Atzmon, M. & Lipman, Y. (2021). SALD: sign agnostic learning with derivatives.
In International Conference on Learning Representations (ICLR). 56, 78
[6] Barnes, C., Shechtman, E., Finkelstein, A., & Goldman, D. B. (2009). Patch-
Match: A randomized correspondence algorithm for structural image editing.
ACM Transactions on Graphics. 90
[7] Bauchet, J.-P. & Lafarge, F. (2020). Kinetic shape reconstruction. ACM Trans-
actions on Graphics (TOG), 39(5), 1–14. 110
[8] Berger, M., Levine, J. A., Nonato, L. G., Taubin, G., & Silva, C. T. (2013). A
benchmark for surface reconstruction. ACM Transaction on Graphics. 16, 30, 32,
86, 90, 93
[9] Berger, M., Tagliasacchi, A., Seversky, L., Alliez, P., Guennebaud, G., Levine,
J., Sharf, A., & Silva, C. (2016). A survey of surface reconstruction from point
clouds. Computer Graphics Forum. 5, 6, 16
113


--- Page 129 ---

[10] Bernardini, F. & Bajaj, C. L. (1997).
Sampling and reconstructing manifolds
using alpha-shapes. Proc. 9th Canad. Conf. Comput. Geom. 25
[11] Bernardini, F., Mittleman, J., Rushmeier, H., Silva, C., & Taubin, G. (1999).
The ball-pivoting algorithm for surface reconstruction.
IEEE transactions on
visualization and computer graphics, 5(4), 349–359. 21, 22
[12] Biljecki, F., Heuvelink, G., Ledoux, H., & Stoter, J. (2018). The eﬀect of acquisi-
tion error and level of detail on the accuracy of spatial analyses. Cartography and
Geographic Information Science, 45(2), 156–176. 111
[13] Biljecki, F., Ledoux, H., Du, X., Stoter, J., Soon, K. H., & Khoo, V. (2016). The
most common geometric and semantic errors in citygml datasets. ISPRS Annals
of Photogrammetry, Remote Sensing & Spatial Information Sciences, 4. 5
[14] Biljecki, F., Stoter, J., Ledoux, H., Zlatanova, S., & C¸¨oltekin, A. (2015). Appli-
cations of 3d city models: State of the art review. ISPRS International Journal
of Geo-Information, 4(4), 2842–2889. 1
[15] Boissonnat, J.-D. (1984). Geometric structures for 3d shape representation. ACM
Transactions on Graphics, 3(4). 6, 23, 25
[16] Boissonnat, J.-D. & Oudot, S. (2005). Provably good sampling and meshing of
surfaces. Graphical Models, 67(5), 405–451. 21
[17] Bolle, R. M. & Vemuri, B. C. (1991). On three-dimensional surface reconstruction
methods. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(01),
1–13. 16
[18] Botsch, M., Kobbelt, L., Pauly, M., Alliez, P., & L´evy, B. (2010). Polygon mesh
processing. CRC press. 11, 16, 17
[19] Boulch, A. (2020). Convpoint: Continuous convolutions for point cloud process-
ing. Computers & Graphics. 39, 60
[20] Boulch, A., de La Gorce, M., & Marlet, R. (2014). Piecewise-planar 3D recon-
struction with edge and corner regularization. Computer Graphic Forum. 78
[21] Boulch, A. & Marlet, R. (2022). Poco: Point convolution for surface reconstruc-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (pp. 6302–6314). 16, 22, 39, 42, 43, 47, 50, 51, 56, 60, 62, 63, 64,
67, 68, 69, 70, 71, 72, 73
[22] Boykov, Y. & Kolmogorov, V. (2004).
An experimental comparison of min-
cut/max-ﬂow algorithms for energy minimization in vision. PAMI. 85
114


--- Page 130 ---

[23] Boykov, Y., Veksler, O., & Zabih, R. (2001). Fast approximate energy minimiza-
tion via graph cuts. PAMI. 89
[24] Breitenmoser, A. & Siegwart, R. (2012). Surface reconstruction and path plan-
ning for industrial inspection with a climbing robot. In 2012 2nd International
Conference on Applied Robotics for the Power Industry (CARPI) (pp. 22–27).:
IEEE. 1
[25] B´odis-Szomor´u, A., Riemenschneider, H., & Van Gool, L. (2016). Eﬃcient volu-
metric fusion of airborne and street-side data for urban reconstruction. In Inter-
national Conference on Pattern Recognition (ICPR). 7, 54, 76, 78
[26] Caraﬀa, L., Br´edif, M., & Vallet, B. (2017). 3D watertight mesh generation with
uncertainties from ubiquitous data. In Asian Conference on Computer Vision
(ACCV). 25, 54, 76, 78
[27] Caraﬀa, L., Marchand, Y., Br´edif, M., & Vallet, B. (2021). Eﬃciently distributed
watertight surface reconstruction. In 3DV. 54
[28] Cazals, F. & Giesen, J. (2006). Delaunay triangulation based surface reconstruc-
tion. In Eﬀective computational geometry for curves and surfaces (pp. 231–276).
Springer. 6, 16, 20, 21, 23, 25
[29] Cazals, F. & Pouget, M. (2005). Estimating diﬀerential quantities using polyno-
mial ﬁtting of osculating jets. Computer Aided Geometric Design. 37, 38, 60, 61,
62, 94
[30] Cernea, D. (2020). OpenMVS: Multi-view stereo reconstruction library. 59, 86,
87, 90
[31] Chabra, R., Lenssen, J. E., Ilg, E., Schmidt, T., Straub, J., Lovegrove, S., &
Newcombe, R. (2020). Deep local shapes: Learning local SDF priors for detailed
3D reconstruction. In European Conference on Computer Vision (ECCV). 16, 27,
54, 78
[32] Chang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., & Yu, F. (2015).
ShapeNet: An Information-Rich 3D Model Repository. Technical report, Stanford
University, Princeton University, Toyota Technological Institute at Chicago. 27,
59, 96
[33] Chauve, A.-L., Labatut, P., & Pons, J.-P. (2010). Robust piecewise-planar 3D
reconstruction and completion from large-scale unstructured point data. In Con-
ference on Computer Vision and Pattern Recognition (CVPR). 76
115


--- Page 131 ---

[34] Chen, Z., Khademi, S., Ledoux, H., & Nan, L. (2021).
Reconstructing com-
pact building models from point clouds using deep implicit ﬁelds. arXiv preprint
arXiv:2112.13142. 110
[35] Chen, Z. & Zhang, H. (2019). Learning implicit ﬁelds for generative shape mod-
eling. In Conference on Computer Vision and Pattern Recognition (CVPR). 22,
26, 27, 54
[36] Chibane, J., Alldieck, T., & Pons-Moll, G. (2020a). Implicit functions in feature
space for 3D shape reconstruction and completion. In Conference on Computer
Vision and Pattern Recognition (CVPR). 56
[37] Chibane, J., Mir, A., & Pons-Moll, G. (2020b). Neural unsigned distance ﬁelds
for implicit function learning. In Conference on Neural Information Processing
Systems (NeurIPS). 56
[38] Choy, C. B., Xu, D., Gwak, J., Chen, K., & Savarese, S. (2016). 3D-R2N2: A
uniﬁed approach for single and multi-view 3D object reconstruction. In European
Conference on Computer Vision (ECCV). 34, 59, 86, 93
[39] C¸i¸cek, ¨O., Abdulkadir, A., Lienkamp, S. S., Brox, T., & Ronneberger, O. (2016).
3d u-net: learning dense volumetric segmentation from sparse annotation.
In
International conference on medical image computing and computer-assisted in-
tervention (pp. 424–432).: Springer. 27
[40] Dai, A., Chang, A., Savva, M., Halber, M., Funkhouser, T., & Nießner, M. (2017).
ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In Conference
on Computer Vision and Pattern Recognition (CVPR). 59
[41] Dai, A., Diller, C., & Nießner, M. (2020).
SG-NN: Sparse generative neural
networks for self-supervised scene completion of RGB-D scans. In Conference on
Computer Vision and Pattern Recognition (CVPR). 54, 56
[42] Dai, A. & Nießner, M. (2019). Scan2Mesh: From unstructured range scans to 3D
meshes. In Conference on Computer Vision and Pattern Recognition (CVPR). 78
[43] Deprelle, T., Groueix, T., Fisher, M., Kim, V., Russell, B., & Aubry, M. (2019).
Learning elementary structures for 3D shape generation and matching. In Con-
ference on Neural Information Processing Systems (NeurIPS). 78
[44] Digne, J. (2014).
An analysis and implementation of a parallel ball pivoting
algorithm. Image Processing On Line, 4, 149–168. 21
[45] Erler, P., Ohrhallinger, S., Mitra, N., & Wimmer, M. (2020). Points2Surf: Learn-
ing implicit surfaces from point clouds. In European Conference on Computer
Vision (ECCV). 16, 22, 50, 56, 60, 62, 63, 64, 67, 68, 69, 70, 71, 72
116


--- Page 132 ---

[46] Gkioxari, G., Johnson, J., & Malik, J. (2019). Mesh R-CNN. In Conference on
Computer Vision and Pattern Recognition (CVPR). 78
[47] Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
http://www.deeplearningbook.org. 11
[48] Gropp, A., Yariv, L., Haim, N., Atzmon, M., & Lipman, Y. (2020). Implicit geo-
metric regularization for learning shapes. In International Conference on Machine
Learning (ICML). 22, 28, 29, 37, 45, 47, 51, 54, 78, 79, 87, 90, 91, 92, 93, 99, 100,
101
[49] Groueix, T., Fisher, M., Kim, V. G., Russell, B. C., & Aubry, M. (2018). AtlasNet:
A papier-mˆach´e approach to learning 3D surface generation. In Conference on
Computer Vision and Pattern Recognition (CVPR). 9, 22, 24, 78
[50] Hamilton, W. L., Ying, R., & Leskovec, J. (2017). Inductive representation learn-
ing on large graphs. In Conference on Neural Information Processing Systems
(NeurIPS). 82
[51] Handa, A., Patraucean, V., Stent, S., & Cipolla, R. (2016). SceneNet: An anno-
tated model generator for indoor scene understanding. In International Conference
on Robotics and Automation (ICRA). 59
[52] Hanocka, R., Metzer, G., Giryes, R., & Cohen-Or, D. (2020). Point2Mesh: A
self-prior for deformable meshes. ACM Transaction on Graphics. 22, 24, 38, 45,
47, 51, 54, 56, 78
[53] Hiep, V. H., Keriven, R., Labatut, P., & Pons, J.-P. (2009).
Towards high-
resolution large-scale multi-view stereo. In 2009 IEEE Conference on Computer
Vision and Pattern Recognition (pp. 1430–1437).: IEEE. 25
[54] Hoppe, H., DeRose, T., Duchamp, T., McDonald, J., & Stuetzle, W. (1992).
Surface reconstruction from unorganized points. In Proceedings of the 19th annual
conference on computer graphics and interactive techniques (pp. 71–78). 17, 25
[55] Huang, J., Su, H., & Guibas, L. (2018).
Robust watertight manifold surface
generation method for ShapeNet models. arXiv preprint arXiv:1802.01698. 86
[56] Huang, J., Zhou, Y., & Guibas, L. (2020). Manifoldplus: A robust and scalable
watertight manifold surface generation method for triangle soups. arXiv preprint
arXiv:2005.11621. 34, 59
[57] Huang, X., Mei, G., Zhang, J., & Abbas, R. (2021). A comprehensive survey on
point cloud registration. arXiv preprint arXiv:2103.02690. 7
117


--- Page 133 ---

[58] Ioﬀe, S. & Szegedy, C. (2015). Batch normalization: Accelerating deep network
training by reducing internal covariate shift. In ICML. 86
[59] Jancosek, M. & Pajdla, T. (2011). Multi-view reconstruction preserving weakly-
supported surfaces. In Conference on Computer Vision and Pattern Recognition
(CVPR). 25, 54, 76, 78
[60] Jancosek, M. & Pajdla, T. (2014). Exploiting visibility information in surface
reconstruction to preserve weakly supported surfaces.
International Scholarly
Research Notices. 25, 54, 76, 77, 78, 87, 94, 95, 104, 105, 106, 107
[61] Jensen, R., Dahl, A., Vogiatzis, G., Tola, E., & Aanæs, H. (2014). Large scale
multi-view stereopsis evaluation. In Conference on Computer Vision and Pattern
Recognition (CVPR). 17, 29, 34, 59, 71
[62] Jiang, C. M., Sud, A., Makadia, A., Huang, J., Nießner, M., & Funkhouser,
T. (2020). Local implicit grid representations for 3D scenes. In Conference on
Computer Vision and Pattern Recognition (CVPR). 16, 22, 37, 45, 47, 51, 54, 56,
60, 62
[63] Kazhdan, M., Bolitho, M., & Hoppe, H. (2006). Poisson surface reconstruction.
In Proceedings of the fourth Eurographics symposium on Geometry processing (pp.
61–70).: Eurographics Association. 25
[64] Kazhdan, M., Chuang, M., Rusinkiewicz, S., & Hoppe, H. (2020). Poisson Surface
Reconstruction with Envelope Constraints.
Computer Graphics Forum, 39(5),
173–182. 26
[65] Kazhdan, M. & Hoppe, H. (2013). Screened Poisson surface reconstruction. ACM
Transaction on Graphics. 4, 9, 22, 24, 26, 28, 38, 39, 42, 43, 45, 47, 49, 50, 51,
56, 60, 77, 87, 91, 92, 93, 94, 99, 100, 101
[66] Kettner, L. (1999). Using generic programming for designing a data structure for
polyhedral surfaces. Computational Geometry, 13(1), 65–90. 20
[67] Kingma, D. P. & Ba, J. (2015). ADAM: A method for stochastic optimization.
In International Conference on Learning Representations (ICLR). 86
[68] Knapitsch, A., Park, J., Zhou, Q.-Y., & Koltun, V. (2017). Tanks and Temples.
ACM Transactions on Graphics (TOG). 17, 29, 34, 59, 71
[69] Kolluri, R., Shewchuk, J. R., & O’Brien, J. F. (2004). Spectral surface recon-
struction from noisy point clouds. In Proceedings of the 2004 Eurographics/ACM
SIGGRAPH symposium on Geometry processing (pp. 11–21). 25
118


--- Page 134 ---

[70] K¨onig, S. & Gumhold, S. (2009). Consistent propagation of normal orientations in
point clouds. In International Workshop on Vision, Modeling, and Visualization
(VMV). 56
[71] Labatut, P., Pons, J. P., & Keriven, R. (2009).
Robust and eﬃcient surface
reconstruction from range data. Computer Graphics Forum (CGF). 9, 12, 22, 25,
38, 39, 42, 43, 45, 47, 49, 50, 51, 54, 76, 78, 85, 87, 90, 91, 93, 99, 100, 101
[72] Lemmens, M. (2020). An Introduction to Pointcloudmetry: Point Clouds from
Laser Scanning and Photogrammetry. Whittles. 11
[73] Li, Z., Zhu, C., & Gold, C. (2004).
Digital terrain modeling: principles and
methodology. CRC press. 1
[74] Liu, M., Zhang, X., & Su, H. (2020). Meshing point clouds with predicted intrinsic-
extrinsic ratio guidance. In European Conference on Computer Vision (pp. 68–
84).: Springer. 22, 23, 78
[75] Liu, S., Saito, S., Chen, W., & Li, H. (2019). Learning to infer implicit surfaces
without 3D supervision. In Conference on Neural Information Processing Systems
(NeurIPS). 79
[76] Lorensen, W. E. & Cline, H. E. (1987). Marching cubes: A high resolution 3d
surface construction algorithm. SIGGRAPH Comput. Graph., 21(4), 163–169. 21,
37, 61
[77] Luo, Y., Mi, Z., & Tao, W. (2021). Deepdt: Learning geometry from delaunay
triangulation for surface reconstruction. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 35 (pp. 2277–2285). 25
[78] M¨antyl¨a, M. (1987). An introduction to solid modeling. Computer Science Press,
Inc. 20
[79] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., & Geiger, A. (2019).
Occupancy networks: Learning 3D reconstruction in function space. In Conference
on Computer Vision and Pattern Recognition (CVPR). 9, 22, 26, 54, 59, 79
[80] Metzer, G., Hanocka, R., Giryes, R., & Cohen-Or, D. (2021a). Self-sampling for
neural point cloud consolidation. ACM Transactions on Graphics (TOG), 40(5),
1–14. 11
[81] Metzer, G., Hanocka, R., Zorin, D., Giryes, R., Panozzo, D., & Cohen-Or, D.
(2021b). Orienting point clouds with dipole propagation. ACM Transactions on
Graphics (TOG). 56
119


--- Page 135 ---

[82] Mi, Z., Luo, Y., & Tao, W. (2020). SSRNet: Scalable 3D surface reconstruction
network. In Conference on Computer Vision and Pattern Recognition (CVPR).
9, 79
[83] Michalkiewicz, M., Pontes, J., Jack, D., Baktashmotlagh, M., & Eriksson, A.
(2019). Implicit surface representations as layers in neural networks. In Interna-
tional Conference on Computer Vision (ICCV). 54
[84] Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R.,
& Ng, R. (2020). NeRF: Representing scenes as neural radiance ﬁelds for view
synthesis. In European Conference on Computer Vision (ECCV). 10, 56
[85] Mostegel, C., Prettenthaler, R., Fraundorfer, F., & Bischof, H. (2017). Scalable
surface reconstruction from point clouds with extreme scale and density diver-
sity. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (pp. 904–913). 25
[86] Niemeyer, M., Mescheder, L., Oechsle, M., & Geiger, A. (2020). Diﬀerentiable vol-
umetric rendering: Learning implicit 3d representations without 3d supervision.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (pp. 3504–3515). 10, 56
[87] Oechsle, M., Peng, S., & Geiger, A. (2021). Unisurf: Unifying neural implicit
surfaces and radiance ﬁelds for multi-view reconstruction. In International Con-
ference on Computer Vision (ICCV). 10, 56, 111
[88] O’neill, B. (2006). Elementary diﬀerential geometry. Elsevier. 17
[89] Park, J. J., Florence, P., Straub, J., Newcombe, R., & Lovegrove, S. (2019).
DeepSDF: Learning continuous signed distance functions for shape representation.
In Conference on Computer Vision and Pattern Recognition (CVPR). 16, 22, 26,
27, 54, 78, 79
[90] Paschalidou, D., Ulusoy, O., Schmitt, C., Van Gool, L., & Geiger, A. (2018).
Raynet: Learning volumetric 3d reconstruction with ray potentials. In Conference
on Computer Vision and Pattern Recognition (CVPR). 56
[91] Peng, S., Jiang, C. M., Liao, Y., Niemeyer, M., Pollefeys, M., & Geiger, A.
(2021). Shape as points: A diﬀerentiable poisson solver. In Conference on Neural
Information Processing Systems (NeurIPS). 9, 22, 27, 28, 38, 42, 43, 45, 47, 50,
51, 60, 62, 63, 64, 67, 68, 69, 70, 71, 72, 111
[92] Peng, S., Niemeyer, M., Mescheder, L., Pollefeys, M., & Geiger, A. (2020). Con-
volutional occupancy networks.
In European Conference on Computer Vision
120


--- Page 136 ---

(ECCV). 16, 22, 27, 32, 38, 42, 43, 47, 50, 51, 56, 58, 59, 60, 61, 62, 63, 64, 65,
66, 67, 68, 69, 70, 71, 72, 76, 79, 87, 90, 91, 92, 93, 94, 97, 99, 100, 101, 102
[93] Petitjean, S. & Boyer, E. (2001). Regular and non-regular point sets: Properties
and reconstruction. Computational Geometry, 19(2-3), 101–126. 21
[94] Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on
point sets for 3d classiﬁcation and segmentation.
In Conference on Computer
Vision and Pattern Recognition (CVPR). 73
[95] Rakotosaona, M.-J., Guerrero, P., Aigerman, N., Mitra, N. J., & Ovsjanikov, M.
(2021). Learning delaunay surface elements for mesh reconstruction. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(pp. 22–31). 16, 22, 23
[96] Rakotosaona, M.-J., La Barbera, V., Guerrero, P., Mitra, N. J., & Ovsjanikov, M.
(2020). Pointcleannet: Learning to denoise and remove outliers from dense point
clouds. In Computer Graphics Forum, volume 39 (pp. 185–203).: Wiley Online
Library. 6, 11
[97] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks
for biomedical image segmentation. In International Conference on Medical image
computing and computer-assisted intervention (pp. 234–241).: Springer. 27
[98] Schertler, N., Savchynskyy, B., & Gumhold, S. (2017). Towards globally optimal
normal orientations for large point clouds. Computer Graphics Forum (CFG). 54,
56, 60, 61, 62
[99] Sch¨ops, T., Sch¨onberger, J. L., Galliani, S., Sattler, T., Schindler, K., Pollefeys,
M., & Geiger, A. (2017). A multi-view stereo benchmark with high-resolution
images and multi-camera videos. In Conference on Computer Vision and Pattern
Recognition (CVPR). 4, 7, 8, 17, 29, 76, 77, 88, 90, 95, 96, 102, 104, 105, 106, 107
[100] Seitz, S., Curless, B., Diebel, J., Scharstein, D., & Szeliski, R. (2006). A compari-
son and evaluation of multi-view stereo reconstruction algorithms. In Conference
on Computer Vision and Pattern Recognition (CVPR). 17, 29, 34, 59, 71
[101] Sharf, A., Lewiner, T., Shamir, A., Kobbelt, L., & Cohen-Or, D. (2006). Compet-
ing fronts for coarse–to–ﬁne surface reconstruction. In Computer Graphics Forum,
volume 25 (pp. 389–398).: Wiley Online Library. 22, 24
[102] Sharp, N. & Ovsjanikov, M. (2020). ”pointtrinet: Learned triangulation of 3d
point sets”. In European Conference on Computer Vision (ECCV). 22, 23, 56, 78
121


--- Page 137 ---

[103] Simonovsky, M. & Komodakis, N. (2017). Dynamic edge-conditioned ﬁlters in
convolutional neural networks on graphs. In Conference on Computer Vision and
Pattern Recognition (CVPR). 89
[104] Sinha, S. N., Mordohai, P., & Pollefeys, M. (2007). Multi-view stereo via graph
cuts on the dual of an adaptive tetrahedral mesh. In 2007 IEEE 11th international
conference on computer vision (pp. 1–8).: IEEE. 25
[105] Solem, J. E. (2012).
Programming Computer Vision with Python: Tools and
algorithms for analyzing images. ” O’Reilly Media, Inc.”. 11
[106] Song, S., Cui, Z., & Qin, R. (2021). Vis2mesh: Eﬃcient mesh reconstruction from
unstructured point clouds of large scenes with learned virtual view visibility. In
International Conference on Computer Vision (ICCV). 56
[107] Strecha, C., von Hansen, W., Gool, L. V., Fua, P., & Thoennessen, U. (2008).
On benchmarking camera calibration and multi-view stereo for high resolution
imagery. In Conference on Computer Vision and Pattern Recognition (CVPR):
IEEE Computer Society. 17, 29
[108] Sulzer, R., Landrieu, L., Boulch, A., Marlet, R., & Vallet, B. (2022). Deep surface
reconstruction from point clouds with visibility information.
In International
Conference on Pattern Recognition (ICPR). 12
[109] Sulzer, R., Landrieu, L., Marlet, R., & Vallet, B. (2021). Scalable surface re-
construction with delaunay-graph neural networks. Computer Graphics Forum,
40(5), 157–167. 9, 12, 16, 22, 25, 38, 42, 43, 47, 50, 51, 54, 56, 60, 62, 67
[110] Tang, J., Lei, J., Xu, D., Ma, F., Jia, K., & Zhang, L. (2021). SA-ConvONet:
Sign-agnostic optimization of convolutional occupancy networks. In International
Conference on Computer Vision (ICCV). 56
[111] Tognola, G., Parazzini, M., Ravazzani, P., Svelto, C., & Grandori, F. (2001).
3d reconstruction of anatomical surfaces from unorganized range data. In 2001
Conference Proceedings of the 23rd Annual International Conference of the IEEE
Engineering in Medicine and Biology Society, volume 3 (pp. 2534–2536).: IEEE.
1
[112] Ummenhofer, B. & Koltun, V. (2021). Adaptive surface reconstruction with mul-
tiscale convolutional kernels. In International Conference on Computer Vision
(ICCV). 56
[113] Vetsch, M., Lombardi, S., Pollefeys, M., & Oswald, M. R. (2022). Neuralmesh-
ing: Diﬀerentiable meshing of implicit neural representations. In B. Andres, F.
122


--- Page 138 ---

Bernard, D. Cremers, S. Frintrop, B. Goldl¨ucke, & I. Ihrke (Eds.), Pattern Recog-
nition (pp. 317–333). Cham: Springer International Publishing. 21
[114] Vu, H. H., Labatut, P., Pons, J. P., & Keriven, R. (2012). High accuracy and
visibility-consistent dense multiview stereo. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence (PAMI). 4, 6, 25, 54, 76, 77, 78, 87, 94, 96, 97, 105,
106, 107
[115] Waechter, M., Moehrle, N., & Goesele, M. (2014). Let there be color! large-scale
texturing of 3D reconstructions. In ECCV. 77, 104, 105, 106, 107
[116] Williams, F., Schneider, T., Silva, C., Zorin, D., Bruna, J., & Panozzo, D. (2018).
Deep geometric prior for surface reconstruction.
In Conference on Computer
Vision and Pattern Recognition (CVPR). 78
[117] Williams, F., Trager, M., Bruna, J., & Zorin, D. (2021). Neural splines: Fitting 3d
surfaces with inﬁnitely-wide neural networks. In Conference on Computer Vision
and Pattern Recognition (CVPR). 56
[118] Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., & Xiao, J. (2015).
3d shapenets: A deep representation for volumetric shapes. In Conference on
Computer Vision and Pattern Recognition (CVPR). 59
[119] Yang, Y., Feng, C., Shen, Y., & Tian, D. (2018). FoldingNet: Point cloud auto-
encoder via deep grid deformation. In Conference on Computer Vision and Pat-
tern Recognition (CVPR). 78
[120] You, C. C., Lim, S. P., Lim, S. C., San Tan, J., Lee, C. K., & Khaw, Y. M. J.
(2020). A survey on surface reconstruction techniques for structured and unstruc-
tured data. In 2020 IEEE Conference on Open Systems (ICOS) (pp. 37–42).:
IEEE. 16
[121] Zhao, W., Lei, J., Wen, Y., Zhang, J., & Jia, K. (2021). Sign-agnostic implicit
learning of surface self-similarities for shape modeling and reconstruction from
raw point clouds. In Conference on Computer Vision and Pattern Recognition
(CVPR). 56, 60
[122] Zhou, Y., Shen, S., & Hu, Z. (2019). Detail preserved surface reconstruction from
point cloud. Sensors. 25, 54, 76, 78
123


--- Page 139 ---

 
```

---

## Section 1.25: Novel algorithms for 3D surface point cloud boundary detection

Source File: Novel algorithms for 3D surface point cloud boundary detection.txt

### Paper Content:

```
# Novel algorithms for 3D surface point cloud boundary detection.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Novel algorithms for 3D surface point cloud boundary detection.pdf
# Output: ../layer2_completion/txt/Novel algorithms for 3D surface point cloud boundary detection.txt


--- Page 1 ---

Novel algorithms for 3D surface point cloud boundary detection
and edge reconstruction
Carmelo Mineo ⇑, Stephen Gareth Pierce, Rahul Summan
Department of Electronic and Electrical Engineering, University of Strathclyde, Royal College Building, 204 George Street, Glasgow G1 1XW, UK
a r t i c l e
i n f o
Article history:
Received 20 October 2017
Received in revised form 5 February 2018
Accepted 6 February 2018
Available online 7 February 2018
Keywords:
Point-cloud
Boundary detection
Edge reconstruction
a b s t r a c t
Tessellated surfaces generated from point clouds typically show inaccurate and jagged boundaries. This
can lead to tolerance errors and problems such as machine judder if the model is used for ongoing man-
ufacturing applications. This paper introduces a novel boundary point detection algorithm and spatial
FFT-based ﬁltering approach, which together allow for direct generation of low noise tessellated surfaces
from point cloud data, which are not based on pre-deﬁned threshold values. Existing detection tech-
niques are optimized to detect points belonging to sharp edges and creases. The new algorithm is tar-
geted at the detection of boundary points and it is able to do this better than the existing methods.
The FFT-based edge reconstruction eliminates the problem of deﬁning a speciﬁc polynomial function
order for optimum polynomial curve ﬁtting. The algorithms were tested to analyse the results and mea-
sure the execution time for point clouds generated from laser scanned measurements on a turbofan
engine turbine blade with varying numbers of member points. The reconstructed edges ﬁt the boundary
points with an improvement factor of 4.7 over a standard polynomial ﬁtting approach. Furthermore,
through adding artiﬁcial noise it has been demonstrated that the detection algorithm is very robust for
out-of-plane noise lower than 25% of the cloud resolution and it can produce satisfactory results when
the noise is lower than 75%.
 2018 Society for Computational Design and Engineering. Publishing Services by Elsevier. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
1. Introduction
Three-dimensional (3D) scanning is increasingly used to anal-
yse objects or environments in diverse applications including
industrial design, orthotics and prosthetics, gaming and ﬁlm pro-
duction, reverse engineering and prototyping, quality control and
documentation of cultural and architectural artefacts (Curless,
1999; Levoy & Whitted, 1985). Conventional reconstruction tech-
niques generate tessellated surfaces from point clouds. Such tessel-
lated models often show inaccurate and jagged boundaries that
can lead to tolerance errors and problems such as machine judder
if the models are used for ongoing manufacturing applications
(Beard, 1997). That is the reason why many existing commercial
computer-aided manufacturing (CAM) applications are not able
to use tessellated models, instead of using precise analytical CAD
models
where
the
surfaces
are
mathematically
represented
(Mineo, Pierce, Nicholson, & Cooper, 2017). Whilst the conversion
of analytical geometries into meshed surfaces is straightforward,
the reverse process of conversion of a tessellated model into an
analytical CAD model is challenging and time-consuming. There
are circumstances where the original CAD model of a component
is not available or deviates from the real part. New CAM software
applications, able to use clean tessellated models, are emerging
(Mineo et al., 2017); they enable the direct use of triangulated
point clouds obtainable through surface mapping techniques.
However, the point clouds obtained through surface mapping are
typically affected by noise. New algorithms for optimum surface
mesh reﬁnement are required to improve the performance of
emerging applications and to overcome the limitations of typical
approaches based on polynomial smoothing. Different technolo-
gies can be used to build coordinate measuring machines (CMM)
or 3D-scanning devices (Curless, 1999). Each technology comes
with its own limitations, advantages and costs. A common factor
for many CMM and 3D scanners is that they can measure the coor-
dinates of a large number of points on an object surface and output
a point cloud of the scanned area. However, point clouds are gen-
erally not directly usable in most 3D applications, and therefore are
usually converted to mesh models, NURBS surface models, or CAD
models (Berger et al., 2017; Hinks, Carr, Truong-Hong, & Laefer,
2012; Truong-Hong, Laefer, Hinks, & Carr, 2011). Tessellated mod-
els have emerged as a favoured technique; they are the easiest
https://doi.org/10.1016/j.jcde.2018.02.001
2288-4300/ 2018 Society for Computational Design and Engineering. Publishing Services by Elsevier.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Peer review under responsibility of Society for Computational Design and
Engineering.
⇑Corresponding author.
E-mail address: carmelo.mineo@strath.ac.uk (C. Mineo).
Journal of Computational Design and Engineering 6 (2019) 81–91
Contents lists available at ScienceDirect
Journal of Computational Design and Engineering
journal homepage: www.elsevier.com/locate/jcde


--- Page 2 ---

form of virtual models obtainable from point clouds with minimal
processing. There are two different approaches to create a triangu-
lar meshed surface from a point cloud: using triangulation meth-
ods or surface reconstruction methods. Triangulation algorithms
use the original points of the input point cloud, using them as
the vertices of the mesh triangles. The Delaunay triangulation,
named after Boris Delaunay for his work on the topic from 1934
(Delaunay, 1934), is the most popular algorithm of this kind. A
bi-dimensional Delaunay triangulation ensures that the circumcir-
cle associated with each triangle contains no other point in its inte-
rior.
This
deﬁnition
extends
naturally
to
three
dimensions
considering spheres instead of circles. Surface reconstruction algo-
rithms differ from the triangulation method since they do not use
the original points as the vertices of the mesh triangles but com-
pute new points, whose density can vary according to the local cur-
vature of the 3D geometry. Surface reconstruction from oriented
points can be cast as a spatial problem, based on the Poisson’s
equation (Calakli & Taubin, 2011; Kazhdan & Hoppe, 2013).
Both approaches are not able to reconstruct the surface bound-
aries accurately, which makes the tessellated models unsuitable to
be used for CAM toolpath generation. Triangulation methods pro-
duce meshed surfaces with jagged boundaries since the original
noisy points of the cloud are used as vertices of the mesh triangles.
Reconstruction methods produce smooth boundaries, but they can
be quite far from the original boundaries of the real surface.
Indeed, Poisson’s surface reconstruction does not follow the
boundary of the point cloud and replaces the original points with
new points laying on a reconstructed continuous surface that sat-
isﬁes the Poisson’s differential equation.
The detection of cloud boundary points and the reconstruction
of smooth boundary edges would allow trimming of the recon-
structed tessellated models, to reﬁne the mesh boundaries. Very
few feature detection methods are optimized to work with point-
sampled geometries only. The major problem of these point based
methods is the lack of knowledge concerning point normal and
connectivity. This makes feature detection a more challenging task
than in mesh-based methods. Gumhold, Wang, and MacLeod
(2001) presented an algorithm that ﬁrst analyses the neighbour-
hood of each point via a principal component analysis (PCA). The
eigenvalues of the correlation matrix are then used to determine
if a point belongs to a feature. This technique for the detection of
features in point clouds is used as a pre-processing step for tessel-
lated
surface
reconstruction
with
sharp
features
(Weber,
Hahmann, & Hagen, 2011). There exist also several reconstruction
methods that preserve sharp features during the surface recon-
struction of a point cloud without pre-processing; for example,
the methods shown by Fleishman, Cohen-Or, and Silva (2005)
and Öztireli, Guennebaud, and Gross (2009).
The existing techniques mentioned above are optimized to
detect points belonging to sharp edges. This paper presents novel
algorithms targeted to the detection of boundary points and the
deterministic reconstruction of accurate and smooth surface
boundaries from 3D point clouds. A smart approach known as
Mesh Following Technique (MFT) (Mineo et al., 2017), for the gen-
eration of robot tool-paths from STL models, has recently been
published. The technique requires virtual tessellated surfaces with
smooth boundary edges.
The algorithms presented in this paper are useful tools to reﬁne
the boundary of tessellated surfaces obtained from 3D scanning
point cloud data. They can be used to trim Delaunay triangulation
or Poisson’s reconstructed surface meshes, facilitating the direct
use of tessellated models, instead of analytical geometries. The
remainder part of the paper describes the algorithms and shows
qualitative and quantitative results, discussing advantages and
disadvantages.
2. Detection of boundary points
Given a mapped surface in the form of a point cloud, the iden-
tiﬁcation of the point cloud borderline, thus the detection of
boundary points, is not a trivial task. The human brain is able to
infer the border of a point cloud by simply looking at the arrange-
ment of the sparse points. In computer science and computational
geometry, a point cloud is an entity without a well-deﬁned bound-
ary. In the bi-dimensional domain, given a ﬁnite set of points, the
problem of detecting the smallest convex polygon that contains all
the given points of the cloud is solved through the quickhull
method (Barber, Dobkin, & Huhdanpaa, 1996). It uses a divide
and conquer approach. This method works well but is only able
to detect the boundary points that are part of the convex polygon
and is only for a set of points. A generalization of quickhull, able to
handle concave regions and holes in the point cloud, is the alpha-
shape approach (Akkiraju et al., 1995). The Computational Geome-
try Algorithms Library (CGAL) (Kettner, Näher, Goodman and
O’Rourke 2004) has a robust implementation of alpha-shape for
2D and 3D point clouds. For each real number a, the approach is
based on the generalized disk of radius 1=a. An edge of the polygon
that contains all the given points (alpha-shape) is drawn between
two members of the ﬁnite point set whenever there exists a gener-
alized disk of radius 1=a containing the entire point set and which
has the property that the two points lie on its boundary. If a ¼ 0,
then the alpha-shape associated with the ﬁnite point set is its ordi-
nary convex hull given by quickhull. The limitation of the alpha-
shape approach is that its performance depends on the set value
of the parameter a. A value of a that produces a satisfactory result
for a point cloud may not be suitable for other point sets since
point clouds can exhibit different point densities. This inconve-
nience is similar to what happens when obtaining a black and
white picture from a grayscale image, through thresholding the
pixel intensities; the optimal threshold value is affected by the
average brightness of the image. Moreover, when the point density
of a point cloud varies between across the cloud, the alpha-shape
result can be satisfactory in some regions and poor in others.
Non-parametric edge extraction methods based on kernel regres-
sion
(Öztireli
et
al.,
2009)
and
on
analysis
of
eigenvalues
(Bazazian, Casas, & Ruiz-Hidalgo, 2015) have been proposed in
recent years. Such methods, however, are optimized for the detec-
tion of internal sharp edges, rather than detecting the point cloud
borderline.
The boundary point detection algorithm presented in this
paper, herein referred as BPD algorithm, does not need the deﬁni-
tion of any threshold values. For every region of the cloud, it
detects as many boundary points as possible, given the local reso-
lution of the region. A 3D-point cloud is unorganized and the
neighbourhood of a point is more complex than that of a pixel in
an image. Generally, in 3D-point clouds, there are three types of
neighbourhoods: spherical neighbourhood, cylindrical neighbour-
hood,
and
k-nearest
neighbours
based
neighbourhood
(Weinmann, Jutzi, Hinz, & Mallet, 2015). The three types of neigh-
bourhoods are based on different search methods. Given a point P,
a spherical neighbourhood is formed by all 3D points in a sphere of
ﬁxed radius around P. A cylindrical neighbourhood is formed by all
those 3D points whose 2D projections onto a plane (e.g. the ground
plane) are within a circle of ﬁxed radius around the projection of P.
The k-nearest neighbourhood (k-NN) search method is non-
parametric and it is used in this work, since it does not need the
deﬁnition of a radius value; it ﬁnds the closest k-members of the
cloud. Fig. 1 shows a point cloud with the k-nearest neighbourhood
of 5 points (A to E), where k is set to 30. The points of the cloud
belonging to the neighbourhoods are indicated through ﬁlled
circles. The other points of the cloud are represented with empty
82
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91


--- Page 3 ---

circles. The semi-transparent circles, centred at the points from A
to E, highlight the best ﬁt planes for each of the neighbourhoods.
The normal directions of such planes are also shown through
arrows pointing outwards from the neighbourhood parent points.
The local cloud resolution for every member of the cloud is esti-
mated through the following steps. Given a point of the cloud Pi,
for every point of its neighbourhood (Pi;j), the minimum distance
(dj;k) between that point and all other neighbours is computed.
The local point cloud resolution (bi) in Pi
is estimated as
bi ¼ li þ 2ri, where li is the mean value of the minimum distances
and ri is their standard deviation. This method of computing the
local resolution is robust, overcoming the problematic noisy nature
of some point clouds collected through optical and photogrammet-
ric method. If the distance values (dj;k) are distributed according to
a Gaussian distribution, the addition of 2ri to the mean value
ensures that 97.6% of the data values are considered and the major
outliers are ignored.
The alpha-shape method may not be able to detect some sec-
tions of the point cloud boundary with high concave curvature if
a is set too low. On the other hand, it may detect unwanted bound-
ary points, if a is too high. The new method described here is cap-
able of detecting boundary points belonging to convex and concave
regions.
The BPD method exploits the fact that there is one and only one
circle that passes through three given points in the 3D space. Given
the unclassiﬁed point Pi, the centre and the radius of all circles that
pass for Pi and any two other points of its neighbourhood are com-
puted. The point is labelled as a boundary point if there is at least
one circle with radius equal or bigger than bi and if the sphere for
Pi, whose centre coincides with the centre of the circle, does not
contain any other point of the neighbourhood. This is the case for
point A and B, shown in Fig. 1. Point A and B are two boundary
points, since it is possible to ﬁnd at least one circle that satisﬁes
the above conditions (Fig. 2). In order to avoid unnecessary compu-
tational efforts and yet consider all possible circles, all the originat-
ing point triples are identiﬁed through the binomial coefﬁcient
(Biggs, 1979). Working with k-nearest neighbourhoods and being
the investigated point Pi always part of the triples, the remaining
points (k-1) are combined in couples with no repetitions. Thus,
the total number of circles is equal to:
n ¼
ðk  1Þ!
2!  ½ðk  1Þ  2! ¼
29!
2  27! ¼ 29  28
2
¼ 406
ð1Þ
Although the presence of at least one circle that satisﬁes the
above conditions allows to classify the investigated point as
belonging to the point cloud boundary, its absence cannot be used
to state that the point is an internal point of the cloud. Indeed the
investigated point Pi can be located on a convex region of the
boundary as well as being an internal point of the cloud. In such
circumstances, although it may exist one circle with radius equal
or bigger than bi, the sphere for Pi, centred at the centre of the cir-
cle, will always contain some points of the neighbourhood. This is
the case for the points C, D and E shown in Fig. 1. For such kind of
points, thus when the point cannot be labelled as a boundary point
through the ﬁrst part of the detection algorithm described above,
the algorithm continues with further operations. Each investigated
point and its neighbours are projected to the best ﬁt plane accord-
ing to the normal vector associated with the point. The resulting
bi-dimensional neighbourhood cloud can be plotted in polar coor-
dinates, with Pi at the pole of the plot. Fig. 3 shows the polar plots
for point C, D and E and their neighbours. Pi is shown in red and its
neighbour points are shown in blue. The blue and red dotted line of
the plots in Fig. 3 highlight, respectively, the minimum and maxi-
mum angle of the angular sector spanned by the points in the
neighbourhood.
The fundamental idea behind the ﬁnal step of the BPD algo-
rithm is that the point of the cloud Pi is a boundary point if it is
not possible to ﬁnd a path that surrounds it and passes through
the neighbour points. Each point on the polar plot is determined
by the distance from the pole (radial coordinate, R) and the angular
coordinate (h). Given a neighbourhood, the developed algorithm
creates a path that surrounds the parent point at the pole. An incre-
mental approach is used. All radial and angular coordinates of the
neighbours are normalized so that the coordinates of the j-th
neighbour point are:
rj ¼ Rj  Rmin
Rmax  Rmin
ð2Þ
#j ¼ hj  hmin
hmax  hmin
ð3Þ
Fig. 2. A and B labelled as boundary points.
Fig. 1. Point cloud with examples of two boundary points on concave regions (A
and B), an inner point (C) and two boundary points on convex regions (D and E).
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
83


--- Page 4 ---

where Rmin, Rmax and hmin, hmax are respectively the minimum and
maximum values of the radial and angular coordinates. In order
to select the starting point of the surrounding path, a characterizing
parameter (c) is given to each neighbour, such that:
cj ¼ rj þ j#j  #bisj
ð4Þ
where #bis is the normalized angle of the direction bisecting the
angular gap comprising all neighbours (between hmin and hmax).
The neighbour point with the minimum value of c, as computed
in Eq. (4), is selected as starting point for the surrounding path.
From the starting point the algorithm progresses through linking
the other points of the neighbourhood. The crossed neighbours
are removed from the list of available points. The characterizing
parameter of the generic p-th remaining point is computed as:
cp ¼ rp þ ½ð#p  #lastÞ  c
ð5Þ
where #last is the normalized angular coordinate of the last crossed
point and c is a factor equal to -1 if sgnð#p  #lastÞ = sgnð#last  #last1Þ
or equal to 1 otherwise. The factor c facilitates the selection of a
point that does not force a change of surrounding direction. For
example, if the last selected point produced a clockwise rotation,
any anti-clockwise rotation is penalised. The neighbour with the
minimum value of c, as computed in Eq. (5), becomes the new last
point of the path.
The incremental linkage of the neighbours keeps track of the
spanned angle around the pole. The sum of the angle increments
considered with their sign (a ¼ P Da) and the sum of their
absolute values (s ¼ P jDaj) is updated each time a new point is
selected after the starting point. The incremental linkage stops
when there are no more linkable neighbour points or when
s > 2p. If jaj P 2p the investigated point is an internal point of
the cloud (e.g. point C in Fig. 3), otherwise it is a boundary point
(e.g. D and E in Fig. 3). The stopping condition of the incremental
algorithm (s > 2p) avoids superﬂuous computation efforts, since
it is often possible to infer if a point belongs to the boundary with-
out linking all neighbours. The solid line in Fig. 3 shows the sur-
rounding path created until s > 2p. For example, it is possible to
determine that C does not belong to the boundary, linking only
10 out of 29 neighbours, since jaj > 2p. It is possible to state that
D is a boundary point, through linking 18 out of 29 neighbours;
it results jaj < 2p, although the sum of the absolute angle incre-
ments is s > 2p. In summary, the BPD method described here is
capable of determining if a point Pi belongs to a concave boundary
section (e.g. point A in Fig. 1) when the local curvature is as high as
1=bi. The method is also able to infer if a point belongs to the
boundary of a hole in the point cloud (e.g. point B in Fig. 1) when
the hole radius is as small as bi. Since bi is the local resolution, the
method works well on point clouds with variable point density.
3. Edge reconstruction
The application of the BPD algorithm to the point cloud sample
shown in Fig. 1 ﬁnds all the boundary points, represented as empty
circles in Fig. 4. Also, the 4 boundary points of the internal circular
hole, whose radius is just above the local point cloud resolution,
are detected as expected. The detected points need to be clustered
so that points belonging to the same boundary are grouped
together. Moreover, the points of every cluster need to be ordered
correctly. These tasks can be fulﬁlled through existing algorithms.
For example, the clustered points can be ordered through algo-
rithms capable of solving the so-called travelling salesman problem
(TSP). Given a random list of points and the distances between
each pair of points, the solution of the TSP is the shortest possible
path that crosses each point exactly once and returns to the origin
point (Schrijver, 2005). Therefore a closed boundary path is
obtained from every cluster. These boundaries, given by the
ordered points linked through line segments (see dashed lines in
Fig. 4) can be quite jagged in some areas. Therefore, it is evident
that such boundaries are not suitable to trim the surface meshes
obtainable from the point cloud. The boundary curves need
smoothing to better resemble the real surface borderlines. This
section of the paper introduces a novel raw boundary smoothing
algorithm, herein referred as RBS algorithm, to improve the recon-
struction of surface point cloud borderlines through accurate
smoothing of the raw boundaries.
Fig. 3. Polar plot of point C, D and E and their neighbours. The solid and dashed lines illustrate the application of the algorithm.
Fig. 4. Closed boundaries partitioned into edges, obtained through clustering and
ordering of the detected boundary points (jagged dashed line) and reconstructed
edges (solid lines).
84
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91


--- Page 5 ---

3.1. Detection of key points
Before applying any smoothing algorithm to the closed loop
boundary curves, it is necessary to highlight that the position of
some of the detected boundary points should be preserved. This
is the case for the borderline corners, where there is a sharp change
in the boundary directionality. Indeed, such points usually play a
crucial role in the deﬁnition of reference systems in CAM applica-
tions for the development of accurate operations, where the cor-
rect registration of the part virtual model is required. Therefore,
the ﬁrst step of the edge reconstruction algorithm is targeted to
detect such key points. Given the i-th point of a closed boundary
path, the point is labelled as corner if the radius of the circle for
the investigated point, the precedent and the successive point is
smaller than the value of the local point cloud resolution, bi (as cal-
culated in the detection algorithm). This approach is able to iden-
tify the corner points from P1 to P5, highlighted with asterisks in
Fig. 4. It is worth noticing here that, although the corners can be
found through applying a threshold value to the angle between
the two borderline segments for each investigated point (Ni, Lin,
Ning, & Zhang, 2016), this approach is advantageous and it is suit-
able to work with point clouds with variable density. The identiﬁed
corners are used to divide the closed loop boundary into sections,
corresponding to the edges of the surface geometry. The external
boundary points of the cloud in Fig. 4 are grouped in 5 edges.
The 4 internal boundary points are grouped in one single closed
loop edge since no corners are found there.
3.2. Limitation of traditional curve smoothing methods
Each edge could be smoothed through ﬁtting a polynomial
curve. Polynomial curve ﬁtting is a common smoothing method
and the functionality is also implemented in CAD/CAM commercial
software applications (e.g. Rhinoceros). Curve ﬁtting is the pro-
cess of approximating a pattern of points with a mathematical
function (Arlinghaus, 1994). Fitted curves can be used to infer
the values of a function where no data are available (Johnson &
Williams, 1976) (e.g. in the gaps between sampled points). The
goal of curve ﬁtting is to model the expected value of a dependent
variable y in terms of the value of an independent variable (or vec-
tor of independent variables) x. In general, the expected value of y
can be modelled as an nth degree polynomial function, yielding the
general polynomial regression model based on the truncated Tay-
lor’s series:
y ¼ a0 þ a1x þ a2x2 þ . . . þ anxn þ e
ð6Þ
where e is a random error with null mean. Given the points of an
edge and the order of the target polynomial function, it is possible
to compute the coefﬁcients of the ﬁtting function. The limitation
of this smoothing approach is that the order of the target function
is typically unknown and curve ﬁtting remains a time-consuming
iterative trial and error process for edge reconstruction. When there
is no theoretical basis for choosing the order of the ﬁtting polyno-
mial function, the edges may be ﬁtted with a spline function com-
posed of a sum of B-splines (Knott, 2012). The places where the
B-splines meet are known as knots. The main difﬁculty in applying
this process is in determining the number of knots to use and where
they should be placed (de Boor, 1968).
3.3. FFT-based reconstruction
The ﬁnal step of the RBS algorithm, presented in this paper,
introduces a robust approach based on the Fast Fourier Transform
(FFT). The FFT is a well-known way to translate the information
contained in a waveform from the time domain to frequency
domain. It is used for the spectral analysis of time-series and
allows the application of high or low-pass ﬁlters, to respectively
attenuate low or high frequencies. Here the FFT is applied to the
pattern of the edge point Cartesian coordinates, to enable the
application of low-pass ﬁlters able to improve the smoothness of
the boundary edges. The exploitation of FFT to spatial patterns
(waveforms sampled in the Cartesian domain rather than the time
domain) is not new (e.g. it has been used for image processing)
(Gonzales & Woods, 1992). However, there is no record of the
FFT being applied to the problem of surface edge reconstruction.
The nuances of the adaptation of FFT to this problem are described
herein.
Consider a series xðkÞ with N samples. Furthermore, assume that
the series outside the range between 0 and N-1 is extended N-peri-
odic, which is xðkÞ ¼ xðk þ NÞ for all k. The FFT of this series will be
denoted XðkÞ, it will also have N samples. The FFT transform
implies speciﬁc relationships between the series index and the fre-
quency domain sample index. For the common case, where the FFT
is applied to series representing a time sequence of length T, the
samples in the frequency domain are spaced by f s ¼ 1=T. The ﬁrst
sample Xð0Þ of the transformed series is the average of the input
series. The frequency sample corresponding to f Ny ¼ N=2T is called
Nyquist frequency. This is the highest frequency component that
should exist in the input series for the FFT to yield uncorrupted
results. More speciﬁcally if there are no frequencies above Nyquist
the original pattern can be exactly reconstructed from the samples
in the frequency domain. For the spatial problem of edge recon-
struction, given the FFT is applied to the edge Cartesian component
pattern, plotted as function of the curvilinear distance (d), the spa-
tial frequency is a measure of how often sinusoidal components (as
determined by the FFT) repeat per unit of distance. The spatial fre-
quency domain representation of any Cartesian component of a
circular edge with radius (R) and length (2pR) contains only one
frequency component f c ¼ 1=2pR. Therefore it is possible to
deduce that, denoting the local curvature radius at the i-th point
of the edge with Ri, the main spatial frequency occurring at that
point is equal to f i ¼ 1=2pRi. According to the Nyquist theorem,
when sampling an analogue signal in the time domain, the sam-
pling rate must be at least equal to 2f max, where f max is the highest
frequency component. The Nyquist rule applied to the spatial
domain means that bi (the local point cloud resolution) limits the
minimum edge radius that is possible to reconstruct at the i-th
point. The smallest radius that is possible to reconstruct will be
the one associated to a circumference of length 2pbi sampled with
2
points,
corresponding
to
the
spatial
frequency
f
 ¼ 2=2pbi ¼ 1=pbi. The maximum alias-free spatial frequency
component will be:
f max ¼ f
=2 ¼ 1=2pbi:
ð7Þ
The smallest edge radius of curvature that is possible to recon-
struct at the i-th point will be equal to Rmax
i
¼ bi.
The
plots
in
Fig.
5a
and
5b
regard,
respectively,
the
x-component pattern of the edge between P2 and P3 and of the
closed loop internal hole edge of the cloud in Fig. 4. The patterns
are plotted as functions of the normalized curvilinear distance of
the edge (d
 ¼ d=D, with D being the total length of the edge).
The original patterns, given by the dashed line that goes through
the x-component samples (shown through round circles), are quite
jagged. Fig. 5 clariﬁes how a periodic waveform is obtained from
the original pattern of each Cartesian component of a given edge.
The pattern is ﬁrst translated along the direction of the ordinate
axis to move the ﬁrst point of the pattern to the origin of the plot.
The pattern is then rotated by the angle a to move the last point of
the pattern on the horizontal axis. A copy of the resulting pattern is
inverted, ﬂipped and appended to the end extremity; it constitutes
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
85


--- Page 6 ---

a complementary portion creating a period with the translated and
rotated version of the original pattern (Fig. 5a). Since the FFT
assumes a constant sampling rate of the input pattern, the original
randomly spaced samples are replaced with interpolated equally
spaced samples. The number of interpolated samples (Np) is chosen
appropriately to give a constant sampling interval (dsÞ, equal or
smaller than the minimum original sampling distance. In order
to ensure a good ﬁltering performance, it is necessary to have suf-
ﬁcient spatial frequency resolution. For such reason the period is
repeated to get a minimum of 1000 samples in the input waveform
of the FFT, giving a frequency resolution of f s ¼ 1=ð1000  dsÞ.
Therefore a low-pass ﬁlter is applied, cancelling all spatial fre-
quency components higher than f max, as expressed in Eq. (7). This
produces a smoothed waveform for the edge component. The FFT
input waveform shown in Fig. 5a, artiﬁcially constructed to ﬁlter
the x-component of the open edge comprised between P2 and P3,
has a null mean value. The original edge extremities lie on the hor-
izontal axis (the mean value line) and are not affected by the low-
pass ﬁltering. The Cartesian component values of the extremities
are preserved. The ﬁrst part of the waveform (the portion up to
the total length D of the original pattern) is rotated by negative a
and translated back to the original position. The smoothed bound-
ary is obtained through ﬁltering all the Cartesian components of all
its edges. The preservation of the original edge extremity points
makes sure that, when a boundary consists of multiple edges,
two consecutive edges share a common point. Therefore, the chain
of edges forms a closed boundary. If a boundary is formed by only
one closed edge, like in the case of the internal hole boundary in
Fig. 4, the extremities of each of its Cartesian components have
the same value (a ¼ 0). Moreover, the complementary portion to
construct the period is a mere horizontally translated copy of the
Cartesian component pattern (after its extremities are brought to
the horizontal axis). The copy of the original pattern is not inverted
nor ﬂipped to create the complementary portion (Fig. 5b). All
points of the closed edge are affected by the ﬁltering. The
smoothed edges relative to the boundaries of the sample point
cloud are shown through solid line curves in Fig. 4.
4. Results and performances
This section of the paper analyses the results obtainable
through the use of the introduced BPD and RBS algorithms. The
computational performances are also examined and quantitative
ﬁgures are reported. Fig. 6 shows a schematic summary of the algo-
rithm steps. The thick dashed line perimeters contain the novel
algorithm components introduced by this paper. The BPD algo-
rithm (Fig. 6a) allows the unlabelled point of a surface point cloud
to be grouped into two groups: boundary points and internal
points. The boundary points are clustered and ordered, through
existing algorithms, to constitute raw closed boundaries (jagged).
The RBS algorithm (Fig. 6b) identiﬁes the boundary corners and
divides each closed boundary into the constituting edges. Every
edge is smoothed through spatial FFT-based ﬁltering. A crucial
advantage of the introduced algorithms is that they are not based
on any threshold values that can be suitable for some point cloud
but not suitable for others. The BPD is capable of labelling all
points, observing the local resolution of the cloud for each point.
The FFT-based edge reconstruction eliminates the problem of
deﬁning a speciﬁc polynomial function order for optimum polyno-
mial curve ﬁtting. In the approach introduced in this paper, the
best edge smoothing performance is also ensured through applying
a spatial low-pass ﬁltering with a frequency deﬁned at every
boundary point as a function of the local cloud resolution.
In order to show the potentialities of the new algorithms, an air-
craft turbine engine fan blade, 640 mm long and 300 mm wide (in
average) was scanned through a coordinate measuring machine.
The FARO Quantum Arm was used in conjunction with a laser pro-
ﬁle mapping probe (Monchalin, Neron, Bouchard, & Heon, 1998).
This 3D scanning equipment has a volumetric maximum deviation
of ±74 lm. The blade surface was scanned to obtain a uniform
point
cloud
with
circa
14
thousand
points
(approximately
72 points per square centimetre).
The cloud points were decimated to obtain four different point
cloud versions, with target resolution respectively equal to 4, 8, 16
and 34 mm. An additional point cloud was generated with variable
point resolution, between 2 and 34 mm. Such generated point
clouds allowed testing the algorithms under controlled situations
and facilitated the analysis of the results. In order to introduce
well-deﬁned internal boundaries, the points found within three
spheres centred at ﬁxed positions and with radii equal to 16, 32
and 64 mm were removed from the clouds. Therefore, each cloud
presents three holes (H1, H2, and H3), with radii approximatively
equal to the original generating spheres. The resulting ﬁve point
clouds are shown by the top row plots in Fig. 7. These plots show
the detected boundary points through darker round point marks.
The external and internal boundary points are detected as
expected. The smallest radius of the hole detectable in a point
cloud depends on the cloud resolution, as described in Section 2.
Only four points of the 16 mm and 32 mm radius holes (H1 and
H2) are detected in the clouds of Fig. 7c and 7d, since the resolution
of these clouds is close to their radii. H1 cannot be detected on the
34 mm resolution cloud (Fig. 7d).
Fig. 8 shows the boundary points detected in the Stanford
bunny (Turk & Levoy, 1994a), an open source computer graphics
test meshed model, obtained through range scanning (Turk &
Levoy, 1994b). Similarly to how many researchers have used this
model, as input for surface reconstruction algorithms, the mesh
connectivity has been stripped away and the vertices have been
treated as an unorganized point cloud in this work. It presents
35,947 points and has an average resolution of 1.2 mm. Boundary
Fig. 5. Creation of periodic waveform for application of spatial FFT to the x-
component pattern of an open edge (a) and a closed edge (b).
86
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91


--- Page 7 ---

Fig. 6. Schematic summary of the algorithm steps: boundary point detection (a) and edge reconstruction (b).
Fig. 7. Detected points (top row) and reconstructed boundary edges (bottom row) for point clouds with ﬁxed resolution of 4 (a), 8 (b), 16 (c) and 34 mm (d) and with variable
resolution between 2 and 34 mm (e).
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
87


--- Page 8 ---

points were detected using a method based on principal compo-
nent analysis (PCA) (Gumhold et al., 2001) (Fig. 8a) and the new
detection algorithm (Fig. 8b). To appreciate the differences in per-
formance it is important to differentiate boundary points and edge
points; the latter points are located in areas where there is a sharp
ripple (like in the bunny ears). The PCA method detected points
that are either on sharp crease lines or on the borderline of the
point cloud holes. The BPD algorithm is targeted to exclusively ﬁnd
borderline points; therefore it found two holes on the base of the
bunny, originating from the original clay model (it was a hollow
model). It should be noted that the BPD algorithm should not
detect the extra edge and crease points detected by PCA (e.g. in
the ears of the bunny). It is designed to detect boundary points
(around holes or areas where the point density drops drastically),
so that a boundary edge can be reconstructed and the tessellated
surface can be trimmed. The BPD algorithm was capable of detect-
ing four additional groups of boundary points (one group under the
bunny chest, one group between the bunny front legs and two
groups on the base), corresponding to the borderline of areas with
poor range scanning coverage. The smallest two of these areas
were not detected by the PCA method, showing how the BPD algo-
rithm is more accurate for the detection of borderline points.
The bottom row plots of Fig. 7 shows the reconstructed bound-
ary edges. The effectiveness of the FFT-based ﬁltering algorithm is
evident observing the smoothness of the edges. The smoothed
boundaries of the internal holes, given by single closed edges,
faithfully reproduces the roundness of the theoretical intersection
between the original generating sphere and the blade surface. Only
the boundaries of H1 and H2, reconstructed through only four
detected points, show visible distortion. Although explaining how
mesh trimming works is out of the scope of this paper, the recon-
structed boundary edges can be used to trim the Poisson mesh,
producing the clean boundary meshes highlighted in Fig. 7.
The algorithms were tested, using a computing machine based
on an Intel Core(TM) i7-6820HQ CPU (2.70 GHz), with 32 Gb of
RAM. The tests were carried out through MATLAB 2016a, running
on the Windows 10 64-bit operating system. Table 1 reports quan-
titative outcomes obtained from the application of the BPD algo-
rithm to the point clouds given in Fig. 7. The initial rows of the
table report the minimum, the mean, the maximum cloud resolu-
tion (b) and its standard deviation (r), followed by the number of
points in the clouds. Therefore the table gives the number of
boundary points detected by the detection algorithm and the time
taken (in milliseconds [ms]) for its complete execution. The
elapsed time is always lower than 3 s for all examined point
clouds.
The point clouds obtained through some 3D scanning methods
are affected by out-of-plane noise, meaning that the sampled
points deviate from their ideal version lying on the surface. It is
important to estimate how much the proposed detection algorithm
is tolerant to such noise. Therefore the detection algorithm was
repeatedly applied to versions of the point clouds with increasing
random noise added to the original points. The noisy clouds were
artiﬁcially obtained by moving each point along the normal direc-
tion of the local k-neighbourhood best ﬁt plane. Each point was
moved by distances equal to a percentage of the local cloud reso-
lution. Starting from 0%, the noise percentage was increased by
1% at every repetition of the detection algorithm. The maximum
percentage value after which at least one of the boundary points
is not detected (false negative) is denoted as X. The value of X is
around 25% for all the clouds of Fig. 7. If the noise percentage is
increased above X, some internal points of the cloud may be
labelled as boundary points (false positives). The maximum per-
centage value after which at least one of the internal points is
labelled as boundary point is denoted as W. The values of W in
Table 1 are all above 75%. Therefore, the detection algorithm is very
Fig. 8. Detected points on the Stanford bunny (Turk & Levoy, 1994a), with a method based on principal component analysis (Gumhold et al., 2001) (a) and with the BPD
algorithm (b).
Table 1
Performances obtained from the application of the BPD algorithm.
Cloud
a
b
c
d
e
Min b [mm]
3.82
7.77
15.53
32.29
1.98
Mean b [mm]
4.19
8.44
16.87
34.52
3.71
Max b [mm]
4.85
9.36
18.98
40.42
34.86
SD (r) [mm]
0.11
0.20
0.52
1.14
4.30
Number of points
12,309
3095
791
203
7458
Boundary points
632
315
156
72
397
Detection time [ms]
2606
944
340
289
1892
X [% of bi]
25%
24%
27%
25%
26%
W [% of bi]
80%
113%
84%
157%
75%
88
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91


--- Page 9 ---

robust for out-of-plane noise lower than 25% of the cloud resolu-
tion and it can produce satisfactory results when the noise is lower
than circa 75%. With noise values between 25% and 75% of the
cloud resolution, the detection algorithm will miss some boundary
points but no outliers will be generated.
Fig. 9 compares, for the 16 mm average resolution point cloud,
polynomial ﬁtting and B-spline edges of 2nd and 3rd order with
the edges reconstructed through the new RBS approach (as
described within the thickest dashed perimeter in Fig. 6b). It is evi-
dent that the polynomial ﬁtting produces unsatisfactory edges,
especially for the internal boundaries. B-spline edges present
excessive wrinkling, thus poor smoothing.
Table 2 gives quantitative results on the performance of the RBS
algorithm, compared to 3rd order polynomial ﬁtting and B-splines,
for all point clouds in Fig. 7.
Although the execution time of the new FFT-based algorithm is
always
one
order
of
magnitude
higher
than
the
time
for
polynomial ﬁtting and for B-spline computation, the smooth edges
produced by the new approach ﬁt the surface contour better than
polynomial ﬁtting and B-splines. The table reports the mean, max-
imum and standard deviation (STD) values for the distances
between the boundary points and the smoothed edges and for
the distance between the reconstructed surface mesh and the
reconstructed edges. In average, the reconstructed edges computed
through the new approach ﬁt the boundary points 4.7 times better
than the 3rd order polynomial edges. Moreover, they follow the
reconstructed
surface
mesh
contour
77%
better
than
the
polynomial
ﬁtting
edges.
Although
the
B-splines
seem
to
follow the reconstructed mesh better than the new approach, 3rd
order B-splines do not produce signiﬁcant smoothing of the
Fig. 9. Comparison of reconstructed edges (red line) with polynomial ﬁtting and B-spline edges (black line) of 2nd and 3rd order.
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
89


--- Page 10 ---

original boundary points; the distance between the B-splines
and the points being circa equal to 0 is not a sign of good
performance. The accuracy with which the polynomial and
B-spline edges ﬁt the boundary points and the reconstructed
surface can be improved by increasing the order of the functions,
but this impacts on the time required to compute the function
coefﬁcients. Moreover, high order polynomials often suffer from
severe ringing between the data points. The RBS approach
reconstructs the optimum edges without the need to specify any
parameter, unlike the function order in the polynomial and the
B-spline ﬁtting.
5. Conclusions
Tessellated surfaces generated from point clouds typically show
inaccurate and jagged boundaries. This can lead to tolerance errors
and problems such as machine judder if the model is used for
ongoing manufacturing applications. This is the reason why many
existing commercial computer-aided manufacturing (CAM) appli-
cations are not able to use tessellated models. This work presented
novel algorithms to reﬁne the boundary of meshed surfaces
obtained from 3D scanning point cloud data. The BPD algorithm
allows the unlabelled point of a surface point cloud to be grouped
into two groups: boundary points and internal points. Existing
detection techniques are optimized to detect points belonging to
sharp edges and creases. The BPD algorithm is targeted to the
detection of boundary points and it is able to do this better than
the existing methods. The RBS algorithm identiﬁes the boundary
corners and divides each closed boundary into the constituting
edges. Every edge is smoothed through spatial FFT-based ﬁltering.
A crucial advantage of the introduced algorithms is that they are
not based on any threshold values that can be suitable for some
point cloud but not suitable for others. The FFT-based edge recon-
struction eliminates the problem of deﬁning a speciﬁc polynomial
function order for optimum polynomial curve ﬁtting. The algo-
rithms were tested to analyse the results and measure the execu-
tion
time
for
point
clouds
generated
from
laser
scanned
measurements on a turbofan engine turbine blade with varying
numbers of member points. Through adding artiﬁcial noise it has
been demonstrated that the BPD algorithm is very robust for out-
of-plane noise lower than 25% of the cloud resolution and it can
produce satisfactory results when the noise is lower than circa
75%. With noise values between 25% and 75% of the cloud resolu-
tion, the detection algorithm will miss some boundary points but
no outliers will be generated. Quantitative results on the perfor-
mance of the RBS algorithm were also presented. The recon-
structed edges computed through the new approach ﬁt the
boundary points by a factor of 4.7 times better than polynomial
edges. Moreover, they follow the reconstructed surface mesh con-
tour with an improvement of 77% compared to the polynomial ﬁt-
ting edges.
Acknowledgements
This work is part of the Autonomous Inspection in Manufactur-
ing and Re-Manufacturing (AIMaReM) project, funded by the UK
Engineering
and
Physical
Science
Research
Council
(EPSRC)
through the grant EP/N018427/1. The authors also wish to thank
Dr. Maxim Morozov for acquiring the point cloud of the turbofan
engine turbine blade considered in this paper.
Appendix A. Supplementary material
Supplementary data associated with this article can be found, in
the online version, at https://doi.org/10.1016/j.jcde.2018.02.001.
References
Akkiraju, N., Edelsbrunner, H., Facello, M., Fu, P., Mücke, E., Varela, C. (1995). Alpha
shapes:
Deﬁnition and software.
In Proceedings
of the 1st international
computational geometry software workshop (p. 66)
Arlinghaus, S. (1994). Practical handbook of curve ﬁtting. CRC Press.
Barber, C. B., Dobkin, D. P., & Huhdanpaa, H. (1996). The quickhull algorithm for
convex hulls. ACM Transactions on Mathematical Software (TOMS), 22, 469–483.
Bazazian, D., Casas, J. R., Ruiz-Hidalgo, J. (2015). Fast and robust edge extraction in
unorganized
point
clouds.
In:
International
conference
on
digital
image
computing: Techniques and applications (DICTA) (pp. 1–8).
Beard, T. (1997). Machining from STL ﬁles. Modern Machine Shop, 69, 90–99.
Berger, M., Tagliasacchi, A., Seversky, L. M., Alliez, P., Guennebaud, G., Levine, J. A.,
et al. (2017). A survey of surface reconstruction from point clouds. Computer
Graphics Forum, 301–329.
Biggs, N. L. (1979). The roots of combinatorics. Historia Mathematica, 6, 109–136.
Calakli, F., & Taubin, G. (2011). SSD: Smooth signed distance surface reconstruction.
Computer Graphics Forum, 1993–2002.
Curless, B. (1999). From range scans to 3D models. ACM SIGGRAPH Computer
Graphics, 33, 38–41.
de Boor, C. (1968). On the convergence of odd-degree spline interpolation. Journal of
Approximation Theory, 1, 452–463.
Delaunay, B. (1934). ‘‘Sur la sphere vide,” Izv. Akad. Nauk SSSR, Otdelenie
Matematicheskii i Estestvennyka Nauk (Vol. 7, pp. 1–2).
Fleishman, S., Cohen-Or, D. & Silva, C. T. (2005). Robust moving least-squares ﬁtting
with sharp features. In ACM transactions on graphics (TOG), (Vol. 0, pp. 544–
552).
Gonzales, R. C., & Woods, R. E. (1992). Digital image processing. Reading, MA:
Addison & Wesley Publishing Company.
Gumhold, S., Wang, X., & MacLeod, R. S. (2001). Feature Extraction From Point
Clouds. In IMR.
Hinks, T., Carr, H., Truong-Hong, L., & Laefer, D. F. (2012). Point cloud data
conversion into solid models via point-based voxelization. Journal of Surveying
Engineering, 139, 72–83.
Johnson, D., & Williams, R. P. D. (1976). Methods of experimental physics:
Spectroscopy. New York: Academic Press.
Table 2
Performances of the RBS algorithm, compared to 3rd order polynomial ﬁtting.
Cloud
a
b
c
d
e
New method
Reconstruction time [ms]
528
305
143
71
385
Mean dist. to points [mm]
0.41
0.84
0.96
1.38
0.22
Max dist. to points [mm]
2.17
4.16
6.73
3.76
2.43
Mean dist. to mesh [mm]
0.07
0.25
0.95
1.20
0.73
Max dist. to mesh [mm]
0.80
1.68
4. 51
4.23
5.77
Pol. ﬁtting
Reconstruction time [ms]
20
19
17
15
19
Mean dist. to points [mm]
3.61
3.78
4.95
5.64
3.83
Max dist. to points [mm]
34.8
33.0
32.7
40.0
30.1
Mean dist. to mesh [mm]
1.00
0.89
1.08
1.43
1.27
Max dist. to mesh [mm]
7.03
5.98
6.56
7.70
12.5
B-Splines
Reconstruction time [ms]
102
60
39
25
69
Mean dist. to points [mm]
0
0
0
0
0
Max dist. to points [mm]
0
0
0
0
0
Mean dist. to mesh [mm]
0.05
0.11
0.24
0.46
0.45
Max dist. to mesh [mm]
0.78
1.12
1.35
3.08
3.30
90
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91


--- Page 11 ---

Kazhdan, M., & Hoppe, H. (2013). Screened Poisson surface reconstruction. ACM
Transactions on Graphics (TOG), 32, 29.
Kettner, L., Näher, S., Goodman, J. E., & O’Rourke, J. (2004). Two computational
geometry libraries: LEDA and CGAL. In Handbook of discrete and computational
geometry (pp. 1435–1463). Chapman & Hall/CRC.
Knott, G. D. (2012). Interpolating cubic splines (Vol. 18). Springer Science & Business
Media.
Levoy, M., & Whitted, T. (1985). The use of points as a display primitive. University of
North Carolina, Department of Computer Science.
Mineo, C., Pierce, S. G., Nicholson, P. I., & Cooper, I. (2017). Introducing a novel mesh
following technique for approximation-free robotic tool path trajectories.
Journal of Computational Design and Engineering, 4(3), 192–202.
Monchalin, J.-P., Neron, C., Bouchard, P., & Heon, R. (1998). Laser-ultrasonics for
inspection
and
characterization
of
aeronautic
materials.
Journal
of
Nondestructive Testing & Ultrasonics (Germany), 3, 002.
Ni, H., Lin, X., Ning, X., & Zhang, J. (2016). Edge detection and feature line tracing in
3d-point clouds by analyzing geometric properties of neighborhoods. Remote
Sensing, 8, 710.
Öztireli, A. C., Guennebaud, G., & Gross, M. (2009). Feature preserving point set
surfaces based on non-linear kernel regression. Computer Graphics Forum,
493–501.
Schrijver, A. (2005). On the history of combinatorial optimization (till 1960).
Handbooks in Operations Research and Management Science, 12, 1–68.
Truong-Hong, L., Laefer, D. F., Hinks, T., & Carr, H. (2011). Flying voxel method with
Delaunay triangulation criterion for façade/feature detection for computation.
Journal of Computing in Civil Engineering, 26, 691–707.
Turk, G. & Levoy, M. (1994). The Stanford bunny, the Stanford 3D scanning
repository.
Turk G, & Levoy, M. (1994). Zippered polygon meshes from range images. In:
Proceedings of the 21st annual conference on Computer graphics and interactive
techniques (pp. 311–318).
Weber, C., Hahmann, S. & Hagen, H. (2011). Methods for feature detection in point
clouds. In OASIcs-OpenAccess Series in Informatics.
Weinmann, M., Jutzi, B., Hinz, S., & Mallet, C. (2015). Semantic point cloud
interpretation based on optimal neighborhoods, relevant features and efﬁcient
classiﬁers. ISPRS Journal of Photogrammetry and Remote Sensing, 105, 286–304.
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
91
```

---

## Section 1.26: Novel algorithms for 3D surface point cloud boundary detection and edge reconstruction

Source File: Novel algorithms for 3D surface point cloud boundary detection and edge reconstruction.txt

### Paper Content:

```
# Novel algorithms for 3D surface point cloud boundary detection and edge reconstruction.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Novel algorithms for 3D surface point cloud boundary detection and edge reconstruction.pdf
# Output: ../layer2_completion/txt/Novel algorithms for 3D surface point cloud boundary detection and edge reconstruction.txt


--- Page 1 ---

Novel algorithms for 3D surface point cloud boundary detection
and edge reconstruction
Carmelo Mineo ⇑, Stephen Gareth Pierce, Rahul Summan
Department of Electronic and Electrical Engineering, University of Strathclyde, Royal College Building, 204 George Street, Glasgow G1 1XW, UK
a r t i c l e
i n f o
Article history:
Received 20 October 2017
Received in revised form 5 February 2018
Accepted 6 February 2018
Available online 7 February 2018
Keywords:
Point-cloud
Boundary detection
Edge reconstruction
a b s t r a c t
Tessellated surfaces generated from point clouds typically show inaccurate and jagged boundaries. This
can lead to tolerance errors and problems such as machine judder if the model is used for ongoing man-
ufacturing applications. This paper introduces a novel boundary point detection algorithm and spatial
FFT-based ﬁltering approach, which together allow for direct generation of low noise tessellated surfaces
from point cloud data, which are not based on pre-deﬁned threshold values. Existing detection tech-
niques are optimized to detect points belonging to sharp edges and creases. The new algorithm is tar-
geted at the detection of boundary points and it is able to do this better than the existing methods.
The FFT-based edge reconstruction eliminates the problem of deﬁning a speciﬁc polynomial function
order for optimum polynomial curve ﬁtting. The algorithms were tested to analyse the results and mea-
sure the execution time for point clouds generated from laser scanned measurements on a turbofan
engine turbine blade with varying numbers of member points. The reconstructed edges ﬁt the boundary
points with an improvement factor of 4.7 over a standard polynomial ﬁtting approach. Furthermore,
through adding artiﬁcial noise it has been demonstrated that the detection algorithm is very robust for
out-of-plane noise lower than 25% of the cloud resolution and it can produce satisfactory results when
the noise is lower than 75%.
 2018 Society for Computational Design and Engineering. Publishing Services by Elsevier. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
1. Introduction
Three-dimensional (3D) scanning is increasingly used to anal-
yse objects or environments in diverse applications including
industrial design, orthotics and prosthetics, gaming and ﬁlm pro-
duction, reverse engineering and prototyping, quality control and
documentation of cultural and architectural artefacts (Curless,
1999; Levoy & Whitted, 1985). Conventional reconstruction tech-
niques generate tessellated surfaces from point clouds. Such tessel-
lated models often show inaccurate and jagged boundaries that
can lead to tolerance errors and problems such as machine judder
if the models are used for ongoing manufacturing applications
(Beard, 1997). That is the reason why many existing commercial
computer-aided manufacturing (CAM) applications are not able
to use tessellated models, instead of using precise analytical CAD
models
where
the
surfaces
are
mathematically
represented
(Mineo, Pierce, Nicholson, & Cooper, 2017). Whilst the conversion
of analytical geometries into meshed surfaces is straightforward,
the reverse process of conversion of a tessellated model into an
analytical CAD model is challenging and time-consuming. There
are circumstances where the original CAD model of a component
is not available or deviates from the real part. New CAM software
applications, able to use clean tessellated models, are emerging
(Mineo et al., 2017); they enable the direct use of triangulated
point clouds obtainable through surface mapping techniques.
However, the point clouds obtained through surface mapping are
typically affected by noise. New algorithms for optimum surface
mesh reﬁnement are required to improve the performance of
emerging applications and to overcome the limitations of typical
approaches based on polynomial smoothing. Different technolo-
gies can be used to build coordinate measuring machines (CMM)
or 3D-scanning devices (Curless, 1999). Each technology comes
with its own limitations, advantages and costs. A common factor
for many CMM and 3D scanners is that they can measure the coor-
dinates of a large number of points on an object surface and output
a point cloud of the scanned area. However, point clouds are gen-
erally not directly usable in most 3D applications, and therefore are
usually converted to mesh models, NURBS surface models, or CAD
models (Berger et al., 2017; Hinks, Carr, Truong-Hong, & Laefer,
2012; Truong-Hong, Laefer, Hinks, & Carr, 2011). Tessellated mod-
els have emerged as a favoured technique; they are the easiest
https://doi.org/10.1016/j.jcde.2018.02.001
2288-4300/ 2018 Society for Computational Design and Engineering. Publishing Services by Elsevier.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Peer review under responsibility of Society for Computational Design and
Engineering.
⇑Corresponding author.
E-mail address: carmelo.mineo@strath.ac.uk (C. Mineo).
Journal of Computational Design and Engineering 6 (2019) 81–91
Contents lists available at ScienceDirect
Journal of Computational Design and Engineering
journal homepage: www.elsevier.com/locate/jcde
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 2 ---

form of virtual models obtainable from point clouds with minimal
processing. There are two different approaches to create a triangu-
lar meshed surface from a point cloud: using triangulation meth-
ods or surface reconstruction methods. Triangulation algorithms
use the original points of the input point cloud, using them as
the vertices of the mesh triangles. The Delaunay triangulation,
named after Boris Delaunay for his work on the topic from 1934
(Delaunay, 1934), is the most popular algorithm of this kind. A
bi-dimensional Delaunay triangulation ensures that the circumcir-
cle associated with each triangle contains no other point in its inte-
rior.
This
deﬁnition
extends
naturally
to
three
dimensions
considering spheres instead of circles. Surface reconstruction algo-
rithms differ from the triangulation method since they do not use
the original points as the vertices of the mesh triangles but com-
pute new points, whose density can vary according to the local cur-
vature of the 3D geometry. Surface reconstruction from oriented
points can be cast as a spatial problem, based on the Poisson’s
equation (Calakli & Taubin, 2011; Kazhdan & Hoppe, 2013).
Both approaches are not able to reconstruct the surface bound-
aries accurately, which makes the tessellated models unsuitable to
be used for CAM toolpath generation. Triangulation methods pro-
duce meshed surfaces with jagged boundaries since the original
noisy points of the cloud are used as vertices of the mesh triangles.
Reconstruction methods produce smooth boundaries, but they can
be quite far from the original boundaries of the real surface.
Indeed, Poisson’s surface reconstruction does not follow the
boundary of the point cloud and replaces the original points with
new points laying on a reconstructed continuous surface that sat-
isﬁes the Poisson’s differential equation.
The detection of cloud boundary points and the reconstruction
of smooth boundary edges would allow trimming of the recon-
structed tessellated models, to reﬁne the mesh boundaries. Very
few feature detection methods are optimized to work with point-
sampled geometries only. The major problem of these point based
methods is the lack of knowledge concerning point normal and
connectivity. This makes feature detection a more challenging task
than in mesh-based methods. Gumhold, Wang, and MacLeod
(2001) presented an algorithm that ﬁrst analyses the neighbour-
hood of each point via a principal component analysis (PCA). The
eigenvalues of the correlation matrix are then used to determine
if a point belongs to a feature. This technique for the detection of
features in point clouds is used as a pre-processing step for tessel-
lated
surface
reconstruction
with
sharp
features
(Weber,
Hahmann, & Hagen, 2011). There exist also several reconstruction
methods that preserve sharp features during the surface recon-
struction of a point cloud without pre-processing; for example,
the methods shown by Fleishman, Cohen-Or, and Silva (2005)
and Öztireli, Guennebaud, and Gross (2009).
The existing techniques mentioned above are optimized to
detect points belonging to sharp edges. This paper presents novel
algorithms targeted to the detection of boundary points and the
deterministic reconstruction of accurate and smooth surface
boundaries from 3D point clouds. A smart approach known as
Mesh Following Technique (MFT) (Mineo et al., 2017), for the gen-
eration of robot tool-paths from STL models, has recently been
published. The technique requires virtual tessellated surfaces with
smooth boundary edges.
The algorithms presented in this paper are useful tools to reﬁne
the boundary of tessellated surfaces obtained from 3D scanning
point cloud data. They can be used to trim Delaunay triangulation
or Poisson’s reconstructed surface meshes, facilitating the direct
use of tessellated models, instead of analytical geometries. The
remainder part of the paper describes the algorithms and shows
qualitative and quantitative results, discussing advantages and
disadvantages.
2. Detection of boundary points
Given a mapped surface in the form of a point cloud, the iden-
tiﬁcation of the point cloud borderline, thus the detection of
boundary points, is not a trivial task. The human brain is able to
infer the border of a point cloud by simply looking at the arrange-
ment of the sparse points. In computer science and computational
geometry, a point cloud is an entity without a well-deﬁned bound-
ary. In the bi-dimensional domain, given a ﬁnite set of points, the
problem of detecting the smallest convex polygon that contains all
the given points of the cloud is solved through the quickhull
method (Barber, Dobkin, & Huhdanpaa, 1996). It uses a divide
and conquer approach. This method works well but is only able
to detect the boundary points that are part of the convex polygon
and is only for a set of points. A generalization of quickhull, able to
handle concave regions and holes in the point cloud, is the alpha-
shape approach (Akkiraju et al., 1995). The Computational Geome-
try Algorithms Library (CGAL) (Kettner, Näher, Goodman and
O’Rourke 2004) has a robust implementation of alpha-shape for
2D and 3D point clouds. For each real number a, the approach is
based on the generalized disk of radius 1=a. An edge of the polygon
that contains all the given points (alpha-shape) is drawn between
two members of the ﬁnite point set whenever there exists a gener-
alized disk of radius 1=a containing the entire point set and which
has the property that the two points lie on its boundary. If a ¼ 0,
then the alpha-shape associated with the ﬁnite point set is its ordi-
nary convex hull given by quickhull. The limitation of the alpha-
shape approach is that its performance depends on the set value
of the parameter a. A value of a that produces a satisfactory result
for a point cloud may not be suitable for other point sets since
point clouds can exhibit different point densities. This inconve-
nience is similar to what happens when obtaining a black and
white picture from a grayscale image, through thresholding the
pixel intensities; the optimal threshold value is affected by the
average brightness of the image. Moreover, when the point density
of a point cloud varies between across the cloud, the alpha-shape
result can be satisfactory in some regions and poor in others.
Non-parametric edge extraction methods based on kernel regres-
sion
(Öztireli
et
al.,
2009)
and
on
analysis
of
eigenvalues
(Bazazian, Casas, & Ruiz-Hidalgo, 2015) have been proposed in
recent years. Such methods, however, are optimized for the detec-
tion of internal sharp edges, rather than detecting the point cloud
borderline.
The boundary point detection algorithm presented in this
paper, herein referred as BPD algorithm, does not need the deﬁni-
tion of any threshold values. For every region of the cloud, it
detects as many boundary points as possible, given the local reso-
lution of the region. A 3D-point cloud is unorganized and the
neighbourhood of a point is more complex than that of a pixel in
an image. Generally, in 3D-point clouds, there are three types of
neighbourhoods: spherical neighbourhood, cylindrical neighbour-
hood,
and
k-nearest
neighbours
based
neighbourhood
(Weinmann, Jutzi, Hinz, & Mallet, 2015). The three types of neigh-
bourhoods are based on different search methods. Given a point P,
a spherical neighbourhood is formed by all 3D points in a sphere of
ﬁxed radius around P. A cylindrical neighbourhood is formed by all
those 3D points whose 2D projections onto a plane (e.g. the ground
plane) are within a circle of ﬁxed radius around the projection of P.
The k-nearest neighbourhood (k-NN) search method is non-
parametric and it is used in this work, since it does not need the
deﬁnition of a radius value; it ﬁnds the closest k-members of the
cloud. Fig. 1 shows a point cloud with the k-nearest neighbourhood
of 5 points (A to E), where k is set to 30. The points of the cloud
belonging to the neighbourhoods are indicated through ﬁlled
circles. The other points of the cloud are represented with empty
82
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 3 ---

circles. The semi-transparent circles, centred at the points from A
to E, highlight the best ﬁt planes for each of the neighbourhoods.
The normal directions of such planes are also shown through
arrows pointing outwards from the neighbourhood parent points.
The local cloud resolution for every member of the cloud is esti-
mated through the following steps. Given a point of the cloud Pi,
for every point of its neighbourhood (Pi;j), the minimum distance
(dj;k) between that point and all other neighbours is computed.
The local point cloud resolution (bi) in Pi
is estimated as
bi ¼ li þ 2ri, where li is the mean value of the minimum distances
and ri is their standard deviation. This method of computing the
local resolution is robust, overcoming the problematic noisy nature
of some point clouds collected through optical and photogrammet-
ric method. If the distance values (dj;k) are distributed according to
a Gaussian distribution, the addition of 2ri to the mean value
ensures that 97.6% of the data values are considered and the major
outliers are ignored.
The alpha-shape method may not be able to detect some sec-
tions of the point cloud boundary with high concave curvature if
a is set too low. On the other hand, it may detect unwanted bound-
ary points, if a is too high. The new method described here is cap-
able of detecting boundary points belonging to convex and concave
regions.
The BPD method exploits the fact that there is one and only one
circle that passes through three given points in the 3D space. Given
the unclassiﬁed point Pi, the centre and the radius of all circles that
pass for Pi and any two other points of its neighbourhood are com-
puted. The point is labelled as a boundary point if there is at least
one circle with radius equal or bigger than bi and if the sphere for
Pi, whose centre coincides with the centre of the circle, does not
contain any other point of the neighbourhood. This is the case for
point A and B, shown in Fig. 1. Point A and B are two boundary
points, since it is possible to ﬁnd at least one circle that satisﬁes
the above conditions (Fig. 2). In order to avoid unnecessary compu-
tational efforts and yet consider all possible circles, all the originat-
ing point triples are identiﬁed through the binomial coefﬁcient
(Biggs, 1979). Working with k-nearest neighbourhoods and being
the investigated point Pi always part of the triples, the remaining
points (k-1) are combined in couples with no repetitions. Thus,
the total number of circles is equal to:
n ¼
ðk  1Þ!
2!  ½ðk  1Þ  2! ¼
29!
2  27! ¼ 29  28
2
¼ 406
ð1Þ
Although the presence of at least one circle that satisﬁes the
above conditions allows to classify the investigated point as
belonging to the point cloud boundary, its absence cannot be used
to state that the point is an internal point of the cloud. Indeed the
investigated point Pi can be located on a convex region of the
boundary as well as being an internal point of the cloud. In such
circumstances, although it may exist one circle with radius equal
or bigger than bi, the sphere for Pi, centred at the centre of the cir-
cle, will always contain some points of the neighbourhood. This is
the case for the points C, D and E shown in Fig. 1. For such kind of
points, thus when the point cannot be labelled as a boundary point
through the ﬁrst part of the detection algorithm described above,
the algorithm continues with further operations. Each investigated
point and its neighbours are projected to the best ﬁt plane accord-
ing to the normal vector associated with the point. The resulting
bi-dimensional neighbourhood cloud can be plotted in polar coor-
dinates, with Pi at the pole of the plot. Fig. 3 shows the polar plots
for point C, D and E and their neighbours. Pi is shown in red and its
neighbour points are shown in blue. The blue and red dotted line of
the plots in Fig. 3 highlight, respectively, the minimum and maxi-
mum angle of the angular sector spanned by the points in the
neighbourhood.
The fundamental idea behind the ﬁnal step of the BPD algo-
rithm is that the point of the cloud Pi is a boundary point if it is
not possible to ﬁnd a path that surrounds it and passes through
the neighbour points. Each point on the polar plot is determined
by the distance from the pole (radial coordinate, R) and the angular
coordinate (h). Given a neighbourhood, the developed algorithm
creates a path that surrounds the parent point at the pole. An incre-
mental approach is used. All radial and angular coordinates of the
neighbours are normalized so that the coordinates of the j-th
neighbour point are:
rj ¼ Rj  Rmin
Rmax  Rmin
ð2Þ
#j ¼ hj  hmin
hmax  hmin
ð3Þ
Fig. 2. A and B labelled as boundary points.
Fig. 1. Point cloud with examples of two boundary points on concave regions (A
and B), an inner point (C) and two boundary points on convex regions (D and E).
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
83
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 4 ---

where Rmin, Rmax and hmin, hmax are respectively the minimum and
maximum values of the radial and angular coordinates. In order
to select the starting point of the surrounding path, a characterizing
parameter (c) is given to each neighbour, such that:
cj ¼ rj þ j#j  #bisj
ð4Þ
where #bis is the normalized angle of the direction bisecting the
angular gap comprising all neighbours (between hmin and hmax).
The neighbour point with the minimum value of c, as computed
in Eq. (4), is selected as starting point for the surrounding path.
From the starting point the algorithm progresses through linking
the other points of the neighbourhood. The crossed neighbours
are removed from the list of available points. The characterizing
parameter of the generic p-th remaining point is computed as:
cp ¼ rp þ ½ð#p  #lastÞ  c
ð5Þ
where #last is the normalized angular coordinate of the last crossed
point and c is a factor equal to -1 if sgnð#p  #lastÞ = sgnð#last  #last1Þ
or equal to 1 otherwise. The factor c facilitates the selection of a
point that does not force a change of surrounding direction. For
example, if the last selected point produced a clockwise rotation,
any anti-clockwise rotation is penalised. The neighbour with the
minimum value of c, as computed in Eq. (5), becomes the new last
point of the path.
The incremental linkage of the neighbours keeps track of the
spanned angle around the pole. The sum of the angle increments
considered with their sign (a ¼ P Da) and the sum of their
absolute values (s ¼ P jDaj) is updated each time a new point is
selected after the starting point. The incremental linkage stops
when there are no more linkable neighbour points or when
s > 2p. If jaj P 2p the investigated point is an internal point of
the cloud (e.g. point C in Fig. 3), otherwise it is a boundary point
(e.g. D and E in Fig. 3). The stopping condition of the incremental
algorithm (s > 2p) avoids superﬂuous computation efforts, since
it is often possible to infer if a point belongs to the boundary with-
out linking all neighbours. The solid line in Fig. 3 shows the sur-
rounding path created until s > 2p. For example, it is possible to
determine that C does not belong to the boundary, linking only
10 out of 29 neighbours, since jaj > 2p. It is possible to state that
D is a boundary point, through linking 18 out of 29 neighbours;
it results jaj < 2p, although the sum of the absolute angle incre-
ments is s > 2p. In summary, the BPD method described here is
capable of determining if a point Pi belongs to a concave boundary
section (e.g. point A in Fig. 1) when the local curvature is as high as
1=bi. The method is also able to infer if a point belongs to the
boundary of a hole in the point cloud (e.g. point B in Fig. 1) when
the hole radius is as small as bi. Since bi is the local resolution, the
method works well on point clouds with variable point density.
3. Edge reconstruction
The application of the BPD algorithm to the point cloud sample
shown in Fig. 1 ﬁnds all the boundary points, represented as empty
circles in Fig. 4. Also, the 4 boundary points of the internal circular
hole, whose radius is just above the local point cloud resolution,
are detected as expected. The detected points need to be clustered
so that points belonging to the same boundary are grouped
together. Moreover, the points of every cluster need to be ordered
correctly. These tasks can be fulﬁlled through existing algorithms.
For example, the clustered points can be ordered through algo-
rithms capable of solving the so-called travelling salesman problem
(TSP). Given a random list of points and the distances between
each pair of points, the solution of the TSP is the shortest possible
path that crosses each point exactly once and returns to the origin
point (Schrijver, 2005). Therefore a closed boundary path is
obtained from every cluster. These boundaries, given by the
ordered points linked through line segments (see dashed lines in
Fig. 4) can be quite jagged in some areas. Therefore, it is evident
that such boundaries are not suitable to trim the surface meshes
obtainable from the point cloud. The boundary curves need
smoothing to better resemble the real surface borderlines. This
section of the paper introduces a novel raw boundary smoothing
algorithm, herein referred as RBS algorithm, to improve the recon-
struction of surface point cloud borderlines through accurate
smoothing of the raw boundaries.
Fig. 3. Polar plot of point C, D and E and their neighbours. The solid and dashed lines illustrate the application of the algorithm.
Fig. 4. Closed boundaries partitioned into edges, obtained through clustering and
ordering of the detected boundary points (jagged dashed line) and reconstructed
edges (solid lines).
84
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 5 ---

3.1. Detection of key points
Before applying any smoothing algorithm to the closed loop
boundary curves, it is necessary to highlight that the position of
some of the detected boundary points should be preserved. This
is the case for the borderline corners, where there is a sharp change
in the boundary directionality. Indeed, such points usually play a
crucial role in the deﬁnition of reference systems in CAM applica-
tions for the development of accurate operations, where the cor-
rect registration of the part virtual model is required. Therefore,
the ﬁrst step of the edge reconstruction algorithm is targeted to
detect such key points. Given the i-th point of a closed boundary
path, the point is labelled as corner if the radius of the circle for
the investigated point, the precedent and the successive point is
smaller than the value of the local point cloud resolution, bi (as cal-
culated in the detection algorithm). This approach is able to iden-
tify the corner points from P1 to P5, highlighted with asterisks in
Fig. 4. It is worth noticing here that, although the corners can be
found through applying a threshold value to the angle between
the two borderline segments for each investigated point (Ni, Lin,
Ning, & Zhang, 2016), this approach is advantageous and it is suit-
able to work with point clouds with variable density. The identiﬁed
corners are used to divide the closed loop boundary into sections,
corresponding to the edges of the surface geometry. The external
boundary points of the cloud in Fig. 4 are grouped in 5 edges.
The 4 internal boundary points are grouped in one single closed
loop edge since no corners are found there.
3.2. Limitation of traditional curve smoothing methods
Each edge could be smoothed through ﬁtting a polynomial
curve. Polynomial curve ﬁtting is a common smoothing method
and the functionality is also implemented in CAD/CAM commercial
software applications (e.g. Rhinoceros). Curve ﬁtting is the pro-
cess of approximating a pattern of points with a mathematical
function (Arlinghaus, 1994). Fitted curves can be used to infer
the values of a function where no data are available (Johnson &
Williams, 1976) (e.g. in the gaps between sampled points). The
goal of curve ﬁtting is to model the expected value of a dependent
variable y in terms of the value of an independent variable (or vec-
tor of independent variables) x. In general, the expected value of y
can be modelled as an nth degree polynomial function, yielding the
general polynomial regression model based on the truncated Tay-
lor’s series:
y ¼ a0 þ a1x þ a2x2 þ . . . þ anxn þ e
ð6Þ
where e is a random error with null mean. Given the points of an
edge and the order of the target polynomial function, it is possible
to compute the coefﬁcients of the ﬁtting function. The limitation
of this smoothing approach is that the order of the target function
is typically unknown and curve ﬁtting remains a time-consuming
iterative trial and error process for edge reconstruction. When there
is no theoretical basis for choosing the order of the ﬁtting polyno-
mial function, the edges may be ﬁtted with a spline function com-
posed of a sum of B-splines (Knott, 2012). The places where the
B-splines meet are known as knots. The main difﬁculty in applying
this process is in determining the number of knots to use and where
they should be placed (de Boor, 1968).
3.3. FFT-based reconstruction
The ﬁnal step of the RBS algorithm, presented in this paper,
introduces a robust approach based on the Fast Fourier Transform
(FFT). The FFT is a well-known way to translate the information
contained in a waveform from the time domain to frequency
domain. It is used for the spectral analysis of time-series and
allows the application of high or low-pass ﬁlters, to respectively
attenuate low or high frequencies. Here the FFT is applied to the
pattern of the edge point Cartesian coordinates, to enable the
application of low-pass ﬁlters able to improve the smoothness of
the boundary edges. The exploitation of FFT to spatial patterns
(waveforms sampled in the Cartesian domain rather than the time
domain) is not new (e.g. it has been used for image processing)
(Gonzales & Woods, 1992). However, there is no record of the
FFT being applied to the problem of surface edge reconstruction.
The nuances of the adaptation of FFT to this problem are described
herein.
Consider a series xðkÞ with N samples. Furthermore, assume that
the series outside the range between 0 and N-1 is extended N-peri-
odic, which is xðkÞ ¼ xðk þ NÞ for all k. The FFT of this series will be
denoted XðkÞ, it will also have N samples. The FFT transform
implies speciﬁc relationships between the series index and the fre-
quency domain sample index. For the common case, where the FFT
is applied to series representing a time sequence of length T, the
samples in the frequency domain are spaced by f s ¼ 1=T. The ﬁrst
sample Xð0Þ of the transformed series is the average of the input
series. The frequency sample corresponding to f Ny ¼ N=2T is called
Nyquist frequency. This is the highest frequency component that
should exist in the input series for the FFT to yield uncorrupted
results. More speciﬁcally if there are no frequencies above Nyquist
the original pattern can be exactly reconstructed from the samples
in the frequency domain. For the spatial problem of edge recon-
struction, given the FFT is applied to the edge Cartesian component
pattern, plotted as function of the curvilinear distance (d), the spa-
tial frequency is a measure of how often sinusoidal components (as
determined by the FFT) repeat per unit of distance. The spatial fre-
quency domain representation of any Cartesian component of a
circular edge with radius (R) and length (2pR) contains only one
frequency component f c ¼ 1=2pR. Therefore it is possible to
deduce that, denoting the local curvature radius at the i-th point
of the edge with Ri, the main spatial frequency occurring at that
point is equal to f i ¼ 1=2pRi. According to the Nyquist theorem,
when sampling an analogue signal in the time domain, the sam-
pling rate must be at least equal to 2f max, where f max is the highest
frequency component. The Nyquist rule applied to the spatial
domain means that bi (the local point cloud resolution) limits the
minimum edge radius that is possible to reconstruct at the i-th
point. The smallest radius that is possible to reconstruct will be
the one associated to a circumference of length 2pbi sampled with
2
points,
corresponding
to
the
spatial
frequency
f
 ¼ 2=2pbi ¼ 1=pbi. The maximum alias-free spatial frequency
component will be:
f max ¼ f
=2 ¼ 1=2pbi:
ð7Þ
The smallest edge radius of curvature that is possible to recon-
struct at the i-th point will be equal to Rmax
i
¼ bi.
The
plots
in
Fig.
5a
and
5b
regard,
respectively,
the
x-component pattern of the edge between P2 and P3 and of the
closed loop internal hole edge of the cloud in Fig. 4. The patterns
are plotted as functions of the normalized curvilinear distance of
the edge (d
 ¼ d=D, with D being the total length of the edge).
The original patterns, given by the dashed line that goes through
the x-component samples (shown through round circles), are quite
jagged. Fig. 5 clariﬁes how a periodic waveform is obtained from
the original pattern of each Cartesian component of a given edge.
The pattern is ﬁrst translated along the direction of the ordinate
axis to move the ﬁrst point of the pattern to the origin of the plot.
The pattern is then rotated by the angle a to move the last point of
the pattern on the horizontal axis. A copy of the resulting pattern is
inverted, ﬂipped and appended to the end extremity; it constitutes
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
85
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 6 ---

a complementary portion creating a period with the translated and
rotated version of the original pattern (Fig. 5a). Since the FFT
assumes a constant sampling rate of the input pattern, the original
randomly spaced samples are replaced with interpolated equally
spaced samples. The number of interpolated samples (Np) is chosen
appropriately to give a constant sampling interval (dsÞ, equal or
smaller than the minimum original sampling distance. In order
to ensure a good ﬁltering performance, it is necessary to have suf-
ﬁcient spatial frequency resolution. For such reason the period is
repeated to get a minimum of 1000 samples in the input waveform
of the FFT, giving a frequency resolution of f s ¼ 1=ð1000  dsÞ.
Therefore a low-pass ﬁlter is applied, cancelling all spatial fre-
quency components higher than f max, as expressed in Eq. (7). This
produces a smoothed waveform for the edge component. The FFT
input waveform shown in Fig. 5a, artiﬁcially constructed to ﬁlter
the x-component of the open edge comprised between P2 and P3,
has a null mean value. The original edge extremities lie on the hor-
izontal axis (the mean value line) and are not affected by the low-
pass ﬁltering. The Cartesian component values of the extremities
are preserved. The ﬁrst part of the waveform (the portion up to
the total length D of the original pattern) is rotated by negative a
and translated back to the original position. The smoothed bound-
ary is obtained through ﬁltering all the Cartesian components of all
its edges. The preservation of the original edge extremity points
makes sure that, when a boundary consists of multiple edges,
two consecutive edges share a common point. Therefore, the chain
of edges forms a closed boundary. If a boundary is formed by only
one closed edge, like in the case of the internal hole boundary in
Fig. 4, the extremities of each of its Cartesian components have
the same value (a ¼ 0). Moreover, the complementary portion to
construct the period is a mere horizontally translated copy of the
Cartesian component pattern (after its extremities are brought to
the horizontal axis). The copy of the original pattern is not inverted
nor ﬂipped to create the complementary portion (Fig. 5b). All
points of the closed edge are affected by the ﬁltering. The
smoothed edges relative to the boundaries of the sample point
cloud are shown through solid line curves in Fig. 4.
4. Results and performances
This section of the paper analyses the results obtainable
through the use of the introduced BPD and RBS algorithms. The
computational performances are also examined and quantitative
ﬁgures are reported. Fig. 6 shows a schematic summary of the algo-
rithm steps. The thick dashed line perimeters contain the novel
algorithm components introduced by this paper. The BPD algo-
rithm (Fig. 6a) allows the unlabelled point of a surface point cloud
to be grouped into two groups: boundary points and internal
points. The boundary points are clustered and ordered, through
existing algorithms, to constitute raw closed boundaries (jagged).
The RBS algorithm (Fig. 6b) identiﬁes the boundary corners and
divides each closed boundary into the constituting edges. Every
edge is smoothed through spatial FFT-based ﬁltering. A crucial
advantage of the introduced algorithms is that they are not based
on any threshold values that can be suitable for some point cloud
but not suitable for others. The BPD is capable of labelling all
points, observing the local resolution of the cloud for each point.
The FFT-based edge reconstruction eliminates the problem of
deﬁning a speciﬁc polynomial function order for optimum polyno-
mial curve ﬁtting. In the approach introduced in this paper, the
best edge smoothing performance is also ensured through applying
a spatial low-pass ﬁltering with a frequency deﬁned at every
boundary point as a function of the local cloud resolution.
In order to show the potentialities of the new algorithms, an air-
craft turbine engine fan blade, 640 mm long and 300 mm wide (in
average) was scanned through a coordinate measuring machine.
The FARO Quantum Arm was used in conjunction with a laser pro-
ﬁle mapping probe (Monchalin, Neron, Bouchard, & Heon, 1998).
This 3D scanning equipment has a volumetric maximum deviation
of ±74 lm. The blade surface was scanned to obtain a uniform
point
cloud
with
circa
14
thousand
points
(approximately
72 points per square centimetre).
The cloud points were decimated to obtain four different point
cloud versions, with target resolution respectively equal to 4, 8, 16
and 34 mm. An additional point cloud was generated with variable
point resolution, between 2 and 34 mm. Such generated point
clouds allowed testing the algorithms under controlled situations
and facilitated the analysis of the results. In order to introduce
well-deﬁned internal boundaries, the points found within three
spheres centred at ﬁxed positions and with radii equal to 16, 32
and 64 mm were removed from the clouds. Therefore, each cloud
presents three holes (H1, H2, and H3), with radii approximatively
equal to the original generating spheres. The resulting ﬁve point
clouds are shown by the top row plots in Fig. 7. These plots show
the detected boundary points through darker round point marks.
The external and internal boundary points are detected as
expected. The smallest radius of the hole detectable in a point
cloud depends on the cloud resolution, as described in Section 2.
Only four points of the 16 mm and 32 mm radius holes (H1 and
H2) are detected in the clouds of Fig. 7c and 7d, since the resolution
of these clouds is close to their radii. H1 cannot be detected on the
34 mm resolution cloud (Fig. 7d).
Fig. 8 shows the boundary points detected in the Stanford
bunny (Turk & Levoy, 1994a), an open source computer graphics
test meshed model, obtained through range scanning (Turk &
Levoy, 1994b). Similarly to how many researchers have used this
model, as input for surface reconstruction algorithms, the mesh
connectivity has been stripped away and the vertices have been
treated as an unorganized point cloud in this work. It presents
35,947 points and has an average resolution of 1.2 mm. Boundary
Fig. 5. Creation of periodic waveform for application of spatial FFT to the x-
component pattern of an open edge (a) and a closed edge (b).
86
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 7 ---

Fig. 6. Schematic summary of the algorithm steps: boundary point detection (a) and edge reconstruction (b).
Fig. 7. Detected points (top row) and reconstructed boundary edges (bottom row) for point clouds with ﬁxed resolution of 4 (a), 8 (b), 16 (c) and 34 mm (d) and with variable
resolution between 2 and 34 mm (e).
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
87
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 8 ---

points were detected using a method based on principal compo-
nent analysis (PCA) (Gumhold et al., 2001) (Fig. 8a) and the new
detection algorithm (Fig. 8b). To appreciate the differences in per-
formance it is important to differentiate boundary points and edge
points; the latter points are located in areas where there is a sharp
ripple (like in the bunny ears). The PCA method detected points
that are either on sharp crease lines or on the borderline of the
point cloud holes. The BPD algorithm is targeted to exclusively ﬁnd
borderline points; therefore it found two holes on the base of the
bunny, originating from the original clay model (it was a hollow
model). It should be noted that the BPD algorithm should not
detect the extra edge and crease points detected by PCA (e.g. in
the ears of the bunny). It is designed to detect boundary points
(around holes or areas where the point density drops drastically),
so that a boundary edge can be reconstructed and the tessellated
surface can be trimmed. The BPD algorithm was capable of detect-
ing four additional groups of boundary points (one group under the
bunny chest, one group between the bunny front legs and two
groups on the base), corresponding to the borderline of areas with
poor range scanning coverage. The smallest two of these areas
were not detected by the PCA method, showing how the BPD algo-
rithm is more accurate for the detection of borderline points.
The bottom row plots of Fig. 7 shows the reconstructed bound-
ary edges. The effectiveness of the FFT-based ﬁltering algorithm is
evident observing the smoothness of the edges. The smoothed
boundaries of the internal holes, given by single closed edges,
faithfully reproduces the roundness of the theoretical intersection
between the original generating sphere and the blade surface. Only
the boundaries of H1 and H2, reconstructed through only four
detected points, show visible distortion. Although explaining how
mesh trimming works is out of the scope of this paper, the recon-
structed boundary edges can be used to trim the Poisson mesh,
producing the clean boundary meshes highlighted in Fig. 7.
The algorithms were tested, using a computing machine based
on an Intel Core(TM) i7-6820HQ CPU (2.70 GHz), with 32 Gb of
RAM. The tests were carried out through MATLAB 2016a, running
on the Windows 10 64-bit operating system. Table 1 reports quan-
titative outcomes obtained from the application of the BPD algo-
rithm to the point clouds given in Fig. 7. The initial rows of the
table report the minimum, the mean, the maximum cloud resolu-
tion (b) and its standard deviation (r), followed by the number of
points in the clouds. Therefore the table gives the number of
boundary points detected by the detection algorithm and the time
taken (in milliseconds [ms]) for its complete execution. The
elapsed time is always lower than 3 s for all examined point
clouds.
The point clouds obtained through some 3D scanning methods
are affected by out-of-plane noise, meaning that the sampled
points deviate from their ideal version lying on the surface. It is
important to estimate how much the proposed detection algorithm
is tolerant to such noise. Therefore the detection algorithm was
repeatedly applied to versions of the point clouds with increasing
random noise added to the original points. The noisy clouds were
artiﬁcially obtained by moving each point along the normal direc-
tion of the local k-neighbourhood best ﬁt plane. Each point was
moved by distances equal to a percentage of the local cloud reso-
lution. Starting from 0%, the noise percentage was increased by
1% at every repetition of the detection algorithm. The maximum
percentage value after which at least one of the boundary points
is not detected (false negative) is denoted as X. The value of X is
around 25% for all the clouds of Fig. 7. If the noise percentage is
increased above X, some internal points of the cloud may be
labelled as boundary points (false positives). The maximum per-
centage value after which at least one of the internal points is
labelled as boundary point is denoted as W. The values of W in
Table 1 are all above 75%. Therefore, the detection algorithm is very
Fig. 8. Detected points on the Stanford bunny (Turk & Levoy, 1994a), with a method based on principal component analysis (Gumhold et al., 2001) (a) and with the BPD
algorithm (b).
Table 1
Performances obtained from the application of the BPD algorithm.
Cloud
a
b
c
d
e
Min b [mm]
3.82
7.77
15.53
32.29
1.98
Mean b [mm]
4.19
8.44
16.87
34.52
3.71
Max b [mm]
4.85
9.36
18.98
40.42
34.86
SD (r) [mm]
0.11
0.20
0.52
1.14
4.30
Number of points
12,309
3095
791
203
7458
Boundary points
632
315
156
72
397
Detection time [ms]
2606
944
340
289
1892
X [% of bi]
25%
24%
27%
25%
26%
W [% of bi]
80%
113%
84%
157%
75%
88
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 9 ---

robust for out-of-plane noise lower than 25% of the cloud resolu-
tion and it can produce satisfactory results when the noise is lower
than circa 75%. With noise values between 25% and 75% of the
cloud resolution, the detection algorithm will miss some boundary
points but no outliers will be generated.
Fig. 9 compares, for the 16 mm average resolution point cloud,
polynomial ﬁtting and B-spline edges of 2nd and 3rd order with
the edges reconstructed through the new RBS approach (as
described within the thickest dashed perimeter in Fig. 6b). It is evi-
dent that the polynomial ﬁtting produces unsatisfactory edges,
especially for the internal boundaries. B-spline edges present
excessive wrinkling, thus poor smoothing.
Table 2 gives quantitative results on the performance of the RBS
algorithm, compared to 3rd order polynomial ﬁtting and B-splines,
for all point clouds in Fig. 7.
Although the execution time of the new FFT-based algorithm is
always
one
order
of
magnitude
higher
than
the
time
for
polynomial ﬁtting and for B-spline computation, the smooth edges
produced by the new approach ﬁt the surface contour better than
polynomial ﬁtting and B-splines. The table reports the mean, max-
imum and standard deviation (STD) values for the distances
between the boundary points and the smoothed edges and for
the distance between the reconstructed surface mesh and the
reconstructed edges. In average, the reconstructed edges computed
through the new approach ﬁt the boundary points 4.7 times better
than the 3rd order polynomial edges. Moreover, they follow the
reconstructed
surface
mesh
contour
77%
better
than
the
polynomial
ﬁtting
edges.
Although
the
B-splines
seem
to
follow the reconstructed mesh better than the new approach, 3rd
order B-splines do not produce signiﬁcant smoothing of the
Fig. 9. Comparison of reconstructed edges (red line) with polynomial ﬁtting and B-spline edges (black line) of 2nd and 3rd order.
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
89
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 10 ---

original boundary points; the distance between the B-splines
and the points being circa equal to 0 is not a sign of good
performance. The accuracy with which the polynomial and
B-spline edges ﬁt the boundary points and the reconstructed
surface can be improved by increasing the order of the functions,
but this impacts on the time required to compute the function
coefﬁcients. Moreover, high order polynomials often suffer from
severe ringing between the data points. The RBS approach
reconstructs the optimum edges without the need to specify any
parameter, unlike the function order in the polynomial and the
B-spline ﬁtting.
5. Conclusions
Tessellated surfaces generated from point clouds typically show
inaccurate and jagged boundaries. This can lead to tolerance errors
and problems such as machine judder if the model is used for
ongoing manufacturing applications. This is the reason why many
existing commercial computer-aided manufacturing (CAM) appli-
cations are not able to use tessellated models. This work presented
novel algorithms to reﬁne the boundary of meshed surfaces
obtained from 3D scanning point cloud data. The BPD algorithm
allows the unlabelled point of a surface point cloud to be grouped
into two groups: boundary points and internal points. Existing
detection techniques are optimized to detect points belonging to
sharp edges and creases. The BPD algorithm is targeted to the
detection of boundary points and it is able to do this better than
the existing methods. The RBS algorithm identiﬁes the boundary
corners and divides each closed boundary into the constituting
edges. Every edge is smoothed through spatial FFT-based ﬁltering.
A crucial advantage of the introduced algorithms is that they are
not based on any threshold values that can be suitable for some
point cloud but not suitable for others. The FFT-based edge recon-
struction eliminates the problem of deﬁning a speciﬁc polynomial
function order for optimum polynomial curve ﬁtting. The algo-
rithms were tested to analyse the results and measure the execu-
tion
time
for
point
clouds
generated
from
laser
scanned
measurements on a turbofan engine turbine blade with varying
numbers of member points. Through adding artiﬁcial noise it has
been demonstrated that the BPD algorithm is very robust for out-
of-plane noise lower than 25% of the cloud resolution and it can
produce satisfactory results when the noise is lower than circa
75%. With noise values between 25% and 75% of the cloud resolu-
tion, the detection algorithm will miss some boundary points but
no outliers will be generated. Quantitative results on the perfor-
mance of the RBS algorithm were also presented. The recon-
structed edges computed through the new approach ﬁt the
boundary points by a factor of 4.7 times better than polynomial
edges. Moreover, they follow the reconstructed surface mesh con-
tour with an improvement of 77% compared to the polynomial ﬁt-
ting edges.
Acknowledgements
This work is part of the Autonomous Inspection in Manufactur-
ing and Re-Manufacturing (AIMaReM) project, funded by the UK
Engineering
and
Physical
Science
Research
Council
(EPSRC)
through the grant EP/N018427/1. The authors also wish to thank
Dr. Maxim Morozov for acquiring the point cloud of the turbofan
engine turbine blade considered in this paper.
Appendix A. Supplementary material
Supplementary data associated with this article can be found, in
the online version, at https://doi.org/10.1016/j.jcde.2018.02.001.
References
Akkiraju, N., Edelsbrunner, H., Facello, M., Fu, P., Mücke, E., Varela, C. (1995). Alpha
shapes:
Deﬁnition and software.
In Proceedings
of the 1st international
computational geometry software workshop (p. 66)
Arlinghaus, S. (1994). Practical handbook of curve ﬁtting. CRC Press.
Barber, C. B., Dobkin, D. P., & Huhdanpaa, H. (1996). The quickhull algorithm for
convex hulls. ACM Transactions on Mathematical Software (TOMS), 22, 469–483.
Bazazian, D., Casas, J. R., Ruiz-Hidalgo, J. (2015). Fast and robust edge extraction in
unorganized
point
clouds.
In:
International
conference
on
digital
image
computing: Techniques and applications (DICTA) (pp. 1–8).
Beard, T. (1997). Machining from STL ﬁles. Modern Machine Shop, 69, 90–99.
Berger, M., Tagliasacchi, A., Seversky, L. M., Alliez, P., Guennebaud, G., Levine, J. A.,
et al. (2017). A survey of surface reconstruction from point clouds. Computer
Graphics Forum, 301–329.
Biggs, N. L. (1979). The roots of combinatorics. Historia Mathematica, 6, 109–136.
Calakli, F., & Taubin, G. (2011). SSD: Smooth signed distance surface reconstruction.
Computer Graphics Forum, 1993–2002.
Curless, B. (1999). From range scans to 3D models. ACM SIGGRAPH Computer
Graphics, 33, 38–41.
de Boor, C. (1968). On the convergence of odd-degree spline interpolation. Journal of
Approximation Theory, 1, 452–463.
Delaunay, B. (1934). ‘‘Sur la sphere vide,” Izv. Akad. Nauk SSSR, Otdelenie
Matematicheskii i Estestvennyka Nauk (Vol. 7, pp. 1–2).
Fleishman, S., Cohen-Or, D. & Silva, C. T. (2005). Robust moving least-squares ﬁtting
with sharp features. In ACM transactions on graphics (TOG), (Vol. 0, pp. 544–
552).
Gonzales, R. C., & Woods, R. E. (1992). Digital image processing. Reading, MA:
Addison & Wesley Publishing Company.
Gumhold, S., Wang, X., & MacLeod, R. S. (2001). Feature Extraction From Point
Clouds. In IMR.
Hinks, T., Carr, H., Truong-Hong, L., & Laefer, D. F. (2012). Point cloud data
conversion into solid models via point-based voxelization. Journal of Surveying
Engineering, 139, 72–83.
Johnson, D., & Williams, R. P. D. (1976). Methods of experimental physics:
Spectroscopy. New York: Academic Press.
Table 2
Performances of the RBS algorithm, compared to 3rd order polynomial ﬁtting.
Cloud
a
b
c
d
e
New method
Reconstruction time [ms]
528
305
143
71
385
Mean dist. to points [mm]
0.41
0.84
0.96
1.38
0.22
Max dist. to points [mm]
2.17
4.16
6.73
3.76
2.43
Mean dist. to mesh [mm]
0.07
0.25
0.95
1.20
0.73
Max dist. to mesh [mm]
0.80
1.68
4. 51
4.23
5.77
Pol. ﬁtting
Reconstruction time [ms]
20
19
17
15
19
Mean dist. to points [mm]
3.61
3.78
4.95
5.64
3.83
Max dist. to points [mm]
34.8
33.0
32.7
40.0
30.1
Mean dist. to mesh [mm]
1.00
0.89
1.08
1.43
1.27
Max dist. to mesh [mm]
7.03
5.98
6.56
7.70
12.5
B-Splines
Reconstruction time [ms]
102
60
39
25
69
Mean dist. to points [mm]
0
0
0
0
0
Max dist. to points [mm]
0
0
0
0
0
Mean dist. to mesh [mm]
0.05
0.11
0.24
0.46
0.45
Max dist. to mesh [mm]
0.78
1.12
1.35
3.08
3.30
90
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 11 ---

Kazhdan, M., & Hoppe, H. (2013). Screened Poisson surface reconstruction. ACM
Transactions on Graphics (TOG), 32, 29.
Kettner, L., Näher, S., Goodman, J. E., & O’Rourke, J. (2004). Two computational
geometry libraries: LEDA and CGAL. In Handbook of discrete and computational
geometry (pp. 1435–1463). Chapman & Hall/CRC.
Knott, G. D. (2012). Interpolating cubic splines (Vol. 18). Springer Science & Business
Media.
Levoy, M., & Whitted, T. (1985). The use of points as a display primitive. University of
North Carolina, Department of Computer Science.
Mineo, C., Pierce, S. G., Nicholson, P. I., & Cooper, I. (2017). Introducing a novel mesh
following technique for approximation-free robotic tool path trajectories.
Journal of Computational Design and Engineering, 4(3), 192–202.
Monchalin, J.-P., Neron, C., Bouchard, P., & Heon, R. (1998). Laser-ultrasonics for
inspection
and
characterization
of
aeronautic
materials.
Journal
of
Nondestructive Testing & Ultrasonics (Germany), 3, 002.
Ni, H., Lin, X., Ning, X., & Zhang, J. (2016). Edge detection and feature line tracing in
3d-point clouds by analyzing geometric properties of neighborhoods. Remote
Sensing, 8, 710.
Öztireli, A. C., Guennebaud, G., & Gross, M. (2009). Feature preserving point set
surfaces based on non-linear kernel regression. Computer Graphics Forum,
493–501.
Schrijver, A. (2005). On the history of combinatorial optimization (till 1960).
Handbooks in Operations Research and Management Science, 12, 1–68.
Truong-Hong, L., Laefer, D. F., Hinks, T., & Carr, H. (2011). Flying voxel method with
Delaunay triangulation criterion for façade/feature detection for computation.
Journal of Computing in Civil Engineering, 26, 691–707.
Turk, G. & Levoy, M. (1994). The Stanford bunny, the Stanford 3D scanning
repository.
Turk G, & Levoy, M. (1994). Zippered polygon meshes from range images. In:
Proceedings of the 21st annual conference on Computer graphics and interactive
techniques (pp. 311–318).
Weber, C., Hahmann, S. & Hagen, H. (2011). Methods for feature detection in point
clouds. In OASIcs-OpenAccess Series in Informatics.
Weinmann, M., Jutzi, B., Hinz, S., & Mallet, C. (2015). Semantic point cloud
interpretation based on optimal neighborhoods, relevant features and efﬁcient
classiﬁers. ISPRS Journal of Photogrammetry and Remote Sensing, 105, 286–304.
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023
```

---

## Section 1.27: Novel_algorithms_for_3D_surface_point_cl

Source File: Novel_algorithms_for_3D_surface_point_cl.txt

### Paper Content:

```
# Novel_algorithms_for_3D_surface_point_cl.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Novel_algorithms_for_3D_surface_point_cl.pdf
# Output: ../layer2_completion/txt/Novel_algorithms_for_3D_surface_point_cl.txt


--- Page 1 ---

Novel algorithms for 3D surface point cloud boundary detection
and edge reconstruction
Carmelo Mineo ⇑, Stephen Gareth Pierce, Rahul Summan
Department of Electronic and Electrical Engineering, University of Strathclyde, Royal College Building, 204 George Street, Glasgow G1 1XW, UK
a r t i c l e
i n f o
Article history:
Received 20 October 2017
Received in revised form 5 February 2018
Accepted 6 February 2018
Available online 7 February 2018
Keywords:
Point-cloud
Boundary detection
Edge reconstruction
a b s t r a c t
Tessellated surfaces generated from point clouds typically show inaccurate and jagged boundaries. This
can lead to tolerance errors and problems such as machine judder if the model is used for ongoing man-
ufacturing applications. This paper introduces a novel boundary point detection algorithm and spatial
FFT-based ﬁltering approach, which together allow for direct generation of low noise tessellated surfaces
from point cloud data, which are not based on pre-deﬁned threshold values. Existing detection tech-
niques are optimized to detect points belonging to sharp edges and creases. The new algorithm is tar-
geted at the detection of boundary points and it is able to do this better than the existing methods.
The FFT-based edge reconstruction eliminates the problem of deﬁning a speciﬁc polynomial function
order for optimum polynomial curve ﬁtting. The algorithms were tested to analyse the results and mea-
sure the execution time for point clouds generated from laser scanned measurements on a turbofan
engine turbine blade with varying numbers of member points. The reconstructed edges ﬁt the boundary
points with an improvement factor of 4.7 over a standard polynomial ﬁtting approach. Furthermore,
through adding artiﬁcial noise it has been demonstrated that the detection algorithm is very robust for
out-of-plane noise lower than 25% of the cloud resolution and it can produce satisfactory results when
the noise is lower than 75%.
 2018 Society for Computational Design and Engineering. Publishing Services by Elsevier. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
1. Introduction
Three-dimensional (3D) scanning is increasingly used to anal-
yse objects or environments in diverse applications including
industrial design, orthotics and prosthetics, gaming and ﬁlm pro-
duction, reverse engineering and prototyping, quality control and
documentation of cultural and architectural artefacts (Curless,
1999; Levoy & Whitted, 1985). Conventional reconstruction tech-
niques generate tessellated surfaces from point clouds. Such tessel-
lated models often show inaccurate and jagged boundaries that
can lead to tolerance errors and problems such as machine judder
if the models are used for ongoing manufacturing applications
(Beard, 1997). That is the reason why many existing commercial
computer-aided manufacturing (CAM) applications are not able
to use tessellated models, instead of using precise analytical CAD
models
where
the
surfaces
are
mathematically
represented
(Mineo, Pierce, Nicholson, & Cooper, 2017). Whilst the conversion
of analytical geometries into meshed surfaces is straightforward,
the reverse process of conversion of a tessellated model into an
analytical CAD model is challenging and time-consuming. There
are circumstances where the original CAD model of a component
is not available or deviates from the real part. New CAM software
applications, able to use clean tessellated models, are emerging
(Mineo et al., 2017); they enable the direct use of triangulated
point clouds obtainable through surface mapping techniques.
However, the point clouds obtained through surface mapping are
typically affected by noise. New algorithms for optimum surface
mesh reﬁnement are required to improve the performance of
emerging applications and to overcome the limitations of typical
approaches based on polynomial smoothing. Different technolo-
gies can be used to build coordinate measuring machines (CMM)
or 3D-scanning devices (Curless, 1999). Each technology comes
with its own limitations, advantages and costs. A common factor
for many CMM and 3D scanners is that they can measure the coor-
dinates of a large number of points on an object surface and output
a point cloud of the scanned area. However, point clouds are gen-
erally not directly usable in most 3D applications, and therefore are
usually converted to mesh models, NURBS surface models, or CAD
models (Berger et al., 2017; Hinks, Carr, Truong-Hong, & Laefer,
2012; Truong-Hong, Laefer, Hinks, & Carr, 2011). Tessellated mod-
els have emerged as a favoured technique; they are the easiest
https://doi.org/10.1016/j.jcde.2018.02.001
2288-4300/ 2018 Society for Computational Design and Engineering. Publishing Services by Elsevier.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Peer review under responsibility of Society for Computational Design and
Engineering.
⇑Corresponding author.
E-mail address: carmelo.mineo@strath.ac.uk (C. Mineo).
Journal of Computational Design and Engineering 6 (2019) 81–91
Contents lists available at ScienceDirect
Journal of Computational Design and Engineering
journal homepage: www.elsevier.com/locate/jcde
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 2 ---

form of virtual models obtainable from point clouds with minimal
processing. There are two different approaches to create a triangu-
lar meshed surface from a point cloud: using triangulation meth-
ods or surface reconstruction methods. Triangulation algorithms
use the original points of the input point cloud, using them as
the vertices of the mesh triangles. The Delaunay triangulation,
named after Boris Delaunay for his work on the topic from 1934
(Delaunay, 1934), is the most popular algorithm of this kind. A
bi-dimensional Delaunay triangulation ensures that the circumcir-
cle associated with each triangle contains no other point in its inte-
rior.
This
deﬁnition
extends
naturally
to
three
dimensions
considering spheres instead of circles. Surface reconstruction algo-
rithms differ from the triangulation method since they do not use
the original points as the vertices of the mesh triangles but com-
pute new points, whose density can vary according to the local cur-
vature of the 3D geometry. Surface reconstruction from oriented
points can be cast as a spatial problem, based on the Poisson’s
equation (Calakli & Taubin, 2011; Kazhdan & Hoppe, 2013).
Both approaches are not able to reconstruct the surface bound-
aries accurately, which makes the tessellated models unsuitable to
be used for CAM toolpath generation. Triangulation methods pro-
duce meshed surfaces with jagged boundaries since the original
noisy points of the cloud are used as vertices of the mesh triangles.
Reconstruction methods produce smooth boundaries, but they can
be quite far from the original boundaries of the real surface.
Indeed, Poisson’s surface reconstruction does not follow the
boundary of the point cloud and replaces the original points with
new points laying on a reconstructed continuous surface that sat-
isﬁes the Poisson’s differential equation.
The detection of cloud boundary points and the reconstruction
of smooth boundary edges would allow trimming of the recon-
structed tessellated models, to reﬁne the mesh boundaries. Very
few feature detection methods are optimized to work with point-
sampled geometries only. The major problem of these point based
methods is the lack of knowledge concerning point normal and
connectivity. This makes feature detection a more challenging task
than in mesh-based methods. Gumhold, Wang, and MacLeod
(2001) presented an algorithm that ﬁrst analyses the neighbour-
hood of each point via a principal component analysis (PCA). The
eigenvalues of the correlation matrix are then used to determine
if a point belongs to a feature. This technique for the detection of
features in point clouds is used as a pre-processing step for tessel-
lated
surface
reconstruction
with
sharp
features
(Weber,
Hahmann, & Hagen, 2011). There exist also several reconstruction
methods that preserve sharp features during the surface recon-
struction of a point cloud without pre-processing; for example,
the methods shown by Fleishman, Cohen-Or, and Silva (2005)
and Öztireli, Guennebaud, and Gross (2009).
The existing techniques mentioned above are optimized to
detect points belonging to sharp edges. This paper presents novel
algorithms targeted to the detection of boundary points and the
deterministic reconstruction of accurate and smooth surface
boundaries from 3D point clouds. A smart approach known as
Mesh Following Technique (MFT) (Mineo et al., 2017), for the gen-
eration of robot tool-paths from STL models, has recently been
published. The technique requires virtual tessellated surfaces with
smooth boundary edges.
The algorithms presented in this paper are useful tools to reﬁne
the boundary of tessellated surfaces obtained from 3D scanning
point cloud data. They can be used to trim Delaunay triangulation
or Poisson’s reconstructed surface meshes, facilitating the direct
use of tessellated models, instead of analytical geometries. The
remainder part of the paper describes the algorithms and shows
qualitative and quantitative results, discussing advantages and
disadvantages.
2. Detection of boundary points
Given a mapped surface in the form of a point cloud, the iden-
tiﬁcation of the point cloud borderline, thus the detection of
boundary points, is not a trivial task. The human brain is able to
infer the border of a point cloud by simply looking at the arrange-
ment of the sparse points. In computer science and computational
geometry, a point cloud is an entity without a well-deﬁned bound-
ary. In the bi-dimensional domain, given a ﬁnite set of points, the
problem of detecting the smallest convex polygon that contains all
the given points of the cloud is solved through the quickhull
method (Barber, Dobkin, & Huhdanpaa, 1996). It uses a divide
and conquer approach. This method works well but is only able
to detect the boundary points that are part of the convex polygon
and is only for a set of points. A generalization of quickhull, able to
handle concave regions and holes in the point cloud, is the alpha-
shape approach (Akkiraju et al., 1995). The Computational Geome-
try Algorithms Library (CGAL) (Kettner, Näher, Goodman and
O’Rourke 2004) has a robust implementation of alpha-shape for
2D and 3D point clouds. For each real number a, the approach is
based on the generalized disk of radius 1=a. An edge of the polygon
that contains all the given points (alpha-shape) is drawn between
two members of the ﬁnite point set whenever there exists a gener-
alized disk of radius 1=a containing the entire point set and which
has the property that the two points lie on its boundary. If a ¼ 0,
then the alpha-shape associated with the ﬁnite point set is its ordi-
nary convex hull given by quickhull. The limitation of the alpha-
shape approach is that its performance depends on the set value
of the parameter a. A value of a that produces a satisfactory result
for a point cloud may not be suitable for other point sets since
point clouds can exhibit different point densities. This inconve-
nience is similar to what happens when obtaining a black and
white picture from a grayscale image, through thresholding the
pixel intensities; the optimal threshold value is affected by the
average brightness of the image. Moreover, when the point density
of a point cloud varies between across the cloud, the alpha-shape
result can be satisfactory in some regions and poor in others.
Non-parametric edge extraction methods based on kernel regres-
sion
(Öztireli
et
al.,
2009)
and
on
analysis
of
eigenvalues
(Bazazian, Casas, & Ruiz-Hidalgo, 2015) have been proposed in
recent years. Such methods, however, are optimized for the detec-
tion of internal sharp edges, rather than detecting the point cloud
borderline.
The boundary point detection algorithm presented in this
paper, herein referred as BPD algorithm, does not need the deﬁni-
tion of any threshold values. For every region of the cloud, it
detects as many boundary points as possible, given the local reso-
lution of the region. A 3D-point cloud is unorganized and the
neighbourhood of a point is more complex than that of a pixel in
an image. Generally, in 3D-point clouds, there are three types of
neighbourhoods: spherical neighbourhood, cylindrical neighbour-
hood,
and
k-nearest
neighbours
based
neighbourhood
(Weinmann, Jutzi, Hinz, & Mallet, 2015). The three types of neigh-
bourhoods are based on different search methods. Given a point P,
a spherical neighbourhood is formed by all 3D points in a sphere of
ﬁxed radius around P. A cylindrical neighbourhood is formed by all
those 3D points whose 2D projections onto a plane (e.g. the ground
plane) are within a circle of ﬁxed radius around the projection of P.
The k-nearest neighbourhood (k-NN) search method is non-
parametric and it is used in this work, since it does not need the
deﬁnition of a radius value; it ﬁnds the closest k-members of the
cloud. Fig. 1 shows a point cloud with the k-nearest neighbourhood
of 5 points (A to E), where k is set to 30. The points of the cloud
belonging to the neighbourhoods are indicated through ﬁlled
circles. The other points of the cloud are represented with empty
82
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 3 ---

circles. The semi-transparent circles, centred at the points from A
to E, highlight the best ﬁt planes for each of the neighbourhoods.
The normal directions of such planes are also shown through
arrows pointing outwards from the neighbourhood parent points.
The local cloud resolution for every member of the cloud is esti-
mated through the following steps. Given a point of the cloud Pi,
for every point of its neighbourhood (Pi;j), the minimum distance
(dj;k) between that point and all other neighbours is computed.
The local point cloud resolution (bi) in Pi
is estimated as
bi ¼ li þ 2ri, where li is the mean value of the minimum distances
and ri is their standard deviation. This method of computing the
local resolution is robust, overcoming the problematic noisy nature
of some point clouds collected through optical and photogrammet-
ric method. If the distance values (dj;k) are distributed according to
a Gaussian distribution, the addition of 2ri to the mean value
ensures that 97.6% of the data values are considered and the major
outliers are ignored.
The alpha-shape method may not be able to detect some sec-
tions of the point cloud boundary with high concave curvature if
a is set too low. On the other hand, it may detect unwanted bound-
ary points, if a is too high. The new method described here is cap-
able of detecting boundary points belonging to convex and concave
regions.
The BPD method exploits the fact that there is one and only one
circle that passes through three given points in the 3D space. Given
the unclassiﬁed point Pi, the centre and the radius of all circles that
pass for Pi and any two other points of its neighbourhood are com-
puted. The point is labelled as a boundary point if there is at least
one circle with radius equal or bigger than bi and if the sphere for
Pi, whose centre coincides with the centre of the circle, does not
contain any other point of the neighbourhood. This is the case for
point A and B, shown in Fig. 1. Point A and B are two boundary
points, since it is possible to ﬁnd at least one circle that satisﬁes
the above conditions (Fig. 2). In order to avoid unnecessary compu-
tational efforts and yet consider all possible circles, all the originat-
ing point triples are identiﬁed through the binomial coefﬁcient
(Biggs, 1979). Working with k-nearest neighbourhoods and being
the investigated point Pi always part of the triples, the remaining
points (k-1) are combined in couples with no repetitions. Thus,
the total number of circles is equal to:
n ¼
ðk  1Þ!
2!  ½ðk  1Þ  2! ¼
29!
2  27! ¼ 29  28
2
¼ 406
ð1Þ
Although the presence of at least one circle that satisﬁes the
above conditions allows to classify the investigated point as
belonging to the point cloud boundary, its absence cannot be used
to state that the point is an internal point of the cloud. Indeed the
investigated point Pi can be located on a convex region of the
boundary as well as being an internal point of the cloud. In such
circumstances, although it may exist one circle with radius equal
or bigger than bi, the sphere for Pi, centred at the centre of the cir-
cle, will always contain some points of the neighbourhood. This is
the case for the points C, D and E shown in Fig. 1. For such kind of
points, thus when the point cannot be labelled as a boundary point
through the ﬁrst part of the detection algorithm described above,
the algorithm continues with further operations. Each investigated
point and its neighbours are projected to the best ﬁt plane accord-
ing to the normal vector associated with the point. The resulting
bi-dimensional neighbourhood cloud can be plotted in polar coor-
dinates, with Pi at the pole of the plot. Fig. 3 shows the polar plots
for point C, D and E and their neighbours. Pi is shown in red and its
neighbour points are shown in blue. The blue and red dotted line of
the plots in Fig. 3 highlight, respectively, the minimum and maxi-
mum angle of the angular sector spanned by the points in the
neighbourhood.
The fundamental idea behind the ﬁnal step of the BPD algo-
rithm is that the point of the cloud Pi is a boundary point if it is
not possible to ﬁnd a path that surrounds it and passes through
the neighbour points. Each point on the polar plot is determined
by the distance from the pole (radial coordinate, R) and the angular
coordinate (h). Given a neighbourhood, the developed algorithm
creates a path that surrounds the parent point at the pole. An incre-
mental approach is used. All radial and angular coordinates of the
neighbours are normalized so that the coordinates of the j-th
neighbour point are:
rj ¼ Rj  Rmin
Rmax  Rmin
ð2Þ
#j ¼ hj  hmin
hmax  hmin
ð3Þ
Fig. 2. A and B labelled as boundary points.
Fig. 1. Point cloud with examples of two boundary points on concave regions (A
and B), an inner point (C) and two boundary points on convex regions (D and E).
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
83
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 4 ---

where Rmin, Rmax and hmin, hmax are respectively the minimum and
maximum values of the radial and angular coordinates. In order
to select the starting point of the surrounding path, a characterizing
parameter (c) is given to each neighbour, such that:
cj ¼ rj þ j#j  #bisj
ð4Þ
where #bis is the normalized angle of the direction bisecting the
angular gap comprising all neighbours (between hmin and hmax).
The neighbour point with the minimum value of c, as computed
in Eq. (4), is selected as starting point for the surrounding path.
From the starting point the algorithm progresses through linking
the other points of the neighbourhood. The crossed neighbours
are removed from the list of available points. The characterizing
parameter of the generic p-th remaining point is computed as:
cp ¼ rp þ ½ð#p  #lastÞ  c
ð5Þ
where #last is the normalized angular coordinate of the last crossed
point and c is a factor equal to -1 if sgnð#p  #lastÞ = sgnð#last  #last1Þ
or equal to 1 otherwise. The factor c facilitates the selection of a
point that does not force a change of surrounding direction. For
example, if the last selected point produced a clockwise rotation,
any anti-clockwise rotation is penalised. The neighbour with the
minimum value of c, as computed in Eq. (5), becomes the new last
point of the path.
The incremental linkage of the neighbours keeps track of the
spanned angle around the pole. The sum of the angle increments
considered with their sign (a ¼ P Da) and the sum of their
absolute values (s ¼ P jDaj) is updated each time a new point is
selected after the starting point. The incremental linkage stops
when there are no more linkable neighbour points or when
s > 2p. If jaj P 2p the investigated point is an internal point of
the cloud (e.g. point C in Fig. 3), otherwise it is a boundary point
(e.g. D and E in Fig. 3). The stopping condition of the incremental
algorithm (s > 2p) avoids superﬂuous computation efforts, since
it is often possible to infer if a point belongs to the boundary with-
out linking all neighbours. The solid line in Fig. 3 shows the sur-
rounding path created until s > 2p. For example, it is possible to
determine that C does not belong to the boundary, linking only
10 out of 29 neighbours, since jaj > 2p. It is possible to state that
D is a boundary point, through linking 18 out of 29 neighbours;
it results jaj < 2p, although the sum of the absolute angle incre-
ments is s > 2p. In summary, the BPD method described here is
capable of determining if a point Pi belongs to a concave boundary
section (e.g. point A in Fig. 1) when the local curvature is as high as
1=bi. The method is also able to infer if a point belongs to the
boundary of a hole in the point cloud (e.g. point B in Fig. 1) when
the hole radius is as small as bi. Since bi is the local resolution, the
method works well on point clouds with variable point density.
3. Edge reconstruction
The application of the BPD algorithm to the point cloud sample
shown in Fig. 1 ﬁnds all the boundary points, represented as empty
circles in Fig. 4. Also, the 4 boundary points of the internal circular
hole, whose radius is just above the local point cloud resolution,
are detected as expected. The detected points need to be clustered
so that points belonging to the same boundary are grouped
together. Moreover, the points of every cluster need to be ordered
correctly. These tasks can be fulﬁlled through existing algorithms.
For example, the clustered points can be ordered through algo-
rithms capable of solving the so-called travelling salesman problem
(TSP). Given a random list of points and the distances between
each pair of points, the solution of the TSP is the shortest possible
path that crosses each point exactly once and returns to the origin
point (Schrijver, 2005). Therefore a closed boundary path is
obtained from every cluster. These boundaries, given by the
ordered points linked through line segments (see dashed lines in
Fig. 4) can be quite jagged in some areas. Therefore, it is evident
that such boundaries are not suitable to trim the surface meshes
obtainable from the point cloud. The boundary curves need
smoothing to better resemble the real surface borderlines. This
section of the paper introduces a novel raw boundary smoothing
algorithm, herein referred as RBS algorithm, to improve the recon-
struction of surface point cloud borderlines through accurate
smoothing of the raw boundaries.
Fig. 3. Polar plot of point C, D and E and their neighbours. The solid and dashed lines illustrate the application of the algorithm.
Fig. 4. Closed boundaries partitioned into edges, obtained through clustering and
ordering of the detected boundary points (jagged dashed line) and reconstructed
edges (solid lines).
84
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 5 ---

3.1. Detection of key points
Before applying any smoothing algorithm to the closed loop
boundary curves, it is necessary to highlight that the position of
some of the detected boundary points should be preserved. This
is the case for the borderline corners, where there is a sharp change
in the boundary directionality. Indeed, such points usually play a
crucial role in the deﬁnition of reference systems in CAM applica-
tions for the development of accurate operations, where the cor-
rect registration of the part virtual model is required. Therefore,
the ﬁrst step of the edge reconstruction algorithm is targeted to
detect such key points. Given the i-th point of a closed boundary
path, the point is labelled as corner if the radius of the circle for
the investigated point, the precedent and the successive point is
smaller than the value of the local point cloud resolution, bi (as cal-
culated in the detection algorithm). This approach is able to iden-
tify the corner points from P1 to P5, highlighted with asterisks in
Fig. 4. It is worth noticing here that, although the corners can be
found through applying a threshold value to the angle between
the two borderline segments for each investigated point (Ni, Lin,
Ning, & Zhang, 2016), this approach is advantageous and it is suit-
able to work with point clouds with variable density. The identiﬁed
corners are used to divide the closed loop boundary into sections,
corresponding to the edges of the surface geometry. The external
boundary points of the cloud in Fig. 4 are grouped in 5 edges.
The 4 internal boundary points are grouped in one single closed
loop edge since no corners are found there.
3.2. Limitation of traditional curve smoothing methods
Each edge could be smoothed through ﬁtting a polynomial
curve. Polynomial curve ﬁtting is a common smoothing method
and the functionality is also implemented in CAD/CAM commercial
software applications (e.g. Rhinoceros). Curve ﬁtting is the pro-
cess of approximating a pattern of points with a mathematical
function (Arlinghaus, 1994). Fitted curves can be used to infer
the values of a function where no data are available (Johnson &
Williams, 1976) (e.g. in the gaps between sampled points). The
goal of curve ﬁtting is to model the expected value of a dependent
variable y in terms of the value of an independent variable (or vec-
tor of independent variables) x. In general, the expected value of y
can be modelled as an nth degree polynomial function, yielding the
general polynomial regression model based on the truncated Tay-
lor’s series:
y ¼ a0 þ a1x þ a2x2 þ . . . þ anxn þ e
ð6Þ
where e is a random error with null mean. Given the points of an
edge and the order of the target polynomial function, it is possible
to compute the coefﬁcients of the ﬁtting function. The limitation
of this smoothing approach is that the order of the target function
is typically unknown and curve ﬁtting remains a time-consuming
iterative trial and error process for edge reconstruction. When there
is no theoretical basis for choosing the order of the ﬁtting polyno-
mial function, the edges may be ﬁtted with a spline function com-
posed of a sum of B-splines (Knott, 2012). The places where the
B-splines meet are known as knots. The main difﬁculty in applying
this process is in determining the number of knots to use and where
they should be placed (de Boor, 1968).
3.3. FFT-based reconstruction
The ﬁnal step of the RBS algorithm, presented in this paper,
introduces a robust approach based on the Fast Fourier Transform
(FFT). The FFT is a well-known way to translate the information
contained in a waveform from the time domain to frequency
domain. It is used for the spectral analysis of time-series and
allows the application of high or low-pass ﬁlters, to respectively
attenuate low or high frequencies. Here the FFT is applied to the
pattern of the edge point Cartesian coordinates, to enable the
application of low-pass ﬁlters able to improve the smoothness of
the boundary edges. The exploitation of FFT to spatial patterns
(waveforms sampled in the Cartesian domain rather than the time
domain) is not new (e.g. it has been used for image processing)
(Gonzales & Woods, 1992). However, there is no record of the
FFT being applied to the problem of surface edge reconstruction.
The nuances of the adaptation of FFT to this problem are described
herein.
Consider a series xðkÞ with N samples. Furthermore, assume that
the series outside the range between 0 and N-1 is extended N-peri-
odic, which is xðkÞ ¼ xðk þ NÞ for all k. The FFT of this series will be
denoted XðkÞ, it will also have N samples. The FFT transform
implies speciﬁc relationships between the series index and the fre-
quency domain sample index. For the common case, where the FFT
is applied to series representing a time sequence of length T, the
samples in the frequency domain are spaced by f s ¼ 1=T. The ﬁrst
sample Xð0Þ of the transformed series is the average of the input
series. The frequency sample corresponding to f Ny ¼ N=2T is called
Nyquist frequency. This is the highest frequency component that
should exist in the input series for the FFT to yield uncorrupted
results. More speciﬁcally if there are no frequencies above Nyquist
the original pattern can be exactly reconstructed from the samples
in the frequency domain. For the spatial problem of edge recon-
struction, given the FFT is applied to the edge Cartesian component
pattern, plotted as function of the curvilinear distance (d), the spa-
tial frequency is a measure of how often sinusoidal components (as
determined by the FFT) repeat per unit of distance. The spatial fre-
quency domain representation of any Cartesian component of a
circular edge with radius (R) and length (2pR) contains only one
frequency component f c ¼ 1=2pR. Therefore it is possible to
deduce that, denoting the local curvature radius at the i-th point
of the edge with Ri, the main spatial frequency occurring at that
point is equal to f i ¼ 1=2pRi. According to the Nyquist theorem,
when sampling an analogue signal in the time domain, the sam-
pling rate must be at least equal to 2f max, where f max is the highest
frequency component. The Nyquist rule applied to the spatial
domain means that bi (the local point cloud resolution) limits the
minimum edge radius that is possible to reconstruct at the i-th
point. The smallest radius that is possible to reconstruct will be
the one associated to a circumference of length 2pbi sampled with
2
points,
corresponding
to
the
spatial
frequency
f
 ¼ 2=2pbi ¼ 1=pbi. The maximum alias-free spatial frequency
component will be:
f max ¼ f
=2 ¼ 1=2pbi:
ð7Þ
The smallest edge radius of curvature that is possible to recon-
struct at the i-th point will be equal to Rmax
i
¼ bi.
The
plots
in
Fig.
5a
and
5b
regard,
respectively,
the
x-component pattern of the edge between P2 and P3 and of the
closed loop internal hole edge of the cloud in Fig. 4. The patterns
are plotted as functions of the normalized curvilinear distance of
the edge (d
 ¼ d=D, with D being the total length of the edge).
The original patterns, given by the dashed line that goes through
the x-component samples (shown through round circles), are quite
jagged. Fig. 5 clariﬁes how a periodic waveform is obtained from
the original pattern of each Cartesian component of a given edge.
The pattern is ﬁrst translated along the direction of the ordinate
axis to move the ﬁrst point of the pattern to the origin of the plot.
The pattern is then rotated by the angle a to move the last point of
the pattern on the horizontal axis. A copy of the resulting pattern is
inverted, ﬂipped and appended to the end extremity; it constitutes
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
85
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 6 ---

a complementary portion creating a period with the translated and
rotated version of the original pattern (Fig. 5a). Since the FFT
assumes a constant sampling rate of the input pattern, the original
randomly spaced samples are replaced with interpolated equally
spaced samples. The number of interpolated samples (Np) is chosen
appropriately to give a constant sampling interval (dsÞ, equal or
smaller than the minimum original sampling distance. In order
to ensure a good ﬁltering performance, it is necessary to have suf-
ﬁcient spatial frequency resolution. For such reason the period is
repeated to get a minimum of 1000 samples in the input waveform
of the FFT, giving a frequency resolution of f s ¼ 1=ð1000  dsÞ.
Therefore a low-pass ﬁlter is applied, cancelling all spatial fre-
quency components higher than f max, as expressed in Eq. (7). This
produces a smoothed waveform for the edge component. The FFT
input waveform shown in Fig. 5a, artiﬁcially constructed to ﬁlter
the x-component of the open edge comprised between P2 and P3,
has a null mean value. The original edge extremities lie on the hor-
izontal axis (the mean value line) and are not affected by the low-
pass ﬁltering. The Cartesian component values of the extremities
are preserved. The ﬁrst part of the waveform (the portion up to
the total length D of the original pattern) is rotated by negative a
and translated back to the original position. The smoothed bound-
ary is obtained through ﬁltering all the Cartesian components of all
its edges. The preservation of the original edge extremity points
makes sure that, when a boundary consists of multiple edges,
two consecutive edges share a common point. Therefore, the chain
of edges forms a closed boundary. If a boundary is formed by only
one closed edge, like in the case of the internal hole boundary in
Fig. 4, the extremities of each of its Cartesian components have
the same value (a ¼ 0). Moreover, the complementary portion to
construct the period is a mere horizontally translated copy of the
Cartesian component pattern (after its extremities are brought to
the horizontal axis). The copy of the original pattern is not inverted
nor ﬂipped to create the complementary portion (Fig. 5b). All
points of the closed edge are affected by the ﬁltering. The
smoothed edges relative to the boundaries of the sample point
cloud are shown through solid line curves in Fig. 4.
4. Results and performances
This section of the paper analyses the results obtainable
through the use of the introduced BPD and RBS algorithms. The
computational performances are also examined and quantitative
ﬁgures are reported. Fig. 6 shows a schematic summary of the algo-
rithm steps. The thick dashed line perimeters contain the novel
algorithm components introduced by this paper. The BPD algo-
rithm (Fig. 6a) allows the unlabelled point of a surface point cloud
to be grouped into two groups: boundary points and internal
points. The boundary points are clustered and ordered, through
existing algorithms, to constitute raw closed boundaries (jagged).
The RBS algorithm (Fig. 6b) identiﬁes the boundary corners and
divides each closed boundary into the constituting edges. Every
edge is smoothed through spatial FFT-based ﬁltering. A crucial
advantage of the introduced algorithms is that they are not based
on any threshold values that can be suitable for some point cloud
but not suitable for others. The BPD is capable of labelling all
points, observing the local resolution of the cloud for each point.
The FFT-based edge reconstruction eliminates the problem of
deﬁning a speciﬁc polynomial function order for optimum polyno-
mial curve ﬁtting. In the approach introduced in this paper, the
best edge smoothing performance is also ensured through applying
a spatial low-pass ﬁltering with a frequency deﬁned at every
boundary point as a function of the local cloud resolution.
In order to show the potentialities of the new algorithms, an air-
craft turbine engine fan blade, 640 mm long and 300 mm wide (in
average) was scanned through a coordinate measuring machine.
The FARO Quantum Arm was used in conjunction with a laser pro-
ﬁle mapping probe (Monchalin, Neron, Bouchard, & Heon, 1998).
This 3D scanning equipment has a volumetric maximum deviation
of ±74 lm. The blade surface was scanned to obtain a uniform
point
cloud
with
circa
14
thousand
points
(approximately
72 points per square centimetre).
The cloud points were decimated to obtain four different point
cloud versions, with target resolution respectively equal to 4, 8, 16
and 34 mm. An additional point cloud was generated with variable
point resolution, between 2 and 34 mm. Such generated point
clouds allowed testing the algorithms under controlled situations
and facilitated the analysis of the results. In order to introduce
well-deﬁned internal boundaries, the points found within three
spheres centred at ﬁxed positions and with radii equal to 16, 32
and 64 mm were removed from the clouds. Therefore, each cloud
presents three holes (H1, H2, and H3), with radii approximatively
equal to the original generating spheres. The resulting ﬁve point
clouds are shown by the top row plots in Fig. 7. These plots show
the detected boundary points through darker round point marks.
The external and internal boundary points are detected as
expected. The smallest radius of the hole detectable in a point
cloud depends on the cloud resolution, as described in Section 2.
Only four points of the 16 mm and 32 mm radius holes (H1 and
H2) are detected in the clouds of Fig. 7c and 7d, since the resolution
of these clouds is close to their radii. H1 cannot be detected on the
34 mm resolution cloud (Fig. 7d).
Fig. 8 shows the boundary points detected in the Stanford
bunny (Turk & Levoy, 1994a), an open source computer graphics
test meshed model, obtained through range scanning (Turk &
Levoy, 1994b). Similarly to how many researchers have used this
model, as input for surface reconstruction algorithms, the mesh
connectivity has been stripped away and the vertices have been
treated as an unorganized point cloud in this work. It presents
35,947 points and has an average resolution of 1.2 mm. Boundary
Fig. 5. Creation of periodic waveform for application of spatial FFT to the x-
component pattern of an open edge (a) and a closed edge (b).
86
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 7 ---

Fig. 6. Schematic summary of the algorithm steps: boundary point detection (a) and edge reconstruction (b).
Fig. 7. Detected points (top row) and reconstructed boundary edges (bottom row) for point clouds with ﬁxed resolution of 4 (a), 8 (b), 16 (c) and 34 mm (d) and with variable
resolution between 2 and 34 mm (e).
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
87
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 8 ---

points were detected using a method based on principal compo-
nent analysis (PCA) (Gumhold et al., 2001) (Fig. 8a) and the new
detection algorithm (Fig. 8b). To appreciate the differences in per-
formance it is important to differentiate boundary points and edge
points; the latter points are located in areas where there is a sharp
ripple (like in the bunny ears). The PCA method detected points
that are either on sharp crease lines or on the borderline of the
point cloud holes. The BPD algorithm is targeted to exclusively ﬁnd
borderline points; therefore it found two holes on the base of the
bunny, originating from the original clay model (it was a hollow
model). It should be noted that the BPD algorithm should not
detect the extra edge and crease points detected by PCA (e.g. in
the ears of the bunny). It is designed to detect boundary points
(around holes or areas where the point density drops drastically),
so that a boundary edge can be reconstructed and the tessellated
surface can be trimmed. The BPD algorithm was capable of detect-
ing four additional groups of boundary points (one group under the
bunny chest, one group between the bunny front legs and two
groups on the base), corresponding to the borderline of areas with
poor range scanning coverage. The smallest two of these areas
were not detected by the PCA method, showing how the BPD algo-
rithm is more accurate for the detection of borderline points.
The bottom row plots of Fig. 7 shows the reconstructed bound-
ary edges. The effectiveness of the FFT-based ﬁltering algorithm is
evident observing the smoothness of the edges. The smoothed
boundaries of the internal holes, given by single closed edges,
faithfully reproduces the roundness of the theoretical intersection
between the original generating sphere and the blade surface. Only
the boundaries of H1 and H2, reconstructed through only four
detected points, show visible distortion. Although explaining how
mesh trimming works is out of the scope of this paper, the recon-
structed boundary edges can be used to trim the Poisson mesh,
producing the clean boundary meshes highlighted in Fig. 7.
The algorithms were tested, using a computing machine based
on an Intel Core(TM) i7-6820HQ CPU (2.70 GHz), with 32 Gb of
RAM. The tests were carried out through MATLAB 2016a, running
on the Windows 10 64-bit operating system. Table 1 reports quan-
titative outcomes obtained from the application of the BPD algo-
rithm to the point clouds given in Fig. 7. The initial rows of the
table report the minimum, the mean, the maximum cloud resolu-
tion (b) and its standard deviation (r), followed by the number of
points in the clouds. Therefore the table gives the number of
boundary points detected by the detection algorithm and the time
taken (in milliseconds [ms]) for its complete execution. The
elapsed time is always lower than 3 s for all examined point
clouds.
The point clouds obtained through some 3D scanning methods
are affected by out-of-plane noise, meaning that the sampled
points deviate from their ideal version lying on the surface. It is
important to estimate how much the proposed detection algorithm
is tolerant to such noise. Therefore the detection algorithm was
repeatedly applied to versions of the point clouds with increasing
random noise added to the original points. The noisy clouds were
artiﬁcially obtained by moving each point along the normal direc-
tion of the local k-neighbourhood best ﬁt plane. Each point was
moved by distances equal to a percentage of the local cloud reso-
lution. Starting from 0%, the noise percentage was increased by
1% at every repetition of the detection algorithm. The maximum
percentage value after which at least one of the boundary points
is not detected (false negative) is denoted as X. The value of X is
around 25% for all the clouds of Fig. 7. If the noise percentage is
increased above X, some internal points of the cloud may be
labelled as boundary points (false positives). The maximum per-
centage value after which at least one of the internal points is
labelled as boundary point is denoted as W. The values of W in
Table 1 are all above 75%. Therefore, the detection algorithm is very
Fig. 8. Detected points on the Stanford bunny (Turk & Levoy, 1994a), with a method based on principal component analysis (Gumhold et al., 2001) (a) and with the BPD
algorithm (b).
Table 1
Performances obtained from the application of the BPD algorithm.
Cloud
a
b
c
d
e
Min b [mm]
3.82
7.77
15.53
32.29
1.98
Mean b [mm]
4.19
8.44
16.87
34.52
3.71
Max b [mm]
4.85
9.36
18.98
40.42
34.86
SD (r) [mm]
0.11
0.20
0.52
1.14
4.30
Number of points
12,309
3095
791
203
7458
Boundary points
632
315
156
72
397
Detection time [ms]
2606
944
340
289
1892
X [% of bi]
25%
24%
27%
25%
26%
W [% of bi]
80%
113%
84%
157%
75%
88
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 9 ---

robust for out-of-plane noise lower than 25% of the cloud resolu-
tion and it can produce satisfactory results when the noise is lower
than circa 75%. With noise values between 25% and 75% of the
cloud resolution, the detection algorithm will miss some boundary
points but no outliers will be generated.
Fig. 9 compares, for the 16 mm average resolution point cloud,
polynomial ﬁtting and B-spline edges of 2nd and 3rd order with
the edges reconstructed through the new RBS approach (as
described within the thickest dashed perimeter in Fig. 6b). It is evi-
dent that the polynomial ﬁtting produces unsatisfactory edges,
especially for the internal boundaries. B-spline edges present
excessive wrinkling, thus poor smoothing.
Table 2 gives quantitative results on the performance of the RBS
algorithm, compared to 3rd order polynomial ﬁtting and B-splines,
for all point clouds in Fig. 7.
Although the execution time of the new FFT-based algorithm is
always
one
order
of
magnitude
higher
than
the
time
for
polynomial ﬁtting and for B-spline computation, the smooth edges
produced by the new approach ﬁt the surface contour better than
polynomial ﬁtting and B-splines. The table reports the mean, max-
imum and standard deviation (STD) values for the distances
between the boundary points and the smoothed edges and for
the distance between the reconstructed surface mesh and the
reconstructed edges. In average, the reconstructed edges computed
through the new approach ﬁt the boundary points 4.7 times better
than the 3rd order polynomial edges. Moreover, they follow the
reconstructed
surface
mesh
contour
77%
better
than
the
polynomial
ﬁtting
edges.
Although
the
B-splines
seem
to
follow the reconstructed mesh better than the new approach, 3rd
order B-splines do not produce signiﬁcant smoothing of the
Fig. 9. Comparison of reconstructed edges (red line) with polynomial ﬁtting and B-spline edges (black line) of 2nd and 3rd order.
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
89
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 10 ---

original boundary points; the distance between the B-splines
and the points being circa equal to 0 is not a sign of good
performance. The accuracy with which the polynomial and
B-spline edges ﬁt the boundary points and the reconstructed
surface can be improved by increasing the order of the functions,
but this impacts on the time required to compute the function
coefﬁcients. Moreover, high order polynomials often suffer from
severe ringing between the data points. The RBS approach
reconstructs the optimum edges without the need to specify any
parameter, unlike the function order in the polynomial and the
B-spline ﬁtting.
5. Conclusions
Tessellated surfaces generated from point clouds typically show
inaccurate and jagged boundaries. This can lead to tolerance errors
and problems such as machine judder if the model is used for
ongoing manufacturing applications. This is the reason why many
existing commercial computer-aided manufacturing (CAM) appli-
cations are not able to use tessellated models. This work presented
novel algorithms to reﬁne the boundary of meshed surfaces
obtained from 3D scanning point cloud data. The BPD algorithm
allows the unlabelled point of a surface point cloud to be grouped
into two groups: boundary points and internal points. Existing
detection techniques are optimized to detect points belonging to
sharp edges and creases. The BPD algorithm is targeted to the
detection of boundary points and it is able to do this better than
the existing methods. The RBS algorithm identiﬁes the boundary
corners and divides each closed boundary into the constituting
edges. Every edge is smoothed through spatial FFT-based ﬁltering.
A crucial advantage of the introduced algorithms is that they are
not based on any threshold values that can be suitable for some
point cloud but not suitable for others. The FFT-based edge recon-
struction eliminates the problem of deﬁning a speciﬁc polynomial
function order for optimum polynomial curve ﬁtting. The algo-
rithms were tested to analyse the results and measure the execu-
tion
time
for
point
clouds
generated
from
laser
scanned
measurements on a turbofan engine turbine blade with varying
numbers of member points. Through adding artiﬁcial noise it has
been demonstrated that the BPD algorithm is very robust for out-
of-plane noise lower than 25% of the cloud resolution and it can
produce satisfactory results when the noise is lower than circa
75%. With noise values between 25% and 75% of the cloud resolu-
tion, the detection algorithm will miss some boundary points but
no outliers will be generated. Quantitative results on the perfor-
mance of the RBS algorithm were also presented. The recon-
structed edges computed through the new approach ﬁt the
boundary points by a factor of 4.7 times better than polynomial
edges. Moreover, they follow the reconstructed surface mesh con-
tour with an improvement of 77% compared to the polynomial ﬁt-
ting edges.
Acknowledgements
This work is part of the Autonomous Inspection in Manufactur-
ing and Re-Manufacturing (AIMaReM) project, funded by the UK
Engineering
and
Physical
Science
Research
Council
(EPSRC)
through the grant EP/N018427/1. The authors also wish to thank
Dr. Maxim Morozov for acquiring the point cloud of the turbofan
engine turbine blade considered in this paper.
Appendix A. Supplementary material
Supplementary data associated with this article can be found, in
the online version, at https://doi.org/10.1016/j.jcde.2018.02.001.
References
Akkiraju, N., Edelsbrunner, H., Facello, M., Fu, P., Mücke, E., Varela, C. (1995). Alpha
shapes:
Deﬁnition and software.
In Proceedings
of the 1st international
computational geometry software workshop (p. 66)
Arlinghaus, S. (1994). Practical handbook of curve ﬁtting. CRC Press.
Barber, C. B., Dobkin, D. P., & Huhdanpaa, H. (1996). The quickhull algorithm for
convex hulls. ACM Transactions on Mathematical Software (TOMS), 22, 469–483.
Bazazian, D., Casas, J. R., Ruiz-Hidalgo, J. (2015). Fast and robust edge extraction in
unorganized
point
clouds.
In:
International
conference
on
digital
image
computing: Techniques and applications (DICTA) (pp. 1–8).
Beard, T. (1997). Machining from STL ﬁles. Modern Machine Shop, 69, 90–99.
Berger, M., Tagliasacchi, A., Seversky, L. M., Alliez, P., Guennebaud, G., Levine, J. A.,
et al. (2017). A survey of surface reconstruction from point clouds. Computer
Graphics Forum, 301–329.
Biggs, N. L. (1979). The roots of combinatorics. Historia Mathematica, 6, 109–136.
Calakli, F., & Taubin, G. (2011). SSD: Smooth signed distance surface reconstruction.
Computer Graphics Forum, 1993–2002.
Curless, B. (1999). From range scans to 3D models. ACM SIGGRAPH Computer
Graphics, 33, 38–41.
de Boor, C. (1968). On the convergence of odd-degree spline interpolation. Journal of
Approximation Theory, 1, 452–463.
Delaunay, B. (1934). ‘‘Sur la sphere vide,” Izv. Akad. Nauk SSSR, Otdelenie
Matematicheskii i Estestvennyka Nauk (Vol. 7, pp. 1–2).
Fleishman, S., Cohen-Or, D. & Silva, C. T. (2005). Robust moving least-squares ﬁtting
with sharp features. In ACM transactions on graphics (TOG), (Vol. 0, pp. 544–
552).
Gonzales, R. C., & Woods, R. E. (1992). Digital image processing. Reading, MA:
Addison & Wesley Publishing Company.
Gumhold, S., Wang, X., & MacLeod, R. S. (2001). Feature Extraction From Point
Clouds. In IMR.
Hinks, T., Carr, H., Truong-Hong, L., & Laefer, D. F. (2012). Point cloud data
conversion into solid models via point-based voxelization. Journal of Surveying
Engineering, 139, 72–83.
Johnson, D., & Williams, R. P. D. (1976). Methods of experimental physics:
Spectroscopy. New York: Academic Press.
Table 2
Performances of the RBS algorithm, compared to 3rd order polynomial ﬁtting.
Cloud
a
b
c
d
e
New method
Reconstruction time [ms]
528
305
143
71
385
Mean dist. to points [mm]
0.41
0.84
0.96
1.38
0.22
Max dist. to points [mm]
2.17
4.16
6.73
3.76
2.43
Mean dist. to mesh [mm]
0.07
0.25
0.95
1.20
0.73
Max dist. to mesh [mm]
0.80
1.68
4. 51
4.23
5.77
Pol. ﬁtting
Reconstruction time [ms]
20
19
17
15
19
Mean dist. to points [mm]
3.61
3.78
4.95
5.64
3.83
Max dist. to points [mm]
34.8
33.0
32.7
40.0
30.1
Mean dist. to mesh [mm]
1.00
0.89
1.08
1.43
1.27
Max dist. to mesh [mm]
7.03
5.98
6.56
7.70
12.5
B-Splines
Reconstruction time [ms]
102
60
39
25
69
Mean dist. to points [mm]
0
0
0
0
0
Max dist. to points [mm]
0
0
0
0
0
Mean dist. to mesh [mm]
0.05
0.11
0.24
0.46
0.45
Max dist. to mesh [mm]
0.78
1.12
1.35
3.08
3.30
90
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023


--- Page 11 ---

Kazhdan, M., & Hoppe, H. (2013). Screened Poisson surface reconstruction. ACM
Transactions on Graphics (TOG), 32, 29.
Kettner, L., Näher, S., Goodman, J. E., & O’Rourke, J. (2004). Two computational
geometry libraries: LEDA and CGAL. In Handbook of discrete and computational
geometry (pp. 1435–1463). Chapman & Hall/CRC.
Knott, G. D. (2012). Interpolating cubic splines (Vol. 18). Springer Science & Business
Media.
Levoy, M., & Whitted, T. (1985). The use of points as a display primitive. University of
North Carolina, Department of Computer Science.
Mineo, C., Pierce, S. G., Nicholson, P. I., & Cooper, I. (2017). Introducing a novel mesh
following technique for approximation-free robotic tool path trajectories.
Journal of Computational Design and Engineering, 4(3), 192–202.
Monchalin, J.-P., Neron, C., Bouchard, P., & Heon, R. (1998). Laser-ultrasonics for
inspection
and
characterization
of
aeronautic
materials.
Journal
of
Nondestructive Testing & Ultrasonics (Germany), 3, 002.
Ni, H., Lin, X., Ning, X., & Zhang, J. (2016). Edge detection and feature line tracing in
3d-point clouds by analyzing geometric properties of neighborhoods. Remote
Sensing, 8, 710.
Öztireli, A. C., Guennebaud, G., & Gross, M. (2009). Feature preserving point set
surfaces based on non-linear kernel regression. Computer Graphics Forum,
493–501.
Schrijver, A. (2005). On the history of combinatorial optimization (till 1960).
Handbooks in Operations Research and Management Science, 12, 1–68.
Truong-Hong, L., Laefer, D. F., Hinks, T., & Carr, H. (2011). Flying voxel method with
Delaunay triangulation criterion for façade/feature detection for computation.
Journal of Computing in Civil Engineering, 26, 691–707.
Turk, G. & Levoy, M. (1994). The Stanford bunny, the Stanford 3D scanning
repository.
Turk G, & Levoy, M. (1994). Zippered polygon meshes from range images. In:
Proceedings of the 21st annual conference on Computer graphics and interactive
techniques (pp. 311–318).
Weber, C., Hahmann, S. & Hagen, H. (2011). Methods for feature detection in point
clouds. In OASIcs-OpenAccess Series in Informatics.
Weinmann, M., Jutzi, B., Hinz, S., & Mallet, C. (2015). Semantic point cloud
interpretation based on optimal neighborhoods, relevant features and efﬁcient
classiﬁers. ISPRS Journal of Photogrammetry and Remote Sensing, 105, 286–304.
C. Mineo et al. / Journal of Computational Design and Engineering 6 (2019) 81–91
91
Downloaded from https://academic.oup.com/jcde/article/6/1/81/5715592 by guest on 31 January 2023
```

---

## Section 1.28: Point_Cloud_Segmentation_for_3D_Edge_Det

Source File: Point_Cloud_Segmentation_for_3D_Edge_Det.txt

### Paper Content:

```
# Point_Cloud_Segmentation_for_3D_Edge_Det.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Point_Cloud_Segmentation_for_3D_Edge_Det.pdf
# Output: ../layer2_completion/txt/Point_Cloud_Segmentation_for_3D_Edge_Det.txt


--- Page 1 ---

Citation: Betsas, T.; Georgopoulos, A.
Point-Cloud Segmentation for 3D
Edge Detection and Vectorization.
Heritage 2022, 5, 4037–4060. https://
doi.org/10.3390/heritage5040208
Academic Editors: Devrim Akca and
Geert Verhoeven
Received: 7 October 2022
Accepted: 5 December 2022
Published: 9 December 2022
Publisher’s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright:
© 2022 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
heritage
Article
Point-Cloud Segmentation for 3D Edge Detection
and Vectorization
Thodoris Betsas *
and Andreas Georgopoulos
Laboratory of Photogrammetry, School of Rural-Surveying and Geoinformatics Engineering NTUA,
15772 Athens, Greece
* Correspondence: betsasth@mail.ntua.gr
Abstract: The creation of 2D–3D architectural vector drawings constitutes a manual, labor-intensive
process. The scientiﬁc community has not provided an automated approach for the production of
2D–3D architectural drawings of cultural-heritage objects yet, regardless of the undoubtable need
of many scientiﬁc ﬁelds. This paper presents an automated method which addresses the problem
of detecting 3D edges in point clouds by leveraging a set of RGB images and their 2D edge maps.
More concretely, once the 2D edge maps have been produced exploiting manual, semi-automated or
automated methods, the RGB images are enriched with an extra channel containing the edge semantic
information corresponding to each RGB image. The four-channel images are fed into a Structure from
Motion–Multi View Stereo (SfM-MVS) software and a semantically enriched dense point cloud is
produced. Then, using the semantically enriched dense point cloud, the points belonging to a 3D
edge are isolated from all the others based on their label value. The detected 3D edge points are
decomposed into set of points belonging to each edge and fed into the 3D vectorization procedure.
Finally, the 3D vectors are saved into a “.dxf” ﬁle. The previously described steps constitute the
3DPlan software, which is available on GitHub. The efﬁciency of the proposed software was evaluated
on real-world data of cultural-heritage assets.
Keywords: edge detection; point-cloud segmentation; photogrammetry; 3D computer vision; cultural
heritage; SfM-MVS
1. Introduction
The creation and analysis of 3D data has attracted increasing interest from the sci-
entiﬁc community during the last decade. The understanding and analysis of 3D data
could provide solutions for perplexing and demanding tasks, e.g., the production of self-
driving cars, the automation of the production of 2D–3D architectural drawings and the
improvement of the 3D perception of robots. There are several 3D data points provided
to the scientiﬁc community for analysis in the form of, e.g., 3D point clouds, textured 3D
meshes and voxels. A major challenge in computer vision and photogrammetry is the
association of 2D–3D data with a semantic meaning, for instance, the identiﬁcation and
detection of scene objects or the materials of a construction presented in a set of images or
in a point cloud, in the same way as human perception. In this regard, several approaches
have been developed so far, investigating the semantic segmentation of 2D–3D data. The
semantic segmentation of 3D data, such as point clouds and meshes, is still an open issue,
especially for complex scenes such as cultural-heritage assets. In general, 2D edges form a
low-level semantic information incorporated into several deep-learning architectures to
extract image features which may then be used for 2D classiﬁcation, 2D object detection
and 2D semantic-segmentation purposes. A plethora of methods developed so far dealing
with 3D point-cloud semantic segmentation exploit various representations of the cloud
such as graphs and voxels or are applied directly on it. Furthermore, deep-learning image
Heritage 2022, 5, 4037–4060. https://doi.org/10.3390/heritage5040208
https://www.mdpi.com/journal/heritage


--- Page 2 ---

Heritage 2022, 5
4038
semantic-segmentation algorithms are combined with the photogrammetric pipeline to pro-
duce semantically rich 3D point clouds [1–3]. Additionally, 2D–3D semantic-segmentation
techniques are exploited to improve other methods such as 3D reconstruction [4,5].
Architectural drawings such as horizontal ﬂoor plans and vertical cross sections con-
stitute a valuable product for a variety of scientiﬁc ﬁelds such as engineers, archaeologists,
and craftsmen. Since ancient times, construction or, indeed, conservation works were based
on drawings. The primary role or function of working drawings is to convert design data
into construction information and to clearly communicate that information to the building
industry, code ofﬁcials, product manufacturers, suppliers, and fabricators. Keeping track
of modiﬁcations and/or additions during documentation is a necessary step in obtaining
an accurate drawing set for the working system. These drawings are a valuable resource
for maintenance and troubleshooting. Up to now, drawings are printed, mainly on paper
sturdy enough to be handled outdoors and resistant to natural phenomena. In recent years,
however, digital technology has advanced remarkably and, consequently, all drawings, 2D
or even 3D, are produced and disseminated in digital form. Either in vector or raster format,
2D drawings can, nowadays, be displayed on ﬂat screens of laptops, tablets or even smart-
phones. However, this display does not fully appeal to ﬁeld experts as expected. Hence,
2D drawings are still necessary and required, as by creating a drawing more than simply
presenting a captured 3D data set, an interpretation of the subject can be provided [6].
•
Accessible, platform-independent information. There are many cases when site-work
steps beyond the workstation or laptop; this may be due to cost, format compatibility
or simple practicality.
•
Clariﬁcation of complex 3D spaces by use of plan, section or perspective views.
•
Simplicity by showing selected information pertinent to a given project.
•
Reliable perception of scale, a printed plot, either dimensioned or with a scale, allows
consistent shared experience of information, etc.
Nowadays, the creation of 2D–3D architectural drawings is a manual and time-
consuming process. Moreover, the completeness in a drawing requires careful examination
of the structure not covered by surface recording methods such as photogrammetry or
laser scanning. Additionally, the scientiﬁc community has not proposed a fully automatic
method for the construction of such drawings so far, regardless of the indispensable need
in many scientiﬁc ﬁelds. In this study, a 3D edge-detection approach which leverages
2D edge semantic information is presented. The proposed 3DPlan algorithm can also be
exploited using high-level semantic information such as image semantic segmentation,
instance segmentation and panoptic segmentation, instead of edge maps, and, thus, may
be used in various applications.
Thus far, semantic-segmentation algorithms in 2D space have been investigated more
than those in 3D space. Therefore, several approaches exploit such methods to ﬁrstly detect
and then project the 2D semantic information into 3D space [7–9]. For example, if the
images are available, image semantic-segmentation algorithms are applied and then the
2D semantic information is projected into 3D space. If the images are not available, ﬁrstly,
the 3D model is artiﬁcially “photographed” from various positions, i.e., 3D–2D projection;
then, 2D semantic-segmentation algorithms are applied to the images and, ﬁnally, the 2D
semantic information is projected back into 3D space. Several applications in drawing
creation could exploit 3D–2D line detectors such as the creation of drawings of building
facades or the creation of drawings using aerial images instead of terrestrial.
The scope of this effort is to propose a 3D edge-detection algorithm and to investigate
the exploitation of those 3D edge points to produce 3D vectors, a major step towards the
automation of the production of 2D–3D architectural vector drawings, which will still
be required by conservation experts for years to come. To be more speciﬁc, the 3DPlan
algorithm detects the 2D edges on each RGB image of a given dataset. Then, it enriches the
given RGB images with an extra channel which contains the 2D edge semantic information
produced in the previous step. The four channel images are passed into the Agisoft-
Metashape [10] or the Mapillary-OpenSfM [11] software and a semantically enriched dense


--- Page 3 ---

Heritage 2022, 5
4039
point cloud is produced. Finally, the edge points of each edge are grouped and isolated
using their labels carried by the semantically enriched point cloud. Finally, the 3D vectors
are created using the detected groups of edge points and saved into a “.dxf” ﬁle. The steps
described are executed automatically using the 3DPlan software. An illustration of the
general idea of the 3DPlan software is presented in Figure 1.
Figure 1. General idea of the 3DPlan algorithm. The products from left to right are: the semantically
enriched point cloud visualized with true colors, semantic colors, the detected 3D edge and the 3D
vector with the dense point cloud.
The rest of the paper is organized as follows: Section 2 presents the related work,
including the traditional and deep-learning 3D edge-detection approaches presented in
the literature. The materials and methods are outlined in Section 3. In Section 4, the
results using the 3DPlan algorithm are presented, exploiting various datasets as well as
testing different parameter combinations. Section 5 presents a discussion of the results, the
contributions of the proposed algorithm as well as the parts requiring further investigation.
Finally, in Section 6, some conclusions for the proposed algorithm are drawn.
2. Related Work
The general idea of this research is the 3D edge detection for automated 2D –3D vector
drawing construction. Several methods developed so far investigate the 3D edge-detection
task, a crucial step before the 3D edge-vectorization procedure. These methods can be


--- Page 4 ---

Heritage 2022, 5
4040
summarized into two categories, the direct and indirect. On the one hand, the direct
methods initially clean the point cloud of the unnecessary parts and the random noise
present. Then, the remaining points are grouped into clusters which are exploited to extract
the desired 3D edges. On the other hand, indirect methods transform the given point
cloud into images on which the 2D edges are detected and then projected back to 3D space,
resulting in the coveted 3D edges. Model-ﬁtting techniques are usually involved with 3D
edge-detection methods. Bienert [12] introduced an edge-vectorization and dimension
approach exploiting smoothed point-cloud proﬁles. The proﬁle is determined by cutting
the given point cloud with a plane parallel to an “origin plane” deﬁned by XY, XZ, YZ or an
arbitrary plane and with speciﬁc thickness, and is smoothed using a line ﬁtting approach.
A plethora of 3D edge-detection methods presented in the literature exploit plane-
intersection techniques. Nguatem, Drauschke and Mayer [13] presented an approach which
detects window and door boundaries in 3D space using plane intersection and approximate
templates of the desired shapes. Lin et al. [14] introduced the line-segment-half-planes
method which uses plane intersection to detect 3D line segments. The given point cloud
was projected into several groups of images while the group that fully describes the object
of interest was selected. Then, the 2D line segments were detected, applying the LSD [15]
algorithm. The extracted lines were ﬁltered to remove the erroneous ones and the remaining
lines were projected back into 3D space in a “V” shape. The two non-parallel planes of
each projected “V” shape were extracted while their intersection is the desired 3D edge.
Mitropoulou and Georgopoulos, Ref. [16], presented an approach which detects 3D edges
using plane intersection and the PCL library [17]. The planes are detected by calculating
the ﬁtted plane to each of the point triplets in the given point cloud. The planes with the
most inliers are selected, and the 3D edges are produced via plane intersection.
Apart from plane intersection, planes could be used to detect 3D edges applying
different techniques. Bazazian, Casas and Ruiz-Hidalgo [18] proposed an approach which
extracts sharp edges in point clouds exploiting a statistical method in combination with
plane ﬁtting. More precisely, the given point cloud was segmented into regions using
the kNN algorithm and then a plane was ﬁtted to each segment using the least squares.
Afterwards, the covariance matrix of each region as well as its eigenvalues and eigenvectors
were calculated. The 3D edges were detected determining if a point lies on an edge or on a
plane, using the surface variation formula.
The statistical quantities and matrices, e.g., normal vector and covariance matrix,
derived from a set of points, constitute worthwhile information useful to segment those
points into groups describing each of the object’s planes. Lu, Liu and Li [19] presented a
fast method for 3D line extraction exploiting statistical quantities and matrices. Initially, the
given point cloud was decomposed into smaller groups. To achieve that, a region-growing
and a region-merging technique were implemented iteratively aiming to accurately detect
each of the object’s planes. Those steps were applied calculating the covariance matrix for
each region and exploiting its eigenvalues and eigenvectors. Finally, the points of each plane
were projected on an image, an edge-detection algorithm was applied, and the detected
edges were projected back into 3D space. Dolapsaki and Georgopoulos [20] presented a
method in which the desired 3D edges are extracted using oriented high-resolution digital
images, the relationships of the analytical geometry and the properties of planes in 3D
space. The ﬁnal 3D edges, in their true position in 3D space, are detected in the available
point cloud by applying some constraints. The provided method is semiautomated while
the 3D edges are detected with under 1 pix accuracy, using a calibrated camera.
Point clouds can be collected using either image-based or laser-scanning techniques.
Both approaches have advantages and disadvantages while, depending on the application,
one or both can be implemented. Alshawabkeh [21] proposed a 3D edge-detection method
using data from both image-based and laser-scanner techniques. Firstly, the point cloud
produced by the laser scanner was converted into a set of depth images. The speciﬁcations
of the depth images are considered with respect to the captured RGB images. The main
goal of this approach was to identify the cracks presented on the facade of the Treasury


--- Page 5 ---

Heritage 2022, 5
4041
monument of Petra in Jordan. Thus, the Canny edge detector [22] was implemented on
the RGB images to extract the presented cracks. Finally, the found cracks were projected
into the 3D space using the depth information provided by the produced depth maps.
Apart from the RGB images, the Canny edge detector can be applied directly to range
images, generated from point clouds collected by laser scanners, and then reprojected into
3D space [23].
In some applications, 3D edges are used as an abstract representation of shapes. Dur-
ing 3D reconstruction procedures, such methods are commonly used as an alternative
to the time-consuming MVS procedure, aiming to generate a group of 3D edges which
fully describes the object of interest. An approach was presented by Hofer et al. [24]
in which a line-matching method is implemented exploiting the principals of epipolar
geometry. Each 3D line is determined using a depth estimation procerdure while the
line direction is calculated using the singular value decomposition technique. Finally, a
bundle-adjustment technique is provided optionally aiming to produce a more accurate 3D
line model. Bazazian and Parés, [25] presented a capsule network, named EDC-Net, which
predicts if a point lies on an edge or not, in 3D space. EDC-Net was initially trained using a
fully supervised learning approach in combination with the ABC [26] dataset. The gener-
ated model was considered as a pretrained model and, thus, was ﬁne-tuned using a weakly
supervised transfer learning approach and the ShapeNet [27] dataset. Qi et al. [28,29] pro-
posed an innovative deep-learning architecture, named PointNet, which was applied on
the 3D object-detection, 3D part-segmentation and 3D semantic-segmentation applications
with remarkable results. The PointNet architecture was the ﬁrst applied directly to 3D
point clouds, introducing a new era in 3D point-cloud analysis. Liu et al. [30] introduced
an end-to-end trainable deep network architecture which automatically generates a 3D
graph representation of a given 3D point cloud of man-made polyhedral objects such as
furniture, mechanical parts, or building interiors, etc.
Building information models (BIMs) are used in point-cloud vectorization methods.
Chuang and Sung [31] proposed a learning approach for point-cloud vectorization, which
includes a “Feature Extraction” and a “Vectorization” procedure. The approach leverages
BIM models during the training process to associate each point with attributes. To be
more speciﬁc, the approach produces 3D dense point clouds from BIMs which include
not only the geometric information but also a semantic meaning, e.g., walls, columns etc.,
exploiting the information incorporated into the BIM. The test of the proposed approach
was conducted using simple 3D dense point clouds, i.e., without semantic meaning, and a
plethora of metrics, e.g., recall, precision etc, as well. Additionally, Bassier et al. [32] labeled
3D vector models using an automated approach. The proposed method contains various
steps. Firstly, the user deﬁnes a set of parameters regarding the data under consideration.
Then, the data are cleaned from the noise present and the planar surfaces are segmented.
Afterwards the “initial labelling” step is performed using some rules. The horizontal planes
are classiﬁed into ﬁve sub categories, e.g., ﬂoors, furniture, etc., and the vertical planes into
two subcategories, i.e., wall and unidentiﬁed. Fourthly, the labels produced are enhanced
for better results. Then, an automated approach gives the opportunity to detect windows
and doors. Finally, the labeled data are exported according to their label. The authors state
that the disadvantage of the proposed approach is that it cannot be implemented using
complex surfaces characterized by a plethora of details and small parts. The described
approaches assume that a BIM model is already available. Several approaches developed
so far investigate the Scan-to-BIM problem proposing semi-automatic [33] and automatic
approaches [34]. Macher et al. [33] proposed a semi-automatic reconstruction approach of
indoor point clouds acquired by laser scanner. The authors stated that point-cloud quality
plays a signiﬁcant role during the process and presented a recommendation list of steps
to achieve high-quality point clouds using laser scanners. The proposed approach can be
divided into two main categories, point-cloud segmentation and 3D reconstruction of walls
and slabs. When the reconstruction is performed, a ﬁle in the IFC format is exported in
which the walls and slabs are included. Finally, an evaluation procedure was conducted


--- Page 6 ---

Heritage 2022, 5
4042
using datasets which were not included in the development part. Ochmann et al. [34]
proposed an automatic 3D reconstruction approach, in which the main goal is to assign a
label to each cell either as “room” or as “outside area”. The proposed method ﬁrstly detects
planes using the RANSAC algorithm. The parameters used by RANSAC are deﬁned
according to the quality of the used indoor point cloud. Then, a point-cloud cleaning
method is applied to eliminate the outdoor points collected by the scanner due to building
openings such as windows and doors. Afterwards, an automatic binary-classiﬁcation
approach is implemented to classify each point as a “room” point or “outside area” point.
Thereafter, two sets of candidates for wall and slab surfaces are extracted. Next, pairs of
contrary surfaces are detected, which are characterized as walls by approximately matching
parallel surfaces with contrary normal vectors. The wall candidates are used to create a 3D
cell complex. Finally, the result is improved using several probabilistic and optimization
steps and then it is exported. Each cell in the exported ﬁle is assigned to one room or the
outside area. The described reconstruction process was evaluated using synthetic and
real-world datasets of the construction domain. It is worth noting that the creation of simple
BIM models is not a time-consuming process in the construction domain if the building is
not characterized by details and heterogeneity. Thus, the BIMs are usually exported into
the construction domain in which the buildings are characterized by simpler geometric
shapes than the detailed cultural-heritage assets, e.g., monuments. Additionally, the 3D
reconstruction of buildings in construction is conducted for entities such as walls, ﬂoors
and slabs, which are usually characterized by continuous and smooth surfaces. Finally,
the construction becomes even more difﬁcult when information such as the materials and
thickness of the structures should be included to BIMs.
Obrock and Gülch, [35] presented a method for producing semantically enriched
point clouds of interiors, combining deep-learning techniques with photogrammetry. More
precisely, a classiﬁer was trained to segment images of interiors into several objects such as
ﬂoor, doors, windows, etc. Then, the blue channel of each image was replaced by a new
channel containing the corresponding ID number for each pixel. One additional variation
in this step was mentioned, in which a new channel carrying the semantic information
could be added to the existing images as a fourth channel, but, as they mentioned, this
was not applied due to technical issues. A vital disadvantage of the proposed method
was the production of false color point clouds due to the missing blue channel. Thus,
Gülch and Obrock [3] proposed an improved method either in the deep-learning or the
photogrammetric part. Firstly, a new deep-learning network was trained using manu-
ally produced ground-truth data as well as the DeepLabv3+ [36] architecture instead of
FCN-8s [37], producing more accurate semantic-segmentation results than the previous
approach. Additionally, the photogrammetric procedure was improved by proposing a
two-step approach which generates both a true-color and a semantically enriched point
cloud. To be more speciﬁc, the SfM algorithm was executed using the original RGB im-
ages calculating the relative translation and rotation of each image to the referenced one.
The calculated parameters, i.e., the exterior orientation of the images, were exploited to
reproject the semantic images generating the semantically enriched point cloud. Finally,
the obvious lines, i.e., the intersection of the wall with the ﬂoor, were extracted using a heat
map and applying the Hough–Transform algorithm. Obviously, the proposed approach
requires the generation of a dense point cloud, which is a time-consuming process, twice
to produce the true colors and the semantically enriched point clouds. Pellis et al. [2]
proposed a new workﬂow for 3D point-cloud semantic segmentation for cultural-heritage
buildings, using photogrammetric and dense image-matching principles. The proposed
workﬂow contains two major steps: ﬁrstly, the images are semantically segmented using
DeepLabv3+ and secondly, the semantic information is projected into 3D spaces exploiting
image masks. Finally, a 3D semantically enriched dense point cloud is produced, for each
semantic category.
Edge detection is a fundamental computer vision problem. Xie and Tu [38] pro-
posed a deep-learning method for 3D edge detection called Holistically-nested Edge


--- Page 7 ---

Heritage 2022, 5
4043
Detection (HED), which leverages convolutional neural networks (CNN). According to
the authors, the HED algorithm outputs better results than the Canny edge detector.
Poma, Riba and Sappa, [39] presented an interesting deep-learning-based 2D edge detector,
named DexiNed, which produces thin edge maps and can be applied to any image without
a previous training procedure. A comparison between the detected 2D edges using state-
of-the-art models [38,40–42] and DexiNed is presented by the authors, providing useful
information for the performance of such methods. Segmentation techniques on point clouds
are divided into four categories (i) edge-based, (ii) region-growing, (iii) model-ﬁtting and
(iv) clustering [8]. Edge-based methods transfer edge-detection information from images
to 3D point clouds aiming to describe the shape of the presented objects. Thus, the 3D
point cloud is segmented into smaller parts describing the objects. Bhanu et al. [43] de-
tect 3D edge points using several methods such as gradient computing and line ﬁtting,
while they state that the 3D edge points constitute valuable information for 3D shape-
recognition purposes.
However, the scientiﬁc community has not proposed a fully automatic approach to
produce architectural vector drawings yet, in comparison with the unquestionable need
from a diversity of scientiﬁc ﬁelds. In this effort, an edge-based 3D point-cloud segmenta-
tion approach exploiting SfM-MVS algorithms is presented to detect 3D edge points and to
investigate the automation of the construction of 2D and 3D architectural drawings.
3. Materials and Methods
The motivation of the 3DPlan algorithm is its contribution to the unsolved problem of
the automation of the construction of 2D–3D architectural vector drawings. The proposed
methodology presents an efﬁcient pipeline for the extraction of 3D edges from point clouds
using 2D edge semantic information from digital images. The major steps of the proposed
methodology are
1.
The enrichment of the RGB images with their edge maps constructing four-channel images;
2.
Theusageofthefour-channelimagesintoimage-based3Dpoint-cloud production workflows;
3.
The separation of the 3D points into edge and non-edge points;
4.
The 3D vectorization of 3D edge points.
3.1. Edge Maps and the Creation of Four-Channel Images
The quality of the detection of the 3D edges is strongly dependent on the quality of
the 2D edge maps. However, this effort is focused on the construction of a general software
which will be used as the base towards the automation of the production of 2D–3D archi-
tectural drawings, and not to investigate the best 2D edge-detection technique, neither the
traditional nor the more sophisticated, e.g., DexiNed [39], etc. The 3DPlan software receives
the edge semantic information in two ways (i) using a live-Canny implementation which is
a user-guided implementation of the Canny algorithm or (ii) using predeﬁned edge maps
(an external semantic source). More concretely, using the live-Canny implementation, two
windows are opened automatically and the user deﬁnes the Canny hysteresis thresholding
parameters with the provided bars. The edge map is updated live in the ﬁrst window while
the second one displays the image under process. When the user is satisﬁed about the de-
picted edge map, terminates the process. The two windows of the live-Canny are depicted
in Figure 2. On the other hand, the external semantic source enables the combination of
the given images with predeﬁned edge maps and, thus, the user is not restricted to using
Canny algorithm by the providing semiautomatic tool. Additionally, the 3DPlan software
can easily adopt the more sophisticated and upcoming 2D edge-detection techniques as an
external input and, thus, be easily updated in the future. Regardless of whether live-Canny
or a different 2D edge-detection method is used, the RGB images are enriched with their
edge map resulting in four-channel images instead of the RGB. Each pixel is associated
with four values, i.e., RGBL and, thus, four-channel images constitute an elegant way to
carry the RGB accompanied with the 2D edge information.


--- Page 8 ---

Heritage 2022, 5
4044
Figure 2. Live-Canny interface.
3.2. Image-Based 3D Point Cloud Software
A major objective of the 3DPlan software is to produce a semantically enriched 3D
point cloud which carries the edge semantic information in the same way as the four-
channel images, i.e., each 3D point is to be associated with four values—RGBL—with
respect to the pixels used to calculate the 3D position. Two well-known Structure from
Motion and Multi View Stereo (SfM-MVS) softwares, the Agisoft-Metashape and Mapillary-
OpenSfM, were exploited during this effort.
3.2.1. Agisoft-Metashape
Agisoft-Metashape is one of the leading photogrammetric software on the market,
and was introduced in 2006 as Agisoft-PhotoScan. It has a user-friendly GUI with many
algorithms included, well-written documentation and a plethora of training videos. Ad-
ditionally, it supports Python and Java API, offering the opportunity to manipulate the
provided algorithms using programming instead of GUI. Although Agisoft-Metashape
is a closed commercial software and therefore it is not possible to modify its source code,
it has a major advantage regarding the scope of this effort: its ability to process multi-
spectral images, e.g., from satellite sensors. Hence, it enables the direct use of the default
photogrammetric pipeline exploiting the created four-channel images. To this end, a se-
mantically enriched point cloud is produced by default and without any modiﬁcation. The
exploitation of the Agisoft-Metashape pipeline from the 3DPlan software is conducted
using the Python API to achieve a non-stop execution of the software. The major steps of
the developed pipeline using the Python API are described in the following steps:
1.
Firstly, the “.psx” project ﬁle and one chunk inside it, are created.
2.
Then, the four-channel images are added to that chunk.
3.
Afterwards, the image-matching and camera-aligning algorithms are executed.
4.
When the sparse point cloud is generated, a depth map of each image is produced.
5.
Finally, the semantically enriched dense point cloud is created using the depth maps
and saved using the “.ply” format.
The steps described are the same as the main workﬂow of the Agisoft-Metashape soft-
ware using the GUI. The 3DPlan software gives the choice to the user to create the semanti-
cally enriched point cloud using the GUI, apart from the developed Python implementation.
3.2.2. Mapillary-OpenSfM
Mapillary-OpenSfM is an open-source SfM pipeline which is distributed under the
BSD-2-Clause license [44] and, thus, it has all the advantages of an open-source project,
such as the ability to adapt the source code to one’s needs. Additionally, the user has a
wide range of conﬁguration choices for each of the workﬂow steps provided using a user-
friendly conﬁguration ﬁle. The Mapillary-OpenSfM software does not have a graphical
user interface (GUI), so the execution and management of its algorithms are performed
using terminal commands.
When the software processes four-channel images, the produced point cloud is exactly
the same as using three channel images. It was concluded that the software ignores the


--- Page 9 ---

Heritage 2022, 5
4045
existence of the fourth channel and, thus, the necessary modiﬁcations of the main branch of
the Mapillary-OpenSfM software, using the GIT version control system, were applied. The
main idea of the modiﬁed Mapillary-OpenSfM workﬂow was to process the label channel
like the RGB channels during the generation of the point cloud, and, ﬁnally, to include the
label value of each point in addition to its position and color values into the point-cloud
ﬁle. More precisely, the source code of the Mapillary-OpenSfM software was modiﬁed to
achieve the following:
•
To eliminate the fourth channel, i.e., 2D edge map, during the feature-extraction process.
•
To consider the fourth channel during the determination of the color values of each
point and, thus, associate each point with four values instead of three.
•
During the execution of the OpenSfM software, the images’ dimensions are changed
for cost purposes. Thus, the software was modiﬁed to scale down the fourth channel
in the same way as the rest of the channels.
•
To change the deﬁnition of the undistorted images format to “.tiff” instead of “.jpg”
using the user-friendly conﬁguration ﬁle.
The changes described above guarantee the correlation of the pixel values of the RGBL
channels during the creation of the 3D point cloud. The correlation of pixel values is crucial
for the correct registration of the RGBL values to the 3D point-cloud ﬁle. The original
Mapillary-OpenSfM software has two variables called “labels" and “class" associated by
default, with an add-on script for manual image-annotation tasks which was not used
during this effort. The source code of Mapillary-OpenSfM was modiﬁed to use the “la-
bels” variable and the “class” position when required in order to register the label values,
extracted from the images’ fourth channel, to the point-cloud ﬁle and so to produce the
semantically enriched point cloud. The Mapillary-OpenSfM source code was forked and
the new branch called OpenSfM_3DPlan https://github.com/thobet/OpenSfM_3DPlan
(accessed on 15 September 2022), which is available on GitHub.
3.2.3. Edge and Non-Edge Points Classiﬁcation and 3D Vectorization
Regardless of the chosen SfM-MVS workﬂow, a semantically enriched point cloud
is generated and stored in the “.ply” format. Then, the edge points are isolated from the
non-edge ones taking into account their label value. The label values range from 0 to 255
(8-bit images) due to the interpolation applied during the SfM-MVS pipeline. To this end,
an empirical threshold value, which is 100 by default, is deﬁned and so the points with
label values greater than the threshold are considered as edge points and stored into a
point-cloud ﬁle. The rest are characterized as non-edge points and excluded from the
upcoming steps. Afterwards, the isolated 3D edge points should be decomposed into
points of each 3D edge presented into the stored point cloud. Two widely used clustering
algorithms, the RANSAC [45,46] and DBSCAN [47], are combined to determine those sets
of points. First and foremost, the DBSCAN algorithm using each points’ neighborhood,
which is deﬁned by the “eps” value, classiﬁes the edge points into the clusters of points
of each edge. The eps value deﬁnes the minimum distance to characterize two points
as neighboring. It is worth noting that the “eps” value should be estimated considering
the image resolution and density of the semantically enriched point cloud. Finally, the
RANSAC algorithm, using a line-ﬁtting formula, is applied to each cluster. The inlier
points produced via the RANSAC implementation are fed into the vectorization step. The
rest are considered as noise. Then, the 3D edge vectorization is performed by exploiting
the ﬁrst and the last point of the each set of inliers. Obviously, the 3D edge-vectorization
approach is not the optimal one and, thus, a better approach should be included in the
future exploiting the line-ﬁtting parameters computed by RANSAC. Finally, the generated
3D vectors are stored using the “.dxf" format. Figure 3 displays the ﬂowchart of the 3DPlan
algorithm. More details about the algorithm and the experiments performed until the ﬁnal
version are discussed in Results (Section 4).


--- Page 10 ---

Heritage 2022, 5
4046
Figure 3. The ﬂowchart of the 3DPlan software (above to below).
4. Results
4.1. Oversimpliﬁed Experiments
The algorithm developed during the oversimpliﬁed experiment is an implementation
of the standard two image epipolar geometry problem called “Triangulation”, written in


--- Page 11 ---

Heritage 2022, 5
4047
Python using the OpenCV [48] library. It produces a non-optimized 3D point cloud for each
image pair, since the bundle adjustment is omitted. However, the scope of this step was to
develop the secondary Python scripts of the 3DPlan algorithm as well as to ﬁnd the most
efﬁcient method to carry the edge semantic information and then to be included in 3DPlan
software in combination with professional SfM-MVS workﬂows for an optimal result. To
this end, two approaches were investigated for the exploitation of the semantic information.
The ﬁrst approach uses a predeﬁned reference image, which is a duplication of the left
image of the image pair, to carry the semantic information (Figure 4). The images used
for the ﬁrst experiment are from the archaeological site of the ancient Kymissala [49–51] in
Rhodes and were captured during the summer ﬁeld course in photogrammetry in 2019,
which was conducted by the Laboratory of Photogrammetry SRSGE NTUA. The second
approach enriches the RGB images with an extra channel (RGBL), which contains the edge
semantic information and then feeds them into the “Triangulation” pipeline, from which a
semantically enriched point cloud is produced.
To be more speciﬁc, the ﬁrst approach separates the workﬂow of the 3D point-cloud
generation into two steps: (i) the calculation of the 3D positions (XYZ) and (ii) the coloring
of the 3D points using the reference images (RGB). The semantic information is associated
with each point during the coloring step as each color represents a different class, i.e., the
true colors of the original image are replaced with the artiﬁcial colors of the reference image.
In fact, during the generation of the point cloud, the pixels from which the points were
created, are cached. The color value of each point is determined using the cached pixel
index and the reference image. Finally, apart from the true-color point cloud, a semantically
enriched point cloud with the false colors of the segmented image is produced (Figure 5).
This approach was developed to avoid the steps back and forth during the production of
the semantically enriched point cloud using the exterior orientations of the RGB images
and the semantically segmented images [3]. In more detail, using the “Triangulation”
algorithm, the semantically enriched point cloud is produced by exploiting the cached
pixel position of the produced 3D point. The RGB values are extracted from the original
RGB images while the labels for each point are extracted from the reference image. The
approach described is, of course, inefﬁcient, especially in the case of datasets with hundreds
of images, since a duplication of the given images is required during the process and
the process is over-complicated. Additionally, the exploitation of the existing SfM-MVS
algorithms, which guarantees an optimal result, in combination with reference images,
constitute a time-consuming process especially in the case of commercial software without
publicly available code.
Figure 4. The image pair and reference image (Experiment 1).
Figure 5. The semantically enriched point cloud (Experiment 1).


--- Page 12 ---

Heritage 2022, 5
4048
On the other hand, the second approach executes the generation and the colorization
steps of the point cloud simultaneously using four-channel images instead of standard
three-channel images. The point cloud produced contains the true colors and semantic
information as well, because the semantic information follows the RGB values during the
process maintaining the correlation between the RGB values and the label value (Figure 6).
The second experiment was conducted using images from the ancient temple of Demeter
in Naxos. Regarding the scope of this effort, edge semantic information is used but higher
level semantic information could also be included as extra channels of the given RGB
images along with the 3DPlan software. Finally, the classiﬁcation of the 3D points, into
semantically meaningful classes, is conducted using the label value of each 3D point. After
many experiments in which the professional SfM-MVS software was used apart from the
“Triangulation” algorithm, the approach with the four-channel images was selected to be
the most efﬁcient between them. The ﬁrst row of Figure 6 depicts the edge maps (top
to bottom). The edge semantic information was produced manually. The depicted edge
maps were converted to binary images (0, 255), constituting the fourth channel of the
original images. Then, Figure 6 depicts the semantically enriched point cloud as a 3D point
cloud and an ASCII ﬁle, in which the XYZ, RGB and L values are included. The orange
ellipses contain the detected 3D edge points, i.e., the 3D points with 255 label value into the
ASCII ﬁle.
Figure 6. The edge semantic information (ﬁrst row); the semantically enriched point cloud using the
triangulation algorithm (second row).
The most important drawback of the experiment with the “Triangulation” algorithm
is the production of a non-reﬁned point cloud and, thus, the proposed algorithm was
combined with professional software, i.e., Agisoft-Metashape or Mapillary-OpenSfM. It
is clear that the semantically enriched point cloud of Figures 5 and 6 are too far from a
well-created 3D point cloud. Consequently, the production of four-channel images, which
carry the edge semantic information, was considered a more efﬁcient and elegant approach
for the creation of a semantically enriched point cloud and the “Triangulation” algorithm
inappropriate for the creation of the semantically enriched 3D point cloud, as was expected.
4.2. Implementation Using Four-Channel Images and the Professional SfM-MVS Software
As discussed earlier the algorithm “Triangulation” developed for the oversimpliﬁed ex-
periment was characterized by major disadvantages. Additionally, the Agisoft-Metashape
and Mapillary-OpenSfM software constitute two of the leading photogrammetric software
on the market; thus, they are a trusted choice for the creation of the semantically enriched
3D point cloud. Section 3.2 presents the, by default, beneﬁts of the Agisoft-Metashape
as well as the Mapillary-OpenSfM software which was modiﬁed to process four-channel
images so as to be combined with the 3DPlan software. Figure 7 depicts two of the RGB


--- Page 13 ---

Heritage 2022, 5
4049
images (ﬁrst row), their corresponding manually annotated edges (second row) and the
binary edge maps (third row) used during this experiment. The set of the binary edge
maps depicted in Figure 7 was created using the red channel of the annotated images
(second row) and identifying the 255 pixels values. Eventually, the RGB images (ﬁrst row)
were enriched with the binary edge maps (third row) and so the four-channel images were
produced. For this experiment, seven images from the Temple of Demeter, Naxos dataset
were used in combination with Agisoft-Metashape and Mapillary-OpenSfM software.
Figure 7. Edge semantic information used with professional SfM-MVS software.
The four-channel images were imported in both professional SfM-MVS software and
so the semantically enriched 3D point clouds were created (Figure 8a). The ﬁrst row
of Figure 8a depicts the semantically enriched point cloud produced using the Agisoft-
Metashape software while the second row is the semantically enriched point cloud pro-
duced using the modiﬁed Mapillary-OpenSfM software. Each row in Figure 8a depicts
exactly the same 3D point cloud, the difference is due to the semantic knowledge which is
carried into the 3D point-cloud ﬁle simultaneously with the RGB colors; it is possible to
change the color and so visualize the semantic information instead of the true-colors point
cloud. The white line in the black point cloud of the ﬁrst row (Agisoft-Metashape) can be
observed, as well as the green line in the red point cloud of the second row (Mapillary-
OpenSfM) of Figure 8a. The visualization was made using Cloud Compare, and the colors
changed from true colors to semantic colors, by selecting scalar index instead of RGB.
Previously, the scalar index was associated with the labels of the point-cloud ﬁle. After-
wards, as is described in Section 3, the 3D points were classiﬁed into edge and non-edge
points. Figure 8b depicts the 3D edge points detected using Agisoft-Metashape (left) and
Mapillary-OpenSfM (right) software.


--- Page 14 ---

Heritage 2022, 5
4050
(a)
(b)
Figure 8. Products using professional SfM-MVS software and manually annotated edges. (a) Se-
mantically enriched dense point clouds using the Agisoft-Metashape (ﬁrst row) and the Mapillary-
OpenSfM software (second row). (b) 3D edge points using the Metashape (left) and the OpenSfM
software (right).
4.3. Implementation of the 3DPlan Software
The proposed algorithm (3DPlan) is divided into three sections: (i) the user interface,
(ii) the environment setting and (iii) the execution of the developed approaches according
to the user’s choices. In fact, it is impossible to produce manually annotated edges like
Figure 7 for all the edges and all the images of a dataset and so automated methods were in-
cluded. To be more speciﬁc, the 3DPlan algorithm includes the live-Canny implementation
(Figure 2), which gives the opportunity to the user to set and evaluate simultaneously the
Canny’s parameters, as well as the “external source” option which gives the opportunity to
the user to exploit any 2D edge-detection method. Moreover, 3DPlan software includes
the developed algorithm, which enriches the given RGB images with the corresponding
edge map as a fourth channel. Additionally, it includes the developed combination of the
DBSCAN and RANSAC algorithms, to group the detected edge points into sets of points
belonging to each edge and the algorithm which uses the detected group of 3D points,
vectorizes them and saves the 3D vectors to a “.dxf” ﬁle.


--- Page 15 ---

Heritage 2022, 5
4051
3DPlan User Interface
The 3DPlan algorithm user interface includes a series of questions which are posed
to the user at the terminal/command line as well as several informative messages during
the execution of the software, e.g., date/time and execution progress. The user must select
which method they prefer, answering some questions (Figure 9). It should be mentioned
that the user interface was implemented with a defensive perspective, providing a set of
useful functions such as the “message”, “error message” and “input check”, handling the
entire process. For instance, Figure 9 depicts a false answer (red circle), and the response
of the 3DPlan algorithm (red rectangle). First and foremost, the user deﬁnes the number
of the channels of the input images (three or four) into the SfM-MVS workﬂow. Secondly,
the user selects the approach that will be executed to create the semantically enriched
point cloud. The choices provided are the (i) Agisoft-Metashape, (ii) Mapillary-OpenSfM
and the (iii) Triangulation. The Agisoft-Metashape approach contains the (i) Python and
(ii) GUI implementation and so the user must select which one will be used during the
process. Regardless of the selected point-cloud production approach, the 3DPlan algorithm
creates a new folder called “Lines” in the 3DPlan directory. Thirdly, the user selects the
approach which will be used to determine the edge semantic information, the ﬁrst choice is
the (i) live-Canny and the second one is the (ii) external semantic source. Finally, the user
must deﬁne the format of the given three-channel images which will be used to produce
the four-channel images. The next steps are executed depending on the selected algorithm,
which will be used to produce the semantically enriched point cloud (Figure 3).
Figure 9. Questions of the 3DPlan user interface.
4.4. The Environment Setting and the Execution of the Algorithm
Simultaneously to the user’s responses to the interface, the 3DPlan algorithm con-
structs the environment within which the algorithm will be executed according to the user
choices, e.g., it creates the directory in which the ﬁnal product will be stored, or manipulates
the OpenSfM software by creating new folders, copying the four-channel images to the
expected from the algorithm folder inside the OpenSfM installation and executing the
terminal commands in order to use OpenSfM algorithms, etc. The OpenSfM installation
and the 3DPlan software must be in the same folder (“parent directory” in Figure 10). The
general folders’ structure is depicted in Figure 10. The three-channel images, i.e., the given
dataset, is stored into the “rgb” folder; the “lib” folder contains the Python source code.
When the four-channel images are created, they are stored into the “Images” folder. The
semantically enriched 3D point cloud, as well as the 3D edge points and the 3D vectors, are
stored in the “Lines” folder. In case of already-produced 2D edge maps, these are stored
into the “semantic_images” folder to be exploited by the 3DPlan algorithm and, ﬁnally, to
create the four-channel images. The “Labels” and “Blur_rgb” folders are used for optional
debugging/monitoring purposes.


--- Page 16 ---

Heritage 2022, 5
4052
Figure 10. The general folders’ structure.
The steps of the proposed approach are presented in Section 3 and Figure 3. The
experiments were conducted using the 3DPlan software in combination with several RGB
images from (i) the Ancient Temple of Demeter and (ii) the Old Police Station in Monolithos,
Rhodes, datasets. The results are depicted in Figures 11a–d and 12a–d.
(a)
(b)
Figure 11. Cont.


--- Page 17 ---

Heritage 2022, 5
4053
(c)
(d)
Figure 11. The proposed approach and the Temple of Demeter in Naxos dataset. (a) The semantically
enriched dense point cloud using the proposed approach and the Temple of Demeter in Naxos dataset.
(b) The detected 3D edges using the proposed approach and the Temple of Demeter in Naxos dataset.
(c) The detected 3D edge points of each edge (left) and 3D noise points using the proposed approach
and the Temple of Demeter in Naxos dataset (right). The colors of each class (left) seem to be the same
for large regions of the point cloud. In fact, there are not the same colors, but colors with minimal
change in color values due to the existence of hundreds of classes. (d) 3D vectors with the sparse
point cloud using the proposed approach and the Temple of Demeter in Naxos dataset.
(a)
(b)
Figure 12. Cont.


--- Page 18 ---

Heritage 2022, 5
4054
(c)
(d)
Figure 12. The proposed approach and the Old Police Station dataset. (a) The semantically enriched
dense point cloud using the proposed approach and the Old Police Station dataset. (b) The detected
3D edges using the proposed approach and the Old Police Station dataset. (c) The detected 3D edge
points of each edge (left) and 3D noise points using the proposed approach and the Old Police Station
dataset (right). The colors of each class (left) seem to be the same for large regions of the point cloud.
In fact, there are not the same colors, but colors with minimal change in color values due to the
existence of hundreds of classes. (d) 3D vectors with the sparse point cloud using the proposed
approach and the Old Police Station dataset.
5. Discussion
The proposed approach has individual steps which should be evaluated separately.
The main scope of this effort was the detection of the 3D edges into the produced point
cloud using 2D edge semantic information. In fact, the quality of the detection of the 3D
edges, using the 3DPlan algorithm, is inextricably linked to the quality of the detection of
the 2D edges. The proposed algorithm contains a live-Canny implementation in which the
user handles Canny drawbacks, such as misdetections due to textured areas or the shadows
present. Additionally, the 3DPlan algorithm includes a blurring implementation using
a Gaussian kernel, which, in combination with the live-Canny, produces better results,
although misdetections and wrong eliminations are not completely confronted. In general,
a plethora of traditional and modern 2D edge-detection approaches are presented in the
literature. Thus, the 3DPlan algorithm offers the user the opportunity to exploit any 2D
edge-detection method easily and effectively using the “external semantic source” choice.
To be more speciﬁc, the user can produce images’ edge maps using a technique of his choice
and then save them into the “semantic images" directory. The 3DPlan algorithm will enrich
the given RGB images with their edge maps included in the “semantic images" directory;
hence, the 3DPlan algorithm can be easily combined with all the existing and upcoming
2D edge-detection techniques, thus following the evolution of such techniques. The main
contributions of the 3DPlan algorithm are:


--- Page 19 ---

Heritage 2022, 5
4055
1.
The creation of four-channel images using the given RGB images and their label channel.
2.
The creation of a semantically enriched point cloud exploiting the four-channel images
in combination with professional SfM-MVS software.
3.
The modiﬁcation of the Mapillary-OpenSfM software in order to create semantically
enriched point clouds using four-channel images.
4.
The detection of the 3D edges with the most feasible accuracy with respect to the
quality of the 2D edge semantic information.
5.
A software, which is available on GitHub (https://github.com/thobet/3DPlan (ac-
cessed on 15 September 2022)) and could be used as a base for more sophisticated
approaches to the automation of the production of 2D–3D architectural drawings.
5.1. Creation of Four-Channel Images
The combination of the RGB images with their label channel for the production of the
RGBL images was evaluated. To be more precise, after the generation of the RGBL images,
they were decomposed into their channels, i.e., R, G, B and L. The primary channels, i.e., R,
G and B, were combined and saved while the L channel was saved separately. The initial
RGB images were compared with the new RGB images by subtracting from each other
and no differences were observed. Hence, it was concluded that the RGB images were
unaffected during the process of adding the extra L channel. Apart from the RGB images,
the same comparison was performed for the label channels. The described procedure is
illustrated in Figure 13.
Figure 13. RGBL image check procedure.
5.2. Classiﬁcation of 3D Points into Edge and Non-Edge Points
Apart from the quality of the 2D edge-detection technique, the classiﬁcation of the 3D
points into edge and non-edge points contributes to the detection of the 3D edge points
with the best possible accuracy. During the classiﬁcation procedure, the label value of each
point is taken into account to classify it as an edge or non-edge point. Ideally, the label
values must be either 0 or 255, representing the non-edge and edge points, respectively. The
generated label values, using the Agisoft-Metashape and the modiﬁed Mapillary-OpenSfM
workﬂows, rang between 0 and 255 because the images are resized during the execution of
the SfM-MVS pipelines to reduce the computational cost of the algorithms. Additionally,
some 2D edges were not detected in all the images in which they are depicted, due to the
weaknesses of the performed 2D edge-detection technique and so the mean value of the
pixels associated with the 3D point is not equal to 255. To this end, a threshold value, which
is used as an empirical classiﬁcation criterion, was included into the 3DPlan algorithm.
The idea is that the 3D points with values larger than the threshold constitute the “strong”
edge points in contrast to those with values smaller than the threshold, which constitute
the “weak” edge points. The label value of the non-edge points is equal to zero. This
process ﬁlters the detected edge points and stores the “strong” ones. The remaining edge
points using a classiﬁcation criterion equal to 0, 100, 150 and 200, respectively, are depicted
in Figure 14.


--- Page 20 ---

Heritage 2022, 5
4056
Figure 14. Remaining edge points using threshold value equal to 0, 100, 150, 200, respectively.
5.3. 3D Edge Points of Each Edge and 3D Vectorization
Afterwards, the “strong” edge points are fed into the DBSCAN algorithm, which
aims to decompose the detected points into clusters presenting the points of each edge.
Two variables are deﬁned, i.e., the “min samples” and the “eps”. The “min samples” sets
the minimum number of points required into a cluster to be validated and deﬁned as a
value larger than two to eliminate sets of clusters, which are more likely to be erroneous
because the clusters with small number of points describe areas with a sparsity of 3D points,
and, thus, are considered as noise. The “eps” value deﬁnes each point’s neighborhood and
so it must be smaller than the average distance between the 3D points in order to separate
them efﬁciently, into points of each 3D edge. Afterwards, the points of each edge are
imported into the RANSAC algorithm, in which the inliers are considered using line ﬁtting
and used into the 3D vectorization step. The results of the combination of the DBSCAN
and RANSAC algorithms as well as the 3D vectorization step need further investigation
(Figures 11c,d and 12c,d). In this effort, the 3D vectorization procedure is conducted using
the ﬁrst and the last point of each cluster produced using the RANSAC algorithm; this
procedure must be changed in order to exploit the information of each point into the
cluster to ﬁt the most accurate 3D line. To this end, the parameters calculated from the
RANSAC algorithm could be exploited during the vectorization step. Moreover, different
clustering algorithms and statistical methods could be applied to future implementations
to separate the 3D points into points of each edge. Additionally, different implementations
using the provided algorithms (DBSCAN and RANSAC) must be performed to investigate
their performance using several combinations of their parameters. The clustering and 3D
vectorization tasks constitute a ﬁrst investigation of the problem and, obviously, further
analysis is essential.
5.4. 3D Point-Cloud Characteristics and Post-Processing
The characteristics of the created 3D point cloud play a signiﬁcant role in the product’s
quality and, thus, the performance of the SfM-MVS workﬂows is vital. More concretely, a
point cloud with reduced noise, dense distribution of points and without a lack of infor-
mation constitute a high-quality product and so the 3DPlan algorithm could detect and
separate the 3D edges with the most feasible accuracy. To this end, the 3DPlan algorithm
was linked with the Agisoft-Metashape and the modiﬁed Mapillary-OpenSfM software.
The semantic information is passed to the semantically enriched point cloud, exploiting
the professional software and the four-channel images. Figures 11a,b and 12a,b depict the
semantic information passed to the 3D space correctly, with respect to the label channels. In
addition to the professional SfM-MVS software, the 3DPlan algorithm includes the “Trian-
gulation” algorithm. The “Triangulation" algorithm is not an accurate and recommended
approach for the 3D edge-detection task, after all, the scope of that algorithm was the devel-
opment of the secondary Python scripts and not the production of a professional SfM-MVS
software. Apart from the generation of the point cloud, a post process of it, e.g., removing
the ground points or the blue points coming from sky, etc., could improve the 3D edge-
detection task. Several experiments using a post-processed point cloud could be performed
to validate the performance of the 3DPlan algorithm using high-quality point clouds. In
addition, software performance and the complexity of the object under investigation affect
the quality of the product. For instance, the Old Police Station dataset, which is built of
stones, presents several misdetections because stones’ edges are similar to noise and, thus,


--- Page 21 ---

Heritage 2022, 5
4057
are eliminated during the process (Figure 15). Additionally, the stones are characterized by
textured areas and shadows, which make the edge-detection procedure difﬁcult. Hence, a
plethora of experiments using different threshold values should be conducted in each case
study, during the execution of the 3DPlan algorithm, to ﬁnd the optimal parameters and so
the most accurate result. Finally, the modiﬁed Mapillary-OpenSfM approach is available
only for the Ubuntu distribution while the Agisoft-Metashape approach is available for
any operating system and so the 3DPlan algorithm can be executed using every operating
system. The Mapillary-OpenSfM is a non-commercial software and, thus, the 3DPlan
algorithm can be used by everyone. Additionally, during the execution of the 3DPlan
algorithm, no extra mathematical equations are included, apart from those used from the
SfM-MVS software and, thus, additional errors are not embedded.
Figure 15. Close view of the 3D edge points (white) and all the rest of the Old Police Station dataset.
6. Concluding Remarks and Future Work
In this effort, an open-source 3D edge-detection and vectorization algorithm written
in Python is presented. To be more speciﬁc, the RGB images, which are used for the point-
cloud production, are enriched with an extra channel, which contains the edge semantic
information. Afterwards, professional SfM-MVS pipelines are exploited to produce the
semantically enriched point cloud. In fact, the 3DPlan algorithm works with point clouds
produced by the SfM-MVS process only. Eventually, the 3D edge points are ﬁrstly separated
and then classiﬁed into the points of each edge. Each cluster is passed into the vectoriza-
tion step from which the 3D vectors are created and saved using the “.dxf” format. The
3DPlan software can be combined with any 2D edge-detection technique as well as with
any 2D image-segmentation technique, e.g., semantic segmentation, instance segmentation,
panoptic segmentation etc., to pass the label channel information into 3D space. Addi-
tionally, it uses professional and guaranteed SfM-MVS software (Agisoft-Metashape and
Mapillary-OpenSfM) for the generation of the semantically enriched point cloud. Apart
from that, during the execution of the 3DPlan algorithm, extra mathematical equations are
not included, such as plane intersection, and, thus, additional errors are not embedded
to the 3D edge-detection process. On the other hand, the clustering approach, i.e., the
combination of the DBSCAN and RANSAC algorithms, suffers from several erroneously
detected clusters. Apart from that, the 3D vectorization procedure is over-simpliﬁed with-
out the exploitation of the entire information of each cluster, i.e., every detected 3D edge
point, and, thus, a further investigation and development of those methods is required.
The presented results contain too many artifacts and the output drawings contain too many
vectors, due to the aforementioned disadvantages, so except from the improvement of the
existing steps, simpliﬁcation and topology analysis steps should be included in 3DPlan
software. Additionally, a GUI of the 3DPlan software could be produced in order to be
convenient for a plethora of users. The detected edges using a georeferenced point cloud
and the 3DPlan software can be compared with the real edges captured by conventional
surveying methods (total station) to evaluate the precision of the process. More precisely,
we could measure a speciﬁc edge using a total station, then digitally measure the same edge


--- Page 22 ---

Heritage 2022, 5
4058
detected by the 3DPlan algorithm and, ﬁnally, subtract one from the other to evaluate the
precision of the detected 3D edge. In fact, the evaluation should be conducted for a large
number of speciﬁc edges, to draw accurate conclusions. In Section 2, some methods which
exploit BIM models are presented. The creation of a detailed BIM is difﬁcult especially into
the cultural-heritage domain, which is characterized by complexity and heterogeneity. In
Section 2, edge-detection methods are investigated more than methods using BIM because
the proposed approach investigates a simple yet efﬁcient pipeline to detect 3D edges ex-
ploiting 2D edge semantic information in combination with professional SfM-MVS software.
Additionally, the poor 2D edge semantic information is one of the major reasons for the
detection of erroneous 3D edge points by the 3DPlan software and, thus, an improved 2D
edge-detection procedure would produce better edge semantic information to feed into
the 3DPlan software. Moreover, a comparison between the traditional and deep-learning
2D edge-detection approaches should be conducted to ﬁnd the most suitable one for the
automation of the production of architectural vector drawings. An investigation of the
efﬁciency of the 3DPlan software applied to objects with several characteristics such as
complexity, size, etc., could be also conducted. Furthermore, another interesting investiga-
tion could be the implementation of the 3DPlan algorithm for the detection of multiple and
more complex objects or a detailed semantic segmentation in 3D space. To conclude, the
3DPlan software extracts 3D edges with the most feasible accuracy, depending on the 2D
edge-detection technique and can be executed by everyone, in any operating system.
Author Contributions: Conceptualization, A.G.; Methodology, A.G. and T.B.; Software, T.B.; Valida-
tion, T.B. and A.G.; Formal analysis, T.B.; Writing—original draft preparation, T.B.; Writing—review
and editing, T.B. and A.G.; Supervision, A.G. All authors have read and agreed to the published
version of the manuscript.
Funding: This research was partially funded by the General Secretariat of Research and Technology
through an EU matching funds project.
Data Availability Statement: The data and material used in this research were provided by the
Laboratory of Photogrammetry of NTUA and originate from previous projects. The python code
of the 3DPlan software is available on Github [https://github.com/thobet/3DPlan (accessed on
15 September 2022)]. The Python code of the new branch of the Mapillary-OpenSfM software is avail-
able on GitHub [https://github.com/thobet/OpenSfM_3DPlan (accessed on 15 September 2022)].
Acknowledgments: The authors would like to thank PhD Candidate in NTUA, Elisavet (Ellie) K.
Stathopoulou for her advice during this effort.The Python code is available on GitHub.
Conﬂicts of Interest: There are no conﬂicts of interest.
Abbreviations
The following abbreviations are used in this manuscript:
DBSCAN
Density-based spatial clustering of applications with noise
MVS
Multi-view stereo
RANSAC
Random sample consensus
SfM
Structure from motion
References
1.
Murtiyoso, A.; Pellis, E.; Grussenmeyer, P.; Landes, T.; Masiero, A. Towards Semantic Photogrammetry: Generating Semantically
Rich Point Clouds from Architectural Close-Range Photogrammetry. Sensors 2022, 22, 966. [CrossRef] [PubMed]
2.
Pellis, E.; Murtiyoso, A.; Masiero, A.; Tucci, G.; Betti, M.; Grussenmeyer, P. An Image-Based Deep Learning Workﬂow for 3D
Heritage Point Cloud Semantic Segmentation. Int. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci.-ISPRS Arch. 2022, 46, 429–434.
[CrossRef]
3.
Gülch, E.; Obrock, L. Automated semantic modelling of building interiors from images and derived point clouds based on deep
learning methods. Int. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci. 2020, 43, 421–426. [CrossRef]
4.
Stathopoulou, E.K.; Remondino, F. Semantic photogrammetry: Boosting image-based 3D reconstruction with semantic labeling.
Int. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci. 2019, 42, W9. [CrossRef]


--- Page 23 ---

Heritage 2022, 5
4059
5.
Stathopoulou, E.K.; Battisti, R.; Cernea, D.; Remondino, F.; Georgopoulos, A. Semantically derived geometric constraints for MVS
reconstruction of textureless areas. Remote Sens. 2021, 13, 1053. [CrossRef]
6.
Blake, B. On Draughtsmanship and the 2 & a Half D World. Available online: https://billboyheritagesurvey.wordpress.com/20
22/09/23/on-draughtsmanship-and-the-2and-a-half-d-world/ (accessed on 4 October 2022).
7.
Minaee, S.; Boykov, Y.Y.; Porikli, F.; Plaza, A.J.; Kehtarnavaz, N.; Terzopoulos, D. Image segmentation using deep learning: A
survey. IEEE Trans. Pattern Anal. Mach. Intell. 2021, 44, 3523–3542. [CrossRef]
8.
Xie, Y.; Tian, J.; Zhu, X.X. Linking points with labels in 3D: A review of point cloud semantic segmentation. IEEE Geosci. Remote
Sens. Mag. 2020, 8, 38–59. [CrossRef]
9.
Zhang, J.; Zhao, X.; Chen, Z.; Lu, Z. A review of deep learning-based semantic segmentation for point cloud. IEEE Access 2019,
7, 179118–179133. [CrossRef]
10.
Agisoft-Metashape. Discover Intelligent Photogrammetry with Metashape. 2016. Available online: http://www.agisoft.com/
(accessed on 4 October 2022).
11.
Mapillary-OpenSfM. An Open-Source Structure from Motion Library That Lets You Build 3D Models from Images. Available
online: https://opensfm.org/ (accessed on 4 October 2022).
12.
Bienert, A. Vectorization, edge preserving smoothing and dimensioning of proﬁles in laser scanner point clouds. In Proceedings
of the XXIst ISPRS Congress, Beijing, China, 3–11 July 2008; Volume 311.
13.
Nguatem, W.; Drauschke, M.; Mayer, H. Localization of Windows and Doors in 3d Point Clouds of Facades.
ISPRS Ann.
Photogramm. Remote Sens. Spat. Inf. Sci. 2014, II-3, 87–94. [CrossRef]
14.
Lin, Y.; Wang, C.; Cheng, J.; Chen, B.; Jia, F.; Chen, Z.; Li, J. Line segment extraction for large scale unorganized point clouds.
ISPRS J. Photogramm. Remote Sens. 2015, 102, 172–183. [CrossRef]
15.
Grompone von Gioi, R.; Jakubowicz, J.; Morel, J.M.; Randall, G. LSD: A line segment detector. Image Process. Line 2012, 2, 35–55.
[CrossRef]
16.
Mitropoulou, A.; Georgopoulos, A. An automated process to detect edges in unorganized point clouds. ISPRS Ann. Photogramm.
Remote. Sens. Spat. Inf. Sci. 2019, 4, 99–105. [CrossRef]
17.
PCL. Point Cloud Library. Available online: https://pointcloudlibrary.github.io/ (accessed on 4 October 2022).
18.
Bazazian, D.; Casas, J.R.; Ruiz-Hidalgo, J. Fast and robust edge extraction in unorganized point clouds. In Proceedings of the
2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA), Adelaide, SA, Australia,
23–25 November 2015; pp. 1–8.
19.
Lu, X.; Liu, Y.; Li, K. Fast 3D line segment detection from unorganized point cloud. arXiv 2019, arXiv:1901.02532.
20.
Dolapsaki, M.M.; Georgopoulos, A. Edge Detection in 3D Point Clouds Using Digital Images. ISPRS Int. J.-Geo-Inf. 2021, 10, 229.
[CrossRef]
21.
Alshawabkeh, Y. Linear feature extraction from point cloud using color information. Herit. Sci. 2020, 8, 28. [CrossRef]
22.
Canny, J.F. Finding Edges and Lines in Images; Technical Report; Massachusetts Inst of Tech Cambridge Artiﬁcial Intelligence Lab:
Cambridge, MA, USA, 1983.
23.
Bao, T.; Zhao, J.; Xu, M. Step edge detection method for 3D point clouds based on 2D range images. Optik 2015, 126, 2706–2710.
[CrossRef]
24.
Hofer, M.; Maurer, M.; Bischof, H. Efﬁcient 3D scene abstraction using line segments.
Comput. Vis. Image Underst. 2017,
157, 167–178. [CrossRef]
25.
Bazazian, D.; Parés, M.E. EDC-Net: Edge detection capsule network for 3D point clouds. Appl. Sci. 2021, 11, 1833. [CrossRef]
26.
Koch, S.; Matveev, A.; Jiang, Z.; Williams, F.; Artemov, A.; Burnaev, E.; Alexa, M.; Zorin, D.; Panozzo, D. Abc: A big cad model
dataset for geometric deep learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
Long Beach, CA, USA, 15–20 June 2019; pp. 9601–9611.
27.
Chang, A.X.; Funkhouser, T.; Guibas, L.; Hanrahan, P.; Huang, Q.; Li, Z.; Savarese, S.; Savva, M.; Song, S.; Su, H. Shapenet: An
information-rich 3d model repository. arXiv 2015, arXiv:1512.03012.
28.
Qi, C.R.; Su, H.; Mo, K.; Guibas, L.J. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 652–660.
29.
Qi, C.R.; Yi, L.; Su, H.; Guibas, L.J. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. arXiv 2018,
arXiv:1706.02413.
30.
Liu, Y.; D’Aronco, S.; Schindler, K.; Wegner, J.D. PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds. arXiv 2021,
arXiv:2103.02766.
31.
Chuang, T.Y.; Sung, C.C. Learning-guided point cloud vectorization for building component modeling. Autom. Constr. 2021,
132, 103978. [CrossRef]
32.
Bassier, M.; Vergauwen, M.; Van Genechten, B. Automated Semantic Labelling of 3D Vector Models for Scan-to-BIM. In Proceed-
ings of the 4th Annual International Conference on Architecture and Civil Engineering (ACE 2016), Singapore, 25–26 April 2016.
[CrossRef]
33.
Macher, H.; Landes, T.; Grussenmeyer, P. From Point Clouds to Building Information Models: 3D Semi-Automatic Reconstruction
of Indoors of Existing Buildings. Appl. Sci. 2017, 7, 1030. [CrossRef]
34.
Ochmann, S.; Vock, R.; Klein, R. Automatic reconstruction of fully volumetric 3D building models from oriented point clouds.
ISPRS J. Photogramm. Remote. Sens. 2019, 151, 251–262. [CrossRef]


--- Page 24 ---

Heritage 2022, 5
4060
35.
Obrock, L.S.; Gülch, E. First steps to automated interior reconstruction from semantically enriched point clouds and imagery. Int.
Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2018, XLII-2, 781–787. [CrossRef]
36.
Chen, L.C.; Zhu, Y.; Papandreou, G.; Schroff, F.; Adam, H. Encoder-decoder with atrous separable convolution for semantic image
segmentation. In Proceedings of the European conference on computer vision (ECCV), Munich, Germany, 8–14 September 2018;
pp. 801–818.
37.
Long, J.; Shelhamer, E.; Darrell, T. Fully convolutional networks for semantic segmentation.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7–12 June 2015; pp. 3431–3440.
38.
Xie, S.; Tu, Z. Holistically-nested edge detection. In Proceedings of the IEEE International Conference on Computer Vision,
Washington, DC, USA, 7–13 December 2015; pp. 1395–1403.
39.
Poma, X.S.; Riba, E.; Sappa, A. Dense extreme inception network: Towards a robust cnn model for edge detection. In Proceedings
of the IEEE/CVF Winter Conference on Applications of Computer Vision, Waikoloa, HI, USA, 4–8 January 2020; pp. 1923–1932.
40.
He, J.; Zhang, S.; Yang, M.; Shan, Y.; Huang, T. Bi-directional cascade network for perceptual edge detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15–20 June 2019; pp. 3828–3837.
41.
Liu, Y.; Cheng, M.M.; Hu, X.; Wang, K.; Bai, X. Richer convolutional features for edge detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 3000–3009.
42.
Wang, Y.; Zhao, X.; Huang, K. Deep crisp boundaries. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 3892–3900.
43.
Bhanu, B.; Lee, S.; Ho, C.C.; Henderson, T. Range data processing: Representation of surfaces by edges. In Proceedings of
the Eighth International Conference on Pattern Recognition, Paris, France, 21–31 October 1986; IEEE Computer Society Press:
Piscataway, NJ, USA, 1986; pp. 236–238.
44.
The 3-Clause BSD License | Open Source Initiative.
Available online: https://opensource.org/licenses/BSD-3-Clause
(accessed on 4 October 2022).
45.
Fischler, M.A.; Bolles, R.C. Random sample consensus: A paradigm for model ﬁtting with applications to image analysis and
automated cartography. Commun. ACM 1981, 24, 381–395. [CrossRef]
46.
Bolles, R.C.; Fischler, M.A. A RANSAC-based approach to model ﬁtting and its application to ﬁnding cylinders in range data. In
Proceedings of the IJCAI, Vancouver, BC, Canada, 24–28 August 1981; Volume 1981, pp. 637–643.
47.
Ester, M.; Kriegel, H.P.; Sander, J.; Xu, X. A density-based algorithm for discovering clusters in large spatial databases with noise.
In Proceedings of the kdd, Portland, OR, USA, 2–4 August 1996; Volume 96, pp. 226–231.
48.
OpenCV. Open Source Computer Vision Library. Available online: https://opencv.org/ (accessed on 4 October 2022).
49.
Stefanakis, M.; Kalogeropoulos, K.; Georgopoulos, A.; Bourbou, C. Exploring the ancient demos of Kymissaleis on Rhodes:
Multdisciplinary experimental research and theoretical issues. In Classical Archaeology in Context: Theory and Practice in Excavation
in the Greek World; Walter de Gruyter GmbH & Co. KG: Berlin, Germany, 2015; pp. 259–314.
50.
Stefanakis, M.I. The Kymissala (Rhodes, Greece) Archaeological Research Project. Archeologia 2015, 66, 47–63.
51.
Georgopoulos, A.; Tapinaki, S.; Stefanakis, M.I. Innovative Methods for Digital Heritage Documentation: The archaeological
site of Kymissala in Rhodes. In Proceedings of the ICOMOS 19th General Assembly and Scientiﬁc Symposium “Heritage and
Democracy”, New Delhi, India, 13–14 December 2017.
```

---

## Section 1.29: Poisson surface reconstruction

Source File: Poisson surface reconstruction.txt

### Paper Content:

```
# Poisson surface reconstruction.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Poisson surface reconstruction.pdf
# Output: ../layer2_completion/txt/Poisson surface reconstruction.txt


--- Page 1 ---

Eurographics Symposium on Geometry Processing (2006)
Konrad Polthier, Alla Sheffer (Editors)
Poisson Surface Reconstruction
Michael Kazhdan1, Matthew Bolitho1 and Hugues Hoppe2
1Johns Hopkins University, Baltimore MD, USA
2Microsoft Research, Redmond WA, USA
Abstract
We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson
formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and
is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a
hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse
linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are pro-
portional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate
reconstruction of surfaces with greater detail than previously achievable.
1. Introduction
Reconstructing 3D surfaces from point samples is a well
studied problem in computer graphics. It allows ﬁtting of
scanned data, ﬁlling of surface holes, and remeshing of ex-
isting models. We provide a novel approach that expresses
surface reconstruction as the solution to a Poisson equation.
Like much previous work (Section 2), we approach the
problem of surface reconstruction using an implicit function
framework. Speciﬁcally, like [Kaz05] we compute a 3D in-
dicator function χ (deﬁned as 1 at points inside the model,
and 0 at points outside), and then obtain the reconstructed
surface by extracting an appropriate isosurface.
Our key insight is that there is an integral relationship be-
tween oriented points sampled from the surface of a model
and the indicator function of the model. Speciﬁcally, the gra-
dient of the indicator function is a vector ﬁeld that is zero
almost everywhere (since the indicator function is constant
almost everywhere), except at points near the surface, where
it is equal to the inward surface normal. Thus, the oriented
point samples can be viewed as samples of the gradient of
the model’s indicator function (Figure 1).
The problem of computing the indicator function thus re-
duces to inverting the gradient operator, i.e. ﬁnding the scalar
function χ whose gradient best approximates a vector ﬁeld
⃗V deﬁned by the samples, i.e. minχ ∥∇χ −⃗V∥. If we apply
the divergence operator, this variational problem transforms
into a standard Poisson problem: compute the scalar func-
1
1
1
0
0
FM
0
0
0
0
0
1
1
1
0
Indicator function
FM
Indicator gradient
0
0
0
0
0
0
Surface
wM
Oriented points
V
G
Figure 1: Intuitive illustration of Poisson reconstruction in 2D.
tion χ whose Laplacian (divergence of gradient) equals the
divergence of the vector ﬁeld ⃗V,
∆χ ≡∇·∇χ = ∇·⃗V.
We will make these deﬁnitions precise in Sections 3 and 4.
Formulating surface reconstruction as a Poisson problem
offers a number of advantages. Many implicit surface ﬁtting
methods segment the data into regions for local ﬁtting, and
further combine these local approximations using blending
functions. In contrast, Poisson reconstruction is a global so-
lution that considers all the data at once, without resorting
to heuristic partitioning or blending. Thus, like radial basis
function (RBF) approaches, Poisson reconstruction creates
very smooth surfaces that robustly approximate noisy data.
But, whereas ideal RBFs are globally supported and non-
decaying, the Poisson problem admits a hierarchy of locally
supported functions, and therefore its solution reduces to a
well-conditioned sparse linear system.
c
⃝The Eurographics Association 2006.


--- Page 2 ---

Kazhdan et al. / Poisson Surface Reconstruction
Moreover, in many implicit ﬁtting schemes, the value
of the implicit function is constrained only near the sam-
ple points, and consequently the reconstruction may con-
tain spurious surface sheets away from these samples. Typ-
ically this problem is attenuated by introducing auxiliary
“off-surface” points (e.g. [CBC∗01, OBA∗03]). With Pois-
son surface reconstruction, such surface sheets seldom arise
because the gradient of the implicit function is constrained at
all spatial points. In particular it is constrained to zero away
from the samples.
Poisson systems are well known for their resilience in the
presence of imperfect data. For instance, “gradient domain”
manipulation algorithms (e.g. [FLW02]) intentionally mod-
ify the gradient data such that it no longer corresponds to any
real potential ﬁeld, and rely on a Poisson system to recover
the globally best-ﬁtting model.
There has been broad interdisciplinary research on solv-
ing Poisson problems and many efﬁcient and robust methods
have been developed. One particular aspect of our problem
instance is that an accurate solution to the Poisson equation
is only necessary near the reconstructed surface. This allows
us to leverage adaptive Poisson solvers to develop a recon-
struction algorithm whose spatial and temporal complexities
are proportional to the size of the reconstructed surface.
2. Related Work
Surface reconstruction
The reconstruction of surfaces
from oriented points has a number of difﬁculties in prac-
tice. The point sampling is often nonuniform. The positions
and normals are generally noisy due to sampling inaccuracy
and scan misregistration. And, accessibility constraints dur-
ing scanning may leave some surface regions devoid of data.
Given these challenges, reconstruction methods attempt to
infer the topology of the unknown surface, accurately ﬁt (but
not overﬁt) the noisy data, and ﬁll holes reasonably.
Several approaches are based on combinatorial structures,
such as Delaunay triangulations (e.g. [Boi84, KSO04]), al-
pha shapes [EM94, BBX95, BMR∗99]), or Voronoi dia-
grams [ABK98, ACK01]. These schemes typically create a
triangle mesh that interpolates all or a most of the points.
In the presence of noisy data, the resulting surface is often
jagged, and is therefore smoothed (e.g. [KSO04]) or reﬁt to
the points (e.g. [BBX95]) in subsequent processing.
Other schemes directly reconstruct an approximating sur-
face, typically represented in implicit form. We can broadly
classify these as either global or local approaches.
Global ﬁtting methods commonly deﬁne the implicit
function as the sum of radial basis functions (RBFs) centered
at the points (e.g. [Mur91, CBC∗01, TO02]). However, the
ideal RBFs (polyharmonics) are globally supported and non-
decaying, so the solution matrix is dense and ill-conditioned.
Practical solutions on large datasets involve adaptive RBF
reduction and the fast multipole method [CBC∗01].
Local ﬁtting methods consider subsets of nearby points at
a time. A simple scheme is to estimate tangent planes and
deﬁne the implicit function as the signed distance to the tan-
gent plane of the closest point [HDD∗92]. Signed distance
can also be accumulated into a volumetric grid [CL96]. For
function continuity, the inﬂuence of several nearby points
can be blended together, for instance using moving least
squares [ABCO∗01,SOS04]. A different approach is to form
point neighborhoods by adaptively subdividing space, for
example with an adaptive octree. Blending is possible over
an octree structure using a multilevel partition of unity, and
the type of local implicit patch within each octree node can
be selected heuristically [OBA∗03].
Our Poisson reconstruction combines beneﬁts of both
global and local ﬁtting schemes. It is global and therefore
does not involve heuristic decisions for forming local neigh-
borhoods, selecting surface patch types, and choosing blend
weights. Yet, the basis functions are associated with the am-
bient space rather than the data points, are locally supported,
and have a simple hierarchical structure that results in a
sparse, well-conditioned system.
Our approach of solving an indicator function is sim-
ilar to the Fourier-based reconstruction scheme of Kazh-
dan [Kaz05]. In fact, we show in Appendix A that our basic
Poisson formulation is mathematically equivalent. Indeed,
the Fast Fourier Transform (FFT) is a common technique
for solving dense, periodic Poisson systems. However, the
FFT requires O(r3 logr) time and O(r3) space where r is
the 3D grid resolution, quickly becoming prohibitive for ﬁne
resolutions. In contrast, the Poisson system allows adaptive
discretization, and thus yields a scalable solution.
Poisson problems
The Poisson equation arises in numer-
ous applications areas. For instance, in computer graph-
ics it is used for tone mapping of high dynamic range im-
ages [FLW02], seamless editing of image regions [PGB03],
ﬂuid mechanics [LGF04], and mesh editing [YZX∗04].
Multigrid Poisson solutions have even been adapted for efﬁ-
cient GPU computation [BFGS03,GWL∗03].
The Poisson equation is also used in heat transfer and
diffusion problems. Interestingly, Davis et al [DMGL02]
use diffusion to ﬁll holes in reconstructed surfaces. Given
boundary conditions in the form of a clamped signed dis-
tance function d, their diffusion approach essentially solves
the homogeneous Poisson equation ∆d = 0 to create an im-
plicit surface spanning the boundaries. They use a local iter-
ative solution rather than a global multiscale Poisson system.
Nehab et al [NRDR05] use a Poisson system to ﬁt a 2.5D
height ﬁeld to sampled positions and normals. Their ap-
proach ﬁts a given parametric surface and is well-suited to
the reconstruction of surfaces from individual scans. How-
ever, in the case that the samples are obtained from the union
of multiple scans, their approach cannot be directly applied
to obtain a connected, watertight surface.
c
⃝The Eurographics Association 2006.


--- Page 3 ---

Kazhdan et al. / Poisson Surface Reconstruction
3. Our Poisson reconstruction approach
The input data S is a set of samples s ∈S, each consisting of a
point s.p and an inward-facing normal s.⃗N, assumed to lie on
or near the surface ∂M of an unknown model M. Our goal is
to reconstruct a watertight, triangulated approximation to the
surface by approximating the indicator function of the model
and extracting the isosurface, as illustrated in Figure 2.
The key challenge is to accurately compute the indicator
function from the samples. In this section, we derive a rela-
tionship between the gradient of the indicator function and
an integral of the surface normal ﬁeld. We then approximate
this surface integral by a summation over the given oriented
point samples. Finally, we reconstruct the indicator function
from this gradient ﬁeld as a Poisson problem.
Deﬁning the gradient ﬁeld Because the indicator function
is a piecewise constant function, explicit computation of its
gradient ﬁeld would result in a vector ﬁeld with unbounded
values at the surface boundary. To avoid this, we convolve
the indicator function with a smoothing ﬁlter and consider
the gradient ﬁeld of the smoothed function. The following
lemma formalizes the relationship between the gradient of
the smoothed indicator function and the surface normal ﬁeld.
Lemma: Given a solid M with boundary ∂M, let χM de-
note the indicator function of M, ⃗N∂M(p) be the inward
surface normal at p ∈∂M, ˜F(q) be a smoothing ﬁlter, and
˜Fp(q) = ˜F(q−p) its translation to the point p. The gradient
of the smoothed indicator function is equal to the vector ﬁeld
obtained by smoothing the surface normal ﬁeld:
∇

χM ∗˜F

(q0) =

∂M
˜Fp(q0)⃗N∂M(p)dp.
(1)
Proof: To prove this, we show equality for each of the com-
ponents of the vector ﬁeld. Computing the partial derivative
of the smoothed indicator function with respect to x, we get:
∂
∂x

q0

χM ∗˜F

=
∂
∂x

q=q0

M
˜F(q−p)dp
=

M

−∂
∂x
˜F(q0 −p)

dp
= −

M ∇·
 ˜F(q0 −p),0,0

dp
=

∂M
 ˜Fp(q0),0,0

,⃗N∂M(p)

dp.
(The ﬁrst equality follows from the fact that χM is equal to
zero outside of M and one inside. The second follows from
the fact that (∂/∂q) ˜F(q−p) = −(∂/∂p) ˜F(q−p). The last
follows from the Divergence Theorem.)
A similar argument shows that the y-, and z-components
of the two sides are equal, thereby completing the proof. □
Approximating the gradient ﬁeld
Of course, we cannot
evaluate the surface integral since we do not yet know the
Figure 2: Points from scans of the “Armadillo Man” model (left),
our Poisson surface reconstruction (right), and a visualization of the
indicator function (middle) along a plane through the 3D volume.
surface geometry. However, the input set of oriented points
provides precisely enough information to approximate the
integral with a discrete summation. Speciﬁcally, using the
point set S to partition ∂M into distinct patches Ps ⊂∂M,
we can approximate the integral over a patch Ps by the value
at point sample s.p, scaled by the area of the patch:
∇(χM ∗˜F)(q) = ∑
s∈S

Ps
˜Fp(q)⃗N∂M(p)dp
≈∑
s∈S
|Ps| ˜Fs.p(q) s.⃗N ≡⃗V(q).
(2)
It should be noted that though Equation 1 is true for any
smoothing ﬁlter ˜F, in practice, care must be taken in choos-
ing the ﬁlter. In particular, we would like the ﬁlter to satisfy
two conditions. On the one hand, it should be sufﬁciently
narrow so that we do not over-smooth the data. And on the
other hand, it should be wide enough so that the integral over
Ps is well approximated by the value at s.p scaled by the
patch area. A good choice of ﬁlter that balances these two
requirements is a Gaussian whose variance is on the order of
the sampling resolution.
Solving the Poisson problem Having formed a vector ﬁeld
⃗V, we want to solve for the function ˜χ such that ∇˜χ = ⃗V.
However, ⃗V is generally not integrable (i.e. it is not curl-
free), so an exact solution does not generally exist. To ﬁnd
the best least-squares approximate solution, we apply the di-
vergence operator to form the Poisson equation
∆˜χ = ∇·⃗V.
In the next section, we describe our implementation of
these steps in more detail.
4. Implementation
We ﬁrst present our reconstruction algorithm under the as-
sumption that the point samples are uniformly distributed
over the model surface. We deﬁne a space of functions with
high resolution near the surface of the model and coarser
resolution away from it, express the vector ﬁeld⃗V as a linear
sum of functions in this space, set up and solve the Poisson
equation, and extract an isosurface of the resulting indicator
function. We then extend our algorithm to address the case
of non-uniformly sampled points.
c
⃝The Eurographics Association 2006.


--- Page 4 ---

Kazhdan et al. / Poisson Surface Reconstruction
4.1. Problem Discretization
First, we must choose the space of functions in which to dis-
cretize the problem. The most straightforward approach is
to start with a regular 3D grid [Kaz05], but such a uniform
structure becomes impractical for ﬁne-detail reconstruction,
since the dimension of the space is cubic in the resolution
while the number of surface triangles grows quadratically.
Fortunately, an accurate representation of the implicit
function is only necessary near the reconstructed surface.
This motivates the use of an adaptive octree both to repre-
sent the implicit function and to solve the Poisson system
(e.g. [GKS02,LGF04]). Speciﬁcally, we use the positions of
the sample points to deﬁne an octree O and associate a func-
tion Fo to each node o ∈O of the tree, choosing the tree and
the functions so that the following conditions are satisﬁed:
1. The vector ﬁeld ⃗V can be precisely and efﬁciently repre-
sented as the linear sum of the Fo.
2. The matrix representation of the Poisson equation, ex-
pressed in terms of the Fo can be solved efﬁciently.
3. A representation of the indicator function as the sum of
the Fo can be precisely and efﬁciently evaluated near the
surface of the model.
Deﬁning the function space Given a set of point samples
S and a maximum tree depth D, we deﬁne the octree O to be
the minimal octree with the property that every point sample
falls into a leaf node at depth D.
Next, we deﬁne a space of functions obtained as the span
of translates and scales of a ﬁxed, unit-integral, base func-
tion F : R3 →R. For every node o ∈O, we set Fo to be the
unit-integral “node function” centered about the node o and
stretched by the size of o:
Fo(q) ≡F
q−o.c
o.w

1
o.w3 .
where o.c and o.w are the center and width of node o.
This space of functions FO,F ≡Span{Fo} has a multires-
olution structure similar to that of traditional wavelet repre-
sentations. Finer nodes are associated with higher-frequency
functions, and the function representation becomes more
precise as we near the surface.
Selecting a base function
In selecting a base function F,
our goal is to choose a function so that the vector ﬁeld ⃗V,
deﬁned in Equation 2, can be precisely and efﬁciently repre-
sented as the linear sum of the node functions {Fo}.
If we were to replace the position of each sample with the
center of the leaf node containing it, the vector ﬁeld ⃗V could
be efﬁciently expressed as the linear sum of {Fo} by setting:
F(q) = ˜F
	 q
2D

.
This way, each sample would contribute a single term (the
normal vector) to the coefﬁcient corresponding to its leaf’s
node function. Since the sampling width is 2−D and the sam-
ples all fall into leaf nodes of depth D, the error arising from
the clamping can never be too big (at most, on the order of
half the sampling width). In the next section, we show how
the error can be further reduced by using trilinear interpola-
tion to allow for sub-node precision.
Finally, since a maximum tree depth of D corresponds to a
sampling width of 2−D, the smoothing ﬁlter should approxi-
mate a Gaussian with variance on the order of 2−D. Thus, F
should approximate a Gaussian with unit-variance.
For efﬁciency, we approximate the unit-variance Gaussian
by a compactly supported function so that (1) the resulting
Divergence and Laplacian operators are sparse and (2) the
evaluation of a function expressed as the linear sum of Fo at
some point q only requires summing over the nodes o ∈O
that are close to q. Thus, we set F to be the n-th convolution
of a box ﬁlter with itself resulting in the base function F:
F(x,y,z) ≡(B(x)B(y)B(z))∗n
with B(t) =
 1 |t| < 0.5
0 otherwise
Note that as n is increased, F more closely approximates
a Gaussian and its support grows larger; in our implemen-
tation we use a piecewise quadratic approximation with
n = 3. Therefore, the function F is supported on the domain
[-1.5,1.5]3 and, for the basis function of any octree node,
there are at most 53-1 = 124 other nodes at the same depth
whose functions overlap with it.
4.2. Vector Field Deﬁnition
To allow for sub-node precision, we avoid clamping a sam-
ple’s position to the center of the containing leaf node and
instead use trilinear interpolation to distribute the sample
across the eight nearest nodes. Thus, we deﬁne our approxi-
mation to the gradient ﬁeld of the indicator function as:
⃗V(q) ≡∑
s∈S
∑
o∈NgbrD(s)
αo,sFo(q)s.⃗N
(3)
where NgbrD(s) are the eight depth-D nodes closest to s.p
and {αo,s} are the trilinear interpolation weights. (If the
neighbors are not in the tree, we reﬁne it to include them.)
Since the samples are uniform, we can assume that the
area of a patch Ps is constant and ⃗V is a good approxima-
tion, up to a multiplicative constant, of the gradient of the
smoothed indicator function. We will show that the choice
of multiplicative constant does not affect the reconstruction.
4.3. Poisson Solution
Having deﬁned the vector ﬁeld⃗V, we would like to solve for
the function ˜χ ∈FO,F such that the gradient of ˜χ is closest
to ⃗V, i.e. a solution to the Poisson equation ∆˜χ = ∇·⃗V.
One challenge of solving for ˜χ is that though ˜χ and the
c
⃝The Eurographics Association 2006.


--- Page 5 ---

Kazhdan et al. / Poisson Surface Reconstruction
coordinate functions of ⃗V are in the space FO,F it is not
necessarily the case that the functions ∆˜χ and ∇·⃗V are.
To address this issue, we need to solve for the function ˜χ
such that the projection of ∆˜χ onto the space FO,F is closest
to the projection of ∇·⃗V. Since, in general, the functions
Fo do not form an orthonormal basis, solving this problem
directly is expensive. However, we can simplify the problem
by solving for the function ˜χ minimizing:
∑
o∈O
⟨∆˜χ −∇·⃗V,Fo⟩

2
= ∑
o∈O
⟨∆˜χ,Fo⟩−⟨∇·⃗V,Fo⟩

2
.
Thus given the |O|-dimensional vector v whose o-th coordi-
nate is vo = ⟨∇·⃗V,Fo⟩, the goal is to solve for the function
˜χ such that the vector obtained by projecting the Laplacian
of ˜χ onto each of the Fo is as close to v as possible.
To express this in matrix form, let ˜χ = ∑o xoFo, so that
we are solving for the vector x ∈R|O|. Then, let us deﬁne the
|O|×|O| matrix L such that Lx returns the dot product of the
Laplacian with each of the Fo. Speciﬁcally, for all o,o′ ∈O,
the (o,o′)-th entry of L is set to:
Lo,o′ ≡
∂2Fo
∂x2 ,Fo′

+
∂2Fo
∂y2 ,Fo′

+
∂2Fo
∂z2 ,Fo′

.
Thus, solving for ˜χ amounts to ﬁnding
min
x∈R|O| ∥Lx−v∥2.
Note that the matrix L is sparse and symmetric. (Sparse
because the Fo are compactly supported, and symmetric be-
cause
 f ′′g = −
 f ′g′.) Furthermore, there is an inherent
multiresolution structure on FO,F, so we use an approach
similar to the multigrid approach in [GKS02], solving the
restriction Ld of L to the space spanned by the depth d func-
tions (using a conjugate gradient solver) and projecting the
ﬁxed-depth solution back onto FO,F to update the residual.
Addressing memory concerns In practice, as the depth in-
creases, the matrix Ld becomes larger and it may not be prac-
tical to store it in memory. Although the number of entries in
a column of Ld is bounded by a constant, the constant value
can be large. For example, even using a piecewise quadratic
base function F, we end up with as many as 125 non-zero
entries in a column, resulting in a memory requirement that
is 125 times larger than the size of the octree.
To address this issue, we augment our solver with a block
Gauss-Seidel solver. That is, we decompose the d-th dimen-
sional space into overlapping regions and solve the restric-
tion of Ld to these different regions, projecting the local so-
lutions back into the d-dimensional space and updating the
residuals. By choosing the number of regions to be a func-
tion of the depth d, we ensure that the size of the matrix used
by the solver never exceeds a desired memory threshold.
4.4. Isosurface Extraction
In order to obtain a reconstructed surface ∂˜M, it is necessary
to ﬁrst select an isovalue and then extract the corresponding
isosurface from the computed indicator function.
We choose the isovalue so that the extracted surface
closely approximates the positions of the input samples. We
do this by evaluating ˜χ at the sample positions and use the
average of the values for isosurface extraction:
∂˜M ≡{q ∈R3  ˜χ(q) = γ}
with
γ = 1
|S| ∑
s∈S
˜χ(s.p).
This choice of isovalue has the property that scaling ˜χ does
not change the isosurface. Thus, knowing the vector ﬁeld ⃗V
up to a multiplicative constant provides sufﬁcient informa-
tion for reconstructing the surface.
To extract the isosurface from the indicator function, we
use a method similar to previous adaptations of the March-
ing Cubes [LC87] to octree representations (e.g. [WG92,
SFYC96, WKE99]). However, due to the nonconforming
properties of our tree, we modify the reconstruction ap-
proach slightly, deﬁning the positions of zero-crossings
along an edge in terms of the zero-crossings computed by
the ﬁnest level nodes adjacent to the edge. In the case that an
edge of a leaf node has more than one zero-crossing associ-
ated to it, the node is subdivided. As in previous approaches,
we avoid cracks arising when coarser nodes share a face with
ﬁner ones by projecting the isocurve segments from the faces
of ﬁner nodes onto the face of the coarser one.
4.5. Non-uniform Samples
We now extend our method to the case of non-uniformly dis-
tributed point samples. As in [Kaz05], our approach is to es-
timate the local sampling density, and scale the contribution
of each point accordingly. However, rather than simply scal-
ing the magnitude of a ﬁxed-width kernel associated with
each point, we additionally adapt the kernel width. This re-
sults in a reconstruction that maintains sharp features in ar-
eas of dense sampling and provides a smooth ﬁt in sparsely
sampled regions.
Estimating local sampling density
Following the ap-
proach of [Kaz05], we implement the density computation
using a kernel density estimator [Par62]. The approach is to
estimate the number of points in a neighborhood of a sam-
ple by “splatting” the samples into a 3D grid, convolving the
“splatting” function with a smoothing ﬁlter, and evaluating
the convolution at each of the sample points.
We implement the convolution in a manner similar to
Equation 3. Given a depth ˆD ≤D we set the density esti-
mator to be the sum of node functions at depth ˆD:
W ˆD(q) ≡∑
s∈S
∑
o∈Ngbr ˆD(s)
αo,sFo(q).
c
⃝The Eurographics Association 2006.


--- Page 6 ---

Kazhdan et al. / Poisson Surface Reconstruction
Since octree nodes at lower resolution are associated with
functions that approximate Gaussians of larger width, the
parameter ˆD provides away for specifying the locality of the
density estimation, with smaller values of ˆD giving sampling
density estimates over larger regions.
Computing the vector ﬁeld
Using the density estimator,
we modify the summation in Equation 3 so that each sam-
ple’s contribution is proportional to its associated area on the
surface. Speciﬁcally, using the fact that the area is inversely
proportional to sampling density, we set:
⃗V(q) ≡∑
s∈S
1
W ˆD(s.p)
∑
o∈NgbrD(s)
αo,sFo(q).
However, adapting only the magnitudes of the sample
contributions results in poor noise ﬁltering in sparsely sam-
pled regions as demonstrated later in Figure 7. Therefore,
we additionally adapt the width of the smoothing ﬁlter ˜F to
the local sampling density. Adapting the ﬁlter width lets us
retain ﬁne detail in regions of dense sampling, while smooth-
ing out noise in regions of sparse sampling.
Using the fact that node functions at smaller depths corre-
spond to wider smoothing ﬁlters, we deﬁne
⃗V(q) ≡∑
s∈S
1
W ˆD(s.p)
∑
o∈NgbrDepth(s.p)(s)
αo,sFo(q).
In this deﬁnition, Depth(s.p) represents the desired depth of
a sample point s ∈S. It is deﬁned by computing the average
sampling density W over all of the samples and setting:
Depth(s.p) ≡min

D,D+log4(W ˆD(s.p)/W)

so that the width of the smoothing ﬁlter with which s con-
tributes to ⃗V is proportional to the radius of its associated
surface patch Ps.
Selecting an isovalue
Finally, we modify the surface ex-
traction step by selecting an isovalue which is the weighted
average of the values of ˜χ at the sample positions:
∂˜M ≡{q ∈R3  ˜χ(q) = γ}
with
γ =
∑
1
W ˆD(s.p) ˜χ(s.p)
∑
1
W ˆD(s.p)
.
5. Results
To evaluate our method we conducted a series of experi-
ments. Our goal was to address three separate questions:
How well does the algorithm reconstruct surfaces? How
does it compare to other reconstruction methods? And, what
are its performance characteristics?
Much practical motivation for surface reconstruction de-
rives from 3D scanning, so we have focused our experiments
on the reconstruction of 3D models from real-world data.
5.1. Resolution
We ﬁrst consider the effects of the maximum octree depth
on the reconstructed surface.
Figure 3 shows our reconstruction results for the “dragon”
model at octree depths 6, 8, and 10. (In the context of recon-
struction on a regular grid, this would correspond to reso-
lutions of 643, 2563, and 10243, respectively.) As the tree
depth is increased, higher-resolution functions are used to ﬁt
the indicator function, and consequently the reconstructions
capture ﬁner detail. For example, the scales of the dragon,
which are too ﬁne to be captured at the coarsest resolution
begin appearing and become more sharply pronounced as
the octree depth is increased.
Figure 3: Reconstructions of the dragon model at octree depths 6
(top), 8 (middle), and 10 (bottom).
5.2. Comparison to Previous Work
We compare the results of our reconstruction algorithm
to the results obtained using Power Crust [ACK01], Ro-
bust Cocone [DG04], Fast Radial Basis Functions (Fas-
tRBF) [CBC∗01], Multi-Level Partition of Unity Implicits
(MPU) [OBA∗03], Surface Reconstruction from Unorga-
nized Points [HDD∗92], Volumetric Range Image Process-
ing (VRIP) [CL96], and the FFT-based method of [Kaz05].
c
⃝The Eurographics Association 2006.


--- Page 7 ---

Kazhdan et al. / Poisson Surface Reconstruction
Figure 4: Reconstructions of the Stanford bunny using Power
Crust (a), Robust Cocone (b), Fast RBF (c), MPU (d), Hoppe et al.’s
reconstruction (e), VRIP (f), FFT-based reconstruction (g), and our
Poisson reconstruction (h).
Our initial test case is the Stanford “bunny” raw dataset of
362,000 points assembled from ten range images. The data
was processed to ﬁt the input format of each algorithm. For
example, when running our method, we estimated a sample’s
normal from the positions of the neighbors; Running VRIP,
we used the registered scans as input, maintaining the regu-
larity of the sampling, and providing the conﬁdence values.
Figure 4 compares the different reconstructions. Since the
scanned data contains noise, interpolatory methods such as
Power Crust (a) and Robust Cocone (b) generate surfaces
that are themselves noisy. Methods such as Fast RBF (c) and
MPU (d), which only constrain the implicit function near
the sample points, result in reconstructions with spurious
surface sheets. Non-interpolatory methods, such as the ap-
proach of [HDD∗92] (e), can smooth out the noise, although
often at the cost of model detail. VRIP (f), the FFT-based
approach (g), and the Poisson approach (h) all accurately re-
construct the surface of the bunny, even in the presence of
noise, and we compare these three methods in more detail.
Figure 5: Reconstructions of a fragment of the Forma Urbis Ro-
mae tablet using VRIP (left) and the Poisson solution (right).
Comparison to VRIP
A challenge in surface reconstruc-
tion is the recovery of sharp features. We compared our
method to VRIP by evaluating the reconstruction of sam-
ple points obtained from fragment 661a of the Forma Ur-
bis Romae (30 scans, 2,470,000 points) and the “Happy
Buddha” model (48 scans, 2,468,000 points), shown in Fig-
ures 5 and 6. In both cases, we ﬁnd that VRIP exhibits a
“lipping” phenomenon at sharp creases. This is due to the
fact that VRIP’s distance function is grown perpendicular to
the view direction, not the surface normal. In contrast, our
Poisson reconstruction, which is independent of view direc-
tion, accurately reconstructs the corner of the fragment and
the sharp creases in the Buddha’s cloak.
Comparison to the FFT-based approach
As Fig-
ure 4 demonstrates, our Poisson reconstruction (h) closely
matches the one obtained with the FFT-based method (g).
Since our method provides an adaptive solution to the same
problem, the similarity is a conﬁrmation that in adapting
the octree to the data, our method does not discard salient,
high-frequency information. We have also conﬁrmed that
our Poisson method maintains the high noise resilience al-
ready demonstrated in the results of [Kaz05].
Though theoretically equivalent in the context of uni-
formly sampled data, our use of adaptive-width ﬁlters (Sec-
tion 4.5) gives better reconstructions than the FFT-based
method on the non-uniform data commonly encountered in
3D scanning. For example, let us consider the region around
the left eye of the “David” model, shown in Figure 7(a). The
area above the eyelid (highlighted in red) is sparsely sam-
pled due to the fact that it is in a concave region and is seen
only by a few scans. Furthermore, the scans that do sample
c
⃝The Eurographics Association 2006.


--- Page 8 ---

Kazhdan et al. / Poisson Surface Reconstruction
Figure 6: Reconstructions of the “Happy Buddha” model using
VRIP (left) and Poisson reconstruction (right).
the region tend to sample at near-grazing angles resulting
in noisy position and normal estimates. Consequently, ﬁxed-
resolution reconstruction schemes such as the FFT-based ap-
proach (b) introduce high-frequency noise in these regions.
In contrast, our method (c), which adapts both the scale and
the variance of the samples’ contributions, ﬁts a smoother re-
construction to these regions, without sacriﬁcing ﬁdelity in
areas of dense sampling (e.g. the region highlighted in blue).
Limitation of our approach
A limitation of our method
is that it does not incorporate information associated with
the acquisition modality. Figure 6 shows an example of this
in the reconstruction at the base of the Buddha. Since there
are no samples between the two feet, our method (right)
connects the two regions. In contrast, the ability to use sec-
ondary information such as line of sight allows VRIP (left)
to perform the space carving necessary to disconnect the two
feet, resulting in a more accurate reconstruction.
5.3. Performance and Scalability
Table 1 summarizes the temporal and spatial efﬁciency of
our algorithm on the “dragon” model, and indicates that the
Figure 7: Reconstruction of samples from the region around the
left eye of the David model (a), using the ﬁxed-resolution FFT ap-
proach (b), and Poisson reconstruction (c).
memory and time requirements of our algorithm are roughly
quadratic in the resolution. Thus, as we increase the oc-
tree depth by one, we ﬁnd that the running time, the mem-
ory overhead, and the number of output triangles increases
roughly by a factor of four.
Tree Depth
Time
Peak Memory
# of Tris.
7
6
19
21,000
8
26
75
90,244
9
126
155
374,868
10
633
699
1,516,806
Table 1: The running time (in seconds), the peak memory usage (in
megabytes), and the number of triangles in the reconstructed model
for the different depth reconstructions of the dragon model. A kernel
depth of 6 was used for density estimation.
The running time and memory performance of our method
in reconstructing the Stanford Bunny at a depth of 9 is com-
pared to the performance of related methods in Table 2. Al-
though in this experiment, our method is neither fastest nor
most memory efﬁcient, its quadratic nature makes it scalable
to higher resolution reconstructions. As an example, Fig-
ure 8 shows a reconstruction of the head of Michelangelo’s
David at a depth of 11 from a set of 215,613,477 samples.
The reconstruction was computed in 1.9 hours and 5.2GB
of RAM, generating a 16,328,329 triangle model. Trying
to compute an equivalent reconstruction with methods such
as the FFT approach would require constructing two voxel
grids at a resolution of 20483 and would require in excess of
100GB of memory.
c
⃝The Eurographics Association 2006.


--- Page 9 ---

Kazhdan et al. / Poisson Surface Reconstruction
Figure 8: Several images of the reconstruction of the head of Michelangelo’s David, obtained running our algorithm with a maximum tree
depth of 11. The ability to reconstruct the head at such a high resolution allows us to make out the ﬁne features in the model such as the inset
iris, the drill marks in the hair, the chip on the eyelid, and the creases around the nose and mouth.
Method
Time
Peak Memory
# of Tris.
Power Crust
380
2653
554,332
Robust Cocone
892
544
272,662
FastRBF
4919
796
1,798,154
MPU
28
260
925,240
Hoppe et al 1992
70
330
950,562
VRIP
86
186
1,038,055
FFT
125
1684
910,320
Poisson
263
310
911,390
Table 2: The running time (in seconds), the peak memory usage
(in megabytes), and the number of triangles in the reconstructed
surface of the Stanford Bunny generated by the different methods.
6. Conclusion
We have shown that surface reconstruction can be expressed
as a Poisson problem, which seeks the indicator function that
best agrees with a set of noisy, non-uniform observations,
and we have demonstrated that this approach can robustly
recover ﬁne detail from noisy real-world scans.
There are several avenues for future work:
• Extend the approach to exploit sample conﬁdence values.
• Incorporate line-of-sight information from the scanning
process into the solution process.
• Extend the system to allow out-of-core processing for
huge datasets.
Acknowledgements
The authors would like to express their thanks to the Stan-
ford 3D Scanning Repository for their generosity in dis-
tributing their 3D models. The authors would also like to
express particular gratitude to Szymon Rusinkiewicz and
Benedict Brown for sharing valuable experiences and ideas,
and for providing non-rigid body aligned David data.
References
[ABCO∗01]
ALEXA M., BEHR J., COHEN-OR D., FLEISHMAN
S., LEVIN D., SILVA C.: Point set surfaces. In Proc. of the
Conference on Visualization ’01 (2001), 21–28.
[ABK98]
AMENTA N., BERN M., KAMVYSSELIS M.: A new
Voronoi-based surface reconstruction algorithm.
Computer
Graphics (SIGGRAPH ’98) (1998), 415–21.
c
⃝The Eurographics Association 2006.


--- Page 10 ---

Kazhdan et al. / Poisson Surface Reconstruction
[ACK01]
AMENTA N., CHOI S., KOLLURI R.: The power crust,
unions of balls, and the medial axis transform. Computational
Geometry: Theory and Applications 19 (2001), 127–153.
[BBX95]
BAJAJ C., BERNARDINI F., XU G.: Automatic recon-
struction of surfaces and scalar ﬁelds from 3d scans. In SIG-
GRAPH (1995), 109–18.
[BFGS03]
BOLZ J., FARMER I., GRINSPUN E., SCHRÖDER P.:
Sparse matrix solvers on the GPU: Conjugate gradients and
multigrid. TOG 22 (2003), 917–924.
[BMR∗99]
BERNARDINI F., MITTLEMAN J., RUSHMEIER H.,
SILVA C., TAUBIN G.: The ball-pivoting algorithm for surface
reconstruction. IEEE TVCG 5 (1999), 349–359.
[Boi84]
BOISSONNAT J.: Geometric structures for three dimen-
sional shape representation. TOG (1984), 266–286.
[CBC∗01]
CARR J., BEATSON R., CHERRIE H., MITCHEL T.,
FRIGHT W., MCCALLUM B., EVANS T.: Reconstruction and
representation of 3D objects with radial basis functions. SIG-
GRAPH (2001), 67–76.
[CL96]
CURLESS B., LEVOY M.: A volumetric method for build-
ing complex models from range images.
Computer Graphics
(SIGGRAPH ’96) (1996), 303–312.
[DG04]
DEY T., GOSWAMI S.: Provable surface reconstruction
from noisy samples. In Proc. of the Ann. Symp. Comp. Geom.
(2004), 428–438.
[DMGL02]
DAVIS J., MARSCHNER S., GARR M., LEVOY M.:
Filling holes in complex surfaces using volumetric diffusion. In
Int. Symp. 3DPVT (2002), 428–438.
[EM94]
EDELSBRUNNER H., MÜCKE E.: Three-dimensional al-
pha shapes. TOG (1994), 43–72.
[FLW02]
FATTAL R., LISCHINKSI D., WERMAN M.: Gradient
domain high dynamic range compression. In SIGGRAPH (2002),
249–256.
[GKS02]
GRINSPUN E., KRYSL P., SCHRÖDER P.:
Charms:
a simple framework for adaptive simulation.
In SIGGRAPH
(2002), 281–290.
[GWL∗03]
GOODNIGHT N., WOOLLEY C., LEWIN G., LUE-
BKE D., HUMPHREYS G.: A multigrid solver for boundary value
problems using programmable graphics hardware. In Graphics
Hardware (2003), 102–111.
[HDD∗92]
HOPPE H., DEROSE T., DUCHAMP T., MCDONALD
J., STUETZLE W.:
Surface reconstruction from unorganized
points. Computer Graphics 26 (1992), 71–78.
[Kaz05]
KAZHDAN M.: Reconstruction of solid models from ori-
ented point sets. SGP (2005), 73–82.
[KSO04]
KOLLURI R., SHEWCHUK J., O’BRIEN J.: Spectral
surface reconstruction from noisy point clouds. In SGP (2004),
11–21.
[LC87]
LORENSEN W., CLINE H.: Marching cubes: A high res-
olution 3d surface reconstruction algorithm. SIGGRAPH (1987),
163–169.
[LGF04]
LOSASSO F., GIBOU F., FEDKIW R.: Simulating water
and smoke with an octree data structure. TOG (SIGGRAPH ’04)
23 (2004), 457–462.
[Mur91]
MURAKI S.: Volumetric shape description of range data
using “blobby model”. Computer Graphics 25 (1991), 227–235.
[NRDR05]
NEHAB D., RUSINKIEWICZ S., DAVIS J., RA-
MAMOORTHI R.: Efﬁciently combining positions and normals
for precise 3D geometry. TOG (SIGGRAPH ’05) 24 (2005).
[OBA∗03]
OHTAKE Y., BELYAEV A., ALEXA M., TURK G.,
SEIDEL H.: Multi-level partition of unity implicits. TOG (2003),
463–470.
[Par62]
PARZEN E.: On estimation of a probability density func-
tion and mode. Ann. Math Stat. 33 (1962), 1065–1076.
[PGB03]
PÉREZ P., GANGNET M., BLAKE A.: Poisson image
editing. TOG (SIGGRAPH ’03) 22 (2003), 313–318.
[SFYC96]
SHEKHAR R., FAYYAD E., YAGEL R., CORNHILL J.:
Octree-based decimation of marching cubes surfaces. In IEEE
Visualization (1996), 335–342.
[SOS04]
SHEN C., O’BRIEN J., SHEWCHUK J.: Interpolating
and approximating implicit surfaces from polygon soup. TOG
(SIGGRAPH ’04) 23 (2004), 896–904.
[TO02]
TURK G., O’BRIEN J.: Modelling with implicit surfaces
that interpolate. In TOG (2002), 855–873.
[WG92]
WILHELMS J., GELDER A. V.: Octrees for faster iso-
surface generation. TOG 11 (1992), 201–227.
[WKE99]
WESTERMANN R., KOBBELT L., ERTL T.: Real-time
exploration of regular volume data by adaptive reconstruction of
iso-surfaces. The Visual Computer 15 (1999), 100–111.
[YZX∗04]
YU Y., ZHOU K., XU D., SHI X., BAO H., GUO B.,
SHUM H.: Mesh editing with Poisson-based gradient ﬁeld ma-
nipulation. TOG (SIGGRAPH ’04) 23 (2004), 641–648.
Appendix A:
The solution to surface reconstruction described in this paper
approaches the problem in a manner similar to the solution
of [Kaz05] in that the reconstructed surface is obtained by
ﬁrst computing the indicator function and then extracting the
appropriate isosurface.
While the two methods seem to approach the problem
of computing the indicator function in different manners
( [Kaz05] uses Stokes’ Theorem to deﬁne the Fourier co-
efﬁcients of the indicator function while we use the Poisson
equation), the two methods are in fact equivalent.
To show this, we use the fact that the Poisson equation
∆u = f where f is periodic can be solved using the Fourier
transform. The Fourier series expansion is −|ζ|2 ˆu(ζ) =
ˆf(ζ), or equivalently ˆu(ζ) = −1
|ζ|2 ˆf(ζ).
Thus, our Poisson equation ∆χ = ∇·⃗V can be solved us-
ing ˆχ = −1
|ζ|2 
∇·⃗V. With the well known identity ˆf ′ = −iζ ˆf
and its generalization 
∇·⃗V = −iζ · ˆ⃗V, we get ˆχ =
i
|ζ|2 ζ · ˆ⃗V,
which is identical to [Kaz05].
c
⃝The Eurographics Association 2006.
```

---

## Section 1.30: ScanNet_ Richly-annotated 3d reconstructions of indoor scenes

Source File: ScanNet_ Richly-annotated 3d reconstructions of indoor scenes.txt

### Paper Content:

```
# ScanNet_ Richly-annotated 3d reconstructions of indoor scenes.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/ScanNet_ Richly-annotated 3d reconstructions of indoor scenes.pdf
# Output: ../layer2_completion/txt/ScanNet_ Richly-annotated 3d reconstructions of indoor scenes.txt


--- Page 1 ---

ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes
Angela Dai1 Angel X. Chang2 Manolis Savva2 Maciej Halber2 Thomas Funkhouser2 Matthias Nießner1,3
1Stanford University
2Princeton University
3Technical University of Munich
www.scan-net.org
Abstract
A key requirement for leveraging supervised deep learn-
ing methods is the availability of large, labeled datasets.
Unfortunately, in the context of RGB-D scene understand-
ing, very little data is available – current datasets cover a
small range of scene views and have limited semantic an-
notations. To address this issue, we introduce ScanNet, an
RGB-D video dataset containing 2.5M views in 1513 scenes
annotated with 3D camera poses, surface reconstructions,
and semantic segmentations. To collect this data, we de-
signed an easy-to-use and scalable RGB-D capture system
that includes automated surface reconstruction and crowd-
sourced semantic annotation. We show that using this data
helps achieve state-of-the-art performance on several 3D
scene understanding tasks, including 3D object classiﬁca-
tion, semantic voxel labeling, and CAD model retrieval.
1. Introduction
Since the introduction of commodity RGB-D sensors,
such as the Microsoft Kinect, the ﬁeld of 3D geometry cap-
ture has gained signiﬁcant attention and opened up a wide
range of new applications. Although there has been sig-
niﬁcant effort on 3D reconstruction algorithms, general 3D
scene understanding with RGB-D data has only very re-
cently started to become popular. Research along seman-
tic understanding is also heavily facilitated by the rapid
progress of modern machine learning methods, such as neu-
ral models. One key to successfully applying theses ap-
proaches is the availability of large, labeled datasets. While
much effort has been made on 2D datasets [17, 44, 47],
where images can be downloaded from the web and directly
annotated, the situation for 3D data is more challenging.
Thus, many of the current RGB-D datasets [74, 92, 77, 32]
are orders of magnitude smaller than their 2D counterparts.
Typically, 3D deep learning methods use synthetic data to
mitigate this lack of real-world data [91, 6].
One of the reasons that current 3D datasets are small is
because their capture requires much more effort, and efﬁ-
Figure 1. Example reconstructed spaces in ScanNet annotated with
instance-level object category labels through our crowdsourced
annotation framework.
ciently providing (dense) annotations in 3D is non-trivial.
Thus, existing work on 3D datasets often fall back to poly-
gon or bounding box annotations on 2.5D RGB-D images
[74, 92, 77], rather than directly annotating in 3D. In the
latter case, labels are added manually by expert users (typi-
cally by the paper authors) [32, 71] which limits their over-
all size and scalability.
In this paper, we introduce ScanNet, a dataset of richly-
annotated RGB-D scans of real-world environments con-
taining 2.5M RGB-D images in 1513 scans acquired in
707 distinct spaces. The sheer magnitude of this dataset
is larger than any other [58, 81, 92, 75, 3, 71, 32]. However,
what makes it particularly valuable for research in scene
understanding is its annotation with estimated calibration
parameters, camera poses, 3D surface reconstructions, tex-
tured meshes, dense object-level semantic segmentations,
and aligned CAD models (see Fig. 2). The semantic seg-
mentations are more than an order of magnitude larger than
any previous RGB-D dataset.
In the collection of this dataset, we have considered two
main research questions: 1) how can we design a frame-
work that allows many people to collect and annotate large
1
arXiv:1702.04405v2  [cs.CV]  11 Apr 2017


--- Page 2 ---

Dataset
Size
Labels
Annotation Tool
Reconstruction
CAD Models
NYU v2 [58]
464 scans
1449 frames
2D LabelMe-style [69]
none
some [25]
TUM [81]
47 scans
none
-
aligned poses (Vicon)
no
SUN 3D [92]
415 scans
8 scans
2D polygons
aligned poses [92]
no
SUN RGB-D [75]
10k frames
10k frames
2D polygons + bounding boxes
aligned poses [92]
no
BuildingParser [3]
265 rooms
265 rooms
CloudCompare [24]
point cloud
no
PiGraphs [71]
26 scans
26 scans
dense 3D, by the authors [71]
dense 3D [62]
no
SceneNN [32]
100 scans
100 scans
dense 3D, by the authors [60]
dense 3D [9]
no
ScanNet (ours)
1513 scans
1513 scans
dense 3D, crowd-sourced MTurk
dense 3D [12]
yes
2.5M frames
labels also proj. to 2D frames
Table 1. Overview of RGB-D datasets for 3D reconstruction and semantic scene understanding. Note that in addition to the 1513 scans in
ScanNet, we also provided dense 3D reconstruction and annotations on all NYU v2 sequences.
amounts of RGB-D data, and 2) can we use the rich annota-
tions and data quantity provided in ScanNet to learn better
3D models for scene understanding?
To investigate the ﬁrst question, we built a capture
pipeline to help novices acquire semantically-labeled 3D
models of scenes.
A person uses an app on an iPad
mounted with a depth camera to acquire RGB-D video,
and then we processes the data off-line and return a com-
plete semantically-labeled 3D reconstruction of the scene.
The challenges in developing such a framework are numer-
ous, including how to perform 3D surface reconstruction ro-
bustly in a scalable pipeline and how to crowdsource seman-
tic labeling. The paper discusses our study of these issues
and documents our experience with scaling up RGB-D scan
collection (20 people) and annotation (500 crowd workers).
To investigate the second question, we trained 3D deep
networks with the data provided by ScanNet and tested their
performance on several scene understanding tasks, includ-
ing 3D object classiﬁcation, semantic voxel labeling, and
CAD model retrieval. For the semantic voxel labeling task,
we introduce a new volumetric CNN architecture.
Overall, the contributions of this paper are:
• A large 3D dataset containing 1513 RGB-D scans of
over 707 unique indoor environments with estimated
camera parameters, surface reconstructions, textured
meshes, semantic segmentations.
We also provide
CAD model placements for a subset of the scans.
• A design for efﬁcient 3D data capture and annotation
suitable for novice users.
• New RGB-D benchmarks and improved results for
state-of-the art machine learning methods on 3D ob-
ject classiﬁcation, semantic voxel labeling, and CAD
model retrieval.
• A complete open source acquisition and annotation
framework for dense RGB-D reconstructions.
2. Previous Work
A large number of RGB-D datasets have been captured
and made publicly available for training and benchmarking
[56, 34, 50, 65, 79, 83, 74, 4, 58, 81, 15, 55, 1, 68, 30, 51, 21,
48, 43, 92, 80, 61, 72, 93, 36, 16, 35, 57, 40, 29, 70, 52, 45,
95, 75, 9, 33, 85, 71, 32, 3, 10, 78, 2].1 These datasets have
been used to train models for many 3D scene understanding
tasks, including semantic segmentation [67, 58, 26, 86], 3D
object detection [73, 46, 27, 76, 77], 3D object classiﬁcation
[91, 53, 66], and others [94, 22, 23].
Most RGB-D datasets contain scans of individual ob-
jects. For example, the Redwood dataset [10] contains over
10,000 scans of objects annotated with class labels, 1,781 of
which are reconstructed with KinectFusion [59]. Since the
objects are scanned in isolation without scene context, the
dataset’s focus is mainly on evaluating surface reconstruc-
tion quality rather than semantic understanding of complete
scenes.
One of the earliest and most popular datasets for RGB-
D scene understanding is NYU v2 [74]. It is composed of
464 short RGB-D sequences, from which 1449 frames have
been annotated with 2D polygons denoting semantic seg-
mentations, as in LabelMe [69]. SUN RGB-D [75] follows
up on this work by collecting 10,335 RGB-D frames an-
notated with polygons in 2D and bounding boxes in 3D.
These datasets have scene diversity comparable to ours, but
include only a limited range of viewpoints, and do not pro-
vide complete 3D surface reconstructions, dense 3D seman-
tic segmentations, or a large set of CAD model alignments.
One of the ﬁrst RGB-D datasets focused on long RGB-
D sequences in indoor environments is SUN3D. It contains
a set of 415 Kinect v1 sequences of 254 unique spaces.
Although some objects were annotated manually with 2D
polygons, and 8 scans have estimated camera poses based
on user input, the bulk of the dataset does not include cam-
era poses, 3D reconstructions, or semantic annotations.
Recently, Armeni et al. [3, 2] introduced an indoor
dataset containing 3D meshes for 265 rooms captured with
a custom Matterport camera and manually labeled with se-
mantic annotations. The dataset is high-quality, but the cap-
1A comprehensive and detailed overview of publicly-accessible RGB-
D datasets is given by [20] at http://www0.cs.ucl.ac.uk/
staff/M.Firman/RGBDdatasets/, which is updated on a regular
basis.


--- Page 3 ---

RGB-D Scanning
3D Reconstruction
Upload
Segmentation
Semantic Labeling
Retrieval + Alignment
Crowd-
sourcing
Figure 2. Overview of our RGB-D reconstruction and semantic annotation framework. Left: a novice user uses a handheld RGB-D device
with our scanning interface to scan an environment. Mid: RGB-D sequences are uploaded to a processing server which produces 3D
surface mesh reconstructions and their surface segmentations. Right: Semantic annotation tasks are issued for crowdsourcing to obtain
instance-level object category annotations and 3D CAD model alignments to the reconstruction.
ture pipeline is based on expensive and less portable hard-
ware. Furthermore, only a fused point cloud is provided
as output. Due to the lack of raw color and depth data, its
applicability to research on reconstruction and scene under-
standing from raw RGB-D input is limited.
The datasets most similar to ours are SceneNN [32] and
PiGraphs [71], which are composed of 100 and 26 densely
reconstructed and labeled scenes respectively. The anno-
tations are done directly in 3D [60, 71]. However, both
scanning and labeling are performed only by expert users
(i.e. the authors), limiting the scalability of the system and
the size of the dataset. In contrast, we design our RGB-D
acquisition framework speciﬁcally for ease-of-use by un-
trained users and for scalable processing through crowd-
sourcing. This allows us to acquire a signiﬁcantly larger
dataset with more annotations (currently, 1513 sequences
are reconstructed and labeled).
3. Dataset Acquisition Framework
In this section, we focus on the design of the framework
used to acquire the ScanNet dataset (Fig. 2). We discuss de-
sign trade-offs in building the framework and relay ﬁndings
on which methods were found to work best for large-scale
RGB-D data collection and processing.
Our main goal driving the design of our framework was
to allow untrained users to capture semantically labeled sur-
faces of indoor scenes with commodity hardware. Thus the
RGB-D scanning system must be trivial to use, the data
processing robust and automatic, the semantic annotations
crowdsourced, and the ﬂow of data through the system han-
dled by a tracking server.
3.1. RGB-D Scanning
Hardware.
There is a spectrum of choices for RGB-D
sensor hardware. Our requirement for deployment to large
groups of inexperienced users necessitates a portable and
low-cost RGB-D sensor setup. We use the Structure sen-
sor [63], a commodity RGB-D sensor with design similar to
the Microsoft Kinect v1. We attach this sensor to a handheld
device such as an iPhone or iPad (see Fig. 2 left) — results
in this paper were collected using iPad Air2 devices. The
iPad RGB camera data is temporally synchronized with the
depth sensor via hardware, providing synchronized depth
and color capture at 30 Hz. Depth frames are captured at a
resolution of 640 × 480 and color at 1296 × 968 pixels. We
enable auto-white balance and auto-exposure by default.
Calibration.
Our use of commodity RGB-D sensors ne-
cessitates unwarping of depth data and alignment of depth
and color data. Prior work has focused mostly on controlled
lab conditions with more accurate equipment to inform cal-
ibration for commodity sensors (e.g., Wang et al. [87]).
However, this is not practical for novice users. Thus the
user only needs to print out a checkerboard pattern, place
it on a large, ﬂat surface, and capture an RGB-D sequence
viewing the surface from close to far away. This sequence,
as well as a set of infrared and color frame pairs viewing the
checkerboard, are uploaded by the user as input to the cali-
bration. Our system then runs a calibration procedure based
on [84, 14] to obtain intrinsic parameters for both depth and
color sensors, and an extrinsic transformation of depth to
color. We ﬁnd that this calibration procedure is easy for
users and results in improved data and consequently en-
hanced reconstruction quality.
User Interface.
To make the capture process simple for
untrained users, we designed an iOS app with a simple live
RGB-D video capture UI (see Fig. 2 left). The user provides
a name and scene type for the current scan and proceeds
to record a sequence. During scanning, a log-scale RGB
feature detector point metric is shown as a “featurefulness”
bar to provide a rough measure of tracking robustness and
reconstruction quality in different regions being scanned.
This feature was critical for providing intuition to users who
are not familiar with the constraints and limitations of 3D
reconstruction algorithms.
Storage.
We store scans as compressed RGB-D data on
the device ﬂash memory so that a stable internet connec-
tion is not required during scanning. The user can upload
scans to the processing server when convenient by press-
ing an “upload” button. Our sensor units used 128 GB iPad
Air2 devices, allowing for several hours of recorded RGB-
D video. In practice, the bottleneck was battery life rather


--- Page 4 ---

than storage space. Depth is recorded as 16-bit unsigned
short values and stored using standard zLib compression.
RGB data is encoded with the H.264 codec with a high bi-
trate of 15 Mbps to prevent encoding artifacts. In addition
to the RGB-D frames, we also record Inertial Measurement
Unit (IMU) data, including acceleration, and angular veloc-
ities, from the Apple SDK. Timestamps are recorded for
IMU, color, and depth images.
3.2. Surface Reconstruction
Once data has been uploaded from the iPad to our
server, the ﬁrst processing step is to estimate a densely-
reconstructed 3D surface mesh and 6-DoF camera poses for
all RGB-D frames. To conform with the goal for an au-
tomated and scalable framework, we choose methods that
favor robustness and processing speed such that uploaded
recordings can be processed at near real-time rates with lit-
tle supervision.
Dense Reconstruction.
We use volumetric fusion [11]
to perform the dense reconstruction, since this approach
is widely used in the context of commodity RGB-D data.
There is a large variety of algorithms targeting this sce-
nario [59, 88, 7, 62, 37, 89, 42, 9, 90, 38, 12]. We chose
the BundleFusion system [12] as it was designed and evalu-
ated for similar sensor setups as ours, and provides real-time
speed while being reasonably robust given handheld RGB-
D video data.
For each input scan, we ﬁrst run BundleFusion [12] at
a voxel resolution of 1 cm3. BundleFusion produces accu-
rate pose alignments which we then use to perform volu-
metric integration through VoxelHashing [62] and extract a
high resolution surface mesh using the Marching Cubes al-
gorithm on the implicit TSDF (4 mm3 voxels). The mesh
is then automatically cleaned up with a set of ﬁltering steps
to merge close vertices, delete duplicate and isolated mesh
parts, and ﬁnally to downsample the mesh to high, medium,
and low resolution versions (each level reducing the number
of faces by a factor of two).
Orientation.
After the surface mesh is extracted, we au-
tomatically align it and all camera poses to a common co-
ordinate frame with the z-axis as the up vector, and the xy
plane aligned with the ﬂoor plane. To perform this align-
ment, we ﬁrst extract all planar regions of sufﬁcient size,
merge regions deﬁned by the same plane, and sort them by
normal (we use a normal threshold of 25◦and a planar off-
set threshold of 5 cm). We then determine a prior for the up
vector by projecting the IMU gravity vectors of all frames
into the coordinates of the ﬁrst frame. This allows us to se-
lect the ﬂoor plane based on the scan bounding box and the
normal most similar to the IMU up vector direction. Finally,
we use a PCA on the mesh vertices to determine the rotation
around the z-axis and translate the scan such that its bounds
are within the positive octant of the coordinate system.
Figure 3. Our web-based crowdsourcing interface for annotating a
scene with instance-level object category labels. The right panel
lists object instances already annotated in the scene with matching
painted colors. This annotation is in progress at ≈35%, with gray
regions indicating unannotated surfaces.
Validation.
This reconstruction process is automatically
triggered when a scan is uploaded to the processing server
and runs unsupervised. In order to establish a clean snap-
shot to construct the ScanNet dataset reported in this paper,
we automatically discard scan sequences that are short, have
high residual reconstruction error, or have low percentage
of aligned frames. We then manually check for and discard
reconstructions with noticeable misalignments.
3.3. Semantic Annotation
After a reconstruction is produced by the processing
server, annotation HITs (Human Intelligence Tasks) are is-
sued on the Amazon Mechanical Turk crowdsourcing mar-
ket. The two HITs that we crowdsource are: i) instance-
level object category labeling of all surfaces in the recon-
struction, and ii) 3D CAD model alignment to the recon-
struction. These annotations are crowdsourced using web-
based interfaces to again maintain the overall scalability of
the framework.
Instance-level Semantic Labeling.
Our ﬁrst annotation
step is to obtain a set of object instance-level labels directly
on each reconstructed 3D surface mesh. This is in contrast
to much prior work that uses 2D polygon annotations on
RGB or RGB-D images, or 3D bounding box annotations.
We developed a WebGL interface that takes as input the
low-resolution surface mesh of a given reconstruction and a
conservative over-segmentation of the mesh using a normal-
based graph cut method [19, 39]. The crowd worker then
selects segments to annotate with instance-level object cate-
gory labels (see Fig. 3). Each worker is required to annotate
at least 25% of the surfaces in a reconstruction, and encour-
aged to annotate more than 50% before submission. Each
scan is annotated by multiple workers (scans in ScanNet are
annotated by 2.3 workers on average).
A key challenge in designing this interface is to enable
efﬁcient annotation by workers who have no prior experi-
ence with the task, or 3D interfaces in general. Our interface
uses a simple painting metaphor where clicking and drag-


--- Page 5 ---

Figure 4. Crowdsourcing interface for aligning CAD models to
objects in a reconstruction. Objects can be clicked to initiate an
assisted search for CAD models (see list of bookshelves in mid-
dle). A suggested model is placed at the position of the clicked
object, and the user then reﬁnes the position and orientation. A
desk, chair, and nightstand have been already placed here.
ging over surfaces paints segments with a given label and
corresponding color. This functions similarly to 2D paint-
ing and allows for erasing and modifying existing regions.
Another design requirement is to allow for freeform text
labels, to reduce the inherent bias and scalability issues of
pre-selected label lists. At the same time, it is desirable
to guide users for consistency and coverage of basic object
types. To achieve this, the interface provides autocomplete
functionality over all labels previously provided by other
workers that pass a frequency threshold (> 5 annotations).
Workers are always allowed to add arbitrary text labels to
ensure coverage and allow expansion of the label set.
Several additional design details are important to ensure
usability by novice workers. First, a simple distance check
for connectedness is used to disallow labeling of discon-
nected surfaces with the same label. Earlier experiments
without this constraint resulted in two undesirable behav-
iors: cheating by painting many surfaces with a few labels,
and labeling of multiple object instances with the same la-
bel. Second, the 3D nature of the data is challenging for
novice users. Therefore, we ﬁrst show a full turntable rota-
tion of each reconstruction and instruct workers to change
the view using a rotating turntable metaphor. Without the
turntable rotation animation, many workers only annotated
from the initial view and never used camera controls despite
the provided instructions.
CAD Model Retrieval and Alignment.
In the second an-
notation task, a crowd worker was given a reconstruction
already annotated with object instances and asked to place
appropriate 3D CAD models to represent major objects in
the scene. The challenge of this task lies in the selection
of closely matching 3D models from a large database, and
in precisely aligning each model to the 3D position of the
corresponding object in the reconstruction.
We implemented an assisted object retrieval interface
Statistic
SceneNN [32]
ScanNet
# of scans
100
1513
# of RGB-D frames
2,475,905
2,492,518
ﬂoor area (avg / sum m2)
22.6 / 2,124
22.6 / 34,453
surface area (avg / sum m2)
75.3 / 7,078
51.6 / 78,595
labeled objects (avg / sum)
15.8 / 1482
24.1 / 36,213
Table 2. Summary statistics for ScanNet compared to the most
similar existing dataset (SceneNN [32]). ScanNet has an order
of magnitude more scans, with 3D surface mesh reconstructions
covering more than ten times the ﬂoor and surface area, and with
more than 36,000 annotated object instances.
where clicking on a previously labeled object in a recon-
struction immediately searched for CAD models with the
same category label in the ShapeNetCore [6] dataset, and
placed one example model such that it overlaps with the ori-
ented bounding box of the clicked object (see Fig. 4). The
worker then used keyboard and mouse-based controls to ad-
just the alignment of the model, and was allowed to submit
the task once at least three CAD models were placed.
Using this interface, we collected sets of CAD mod-
els aligned to each ScanNet reconstruction.
Preliminary
results indicate that despite the challenging nature of this
task, workers select semantically appropriate CAD models
to match objects in the reconstructions. The main limitation
of this interface is due to the mismatch between the cor-
pus of available CAD models and the objects observed in
the ScanNet scans. Despite the diversity of the ShapeNet
CAD model dataset (55K objects), it is still hard to ﬁnd ex-
act instance-level matches for chairs, desks and more rare
object categories. A promising way to alleviate this limi-
tation is to algorithmically suggest candidate retrieved and
aligned CAD models such that workers can perform an eas-
ier veriﬁcation and adjustment task.
4. ScanNet Dataset
In this section, we summarize the data we collected us-
ing our framework to establish the ScanNet dataset. This
dataset is a snapshot of available data from roughly one
month of data acquisition by 20 users at locations in several
countries. It has annotations by more than 500 crowd work-
ers on the Mechanical Turk platform. Since the presented
framework runs in an unsupervised fashion and people are
continuously collecting data, this dataset continues to grow
organically. Here, we report some statistics for an initial
snapshot of 1513 scans, which are summarized in Table 2.
Fig. 5 plots the distribution of scanned scenes over differ-
ent types of real-world spaces. ScanNet contains a variety
of spaces such as ofﬁces, apartments, and bathrooms. The
dataset contains a diverse set of spaces ranging from small
(e.g., bathrooms, closets, utility rooms) to large (e.g., apart-
ments, classrooms, and libraries). Each scan has been anno-
tated with instance-level semantic category labels through


--- Page 6 ---

Figure 5. Distribution of the scans in ScanNet organized by type.
our crowdsourcing task. In total, we deployed 3,391 anno-
tation tasks to annotate all 1513 scans.
The text labels used by crowd workers to annotate object
instances are all mapped to the object category sets of NYU
v2 [58], ModelNet [91], ShapeNet [6], and WordNet [18]
synsets. This mapping is made more robust by a preprocess
that collapses the initial text labels through synonym and
misspelling detection.
In addition to reconstructing and annotating the 1513
ScanNet scans, we have processed all the NYU v2 RGB-D
sequences with our framework. The result is a set of dense
reconstructions of the NYU v2 spaces with instance-level
object annotations in 3D that are complementary in nature
to the existing image-based annotations.
We also deployed the CAD model alignment crowd-
sourcing task to collect a total of 107 virtual scene inter-
pretations consisting of aligned ShapeNet models placed on
a subset of 52 ScanNet scans by 106 workers. There were a
total of 681 CAD model instances (of 296 unique models)
retrieved and placed on the reconstructions, with an average
of 6.4 CAD model instances per annotated scan.
For more detailed statistics on this ﬁrst ScanNet dataset
snapshot, please see the appendix.
5. Tasks and Benchmarks
In this section, we describe the three tasks we developed
as benchmarks for demonstrating the value of ScanNet data.
Train/Test split statistics.
Table 3 shows the test and
training splits of ScanNet in the context of the object classi-
ﬁcation and dense voxel prediction benchmarks. Note that
our data is signiﬁcantly larger than any existing compara-
ble dataset. We use these tasks to demonstrate that Scan-
Net enables the use of deep learning methods for 3D scene
understanding tasks with supervised training, and compare
performance to that using data from other existing datasets.
5.1. 3D Object Classiﬁcation
With the availability of large-scale synthetic 3D datasets
such as [91, 6] and recent advances in 3D deep learn-
Scans
Instances
#Train #Test
#Train #Test
Object
Classiﬁcation
ScanNet
1205
312
9305
2606
NYU
452
80
3260
613
SceneNN
70
12
377
66
Semantic Voxel
Labeling
ScanNet
1201
312
80554 21300
Table 3. Train/Test split for object classiﬁcation and dense voxel
prediction tasks. Note that the number of instances does not in-
clude the rotation augmentation.
ing, research has developed approaches to classify ob-
jects using only geometric data with volumetric deep nets
[91, 82, 52, 13, 66]. All of these methods train on purely
synthetic data and focus on isolated objects. Although they
show limited evaluation on real-world data, a larger evalu-
ation on realistic scanning data is largely missing. When
training data is synthetic and test is performed on real data,
there is also a signiﬁcant discrepancy of test performance,
as data characteristics, such as noise and occlusions pat-
terns, are inherently different.
With ScanNet, we close this gap as we have captured a
sufﬁciently large amount of 3D data to use real-world RGB-
D input for both training and test sets. For this task, we use
the bounding boxes of annotated objects in ScanNet, and
isolate the contained geometry. As a result, we obtain local
volumes around each object instance for which we know the
annotated category. The goal of the task is to classify the
object represented by a set of scanned points within a given
bounding box. For this benchmark, we use 17 categories,
with 9, 677 train instances and 2, 606 test instances.
Network and training.
For object classiﬁcation, we fol-
low the network architecture of the 3D Network-in-Network
of [66], without the multi-orientation pooling step. In order
to classify partial data, we add a second channel to the 303
occupancy grid input, indicating known and unknown re-
gions (with 1 and 0, respectively) according to the camera
scanning trajectory. As in Qi et al. [66], we use an SGD
solver with learning rate 0.01 and momentum 0.9, decaying
the learning rate by half every 20 epochs, and training the
model for 200 epochs. We augment training samples with
12 instances of different rotations (including both elevation
and tilt), resulting in a total training set of 111, 660 samples.
Benchmark performance.
As a baseline evaluation, we
run the 3D CNN approach of Qi et al. [66]. Table 4 shows
the performance of 3D shape classiﬁcation with different
train and test sets. The ﬁrst two columns show results on
synthetic test data from ShapeNet [6] including both com-
plete and partial data. Naturally, training with the corre-
sponding synthetic counterparts of ShapeNet provides the
best performance, as data characteristics are shared. How-
ever, the more interesting case is real-world test data (right-


--- Page 7 ---

most two columns); here, we show results on test sets of
SceneNN [32] and ScanNet. First, we see that training on
synthetic data allows only for limited knowledge transfer
(ﬁrst two rows). Second, although the relatively small Sce-
neNN dataset is able to learn within its own dataset to a
reasonable degree, it does not generalize to the larger vari-
ety of environments found in ScanNet. On the other hand,
training on ScanNet translates well to testing on SceneNN;
as a result, the test results on SceneNN are signiﬁcantly
improved by using the training data from ScanNet.
In-
terestingly enough, these results can be slightly improved
when mixing training data of ScanNet with partial scans of
ShapeNet (last row).
Synthetic Test Sets
Real Test Sets
Training Set
ShapeNet ShapeNet Partial SceneNN ScanNet
ShapeNet
92.5
37.6
68.2
39.5
ShapeNet Partial
88.5
92.1
72.7
45.7
SceneNN
19.9
27.7
69.8
48.2
NYU
26.2
26.6
72.7
53.2
ScanNet
21.4
31.0
78.8
74.9
ScanNet +ShapeNet Par.
79.7
89.8
81.2
76.6
Table 4. 3D object classiﬁcation benchmark performance. Per-
centages give the classiﬁcation accuracy over all models in each
test set (average instance accuracy).
Retrieval from ShapeNet
Train
Top 1 NN
Top 3 NNs
ShapeNet
10.4%
8.0%
ScanNet
12.7%
11.7%
ShapeNet + ScanNet
77.5%
77.0%
Table 6. 3D model retrieval benchmark performance.
Nearest
neighbor models are retrieved for ScanNet objects from ShapeNet-
Core. Percentages indicate average instance accuracy of retrieved
model to query region.
5.2. Semantic Voxel Labeling
A common task on RGB data is semantic segmentation
(i.e. labeling pixels with semantic classes) [49]. With our
data, we can extend this task to 3D, where the goal is to
predict the semantic object label on a per-voxel basis. This
task of predicting a semantic class for each visible 3D voxel
has been addressed by some prior work, but using hand-
crafted features to predict a small number of classes [41,
86], or focusing on outdoor environments [8, 5].
Data Generation.
We ﬁrst voxelize a scene and obtain
a dense voxel grid with 2cm3 voxels, where every voxel
stores its TSDF value and object class annotation (empty
space and unlabeled surface points have their own respec-
tive classes). We now extract subvolumes of the scene vol-
ume, of dimension 2 × 31 × 31 × 62 and spatial extent
1.5m × 1.5m × 3m; i.e., a voxel size of ≈4.8cm3; the
two channels represent the occupancy and known/unknown
space according to the camera trajectory. These sample vol-
umes are aligned with the xy-ground plane.For ground truth
data generation, voxel labels are propagated from the scene
voxelization to these sample volumes. The samples are cho-
sen that ≥2% of the voxels are occupied (i.e., on the sur-
face), and ≥70% of these surface voxels have valid an-
notations; samples not meeting these criteria are discarded.
Across ScanNet, we generate 93, 721 subvolume examples
for training, augmented by 8 rotations each (i.e., 749, 768
training samples), from 1201 training scenes. In addition,
we extract 18, 750 sample volumes for testing, which are
also augmented by 8 rotations each (i.e., 150, 000 test sam-
ples) from 312 test scenes. We have 20 object class labels
plus 1 class for free space.
Network and training.
For the semantic voxel labeling
task, we propose a network which predicts class labels for
a column of voxels in a scene according to the occupancy
characteristics of the voxels’ neighborhood. In order to in-
fer labels for an entire scene, we use the network to predict
a label for every voxel column at test time (i.e., every xy
position that has voxels on the surface). The network takes
as input a 2×31×31×62 volume and uses a series of fully
convolutional layers to simultaneously predict class scores
for the center column of 62 voxels. We use ReLU and batch
normalization for all layers (except the last) in the network.
To account for the unbalanced training data over the class
labels, we weight the cross entropy loss with the inverse log
of the histogram of the train data.
We use an SGD solver with learning rate 0.01 and mo-
mentum 0.9, decaying the learning rate by half every 20
epochs, and train the model for 100 epochs.
Quantitative Results.
The goal of this task is to predict
semantic labels for all visible surface voxels in a given 3D
scene; i.e., every voxel on a visible surface receives one
of the 20 object class labels. We use NYU2 labels, and
list voxel classiﬁcation results on ScanNet in Table 7. We
achieve an voxel classiﬁcation accuracy of 73.0% over the
set of 312 test scenes, which is based purely on the geomet-
ric input (no color is used).
In Table 5, we show our semantic voxel labeling results
on the NYU2 dataset [58]. We are able to outperform previ-
ous methods which are trained on limited sets of real-world
data using our volumetric classiﬁcation network. For in-
stance, Hermans et al. [31] classify RGB-D frames using
a dense random decision forest in combination with a con-
ditional random ﬁeld. Additionally, SemanticFusion [54]
uses a deep net trained on RGB-D frames, and regularize
the predictions with a CRF over a 3D reconstruction of the
frames; note that we compare to their classiﬁcation results


--- Page 8 ---

ﬂoor
wall
chair
table
window
bed
sofa
tv
objs.
furn.
ceil.
avg.
Hermans et al. [31]
91.5
71.8
41.9
27.7
46.1
68.4
28.5
38.4
8.6
37.1
83.4
49.4
SemanticFusion [54]∗
92.6
86.0
58.4
34.0
60.5
61.7
47.3
33.9
59.1
63.7
43.4
58.2
SceneNet [28]
96.2
85.3
61.0
43.8
30.0
72.5
62.8
19.4
50.0
60.4
74.1
59.6
Ours (ScanNet + NYU)
99.0
55.8
67.6
50.9
63.1
81.4
67.2
35.8
34.6
65.6
46.2
60.7
Table 5. Dense pixel classiﬁcation accuracy on NYU2 [58]. Note that both SemanticFusion [54] and Hermans et. al. [31] use both geometry
and color, and that Hermans et al. uses a CRF, unlike our approach which is geometry-only and has only unary predictions. The reported
SemanticFusion classiﬁcation is on the 13 class task (13 class average accuracy of 58.9%).
Class
% of Test Scenes
Accuracy
Floor
35.7%
90.3%
Wall
38.8%
70.1%
Chair
3.8%
69.3%
Sofa
2.5%
75.7%
Table
3.3%
68.4%
Door
2.2%
48.9%
Cabinet
2.4%
49.8%
Bed
2.0%
62.4%
Desk
1.7%
36.8%
Toilet
0.2%
69.9%
Sink
0.2%
39.4%
Window
0.4%
20.1%
Picture
0.2%
3.4%
Bookshelf
1.6%
64.6%
Curtain
0.7%
7.0%
Shower Curtain
0.04%
46.8%
Counter
0.6%
32.1%
Refrigerator
0.3%
66.4%
Bathtub
0.2%
74.3%
OtherFurniture
2.9%
19.5%
Total
-
73.0%
Table 7. Semantic voxel label prediction accuracy on ScanNet test
scenes.
before the CRF regularization. SceneNet trains on a large
synthetic dataset and ﬁne-tunes on NYU2. Note that in con-
trast to Hermans et al. and SemanticFusion, neither we nor
SceneNet use RGB information.
Note that we do not explicitly enforce prediction con-
sistency between neighboring voxel columns when the test
volume is slid across the xy plane. This could be achieved
with a volumetric CRF [64], as used in [86]; however, our
goal in this task to focus exclusively on the per-voxel clas-
siﬁcation accuracy.
5.3. 3D Object Retrieval
Another important task is retrieval of similar CAD mod-
els given (potentially partial) RGB-D scans. To this end,
one wants to learn a shape embedding where a feature de-
scriptor deﬁnes geometric similarity between shapes. The
core idea is to train a network on a shape classiﬁcation task
where a shape embedding can be learned as byproduct of
the classiﬁcation task. For instance, Wu et al. [91] and Qi et
al. [66] use this technique to perform shape retrieval queries
within the ShapeNet database.
With ScanNet, we have established category-level corre-
spondences between real-world objects and ShapeNet mod-
els. This allows us to train on a classiﬁcation problem where
both real and synthetic data are mixed inside of each cate-
gory using real and synthetic data within shared class labels.
Thus, we can learn an embedding between real and syn-
thetic data in order to perform model retrieval for RGB-D
scans. To this end, we use the volumetric shape classiﬁ-
cation network by Qi et al. [66], we use the same training
procedure as in Sec. 5.1. Nearest neighbors are retrieved
based on the ℓ2 distance between the extracted feature de-
scriptors, and measured against the ground truth provided
by the CAD model retrieval task. In Table 6, we show ob-
ject retrieval results using objects from ScanNet to query
for nearest neighbor models from ShapeNetCore. Note that
training on ShapeNet and ScanNet independently results in
poor retrieval performance, as neither are able to bridge the
gap between the differing characteristics of synthetic and
real-world data. Training on both ShapeNet and ScanNet
together is able to ﬁnd an embedding of shape similarities
between both data modalities, resulting in much higher re-
trieval accuracy.
6. Conclusion
This paper introduces ScanNet:
a large-scale RGB-
D dataset of 1513 scans with surface reconstructions,
instance-level object category annotations, and 3D CAD
model placements. To make the collection of this data pos-
sible, we designed a scalable RGB-D acquisition and se-
mantic annotation framework that we provide for the ben-
eﬁt of the community. We demonstrated that the richly-
annotated scan data collected so far in ScanNet is useful in
achieving state-of-the-art performance on several 3D scene
understanding tasks; we hope that ScanNet will inspire fu-
ture work on many other tasks.
Acknowledgments
This project is funded by Google Tango, Intel, NSF
(IIS-1251217 and VEC 1539014/1539099), and a Stanford
Graduate fellowship. We also thank Occipital for donat-
ing structure sensors and Nvidia for hardware donations, as
well as support by the Max-Planck Center for Visual Com-
puting and the Stanford CURIS program. Further, we thank
Toan Vuong, Joseph Chang, and Helen Jiang for help on the
mobile scanning app and the scanning process, and Hope
Casey-Allen and Duc Nugyen for early prototypes of the
annotation interfaces. Last but not least, we would like to
thank all the volunteers who helped with scanning and get-


--- Page 9 ---

ting us access to scanning spaces.
References
[1] A. Aldoma, F. Tombari, L. Di Stefano, and M. Vincze. A
global hypotheses veriﬁcation method for 3D object recog-
nition. In European Conference on Computer Vision, pages
511–524. Springer, 2012. 2
[2] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese. Joint 2d-3d-
semantic data for indoor scene understanding. arXiv preprint
arXiv:1702.01105, 2017. 2
[3] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,
M. Fischer, and S. Savarese. 3D semantic parsing of large-
scale indoor spaces. CVPR, 2016. 1, 2
[4] I. B. Barbosa, M. Cristani, A. Del Bue, L. Bazzani, and
V. Murino. Re-identiﬁcation with RGB-D sensors. In Eu-
ropean Conference on Computer Vision, pages 433–442.
Springer, 2012. 2
[5] M. Blaha, C. Vogel, A. Richard, J. D. Wegner, T. Pock, and
K. Schindler.
Large-scale semantic 3d reconstruction: an
adaptive multi-resolution model for multi-class volumetric
labeling. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 3176–3184,
2016. 7
[6] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. ShapeNet: An information-rich 3D model repository.
arXiv preprint arXiv:1512.03012, 2015. 1, 5, 6, 21, 22
[7] J. Chen, D. Bautembach, and S. Izadi. Scalable real-time
volumetric surface reconstruction.
ACM Transactions on
Graphics (TOG), 32(4):113, 2013. 4
[8] I. Cherabier, C. H¨ane, M. R. Oswald, and M. Pollefeys.
Multi-label semantic 3d reconstruction using voxel blocks.
In 3D Vision (3DV), 2016 Fourth International Conference
on, pages 601–610. IEEE, 2016. 7
[9] S. Choi, Q.-Y. Zhou, and V. Koltun. Robust reconstruction
of indoor scenes. In 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 5556–5565.
IEEE, 2015. 2, 4, 15
[10] S. Choi, Q.-Y. Zhou, S. Miller, and V. Koltun. A large dataset
of object scans. arXiv:1602.02481, 2016. 2, 15
[11] B. Curless and M. Levoy. A volumetric method for building
complex models from range images. In Proceedings of the
23rd annual conference on Computer graphics and interac-
tive techniques, pages 303–312. ACM, 1996. 4
[12] A. Dai, M. Nießner, M. Zoll¨ofer, S. Izadi, and C. Theobalt.
BundleFusion: Real-time globally consistent 3D reconstruc-
tion using on-the-ﬂy surface re-integration. arXiv preprint
arXiv:1604.01093, 2016. 2, 4, 20
[13] A. Dai, C. R. Qi, and M. Nießner. Shape completion us-
ing 3d-encoder-predictor cnns and shape synthesis. arXiv
preprint arXiv:1612.00101, 2016. 6
[14] M. Di Cicco, L. Iocchi, and G. Grisetti. Non-parametric cal-
ibration for depth sensors. Robotics and Autonomous Sys-
tems, 74:309–317, 2015. 3, 20
[15] F. Endres, J. Hess, N. Engelhard, J. Sturm, D. Cremers, and
W. Burgard. An evaluation of the RGB-D SLAM system. In
Robotics and Automation (ICRA), 2012 IEEE International
Conference on, pages 1691–1696. IEEE, 2012. 2
[16] N. Erdogmus and S. Marcel. Spooﬁng in 2D face recognition
with 3D masks and anti-spooﬁng with Kinect. In Biometrics:
Theory, Applications and Systems (BTAS), 2013 IEEE Sixth
International Conference on, pages 1–6. IEEE, 2013. 2
[17] M. Everingham, L. Van Gool, C. K. Williams, J. Winn,
and A. Zisserman.
The PASCAL visual object classes
(VOC) challenge. International journal of computer vision,
88(2):303–338, 2010. 1
[18] C. Fellbaum. WordNet. Wiley Online Library, 1998. 6, 12,
21
[19] P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient graph-
based image segmentation. International Journal of Com-
puter Vision, 59(2):167–181, 2004. 4
[20] M. Firman. RGBD datasets: Past, present and future. In
CVPR Workshop on Large Scale 3D Data: Acquisition, Mod-
elling and Analysis, 2016. 2
[21] S. Fothergill, H. Mentis, P. Kohli, and S. Nowozin. Instruct-
ing people for training gestural interactive systems. In Pro-
ceedings of the SIGCHI Conference on Human Factors in
Computing Systems, pages 1737–1746. ACM, 2012. 2
[22] D. F. Fouhey, A. Gupta, and M. Hebert.
Data-driven 3D
primitives for single image understanding. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 3392–3399, 2013. 2
[23] D. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor
origami world. In European Conference on Computer Vision,
pages 687–702. Springer, 2014. 2
[24] D. Girardeau-Montaut. CloudCompare3D point cloud and
mesh processing software. OpenSource Project, 2011. 2
[25] R. Guo and D. Hoiem. Support surface prediction in indoor
scenes. In Proceedings of the IEEE International Conference
on Computer Vision, pages 2144–2151, 2013. 2
[26] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organiza-
tion and recognition of indoor scenes from RGB-D images.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 564–571, 2013. 2
[27] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik. Learning
rich features from RGB-D images for object detection and
segmentation. In European Conference on Computer Vision,
pages 345–360. Springer, 2014. 2
[28] A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and
R. Cipolla.
Scenenet: Understanding real world indoor
scenes with synthetic data. arXiv preprint arXiv:1511.07041,
2015. 8
[29] A. Handa, T. Whelan, J. McDonald, and A. J. Davison. A
benchmark for RGB-D visual odometry, 3D reconstruction
and SLAM.
In 2014 IEEE International Conference on
Robotics and Automation (ICRA), pages 1524–1531. IEEE,
2014. 2
[30] V. Hedau, D. Hoiem, and D. Forsyth. Recovering free space
of indoor scenes from a single image. In Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on,
pages 2807–2814. IEEE, 2012. 2
[31] A. Hermans, G. Floros, and B. Leibe. Dense 3D semantic
mapping of indoor scenes from RGB-D images. In Robotics


--- Page 10 ---

and Automation (ICRA), 2014 IEEE International Confer-
ence on, pages 2631–2638. IEEE, 2014. 7, 8
[32] B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F.
Yu, and S.-K. Yeung.
SceneNN: A scene meshes dataset
with annotations. In International Conference on 3D Vision
(3DV), volume 1, 2016. 1, 2, 3, 5, 7, 12, 15, 16
[33] M. Innmann, M. Zollh¨ofer, M. Nießner, C. Theobalt, and
M. Stamminger.
VolumeDeform:
Real-time volumetric
non-rigid reconstruction. arXiv preprint arXiv:1603.08161,
2016. 2
[34] C. Ionescu, F. Li, and C. Sminchisescu. Latent structured
models for human pose estimation. In 2011 International
Conference on Computer Vision, pages 2220–2227. IEEE,
2011. 2
[35] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu.
Human3.6M: Large scale datasets and predictive methods
for 3D human sensing in natural environments.
IEEE
transactions on pattern analysis and machine intelligence,
36(7):1325–1339, 2014. 2
[36] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz,
K. Saenko, and T. Darrell.
A category-level 3D object
dataset: Putting the Kinect to work.
In Consumer Depth
Cameras for Computer Vision, pages 141–165. Springer,
2013. 2
[37] O. Kahler, V. Adrian Prisacariu, C. Yuheng Ren, X. Sun,
P. Torr, and D. Murray.
Very high frame rate vol-
umetric integration of depth images on mobile devices.
IEEE Transactions on Visualization and Computer Graph-
ics, 21(11):1241–1250, 2015. 4
[38] O. K¨ahler, V. A. Prisacariu, and D. W. Murray. Real-time
large-scale dense 3D reconstruction with loop closure. In
European Conference on Computer Vision, pages 500–516.
Springer, 2016. 4
[39] A. Karpathy, S. Miller, and L. Fei-Fei.
Object discovery
in 3D scenes via shape analysis. In Robotics and Automa-
tion (ICRA), 2013 IEEE International Conference on, pages
2088–2095. IEEE, 2013. 4
[40] M. Kepski and B. Kwolek.
Fall detection using ceiling-
mounted 3D depth camera. In Computer Vision Theory and
Applications (VISAPP), 2014 International Conference on,
volume 2, pages 640–647. IEEE, 2014. 2
[41] B.-s. Kim, P. Kohli, and S. Savarese. 3d scene understand-
ing by voxel-crf. In Proceedings of the IEEE International
Conference on Computer Vision, pages 1425–1432, 2013. 7
[42] M. Klingensmith, I. Dryanovski, S. Srinivasa, and J. Xiao.
Chisel: Real time large scale 3D reconstruction onboard a
mobile device using spatially hashed signed distance ﬁelds.
In Robotics: Science and Systems, 2015. 4
[43] H. S. Koppula, R. Gupta, and A. Saxena. Learning human
activities and object affordances from RGB-D videos. The
International Journal of Robotics Research, 32(8):951–970,
2013. 2
[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet
classiﬁcation with deep convolutional neural networks. In
Advances in neural information processing systems, pages
1097–1105, 2012. 1
[45] Y. Li, A. Dai, L. Guibas, and M. Nießner. Database-assisted
object retrieval for real-time 3D reconstruction. In Computer
Graphics Forum, volume 34, pages 435–446. Wiley Online
Library, 2015. 2
[46] D. Lin, S. Fidler, and R. Urtasun. Holistic scene understand-
ing for 3D object detection with RGBD cameras. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 1417–1424, 2013. 2
[47] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon objects in context. In European Conference on Com-
puter Vision, pages 740–755. Springer, 2014. 1
[48] L. Liu and L. Shao. Learning discriminative representations
from RGB-D video data. In IJCAI, volume 1, page 3, 2013.
2
[49] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3431–3440, 2015. 7
[50] M. Luber, L. Spinello, and K. O. Arras. People tracking in
RGB-D data with on-line boosted target models. In 2011
IEEE/RSJ International Conference on Intelligent Robots
and Systems, pages 3844–3849. IEEE, 2011. 2
[51] J. Mason, B. Marthi, and R. Parr. Object disappearance for
object discovery. In 2012 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems, pages 2836–2843.
IEEE, 2012. 2
[52] O. Mattausch, D. Panozzo, C. Mura, O. Sorkine-Hornung,
and R. Pajarola.
Object detection and classiﬁcation from
large-scale cluttered indoor scans. In Computer Graphics Fo-
rum, volume 33, pages 11–21. Wiley Online Library, 2014.
2, 6
[53] D. Maturana and S. Scherer. VoxNet: A 3D convolutional
neural network for real-time object recognition. In Intelligent
Robots and Systems (IROS), 2015 IEEE/RSJ International
Conference on, pages 922–928. IEEE, 2015. 2
[54] J. McCormac, A. Handa, A. Davison, and S. Leutenegger.
Semanticfusion: Dense 3d semantic mapping with convo-
lutional neural networks. arXiv preprint arXiv:1609.05130,
2016. 7, 8
[55] S. Meister, S. Izadi, P. Kohli, M. H¨ammerle, C. Rother, and
D. Kondermann. When can we use KinectFusion for ground
truth acquisition. In Workshop on Color-Depth Camera Fu-
sion in Robotics, IROS, volume 2, 2012. 2
[56] A. Mian, M. Bennamoun, and R. Owens. On the repeatabil-
ity and quality of keypoints for local feature-based 3D ob-
ject retrieval from cluttered scenes. International Journal of
Computer Vision, 89(2-3):348–361, 2010. 2
[57] R. Min, N. Kose, and J.-L. Dugelay.
KinectFaceDB: A
Kinect database for face recognition.
IEEE Transactions
on Systems, Man, and Cybernetics: Systems, 44(11):1534–
1548, 2014. 2
[58] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor
segmentation and support inference from RGBD images. In
ECCV, 2012. 1, 2, 6, 7, 8, 15, 21
[59] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and
A. Fitzgibbon. KinectFusion: Real-time dense surface map-
ping and tracking. In Mixed and augmented reality (ISMAR),


--- Page 11 ---

2011 10th IEEE international symposium on, pages 127–
136. IEEE, 2011. 2, 4
[60] D. T. Nguyen, B.-S. Hua, L.-F. Yu, and S.-K. Yeung. A ro-
bust 3D-2D interactive tool for scene segmentation and an-
notation. arXiv preprint arXiv:1610.05883, 2016. 2, 3
[61] B. Ni, G. Wang, and P. Moulin. RGBD-HuDaAct: A color-
depth video database for human daily activity recognition. In
Consumer Depth Cameras for Computer Vision, pages 193–
208. Springer, 2013. 2
[62] M. Nießner, M. Zollh¨ofer, S. Izadi, and M. Stamminger.
Real-time 3D reconstruction at scale using voxel hashing.
ACM Transactions on Graphics (TOG), 32(6):169, 2013. 2,
4, 15
[63] Occipital. Occipital: The structure sensor, 2016. 3
[64] K. Phillip and V. Koltun. Efﬁcient inference in fully con-
nected crfs with gaussian edge potentials. Adv. Neural Inf.
Process. Syst, 2011. 8
[65] F. Pomerleau, S. Magnenat, F. Colas, M. Liu, and R. Sieg-
wart. Tracking a depth camera: Parameter exploration for
fast ICP. In 2011 IEEE/RSJ International Conference on In-
telligent Robots and Systems, pages 3824–3829. IEEE, 2011.
2
[66] C. R. Qi, H. Su, M. Niessner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view CNNs for object classi-
ﬁcation on 3D data. arXiv preprint arXiv:1604.03265, 2016.
2, 6, 8
[67] X. Ren, L. Bo, and D. Fox. RGB-(D) scene labeling: Fea-
tures and algorithms. In Computer Vision and Pattern Recog-
nition (CVPR), 2012 IEEE Conference on, pages 2759–2766.
IEEE, 2012. 2
[68] A. Richtsfeld, T. M¨orwald, J. Prankl, M. Zillich, and
M. Vincze. Segmentation of unknown objects in indoor en-
vironments.
In 2012 IEEE/RSJ International Conference
on Intelligent Robots and Systems, pages 4791–4796. IEEE,
2012. 2
[69] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Free-
man. LabelMe: a database and web-based tool for image
annotation. International journal of computer vision, 77(1-
3):157–173, 2008. 2
[70] M. Savva, A. X. Chang, P. Hanrahan, M. Fisher, and
M. Nießner.
SceneGrok:
Inferring action maps in 3D
environments.
ACM Transactions on Graphics (TOG),
33(6):212, 2014. 2
[71] M. Savva, A. X. Chang, P. Hanrahan, M. Fisher, and
M. Nießner. PiGraphs: Learning interaction snapshots from
observations. ACM Transactions on Graphics (TOG), 35(4),
2016. 1, 2, 3, 15, 16
[72] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and
A. Fitzgibbon. Scene coordinate regression forests for cam-
era relocalization in RGB-D images. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2930–2937, 2013. 2
[73] A. Shrivastava and A. Gupta. Building part-based object de-
tectors via 3D geometry. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 1745–1752,
2013. 2
[74] N. Silberman and R. Fergus. Indoor scene segmentation us-
ing a structured light sensor. In Proceedings of the Inter-
national Conference on Computer Vision - Workshop on 3D
Representation and Recognition, 2011. 1, 2
[75] S. Song, S. P. Lichtenberg, and J. Xiao. SUN RGB-D: A
RGB-D scene understanding benchmark suite. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 567–576, 2015. 1, 2
[76] S. Song and J. Xiao. Sliding shapes for 3D object detection in
depth images. In European Conference on Computer Vision,
pages 634–651. Springer, 2014. 2
[77] S. Song and J. Xiao.
Deep sliding shapes for amodal
3D object detection in RGB-D images.
arXiv preprint
arXiv:1511.02300, 2015. 1, 2
[78] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and
T. Funkhouser.
Semantic scene completion from a single
depth image. arXiv preprint arXiv:1611.08974, 2016. 2
[79] L. Spinello and K. O. Arras. People detection in RGB-D
data. In 2011 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems, pages 3838–3843. IEEE, 2011.
2
[80] S. Stein and S. J. McKenna.
Combining embedded ac-
celerometers with computer vision for recognizing food
preparation activities. In Proceedings of the 2013 ACM inter-
national joint conference on Pervasive and ubiquitous com-
puting, pages 729–738. ACM, 2013. 2
[81] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers.
A benchmark for the evaluation of RGB-D SLAM
systems. In 2012 IEEE/RSJ International Conference on In-
telligent Robots and Systems, pages 573–580. IEEE, 2012.
1, 2
[82] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3D shape
recognition. In Proc. ICCV, 2015. 6
[83] J. Sung, C. Ponce, B. Selman, and A. Saxena. Human activ-
ity detection from RGBD images. plan, activity, and intent
recognition, 64, 2011. 2
[84] A. Teichman, S. Miller, and S. Thrun. Unsupervised intrinsic
calibration of depth sensors via SLAM. In Robotics: Science
and Systems, volume 248, 2013. 3, 20
[85] J. Valentin, A. Dai, M. Nießner, P. Kohli, P. Torr, S. Izadi,
and C. Keskin. Learning to navigate the energy landscape.
arXiv preprint arXiv:1603.05772, 2016. 2
[86] J. Valentin, V. Vineet, M.-M. Cheng, D. Kim, J. Shotton,
P. Kohli, M. Nießner, A. Criminisi, S. Izadi, and P. Torr. Se-
manticPaint: Interactive 3D labeling and learning at your ﬁn-
gertips. ACM Transactions on Graphics (TOG), 34(5):154,
2015. 2, 7, 8
[87] H. Wang, J. Wang, and W. Liang. Online reconstruction of
indoor scenes from RGB-D streams. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3271–3279, 2016. 3
[88] T. Whelan, M. Kaess, M. Fallon, H. Johannsson, J. Leonard,
and J. McDonald. Kintinuous: Spatially extended KinectFu-
sion. 2012. 4
[89] T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker,
and A. J. Davison. ElasticFusion: Dense SLAM without a


--- Page 12 ---

pose graph. Proc. Robotics: Science and Systems, Rome,
Italy, 2015. 4
[90] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison,
and S. Leutenegger. ElasticFusion: Real-time dense SLAM
and light source estimation.
The International Journal of
Robotics Research, page 0278364916669237, 2016. 4
[91] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumetric
shapes. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1912–1920, 2015. 1,
2, 6, 8, 21
[92] J. Xiao, A. Owens, and A. Torralba. SUN3D: A database
of big spaces reconstructed using sfm and object labels. In
Computer Vision (ICCV), 2013 IEEE International Confer-
ence on, pages 1625–1632. IEEE, 2013. 1, 2
[93] B. Zeisl, K. Koser, and M. Pollefeys. Automatic registration
of RGB-D scans via salient directions. In Proceedings of
the IEEE international conference on computer vision, pages
2808–2815, 2013. 2
[94] J. Zhang, C. Kan, A. G. Schwing, and R. Urtasun. Estimating
the 3D layout of indoor scenes and its clutter from depth sen-
sors. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1273–1280, 2013. 2
[95] M. Zollh¨ofer, A. Dai, M. Innmann, C. Wu, M. Stamminger,
C. Theobalt, and M. Nießner. Shading-based reﬁnement on
volumetric signed distance functions. ACM Transactions on
Graphics (TOG), 34(4):96, 2015. 2
A. Dataset Statistics and Comparisons
In this section, we provide thorough statistics on the
construction and composition of ScanNet dataset, and also
compare it to the most similar datasets from prior work.
A.1. Example Annotated Reconstructions
Fig. 6 shows six example annotated reconstructions for
a variety of spaces. For each reconstruction, the surface
mesh with colors is shown, as well as a visualization with
category labels for each object collected using our crowd-
sourced annotation interface. Category labels are consistent
between spaces and are mapped to WordNet [18] synsets.
In addition to the category label, separate object instance
labels are also available to indicate multiple instances of a
given category, such as distinct chairs around a conference
table in the fourth row of Fig. 6.
Fig. 7 shows a larger set of reconstructed spaces in Scan-
Net to illustrate the variety of spaces that are part of the
dataset. The scans range from small spaces with just a few
objects (e.g., toilets), to large areas with dozens of objects
(e.g., classrooms and studio apartments).
A.2. Dataset Construction Statistics
The construction of ScanNet was carried out with the
RGB-D acquisition and annotation framework described in
the main paper.
In order to provide an intuition of the
ScanNet
Category
Count
wall
6226
chair
4279
ﬂoor
3212
table
2223
door
1181
couch
1048
cabinet
937
desk
733
shelf
732
bed
699
ofﬁce chair
669
trashcan
561
pillow
490
sink
470
window
398
toilet
397
picture
351
bookshelf
328
monitor
308
curtain
280
computer
274
armchair
264
bathtub
253
coffee table
239
box
231
dining chair
230
refrigerator
226
book
221
lamp
218
towel
216
kitchen cabinet
203
drawer
202
tv
187
nightstand
182
counter
179
dresser
177
clothes
164
countertop
163
stool
130
plant
130
cushion
116
ceiling
114
bedframe
111
keyboard
107
end table
105
toilet paper
104
bag
104
backpack
100
blanket
94
dining table
94
SceneNN [32]
Category
Count
chair
194
table
53
ﬂoor
44
seat
41
desk
39
monitor
31
sofa
25
cabinet
25
door
24
box
23
keyboard
23
trash bin
21
wall
20
pillow
19
fridge
18
stand
18
bag
17
bed
16
window
14
sink
13
printer
12
computer
12
chair01
12
desk1
11
monitor01
10
shelves
10
shelf
10
chair1
10
chair02
10
fan
9
basket
9
desk2
9
laptop
9
trashbin
9
kettle
9
microwave
9
monitor1
8
stove
8
chair2
8
bike
7
blanket
7
drawer
7
lamp
7
wall02
7
wall01
7
wall04
7
backpack
7
cup
7
chair3
7
whiteboard
7
Table 8. Total counts of annotated object instances of the 50 largest
categories in ScanNet (left), and in SceneNN [32] (right), the most
similar annotated RGB-D reconstruction dataset. ScanNet con-
tains far more annotated object instances, and the annotated la-
bels are processed for consistency to remove duplicates such as
“chair01” in SceneNN.
scalability of our framework, we report timing statistics
for both the reconstruction and annotation steps. The me-
dian reconstruction processing time (including data conver-
sion, dense voxel fusion, surface mesh extraction, align-


--- Page 13 ---

Figure 6. Example annotated scans in ScanNet. Left: reconstructed surface mesh with original colors. Middle: color indicates category
label consistently across all scans. Right: each object instance shown with a different randomly assigned color.


--- Page 14 ---

Figure 7. A variety of example annotated scans in ScanNet. Colors indicate category consistently across all scans.


--- Page 15 ---

ment, cleanup, and preview thumbnail image rendering) is
11.3 min for each scene. A few outliers exist with signiﬁ-
cantly higher processing times (on the order of hours), due
to unplanned processing server downtime during our data
collection (mainly software updates), resulting in a higher
mean reconstruction time of 14.9 min.
After reconstruction is complete, each scan is annotated
by several crowd workers on Amazon Mechanical Turk (2.3
workers on average per scan). The median annotation time
per crowd worker is 12.0 min (mean time is 17.3 min, again
due to a few outlier workers who take signiﬁcantly longer).
Aggregating the time taken across workers for annotating
each of the 1513 scans in ScanNet, the median time per
scan is 16.8 min, and the mean time per scan is 22.3 min.
A.3. Dataset Composition Statistics
The construction of the ScanNet dataset is motivated by
the lack of large, annotated, densely reconstructed RGB-D
dataset of 3D scenes that are publicly available in the aca-
demic community. Existing RGB-D datasets either have
full scene-level annotations only for a subset of RGB-D
frames (e.g., NYU v2 depth [58]), or they focus on anno-
tating decontextualized objects and not scenes (e.g., Choi et
al. [10]). The two datasets that do annotate densely recon-
structed RGB-D spaces at the scene level are the SceneNN
dataset by Hua et al. [32] and the smaller PiGraphs dataset
by Savva et al. [71].
SceneNN consists of 94 RGB-D scans captured using
Asus Xtion Pro devices and reconstructed with the method
of Choi et al. [9].
The resulting densely-fused surface
meshes are fully segmented at the level of meaningful ob-
jects. However, only a small set of segments are annotated
with semantic labels. On the other hand, the PiGraphs [71]
dataset consists of 26 RGB-D scans captured with Kinect v1
devices and reconstructed with the VoxelHashing approach
of Nießner et al. [62]. This dataset has more complete and
clean semantic labels, including object parts and object in-
stances. However, it contains very few scenes and is limited
in the variety of environments, consisting mostly of ofﬁces
and conference rooms. To illustrate the large gap in quan-
tity of annotated semantic labels between these two datasets
and ScanNet, Fig. 8 plots histograms of the total number of
labeled object instances and the total numbers of unique se-
mantic labels for each scan.
In order to demonstrate how our category labels map
to other data, we plot the distribution of annotated object
labels corresponded to the ShapeNetCore 3D CAD model
categories in Fig. 9.
This mapping is leveraged during
our CAD model alignment and retrieval task to automati-
cally suggest instances of CAD models from ShapeNet that
match the label of a given object category in the reconstruc-
tion.
We can also obtain 2D annotations on the input RGB-D
Figure 9. Top 25 most frequent annotation labels in ScanNet scans
mapped to ShapeNetCore classes. ScanNet has thousands of 3D
reconstructed instances of common objects such as chairs, tables,
and cabinets.
sequences by projecting our 3D annotations into each frame
using the corresponding camera pose. This way, we ob-
tain an average of 76% annotation coverage of all pixels per
scene by using the previously obtained 3D annotations.
A.4. NYUv2 Reconstruction and Comparison
Here, we discuss how ScanNet relates to NYUv2, one of
the most popular RGB-D dataset with annotations. In order
to compare the data in ScanNet with the data in NYUv2,
we reconstructed and annotated all the RGB-D sequences
in NYUv2 using our framework. (Note that for 9 sequences
of the NYUv2 dataset, our framework did not obtain valid
camera poses for > 50% of the frames, so we did not com-
pute reconstructions and annotations for these sequences.)
Moreover, we created a set of surface mesh semantic anno-
tations for the NYUv2 reconstructions by projecting every
pixel of the annotated RGB-D frames with valid depth and
label into world space using our computed camera poses,
and assigning the corresponding object label to the clos-
est surface mesh vertices (within 0.04cm, using a kd-tree
lookup).
We then compare the total surface area of the recon-
structed meshes that was annotated using projection from
the annotated NYUv2 frames, and using our annotation
pipeline. Fig. 10 plots the percentage of reconstructed sur-
faces in NYUv2 that were annotated with each approach,
as well as the percentage distribution for the ScanNet re-
constructions for comparison. Note that we exclude the 9
sequences for which we do not have enough valid camera
poses.
A noticeable difference between the RGB-D sequences
in NYUv2 and those in ScanNet is that overall, the ScanNet
sequences are more complete surface reconstructions of the
real-world spaces. Most importantly, the NYUv2 original
frames in general do not cover a sufﬁcient number of view-
points of the space to ensure full reconstruction of semanti-
cally meaningful complete objects. Fig. 11 shows a compar-
ison of several reconstructed scenes from NYUv2 RGB-D


--- Page 16 ---

PiGraphs
SceneNN
ScanNet
0
10
20
30
40
50
Labeled Objects
0
10
20
30
40
50
Labeled Objects
0
10
20
30
40
50
Labeled Objects
0
50
100
150
200
250
300
350
400
450
500
550
600
Number of Scans
PiGraphs
SceneNN
ScanNet
0 10 20
30 40 50 60
70 80
Unique Labels
0 10 20
30 40 50 60
70 80
Unique Labels
0 10 20
30 40 50 60
70 80
Unique Labels
0
50
100
150
200
250
300
350
400
450
Number of Scans
Figure 8. Histograms of the total number of objects labeled per scan (top) and total number of unique labels per scan (bottom) in the
PiGraphs [71], SceneNN [32] and our dataset (ScanNet). The histograms show that ScanNet has many annotated objects over a larger
number of scans, ranging in complexity with regards to the total number of objets per scan.


--- Page 17 ---

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
NYU Surface Coverage (Original Frames)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
NYU Surface Coverage (3D Re-annotation)
0%
2%
4%
6%
8%
10%
12%
14%
% of Scans
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
ScanNet Surface Coverage (3D Annotation)
0%
2%
4%
6%
8%
10%
12%
14%
16%
18%
% of Scans
Figure 10. Histograms of the percentage of total reconstruction surface area per scan that is semantically labeled for: NYU v2 reconstruc-
tions using projection of RGB-D annotated frames (left), for NYU v2 reconstructions using our 3D annotation interface (middle), and for
ScanNet reconstructions similarly annotated with our interface (right).
sequences vs comparable reconstructions from ScanNet. As
shown in the top-down views, the NYU reconstructions are
much more sparse than the ScanNet reconstructions. This
disparity makes a more direct comparison with ScanNet re-
constructions hard to quantify. However, we can conclude
that projecting the annotated NYUv2 RGB-D frames to re-
constructions is not sufﬁcient to semantically annotate the
spaces, as is clear from the far lower surface coverage dis-
tribution for NYUv2 in Fig. 10.
B. Tasks
Here we provide more details about the 3D scene under-
standing tasks and benchmarks discussed in the main paper.
B.1. Semantic Voxel Labeling
For the semantic voxel labeling task, we propose a net-
work which predicts class labels for each column of a vox-
elized scene. As shown in Fig. 12, our network takes as
input a 2 × 31 × 31 × 62 volume and uses a series of fully
convolutional layers to simultaneously predict class scores
for the center column of 62 voxels. We leverage informa-
tion from the voxel neighborhood of both occupied space
(voxels on the surface) and known space (voxels in front of
a surface according to the camera trajectory) to describe the
input partial data from a scan.
At test time, we slide the network through a scan through
a voxelized scan along the xy-plane, and each column is
predicted independently. Fig. 13 visualizes several ScanNet
test scans with voxel label predictions, alongside the ground
truth annotations from our crowdsourced labeling task.
Figure 11. Comparison of reconstructed Bathroom (top), Bedroom
(middle), and Kitchen (bottom) from NYUv2 RGB-D frames
(left), and a comparable reconstruction from ScanNet (right).
For each NYU scene, we show an example color frame, the
rough corresponding region of the view in the reconstructed scene
(light blue box), and a top down view of the reconstruction.
While NYUv2 reconstruction look complete from some view-
points, much of the scene is left uncovered (see top down views).
In constrast, ScanNet reconstruction have a much more complete
coverage of the space and allow for denser annotation.
C. Dataset Acquisition Framework
This section provides more details for speciﬁc steps
in our RGB-D data acquisition framework which was de-
scribed in the main paper. To enable scalable dataset ac-
quisition, we designed our data acquisition framework for
1) ease of use during capture, 2) robust reconstruction, 3)
rapid crowdsourcing, 4) visibility into the collected data and
its metadata. For 1) we developed an iPad app (see Ap-


--- Page 18 ---

Figure 12. Deep Neural Network architecture for our semantic voxel label prediction task. The network is mainly composed of 3D
convolutions that process the geometry of a scene using a 3D voxel grid representation.
Figure 14. Our RGB-D recording app on an iPad Air2 with at-
tached Structure sensor (showing color stream at the top and depth
stream at the bottom). The app allows novice users to record RGB-
D videos and upload to a server for reconstruction and annotation.
pendix C.1) with an easy-to-use interface, reasonable scan-
ning presets, and minimalistic user controls. To ensure good
reconstruction with minimal user interaction during scan-
ning, we tested different exposure time settings and enabled
auto white balancing (see Appendix C.1). We also estab-
lished a simple calibration process that novice users could
carry out (see Appendix C.2), and ofﬂoaded RGB-D recon-
struction to the cloud (see Appendix C.3). Finally, we de-
veloped web-based UIs for crowdsourcing semantic anno-
tation tasks as described in Appendix C.4, and for managing
the collected data as described in Appendix C.6.
C.1. RGB-D Acquisition UI
Fig. 14 shows our RGB-D recording app on the iPad. We
designed an iPad app with a simple camera-based UI and a
minimalistic set of controls. Before scanning, the user en-
ters a user name, a scene name, and selects the type of room
being scanned. The user then presses a single button to start
and stop a scan recording. The interface can be toggled
between visualizing the color stream and the depth stream
overlaid on the color.
We found that the most challenging part of scanning
for novice users was acquiring an intuition as to what re-
gions during scanning are likely to result in poor track-
ing and failed reconstruction. To alleviate this, we added
a “progress bar”-style visualization during active scan-
ning which indicates the featurefulness of the region being
scanned. The bar ranges from full green, indicating high
feature count, to near-empty black, indicating low feature
count and high likelihood of tracking loss. This UI element
was helpful for quickly familiarizing users with the scan-
ning process. After scanning, the user can view a list of
scans on the device and select to upload the scan data to a
processing server. During upload, a progress bar is shown
and scanning is disabled. Upon completion of the upload,
the checksums of scan data on the server are veriﬁed against
local data and the scans are automatically deleted to provide
more memory for scanning.
Auto white balancing and Exposure Settings
Another
challenge towards performing reconstruction in uncon-
trolled scenarios is the wide variety of illumination con-
ditions. Since our scanning app was designed for novice
users, we opted to provide a reasonable set of presets and al-
low for manual override only when deemed necessary. By
default, we enabled continuous automatic whitepoint bal-
ancing as implemented by the iOS SDK. We also enabled
dynamic exposure selection again as implemented by the
iOS SDK, but instructed users that they could manually ad-
just exposure if necessary to make overly dark locations
brighter, or overly bright locations darker. The exposure
setting can have a signiﬁcant impact on the amount of mo-
tion blur during scanning. However, we found that inex-
perienced users preferred to rely on dynamic exposure, and
typically moved relatively slowly during scanning, making
motion blur less of an issue. The average exposure time
during scans with dynamic exposure was close to 30 ms.


--- Page 19 ---

Figure 13. Semantic voxel labeling of 3D scans in ScanNet using our 3D CNN architecture. Voxel colors indicate predicted or ground truth
category.


--- Page 20 ---

C.2. Sensor Calibration
Sensor calibration is a critical, yet often overlooked part
of RGB-D data acquisition. Our experiments showed that
depth-to-color calibration is an important step in acquir-
ing good 3D reconstructions from RGB-D sequences (see
Fig. 15).
Depth To Color Calibration
To align a depth image D
to color image C, we need to estimate intrinsic parameters
of both sensors, the infrared camera KD and color cam-
era KC, as well as extrinsic transformation TD→C. In our
experiments we have found that using the set of intrinsic pa-
rameters of focal length, center of projection, and two bar-
rel distortion coefﬁcients models worked well for the used
cameras. To obtain calibration parameters KD and KC we
capture a series of color-infrared pairs showing an asym-
metric checkerboard grid. We then estimate calibration pa-
rameters for each camera with Matlab’s CameraCalibrator
application. During this procedure we additionally obtain
the world positions of calibration grid corners, and use them
to estimate the transformation TD→C.
Depth Distortion Calibration
Previous work suggests
that for consumer-level depth cameras there exists depth-
dependent distortion that increases as camera moves away
from the surface. Thus, we decided to augment our set of
intrinsic parameters for depth cameras with a undistortion
lookup table, as ﬁrst suggested in Teichman et al. [84]. This
look up table is a function f(x, y, d), of spatial coordinates
x, y and observed depth d, returning a multiplication fac-
tor m used to obtain undistorted depth d′ = md. The ta-
ble is computed from training pairs of observed and ground
truth depths d and dt. However, unlike Teichman’s unsuper-
vised approach, which produces training pairs using care-
fully taken ’calibration sequences’, we decided to design a
supervised approach similar to that of Di Cicco [14]. How-
ever, we found that at large distances the depth distortion
becomes so severe that approaches based on ﬁtting planes
to depth data are bound to fail. Thus to obtain training pairs
{d, dt}, we capture a color-depth video sequence of a large
ﬂat wall with a calibration target at the center, as the user
moves away and towards the wall. To ensure successfull
calibration process user needs to ensure that the viewed wall
is the only observed surface and that it covers the entire ﬁeld
of view. With the captured color-depth sequence and previ-
ously estimated KD, KC, TD→C we can recover the the
world positions of the calibration grid corners, effectively
obtaining the ground truth plane locations for each of the
captured depth images. For each pixel x, y with depth d, we
then shoot a ray through x, y to intersect with the related
plane. dt can be recovered from the point of intersection.
The rest of our undistortion pipeline follows closely the that
Figure 15. Comparison of calibration results. In the top row, we
show results of calibration on a ﬂat wall. As the distance increases
the distortion becomes quite severe, motivating the need for depth
distortion calibration. In the bottom row, we show results of frame-
to-frame tracking on raw and calibrated data.
of Teichman et al. [84]. We found that undistorting depth
images obtained by a Structure sensor leads to signiﬁcantly
improved tracking.
C.3. Surface Reconstruction
Given a calibrated RGB-D sequence as input, a fused
3D surface reconstruction is obtained using the BundleFu-
sion framework [12], as described in the main paper. The
reconstruction is then cleaned by merging vertices within
1 mm of each other, and removing connected components
with fewer than 7500 triangles. Following this cleanup step,
two quadric edge collapse decimation steps are performed
to produce lower triangle count versions of each surface
mesh. Each decimation halves the number of triangles in
the surface mesh, reducing the size of the original meshes
from an average of 146 MB to 5.82 MB for the low res-
olution mesh. The mesh decimation step is important for
reducing data transfer requirements and improving loading
times during the crowdsourced annotation using our web-
based UI.
C.4. Crowdsourced Annotation UI
We deployed our semantic annotation task to crowd
workers on the Amazon Mechanical Turk platform. Each
annotation task began with an introduction (see Fig. 16)
providing a basic overview of the task. The worker was
then shown a reconstruction and asked to paint all object
instances with a color and corresponding label. The worker
was required to annotate at least 25% of the surface area of
the reconstruction, and encouraged to cover at least 50%.
Once the worker was done, they could submit by pressing
a button. Workers were compensated with $0.50 for each
annotation task performed.
The CAD model retrieval and alignment task began with
a view of an already semantically annotated reconstruc-
tion and asked workers to click on objects to retrieve and


--- Page 21 ---

place appropriate CAD models. Fig. 17 shows the initial in-
structions for an example reconstruction with several chairs.
Workers for this task were required to place at least three
objects before submitting. Once the worker was done, they
were compensated with $1.00 for each completed task.
C.5. Label cleaning and propagation
Labeling is performed on the surface mesh reconstruc-
tion, with several workers labeling each scan. To ensure that
labels are consistent across workers, we use standard NLP
techniques to clean up the labels. First, we use a manually
curated list of good labels and their synonyms to compute a
map to a single canonical label for each set, also including
common misspellings by a small edit distance threshold of
the given label. Labels with less than 5 counts are deemed
unreliable and ignored in all statistics. Labels with more
than 20 counts are manually examined and added to the list
of good labels or collapsed as a synonym of a good label.
The list of these frequent collapsed labels is also mapped to
WordNet [18] synsets when possible, and to other common
label sets that are commonly used for RGB-D and 3D CAD
data (NYUv2 [58], ModelNet [91], and ShapeNetCore [6]).
Using the cleaned labels, we then compute an aggre-
gated consensus labeling of each scene, since any individual
crowdsourced annotation of a scene may not cover the en-
tire scene, or may contain some errors. For each segment in
the over-segmentation of a scene mesh, we ﬁrst take the ma-
jority vote label. This groups together instances of the same
class of objects, so we also compute a labeling purely based
on geometric overlap; that is, we greedily take the unions
of annotations which have ≥50% overlap of segments. We
then take the maximal intersections between these two la-
belings to obtain the ﬁnal consensus.
After we have obtained the aggregated consensus seman-
tic annotation for a scene, we then propagate these labels to
the high-resolution mesh as well as to the 2D frames of the
input RGB-D sequence. To propagate the labels to the high
resolution mesh, we compute a kd-tree over the mesh ver-
tices of the labeled coarse mesh, and we label each vertex
of the high resolution mesh according to a nearest neighbor
lookup in the kd-tree. We project the 3D semantic annota-
tions to the input 2D frames by rendering the labeled mesh
from the camera poses of each frame, and follow this with
a joint dilation ﬁlter with the original RGB image and joint
erosion ﬁlter with the original RGB image.
C.6. Management UI
To enable scalability of our RGB-D acquisition and an-
notation, and continual transparency into the progress of
scans throughout our framework, we created a web-based
management UI to track and organize all data (see Fig. 19).
When a user is ﬁnished scanning and presses the upload
button on an iPad device, their scan data is automatically
Figure 16. Instructions provided to crowd workers for our semantic
annotation task. Top: instructions before the beginning of the task.
Bottom: interface instructions during annotation.
uploaded to our processing server, placed into a reconstruc-
tion queue, and immediately made visible in the manage-
ment UI. As the reconstruction proceeds through the var-


--- Page 22 ---

Figure 17. Instructions provided to crowd workers for our CAD
model alignment task. The worker clicks on colored objects to
retrieve and place CAD models.
Figure 18. ShapeNetCore [6] CAD models retrieved and placed
on ScanNet scans by crowd workers (scan mesh is transparent and
CAD models are opaque). From top left clockwise: a classroom,
bedroom, bathroom, and lounge scan.
ious stages of data conversion, calibration, pose optimiza-
tion and RGB-D fusion, alignment, cleanup, decimation,
and segmentation, progress is visualized in the manage-
ment UI. Thumbnail renderings of the generated surface re-
construction, and statistics such as total number of frames,
reconstructed ﬂoor area etc. are automatically computed
and can be used for ﬁltering and sorting of the reconstruc-
tions. Similarly, during crowdsourced annotation, worker
progress and aggregated annotated surface area statistics
are visible and usable for sorting and ﬁltering of the scan
database.
Figure 19. Our web-based data management UI for ScanNet scan
data.
```

---

## Section 1.31: Semi-sharp subdivision surface fitting based on feature lines approximation

Source File: Semi-sharp subdivision surface fitting based on feature lines approximation.txt

### Paper Content:

```
# Semi-sharp subdivision surface fitting based on feature lines approximation.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Semi-sharp subdivision surface fitting based on feature lines approximation.pdf
# Output: ../layer2_completion/txt/Semi-sharp subdivision surface fitting based on feature lines approximation.txt


--- Page 1 ---

HAL Id: hal-01437625
https://hal.science/hal-01437625v1
Submitted on 20 Nov 2022
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Distributed under a Creative Commons Attribution - NonCommercial 4.0 International License
Semi-Sharp Subdivision Surface Fitting Based on
Feature Line Approximation
Guillaume Lavoué, Florent Dupont
To cite this version:
Guillaume Lavoué, Florent Dupont. Semi-Sharp Subdivision Surface Fitting Based on Feature Line
Approximation. Computers and Graphics, 2009, 33 (2), pp.151-161. ￿10.1016/j.cag.2009.01.004￿. ￿hal-
01437625￿


--- Page 2 ---

Semi-sharp subdivision surface ﬁtting based on feature lines approximation
Guillaume Lavoue´ a,b,, Florent Dupont a,c
a Universite´ de Lyon, CNRS
b INSA-Lyon, LIRIS, UMR5205, F-69621, France
c Universite Lyon 1, LIRIS, UMR5205, F-69622, France
This paper presents an algorithm for approximating arbitrary polygonal meshes with subdivision
surfaces, with the objective of preserving the relevant features of the object while searching the coarsest
possible control mesh. The main idea is to ﬁrstly extract the feature lines of the object, and secondly
construct the subdivision surface over this network. Control points are created by approximating these
lines while the connectivity is built with respect to the anisotropy of the object. Our algorithm
reinforces the similarity between the subdivision surface and the original shape by affecting an integer
sharpness degree to each control edge in order to accurately reproduce the different curvature radii of
corresponding ﬁllets and blends.
1. Introduction
Finding an optimal and concise representation for a 3D model
is particularly crucial in computer graphics and computer-aided
design. Indeed, obtaining a synthetic representation of a shape,
usually deﬁned as a redundant dense polygonal mesh, is of interest
for many applications: animation, compression, recognition and
reverse engineering. Subdivision surfaces combine a lot of proper-
ties quite relevant for this issue: this model is very compact,
can represent an arbitrary topology, allows a local control and is
intrinsically multi-resolution. For these reasons, subdivision surfaces
are more and more popular in computer graphics and have been
integrated to the MPEG4 standard. In this context, approximating a
dense verbose polygonal mesh with that model becomes even more
pertinent.
Hence, we present an algorithm for subdivision surface ﬁtting
from
arbitrary
polygonal
meshes,
and
particularly
scanned
models; this algorithm follows our previous approach [1] which
was speciﬁcally designed for piecewise smooth hand-made CAD
models. Our main objective is not to focus on approximation error
but rather to preserve the relevant features of the object while
searching the coarsest possible control mesh. Our algorithm
involves three main parts: a ﬁrst step extracts a smooth feature
line network from the object, using segmentation and smoothing
of the patch boundaries. A second process approximates these
feature lines with subdivision curves and creates a coarse base
mesh by linking corresponding control points. Finally, a relaxing
process affects a sharpness degree to each control edge and
optimizes the control point positions in order to ﬁt the target
object. The whole algorithm is summarized in Fig. 1. Our main
contributions are the following:
 The global subdivision surface ﬁtting framework based on
feature line extraction and approximation, allowing to obtain a
near optimal vertex number.
 The feature line extraction, which extracts smooth feature lines
from arbitrary objects in a very simple way.
 The base mesh construction which is (1) independent of the
connectivity of the target surface, (2) correct even for complex
topology and (3) adapted to the anisotropy of the target object.
 The sharpness relaxation, which assign to each control edge a
semi-sharpness degree according to the curvature of the target
surface.
2. Previous work
2.1. Subdivision surface ﬁtting
Many authors have investigated subdivision surface ﬁtting,
since this issue is quite interesting for compression, reverse
engineering, etc. Most of the existing algorithms rely on the same
scheme: ﬁrst a coarse base control mesh is constructed by one
of the following approaches: simplifying the original dense mesh
[2–5], face clustering [6], triangulating an octree partition [7],
shrinking an initial mesh towards the surface [8] or global
parameterization and quad dominant remeshing as considered
by Le´vy et al. [9].
 Corresponding author. Tel.: +33 472 437136.
E-mail address: glavoue@liris.cnrs.fr (G. Lavoue´).
1


--- Page 3 ---

Once this coarse base mesh has been constructed, a second
step optimizes its geometry in order to ﬁt exactly the target object
using a global [10,2,4,3] or local [8,11] distance minimization.
Since the base mesh does not own a perfect connectivity, several
authors have also investigated connectivity optimization proce-
dures. Hoppe et al. [10] optimize the connectivity by trying to
collapse, split or swap each edge of the control polyhedron. Their
algorithm produces high quality models but need of course an
extensive computing time. Recently, Marinov and Kobbelt [4]
subdivide faces associated with high errors and ﬂip some edges
to regularize vertex valences, similar to [7]. Lavoue´ et al. [1] also
consider such local enrichment process piloted by the error
distribution. Finally, Le´vy et al. [9] optimize the connectivity of the
control mesh by analysing curvature directions of the target
surface.
2.2. Feature preserving remeshing
Very recent remeshing techniques [12–17] are mostly quad-
dominant and outline the importance for the connectivity of
a mesh to follow the salient features of the object and to align
with the geometry (sharp edges, lines of curvature etc.). The main
reason is given by D’Azevedo [18]: the convergence is improved by
such alignment, for both remeshing or ﬁtting. This is linked to the
concept of optimality: for a given number of elements, a mesh
best approximates a smooth surface if its connectivity follows the
lines of curvature. This feature alignment issue is as well relevant
for subdivision surface ﬁtting: the control mesh must respect and
follow the features of the object and especially if the goal is
to provide fewer vertices as possible. Our algorithm considers
segmentation to ﬁnd feature lines, and then curvature tensor
analysis to complete the connectivity. Our algorithm bears
similarities with the recent remeshing technique from Marinov
and Kobbelt [16] which samples equidistantly feature lines to
remesh the target object. However, our objective is not to produce
a nice control mesh but rather an efﬁcient subdivision surface that
correctly approximates a given shape while containing as less
control points as possible.
2.3. Lofting
This feature line preservation shares some similarities with
another class of algorithms, often referred to as lofting and
of particular interest for designers. These algorithms start from
a network of curves and generate a smooth surface which
interpolates
this
network.
Subdivision
surfaces
represent
a
powerful framework for this task [19–22]. Our algorithm bears
many similarities with a lofting scheme since it considers above
all feature lines to construct the subdivision surface.
A lofting algorithm has to resolve three principal difﬁculties:
ﬁrstly the curve network (usually a set of B-Splines) has to be
compatible with a smooth or piecewise smooth surface, in other
words, is it possible to construct a smooth surface over this
network? Schaefer et al. [22] deﬁne the curves with control polygons
associated with a speciﬁc subdivision scheme to insure this property.
Since our feature lines directly come from a real object to
approximate, we do not encounter compatibility problems.
The second major issue is the skinning step, which consists is
constructing the connectivity of the base control mesh. Basically
many authors consider that the curve network is deﬁned by a
control polygon network, thus for each cycle of polygons bounding
each patch, they construct the corresponding polyhedron by
linking boundary control points and creating some others in order
to have mostly quadrilateral and regular faces. Our scheme
considers only boundary control points and links them according
to curvature directions in order to obtain the most compact and
optimal base mesh.
Most
of
the
authors
consider
Catmull–Clark
subdivision
surfaces in their lofting algorithm; since this kind of surface is
not theoretically capable of interpolating a net of curves, they
modify the original scheme by considering special rules near the
curve network [19,22], or by introducing portions of surface
that speciﬁcally deﬁne the curve like the polygonal complexes
from Nasri [20,21]. In our case we consider a simple existing
subdivision scheme [23], since we do not search a perfect
interpolation but rather a quite good approximation.
2.4. Subdivision semi-sharpness
Many subdivision rules exist, some of them are adapted for
triangular control meshes, like Loop [24], and others are adapted
for quadrilateral ones, like Catmull–Clark [25]. We have chosen
the hybrid quad/triangle scheme developed by Stam and Loop
[23], since we want to adapt the control mesh connectivity to the
shape of the input object and thus we may obtain faces from
different degrees. This scheme reproduces Catmull–Clark on quad
regions and Loop on triangle regions. The control mesh is ﬁrstly
linearly subdivided and then each point is replaced by a linear
combination of itself and its direct neighbours following a
smoothing mask (see Fig. 2, left).
Fig. 1. Overview of our subdivision surface ﬁtting algorithm. Top row: The input
RockerArm mesh (15K vertices) and result of the segmentation (35 regions). Middle
row: Feature line network (left) and control polygon network created from
subdivision curve approximation (right). Bottom row: Subdivision control mesh
(160 vertices) with different sharpness degrees and associated limit surface.
2


--- Page 4 ---

Special rules have been introduced by Hoppe et al. [10] for
handling boundary edges, in such a way that the boundary curve
of the limit surface does not depend on any interior control
vertices. These rules can also be used to introduce sharp creases
(see
Fig.
3, right).
Fig. 2
(right)
illustrates
the smoothing
coefﬁcients for vertices shared by two sharp edges.
These inﬁnite sharp edges (see Fig. 3, right), introduced by
Hoppe et al. [10] are quite convenient to represent piecewise
smooth surfaces. However, real-world surfaces are never inﬁnitely
sharp, thus DeRose et al. [26] have introduced semi-sharp edges
whose sharpness is an integer sh which can vary from zero
(meaning smooth) to inﬁnite. This semi-sharp subdivision is then
processed by using sharp rules during the ﬁrst sh subdivision
steps, followed by the use of smooth rules for subsequent
subdivision steps.
With this semi-sharpness concept, the same control edge can
represent blends or ﬁllets associated with different curvature radii
(see Fig. 3). Our algorithm assigns to each edge of the control
mesh the appropriate sharpness degree; to our knowledge no
other existing algorithm carries out such a process.
3. Overview
Given an input polygonal mesh, we ﬁrstly process a decom-
position into several regions Ri, using a modiﬁed version of the
segmentation algorithm (VSA) from Cohen-Steiner et al. [27] (see
Fig. 1, top right). We then extract the network of corresponding
boundaries and apply a smoothing mask to obtain a smooth
feature line network (see Section 4, see Fig. 1, middle left).
The feature lines are then approximated with subdivision
curves, near optimal in terms of control points number, using the
algorithm from Lavoue´ et al. [28]. We thus obtain a control
polygon network (see Fig. 1, middle right). Each region Ri is then
treated separately: from the control polygon surrounding the
region, we create edges and facets by linking control points
with respect to its lines of curvature. We obtain a set of control
meshes Mi which are then assembled together; boundary edges
between them are marked as sharp in order to ﬁt correctly the
input object after subdivisions. We obtain the sharp control mesh
(see Section 5).
At this point, we have created a piecewise smooth subdivision
surface with sharp edges at the emplacement of extracted feature
lines (boundaries between regions Ri). Since this sharpness does
not necessarily correspond to the object aspect, a process relaxes
the sharpness by associating to each control edge an integer
sharpness degree instead of a Boolean value (sharp or not). For this
task we analyse the curvature of the input surface around feature
lines. Finally, since changing the sharpness of the control mesh
induces a shrinking of the limit surface, we perform a geometric
optimization by iteratively relocating control points in order
to minimize a global quadratic distance to the input surface
(see Section 6, see Fig. 1, bottom left).
This algorithm improves over previous work [1] with the
following improvements:
 Previous algorithm was limited to carefully designed CAD
mechanical objects with optimized triangulation, whereas our
new algorithm can approximate arbitrary models and particu-
larly scanned objects.
 The feature line extraction of our previous algorithm was only
suited to CAD models: it was based on a speciﬁc CAD
segmentation step [29]. Our new algorithm produces smooth
feature lines for arbitrary models.
 The creation of the base control mesh, in particular the edge
score (see Section 5.2), was improved regarding to robustness
and quality of the connectivity.
 Previous algorithm was creating sharp edges along feature
lines. While this is well suited to some speciﬁc objects, it is not
for all. Our new algorithm introduces sharpness relaxation
which assigns to each control edge the appropriate integer
sharpness degree.
 A geometric optimization was processed for every region
separately and thus with few degrees of freedom. We introduce
a global optimization process which allows minimizing a
global asymmetric error between the target object and the
approximating subdivision surface.
4. Feature line extraction
Feature lines of a surface carry the visually most salient
characteristics. They are usually described as local extrema of
principal curvatures along corresponding principal directions.
Several algorithms exist to extract smooth feature lines from an
input mesh [30,31], they are mostly based on computation of high
order derivatives of principal curvatures, then thresholding and
smoothing processes.
Existing algorithms are quite complex and provide a set of
smooth lines which are not especially connected and thus do not
always form a partition of the mesh; however, our algorithm
needs a partition into regions to construct the topology and
connectivity of the base control mesh. Hence, we deﬁne our
Fig. 2. Smoothing masks for Loop [24] subdivision rules. (a) Standard vertex. (b)
Sharp (or boundary) vertex. a and b represent, respectively, the weights associated
with a vertex and its neighbourhood in the smoothing operation.
Fig. 3. Example of semi-sharp edges. The control mesh is the cube drawn in
wireframe. Smooth edges are black and colour edges are semi-sharp. From left to
right: Sharpness degree: 0 (black), 1 (blue), 2 (green) and inﬁnite (red).
3


--- Page 5 ---

feature lines as the boundaries of a set of regions Ri issued from an
appropriate segmentation of the mesh.
4.1. Segmentation
The goal is to obtain a partition of the mesh, such as the
corresponding boundaries represent a coherent set of feature lines
which aligns to geometric salient parts and does not contain too
many unnecessary lines. Hence, the segmentation has to create a
partition such as each region Ri bears a geometrical meaning.
The VSA algorithm from Cohen-Steiner et al. [27] complies well
with our requirements: it is fully automatic and decomposes
the mesh into a set of regions, as ﬂat as possible, aligning with
the geometric structure of the input mesh and capturing its
anisotropy. The VSA is a global optimization method, starting from
initial seeds and updating the shape of the regions at each
iteration. Of course, according to the initial seed positions the
algorithm can fall to a local minimum. Hence we adopt an
incremental technique, quite similar to the farthest-point initiali-
zation proposed by the authors: we add one region at a time,
perform a partitioning (with a ﬁxed number of iterations), and
then add a new region at the maximum error position. Fig. 4 (left)
illustrates the segmentation of the RockerArm model into 45
regions.
Regarding to our feature line extraction objective, the standard
VSA algorithm is not completely satisfactory, since it produces
only approximately ﬂat regions. For instance the cylindrical part
at the centre of the object in Fig. 4 (left) is cut into several regions
whereas we would rather require one single region, according to
our feature line deﬁnition (local extrema of principal curvatures
along corresponding principal directions). Moreover, each region
is further approximated with a subdivision surface constructed by
linking its boundary control points; this kind of subdivision
surface is able to represent not only ﬂat regions but also
anisotropic parts (i.e. surfaces with clearly notable curvature
direction, like elliptic or parabolic parts for instance).
These reasons have led us to process a region merging after the
VSA, according to anisotropy similarity: we merge two regions Ri
and Rj if they share the same curvature value and the same
minimum curvature direction. We simply process an anisotropy
similarity score (ASC) between two regions Ri and Rj:
ASCðRi; RjÞ ¼ kdi
min  dj
mink  jci
max  cj
maxj
(1)
di
min and ci
max are, respectively, the minimum curvature direction
and maximum curvature value of the ith region. These values are
calculated by averaging vertex values over the region. For each
vertex, the curvature tensor has been calculated using the normal
cycle algorithm [32] and the principal curvature values and
directions have been extracted; they correspond, respectively, to
the eigenvalues and eigenvectors of the curvature tensor.
We then construct a region adjacency graph, and merge
iteratively pairs of regions associated with smallest scores. The
merging operation stops when a ﬁxed number of regions (chosen
by the user) is obtained or when a minimum score threshold is
reached; in practice such threshold is hard to ﬁnd a priori but can
be learned from training data for instance. Fig. 4 (right) illustrates
the partition after 10 merging operations processed on the
segmented model on the left. Regions associated with a similar
anisotropy have been correctly merged.
4.2. Feature line smoothing
Our feature line network is represented by the network of
boundaries between regions of the partition. This network is
composed with sets of connected pieces of boundary (polygonal
curves) separated by anchor vertices (vertices adjacent to at least
three regions). These paths between regions are quite jagged since
the connectivity of a scanned model, for instance, does not exactly
follow the natural features of the object, on the contrary to hand-
designed CAD parts. Our further process needs smooth feature
lines, particularly for the curve approximation step. Thus we
process a Laplace smoothing of the polygonal curves representing
each piece of boundary (anchor vertices do not move). The main
disadvantage of Laplace curve smoothing is the shrinkage effect
that deviates the boundary polyline from the surface; fortunately
it is not a problem if the curves do not lie precisely on the
mesh since our goal is not to produce a perfect ﬁtting but rather
to have a correct approximation associated with a very coarse
control mesh. Moreover, the further global optimization process
Fig. 5. A part of the segmented Blade mesh (15K vertices, 45 regions) (left), jagged feature line network (middle), smoothed feature line network (right).
Fig. 4. Segmentation of the RockerArm model. Results of the VSA (45 regions) (left).
Results after merging (35 regions) (right).
4


--- Page 6 ---

(see Section 6.2) will reduce this shrinkage resilient error. Fig. 5
shows the segmented Blade object (left), the jagged boundary
network (centre) and the feature line network after smoothing
(right).
5. Sharp control mesh construction
Our goal is to create the coarsest possible base control mesh,
while respecting the shape and the main features of the input
object. We consider that a correct approximating subdivision
surface should respect/approximate the main feature lines of the
object, and thus has to contain at least the number of control
vertices necessary to approximate these feature lines. Hence our
process is the following: we ﬁrst approximate the feature line
network with subdivision curves and then edges and facets of the
base control mesh are created by linking only their feature control
points. With this process we create a near optimal control mesh
since only necessary vertices are created, moreover, we link them
with respect to the lines of curvature of the object.
5.1. Feature line approximation
To create the set of feature control points, we approximate the
network of feature lines with subdivision curves. Each smooth
line between two anchor points is approximated separately (see
Fig. 6), then we obtain a control polygon network.
We use the approximation algorithm from Lavoue´ et al. [28]:
given a smooth polyline and a maximum error value, this
algorithm creates an approximating subdivision curve with a
minimal number of control points.
5.2. Local control mesh construction
At this point, we process each region separately: for each
segmented region (see Fig. 7, top left), the closed cycle B of control
polygons corresponding to its boundary is extracted from the
network (see plain lines in Fig. 7, bottom left). Our task is to form
a local control mesh P whose boundary is exactly B and without
any additional control points (see Fig. 7, bottom right), thus we
construct P by creating control edges (and thus facets) linking
vertices of B (see dotted lines in Fig. 7, bottom left).
Each region has been extracted due to planarity or anisotropy
similarity criteria, thus there exist basically two classes of regions:
planar or parabolic (i.e. anisotropic). For the planar case our
objective is to create facets with correct proportions; thus we
chose control edges associated with the smallest lengths similarly
to the lofting algorithms from Nasri [20,21]. For the parabolic case
we create edges with respect to the anisotropy and therefore
edges coherent with the minimum curvature directions of the
region (see Fig. 7, top right). In order to take into account these
two cases, a score S is processed for each potential control edge E.
Edge score deﬁnition: The mechanism is illustrated by Fig. 8: for
each potential edge E, we consider its vertices Pi, Pj and the
projections ˜Pi, ˜Pj of their respective limit positions on the patch
boundary. Then the pseudo geodesic path between these limit
Fig. 6. Construction of the control polygon network. From left to right: results of the segmentation, the jagged boundary between two anchor points, result after smoothing,
subdivision curve approximation (ﬁve control points), the complete control polygon network.
Fig. 7. Local control mesh construction. Top row: A region from the RockerArm
object and its directions of minimum curvature. Bottom left: The closed cycle of
control polygons corresponding to its boundaries and the created edges (dotted
segments). Bottom right: The corresponding local control mesh.
5


--- Page 7 ---

positions is calculated by applying the Dijkstra algorithm on the
vertices of the target region (see Fig. 8, right); this greedy
algorithm [33] computes the shortest path between two points of
a graph. Finally, the curvature tensors of the n vertices Vi of this
path are extracted, and particularly the minimum curvature
directions. The score SðEÞ is then deﬁned as follows:
SðEÞ ¼
Pn
i¼1y mini
n

X
n1
i¼1
kViþ1  Vik
(2)
with y mini the angle between the minimum curvature direction
of the vertex Vi and the segment ˜Pi ˜Pj. The ﬁrst term favours edges
coherent with minimum curvature lines and the second term
prevents incoherent edges while favouring short ones.
Connectivity construction: Once scores S have been associated
to every potential edges between all the pairs of boundary control
points from B, the algorithm constructs the potential edge
associated with the smallest score (dotted segment associated
with number 1 in Fig. 7), and the contour is cut along this
edge, creating two sub-contours. This algorithm is repeated
recursively on sub-contours until it remains only plane contours
(the corresponding control points all lie in the same plane). The
planarity is determined by a threshold on the dot products of the
contour segments with its average normal. Then for each plane
contour, we check its convexity, if it is convex, we create a facet,
and if not, we decompose it into convex parts, using the algorithm
from Hertel and Mehlhorn [34]. Our algorithm is also applicable to
regions with holes and thus associated with several cycles of
control polygons. The solution for such cases was proposed and
detailed in our previous algorithm [1]: a single oriented contour
including every boundary control polygons is constructed, linking
them by creating edges and doubling some control points.
5.3. Local control mesh assembly
Once local control meshes corresponding to every regions
have been constructed, they are glued together to form a global
control mesh. Boundary edges are tagged as sharp, to insure that
boundary constrains are respected between patches. Moreover,
this insures that boundary edges match the feature lines after
subdivision.
The obtained subdivision surface is piecewise smooth and
gives a quite good approximation of the object (see Fig. 9),
without any global optimization process. At this point the main
drawback is that we introduce sharp edges in the resulting
subdivision surface (at the boundaries between patches) which
can produce
unpleasant
discontinuities
for
smooth
objects.
However, such piecewise smooth reconstruction of scanned
mechanical parts can be required for CAD applications.
Fig. 8. Mechanism for edge score deﬁnition.
Fig. 9. Sharp control mesh examples. Top row: The RockerArm model (15K vertices), the associated control mesh (160 vertices) with sharp edges in red and the limit
surface. Bottom row: The Blade model (15K vertices), the associated control mesh (187 vertices) and the limit surface.
6


--- Page 8 ---

We have to notice that this framework focuses on obtaining the
coarsest possible control mesh and thus our objective is not to
obtain nice shaped quadrangles or regular connectivity (in terms
of vertex valence). Hence the created control mesh can possibly
have high degree facets or high valence vertices. However, in our
experiments, this has not perturbed the results nor induced
visible artefact.
6. Semi-sharp control mesh construction
The ﬁrst part of our algorithm (see previous section) produces
a piecewise smooth approximating subdivision surface, associated
with a quite coarse control polyhedron. We now have to optimize
this surface: ﬁrstly the sharpness of control edges is relaxed in
order to reproduce the curved aspect of the target object and then
a global geometry optimization displaces the control points to
minimize an asymmetric quadratic error.
6.1. Sharpness relaxation
Since most of the natural 3D objects, especially scanned
datasets, are rather smooth, the sharp creases introduced by the
ﬁrst step of our process have to be relaxed. In order to reproduce
more precisely the shape aspect of the target mesh while keeping
the same control point number, we associate to each sharp
edge, an integer sharpness degree from 0 (smooth) to 3 (sharp)
according to the rules introduced by DeRose et al. [26] (see
Section 2.4). We have considered an integer sharpness degree
instead of a real value by reason of simplicity, indeed this
mechanism has thus easily been integrated to the further
geometric optimization (see next section). Moreover, the max-
imum sharpness degree has been limited to 3 in order to speed-up
the process, knowing that this value is sufﬁcient to represent a
visually sharp feature.
For each inﬁnite sharp edge, introduced by the previous
process, we determine the appropriate degree so as to reproduce
the curvature radius of the corresponding ﬁllet or blend on
the target mesh. Fig. 10 illustrates this process: In order to
automatically determine the sharpness degree ShðEiÞ of the
control edge Ei, we associate this edge with different values from
0 to 3. We then compare the curvature radii of the different
resulting surfaces (the three pictures at bottom right) with the
target object (the picture at bottom left). Finally, we choose the
degree that produces the most similar curvature radius (ShðEiÞ ¼ 2
for the example).
Practically, starting from the control polyhedron P1 containing
inﬁnite sharp edges, we create four copies P0, P1, P2 and P3 where
sharp edges are all associated with a degree, respectively, equal to
0,1, 2 and 3. These control meshes are then subdivided (three
iterations) to produce dense meshes Ps
0, Ps
1, Ps
2 and Ps
3. For each
sharp control edge Ei, we choose the appropriate degree by the
following process:
 For each mesh Ps
j, we extract the vertices issued from
subdivisions of Ei and we compute the mean CjðEiÞ of their
maximum curvature values.
 Ei is associated with a boundary between two regions issued
from the segmentation process (see Section 4.1); thus we
extract from the original mesh the vertices from this boundary
Fig. 10. Mechanism for sharpness degree determination.
Fig. 11. Evolution of the root mean square error EL2, along with the number of
optimization iterations, for the RockerArm approximating surface.
7


--- Page 9 ---

and compute the mean COrigðEiÞ of their maximum curvature
values.
 We choose the sharpness degree j such as CjðEiÞ is the most
similar to COrigðEiÞ.
ShðEiÞ ¼ argminjðjCjðEiÞ  COrigðEiÞjÞ
(3)
6.2. Geometry optimization
Relaxing the sharpness of the control edges induces a shrinking
of the limit surface, and thus the control points have to be moved
in order to match correctly the target object. For this task, a global
geometry optimization is conducted, which relocates iteratively
the control points by minimizing a sum of quadratic distances to
the target surface.
(1) Several sample points Sk are chosen on the subdivision
surface,
they
correspond
to
vertices
of
the
subdivided
polyhedron at a ﬁner level l0. The associated footpoints
(projections of the sample points on the target surface)
are extracted. Sample points Sk can be computed as linear
combinations
of
the
initial
control
points
P0
i
(see
the
subdivision rules presented in Section 2.4 and Fig. 2); they
correspond to control points Pl0
i at the ﬁner level l0.
Sk ¼ CkðP0
1; P0
2; . . . ; P0
nÞ
(4)
(2) The functions Ck are determined using iterative multiplica-
tions
of
the
subdivision
matrices
associated
with
our
subdivision rules including semi-sharpness processing (see
Section 2.4 and Fig. 2 for the sharp subdivision rules).
(3) For all Sk, we express the squared distance Fk
d to the target
surface using the quadratic distance approximants deﬁned by
Pottmann and Leopoldseder [35]. The minimization of their
sum F gives the new positions of the control points P0
i .
F ¼
X
k
Fk
dðSkÞ ¼
X
k
Fk
dðCkðP0
1; P0
2; . . . ; P0
nÞÞ
(5)
The minimization of this quadratic function leads to the
solution of a linear squared system.
The point to surface quadratic distance approximants from
Pottmann and Leopoldseder [35] (recently used for subdivision
surface ﬁtting by Marinov and Kobbelt [4] and Cheng et al. [7])
Fig. 12. Left column: The Hand model. Middle column: Control mesh before
geometry optimization (top) and the limit surface (bottom). Right column: The
output control mesh after geometry optimization (three iterations) and the limit
surface.
Fig. 14. The RockerArm model, output control mesh, limit surface and its distance map to the original shape.
Fig. 13. Segmentation, output control mesh and limit surface for the Blade model.
8


--- Page 10 ---

leads to a much faster convergence than traditional point to point
distance (used by Ma et al. [3], for instance).
Steps (1)–(3) are repeated for three iterations since it has
proven to be sufﬁcient to obtain good visual results and a good
trade-off between processing time and quality. Fig. 11 illustrates,
for the RockerArm model, the evolution of the root mean square
error EL2 of the approximating surface, along with the number
of iterations; the convergence is very fast and the precision gain is
marginal after three iterations. Depending on the case, the
convergence may be not completely reached, but our objective
is not to focus on a very punctilious approximation. Concerning
the choice of the number of sample points Sk, we have chosen
l0 ¼ 2 reﬁnements for all examples in this article. As for each
reﬁnement, the number of vertices increases by a factor of at least
four, the number of equations is about sixteen times the number
of unknowns. That ensures a stable solution when solving Eq. (5)
in the least squares sense. Fig. 12 illustrates a result of the
optimization algorithm (three iterations).
7. Experiments and results
We have tested our algorithm on several models from different
natures: scanned mechanical models (see Figs. 13 and 14),
scanned organic data (see Fig. 12) and hand-designed model with
sharp edges (see Fig. 15). Table 1 presents some statistics about
our algorithm.
Properties of the approximating subdivision surfaces: The algo-
rithm provides extremely coarse subdivision control meshes
(less than 200 vertices for the presented examples) even for
complex shapes. Moreover, corresponding limit surfaces present a
very satisfying visual similarity with original objects. Thank to
the semi-sharpness optimization, limit surfaces well reproduce
the curved aspect of the original shapes. This is particularly visible
on the scanned mechanical pieces which present rolling ball
blends with a large variety of curvature radii.
The approximation errors: Mean (EL1) and root mean squared
(EL2) approximation errors are quite small, while maximal errors
(EL1) are larger. This is due to our approximation mechanism:
since we want to obtain very coarse control meshes, it sometimes
lacks degrees of freedom to well approximate some parts of the
object, particularly tiny details. The distance map from the limit
surface to the original shape presented in Fig. 14 well illustrates
this repartition of the error in some localized areas (red parts).
A solution to this drawback could be
to conduct a
local
enrichment of the mesh in such high error parts, or to consider
a ﬁner segmentation. However, our objective is not to focus on
approximation error but rather to preserve the main features of
the object.
Processing time: Processing times are illustrated in the last
three columns of Table 1. The whole ﬁrst part of the algorithm
(feature line extraction and sharp control mesh construction,
see Sections 4 and 5) takes about 30 s for a mesh with 30K faces
(on a 2 GHz XEON bi-processor). The sharpness relaxation is also
quite fast (less than 10 s for the examples); however, the geometry
optimization (see Section 6.2) can take several minutes, particu-
larly because of the multiplications of the large subdivision
matrices. We could consider a local process to optimize the
positions of vertices after the sharpness relaxation, in particular
some rules could be found that calculate the shrinking due to the
sharpness change.
Applications: Applications of our ﬁtting process are numerous:
even if the approximation is not highly precise, such coarse
control meshes may represent a good start point for reverse
engineering of scanned mechanical parts. In particular such
control polyhedron may facilitate B-Spline surface retrieval;
indeed several methods build a network of B-Spline patches
starting from a subdivision control polyhedron [36,37]. The semi-
sharp subdivision representation is particularly used for character
animation, in the context of 3D movies [26]. Our algorithm can
retrieve the semi-sharp control mesh from a scanned humanoid
or other organic model. Applying animation parameters to this
coarse control mesh is far easier than animating the original dense
mesh. Finally, the approximating subdivision surface can be
considered for compression, indeed the control mesh is extremely
compact in terms of amount of data and leads to a quite satisfying
approximation after subdivisions. For instance the encoding of the
RockerArm object with the compression algorithm from Touma
and Gotsman [38] (12 bits precision) gives a 35 kbytes binary
stream; the encoding of the associated control mesh (see Fig. 14)
with 12 bits quantization associated with prediction for geometry,
the Faceﬁxer algorithm [39] for connectivity and 2 bits per edge
for the sharpness degrees gives about 900 bytes. This kind of lossy
high rate compression is particularly adapted for transmission on
low bandwidth channel, moreover, the properties of subdivision
surface allow to display the object to the desired resolution
according to the terminal capacity for instance.
Comparison with other algorithms: We have compared our
results with two algorithms: (1) simpliﬁcation then geometric
optimization, a basic scheme followed by many authors [2–4] and
(2) the approach from Kanai [5]. Table 2 and Fig. 16 illustrate the
results. For both algorithms (1) and (2), we have created
subdivision surfaces associated with 200 control points against
only 160 for our method; thus the three models are associated
with approximatively the same data size (our model contains less
vertices but the supplementary sharpness information to encode).
Table 1
Statistics of our subdivision surface ﬁtting algorithm for various 3D models.
NbReg
F=V Orig
F=V Ctrl
EL1
EL2
EL1
Ctrl (4–5)
Relax (6.1)
Optim (6.2)
Blade
45
30K/15K
260/187
1:25
2:45
25:4
00:32
00:09
04:36
RockerArm
45
30.2K/15.1K
253/160
1:19
1:60
8:0
00:25
00:08
02:59
Hand
20
5K/2.5K
200/119
2:13
3:71
23:3
00:07
00:06
00:45
Cup
25
11.3K/5.7K
215/123
1:93
2:64
26:1
00:19
00:06
02:05
Number of regions from the segmentation, face/vertex number from original surface and control mesh. Approximation error (103), objects are normalized in a unit cube.
Processing times (min:s) of sharp control mesh construction, sharpness relaxation and geometry optimization.
Fig. 15. The Cup model, output control mesh and limit surface.
9


--- Page 11 ---

In terms of geometric approximation errors, our method
provides better results than both others, for EL1, EL2 and EL1. For
instance, the root mean square error (EL2) is 1:60  103 for our
algorithm against 2:20  103 for the simpliﬁcation-based ap-
proach and 5:63  103 for the Kanai algorithm [5]. The Hausdorff
distance (EL1) is also much better with our approximation
(8:0  103 against respectively 14:7  103 and 31:1  103).
Regarding the visual quality of the approximating shape (see
Fig. 16), the subdivision surface associated with our semi-sharp
control mesh appears once again much better than others. In
particular, the centre cylindrical part is very similar to the original
one in comparison with both other algorithms; the extremities
of the model are also very nicely approximated and present no
visible artifact, contrarily to both other methods. Some tiny details
are nevertheless missing at the bottom of the model presented in
the second row, by reason of the high coarseness of the control
mesh.
Fig. 17 illustrates the control meshes corresponding to the
approximation; compared with the other methods, the control
edges produced by our algorithm well follow the main lines of the
Fig. 16. Comparison of the approximation results of the RockerArm object for different algorithms.
Fig. 17. Comparison of the control meshes corresponding to the approximation of the RockerArm object for different algorithms.
Table 2
Face/vertex numbers from control mesh (F=V Ctrl) and approximation error
(103Þ for different algorithms, for the RockerArm model.
F=V Ctrl
EL1
EL2
EL1
Simplif-optim
400/200
1:39
2:20
14:7
Kanai [5]
400/200
4:07
5:63
31:1
Our method
253/160
1:19
1:60
8:0
10


--- Page 12 ---

shape. Moreover, for both other algorithms, the control meshes
present thin and elongated triangles, and also some degenerated
cases like faces crossing themselves, or ﬂipping. These phenom-
ena are mainly caused by the geometric optimization steps which
can produce unstable results. In our case, the base mesh (before
optimization) is constrained by the feature lines of the object and
is very close to the ﬁnal result, hence the geometry optimization
does not introduce such artifacts.
8. Conclusion
We have presented an original subdivision surface ﬁtting
algorithm
based
on
feature
line
approximation,
anisotropy
analysis for connectivity construction and edge sharpness relaxa-
tion. These mechanisms yield to a subdivision surface associated
with a very coarse control polyhedron and respecting the visual
aspect and the relevant features of the object. This approximating
surface is quite pertinent regarding to many applications: reverse
engineering, animation or compression.
In the case of noised 3D objects we obtain of course a
smoothed approximation, indeed our objective is not to represent
accurately each detail because the size of the control mesh will
blow up. However, it could be interesting to associate this smooth
approximation to a multi-resolution bump map or vector ﬁeld. We
also plan to drastically reduce the computing time by suppressing
the global geometry optimization; a solution could be to evaluate
quantitatively the shrinking (direction and strength) induced by
the sharpness relaxation and to displace the vertices accordingly.
References
[1] Lavoue´ G, Dupont F, Baskurt A. A framework for quad/triangle subdivision
surface ﬁtting: application to mechanical objects. Computer Graphics Forum
2007;26(1):1–14.
[2] Lee A, Moreton H, Hoppe H. Displaced subdivision surfaces. In: ACM
SIGGRAPH; 2000. p. 85–94.
[3] Ma W, Ma X, Tso S-K, Pan Z. A direct approach for subdivision surface ﬁtting
from a dense triangle mesh. Computer Aided Design 2004;36(16):525–36.
[4] Marinov M, Kobbelt L. Optimization methods for scattered data approxima-
tion with subdivision surfaces. Journal of Graphical Models 2005;67(5):
452–73.
[5] Kanai T. Meshtoss-converting subdivision surfaces from dense meshes. In:
6th international workshop on vision, modeling and visualization; 2001.
p. 325–32.
[6] Marinov M, Kobbelt L. Automatic generation of structure-preserving multi-
resolution models. Computer Graphics Forum 2005;24(3):479–86.
[7] Cheng K-S-D, Wang W, Qin H, Wong K-Y-K, Yang H-P, Liu Y. Fitting subdivision
surfaces to unorganized point data using SDM. In: IEEE Paciﬁc graphics; 2004.
p. 16–24.
[8] Susuki H. Subdivision surface ﬁtting to a range of points. In: IEEE Paciﬁc
graphics; 1999. p. 158–67.
[9] Li W-C, Ray N, Le´vy B. Automatic and interactive mesh to t-spline conversion.
In: Eurographics/ACM SIGGRAPH symposium on geometry processing; 2006.
[10] Hoppe H, DeRose T, Duchamp T, Halstead M, Jin H, McDonald J, et al.
Piecewise smooth surface reconstruction. In: ACM SIGGRAPH, 28; 1994.
p. 295–302.
[11] Litke N, Levin A, Schroder P. Fitting subdivision surfaces. In: IEEE Visualiza-
tion; 2001. p. 319–24.
[12] Alliez P, Cohen-Steiner D, Devillers O, Le´vy B, Desbrun M. Anisotropic
polygonal remeshing. ACM Transactions on Graphics 2003;22(3):485–93.
[13] Boier-Martin I, Rushmeier H, Jin J. Parameterization of triangle meshes over
quadrilateral domains. In: Eurographics/ACM SIGGRAPH symposium on
geometry processing; 2004. p. 193–203.
[14] Tong Y, Alliez P, Cohen-Steiner D, Desbrun M. Designing quadrangulations
with discrete harmonic forms. In: Eurographics/ACM SIGGRAPH symposium
on geometry processing; 2006. p. 201–10.
[15] Ray N, Li W-C, Le´vy B, Sheffer A, Alliez P. Periodic global parameterization.
ACM Transactions on Graphics 2006;25(4):1460–1485.
[16] Marinov M, Kobbelt L. A robust two-step procedure for quad-dominant
remeshing. Computer Graphics Forum 2006;25(3):537–46.
[17] Dong S, Bremer P-T, Garland M, Pascucci V, Hart JC. Spectral surface
quadrangulation. In: ACM SIGGRAPH; 2006. p. 1057–66.
[18] D’Azevedo EF. Are bilinear quadrilaterals better than linear triangles? SIAM
Journal On Scientiﬁc Computing 2000;22(1):198–217.
[19] Levin A. Interpolating nets of curves by smooth subdivision surfaces. In: ACM
SIGGRAPH; 1999. p. 57–64.
[20] Nasri A-H.
Interpolating
an
unlimited number
of
curves
meeting
at
extraordinary points on subdivision surfaces. Computer Graphics Forum
2003;22(1):87–98.
[21] Nasri A-H, Abbas A, Hasbini I. Skinning Catmull–Clark subdivision surfaces
with incompatible cross-sectional curves. In: IEEE Paciﬁc graphics; 2003.
p. 102–11.
[22] Schaefer S, Warren J, Zorin D. Lofting curve networks using subdivision
surfaces. In: Eurographics/ACM SIGGRAPH symposium on geometry proces-
sing; 2004. p. 103–14.
[23] Stam J, Loop C. Quad/triangle subdivision. Computer Graphics Forum
2003;22(1):79–85.
[24] C. Loop, Smooth subdivision surfaces based on triangles. Master’s thesis, Utah
University; 1987.
[25] Catmull E, Clark J. Recursively generated B-Spline surfaces on arbitrary
topological meshes. Computer-Aided Design 1978;10(6):350–5.
[26] DeRose T, Kass M, Truong T. Subdivision surfaces in character animation. In:
ACM SIGGRAPH. New York: ACM; 1998. p. 85–94.
[27] Cohen-Steiner D, Alliez P, Desbrun M. Variational shape approximation. In:
ACM SIGGRAPH; 2004. p. 905–14.
[28] Lavoue´ G, Dupont F, Baskurt A. A new subdivision based approach for
piecewise smooth approximation of 3d polygonal curves. Pattern Recognition
2005;38(8):1139–51.
[29] Lavoue´ G, Dupont F, Baskurt A. A new cad mesh segmentation method,
based on curvature tensor analysis. Computer-Aided Design 2005;37(10):
975–87.
[30] Ohtake Y, Belyaev A, Seidel H-P. Ridge-valley lines on meshes via implicit
surface ﬁtting. In: ACM SIGGRAPH; 2004. p. 609–12.
[31] Hildebrandt K, Polthier K, Wardetzky M. Smooth feature lines on surface
meshes. In: Eurographics/ACM SIGGRAPH symposium on geometry proces-
sing; 2005. p. 85–90.
[32] Cohen-Steiner D, Morvan J. Restricted Delaunay triangulations and normal
cycle. In: 19th annual ACM symposium on computational geometry; 2003.
[33] Dijkstra E-W. A note on two problems in connexion with graphs. In:
Numerische Mathematik; 1959. p. 269–71.
[34] Hertel S, Mehlhorn K. Fast triangulation of simple polygons. In: Proceedings
of the international FCT-conference on fundamentals of computation theory.
Lecture notes in computer science, vol. 158. Berlin: Springer; 1983. p. 207–18.
[35] Pottmann H, Leopoldseder S. A concept for parametric surface ﬁtting which
avoids the parametrization problem. Computer Aided Geometric Design
2003;20(6):343–62.
[36] Peters J. Biquartic c1-surface splines over irregular meshes. Computer-Aided
Design 1995;27(12):895–903.
[37] Peters J. Patching Catmull–Clark meshes. In: ACM SIGGRAPH; 2000. p. 255–8.
[38] Touma C, Gotsman C. Triangle mesh compression. In: Graphics Interface;
1998. p. 26–34.
[39] Isenburg M, Snoeyink J. Face ﬁxer: compressing polygon meshes with
properties. In: ACM SIGGRAPH; 2001. p. 263–70.
11
```

---

## Section 1.32: Shape-aware surface reconstruction from sparse 3D point-clouds

Source File: Shape-aware surface reconstruction from sparse 3D point-clouds.txt

### Paper Content:

```
# Shape-aware surface reconstruction from sparse 3D point-clouds.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Shape-aware surface reconstruction from sparse 3D point-clouds.pdf
# Output: ../layer2_completion/txt/Shape-aware surface reconstruction from sparse 3D point-clouds.txt


--- Page 1 ---

Shape-aware Surface Reconstruction from Sparse 3D Point-Clouds
Florian Bernard1,2,∗, Luis Salamanca2, Johan Thunberg2, Alexander Tack3, Dennis Jentsch3, Hans Lamecker3,4, Stefan
Zachow3,4, Frank Hertel1, Jorge Goncalves2, Peter Gemmar2,5
Abstract
The reconstruction of an object’s shape or surface from a set of 3D points plays an important role in medical image
analysis, e.g. in anatomy reconstruction from tomographic measurements or in the process of aligning intra-operative
navigation and preoperative planning data. In such scenarios, one usually has to deal with sparse data, which signiﬁcantly
aggravates the problem of reconstruction. However, medical applications often provide contextual information about the
3D point data that allow to incorporate prior knowledge about the shape that is to be reconstructed. To this end, we
propose the use of a statistical shape model (SSM) as a prior for surface reconstruction. The SSM is represented by a
point distribution model (PDM), which is associated with a surface mesh. Using the shape distribution that is modelled
by the PDM, we formulate the problem of surface reconstruction from a probabilistic perspective based on a Gaussian
Mixture Model (GMM). In order to do so, the given points are interpreted as samples of the GMM. By using mixture
components with anisotropic covariances that are “oriented” according to the surface normals at the PDM points, a
surface-based ﬁtting is accomplished. Estimating the parameters of the GMM in a maximum a posteriori manner yields
the reconstruction of the surface from the given data points. We compare our method to the extensively used Iterative
Closest Points method on several diﬀerent anatomical datasets/SSMs (brain, femur, tibia, hip, liver) and demonstrate
superior accuracy and robustness on sparse data.
Keywords:
Sparse shape reconstruction, statistical shape model, point distribution model, Gaussian mixture model,
expected conditional maximisation.
1. Introduction
The reconstruction of an object’s shape or surface from
a set of 3D points is a highly relevant problem in medi-
cal image analysis. It appears for example in image seg-
mentation, where images provide implicit information on
the location of anatomical structures via intensity levels,
which is frequently converted into geometric information
via some kind of feature extraction method. Another sce-
nario is computer-assisted surgery, where a pre-operative
therapy plan is transferred to the operating room by means
of a navigation system.
In contrast to mere 3D point-clouds that may repre-
sent virtually any object, image data of medical objects
yield additional contextual information that can be used
to adopt prior knowledge about the anatomical structures
to be reconstructed.
Heckel et al. (2011) make use of
the variational interpolation method (Turk and O’Brien,
1999), which essentially uses a general prior on the surface
smoothness. Going one step further, Pauly et al. (2005)
∗Corresponding author
1Centre Hospitalier de Luxembourg, Luxembourg
2Luxembourg Centre for Systems Biomedicine, University of Lux-
embourg, Esch-sur-Alzette, Luxembourg
3Zuse Institute Berlin (ZIB), Germany
41000shapes GmbH, Berlin, Germany
5Trier University of Applied Sciences, Trier, Germany
and Gal et al. (2007) have considered templates for 3D scan
completion that are matched to measurements. However,
these methods are limited since the available measure-
ments are assumed to be suﬃciently dense (Berger et al.,
2014).
To tackle this limitation, Bernard et al. (2015)
suggested the use of a statistical shape model (SSM) for
surface reconstruction.
Because the class of anatomical
structures is known for clinical routine tasks such as seg-
mentation, registration, or intra-operative navigation, it is
possible to use their shapes as geometric priors.
Our main contribution of this work is the introduction
of a surface-based SSM ﬁtting procedure in order to re-
construct a surface from a sparse 3D point-cloud.
Us-
ing a point distribution model (PDM) (Cootes and Tay-
lor, 1992), we incorporate a prior into our reconstruction
framework that captures the likely shape of the object
to be extracted.
In doing so, we reformulate the prob-
lem of surface reconstruction from a probabilistic per-
spective, embedding the prior distribution of the SSM
parameters that explain plausible shapes into the objec-
tive function. Our evaluation considers several diﬀerent
anatomical structures and SSMs (brain, femur, tibia, hip,
liver) and sparse data point scenarios, which may occur
in diﬀerent applications, ranging from interactive segmen-
tation to intra-operative registration for navigation. We
are able to show superior accuracy and robustness com-
pared to the extensively used Iterative Closest Point (ICP)
1


--- Page 2 ---

method (Besl and McKay, 1992). Rather than restrict-
ing ourselves to particular applications by investigating
application-speciﬁc aspects, our goal is to demonstrate the
general applicability of the proposed approach in order to
emphasize that the method may be useful in a wide range
of settings. The surface-based ﬁtting procedure is achieved
by the following methodological contributions:
• By extending existing point-set registration proce-
dures based on Gaussian Mixture Models (GMMs)
(Myronenko et al., 2007; Myronenko and Song,
2010; Horaud et al., 2011; Zheng, 2013) such that
anisotropic covariances are used in combination with
a PDM as transformation model, we obtain a shape-
aware surface reconstruction method that is superior
to ICP with respect to robustness and accuracy.
• Before, only spherical (isotropic) GMMs accounting
for a point-based matching have been used (Zheng,
2013; Bernard et al., 2015).
We now complement
these works by presenting a formulation that is based
on anisotropic GMMs that are “oriented” by the sur-
face normals, accounting for a surface-based ﬁtting.
• A
rigorous and self-contained derivation
of
the
surface-based ﬁtting method is presented, leading to
an Expected Conditional Maximisation (ECM) algo-
rithm (Meng and Rubin, 1993). ECM shares the same
convergence properties as the Expectation Maximisa-
tion (EM) method (Dempster et al., 1977) while being
more general.
• We develop a fast approximation of the ECM-based
ﬁtting method that has the same computational com-
plexity as the spherical GMM-based method. Numer-
ical simulations show that it is less prone to unwanted
local optima compared to the original ECM-based
method.
This article is organised as follows: section 2 summarises
previous research relevant to our methodology.
In sec-
tions 3 and 4, we introduce our notation and formally
state the considered surface reconstruction problem.
In
addition, we recapitulate the background of PDMs, prob-
abilistic point-set registration, and the Expectation Max-
imisation method. In section 5 we present our novel shape-
aware surface reconstruction method, including a time
complexity and convergence analysis. Section 6 comprises
experiments conducted using the proposed methods. In
section 7 we conclude this work.
2. Related Work
A plethora of methods for general surface reconstruc-
tion has been presented in the literature so far (see e.g.
Raya and Udupa (1990); Bolle and Vemuri (1991); Her-
man et al. (1992); Hoppe et al. (1992); Edelsbrunner and
M¨ucke (1994); Bajaj et al. (1995); Amenta et al. (1998);
Bernardini et al. (1999); Treece et al. (2000); Kazhdan
et al. (2006); Schroers et al. (2014)). Many of them are
summarised and described in the State-of-the-Art Report
(STAR) by Berger et al. (2014). In the remainder of the
present section, we will discuss only those surface recon-
struction methods that go beyond pure smoothness as-
sumptions and make use of more explicit shape priors.
For the completion of 2D shapes, Guo et al. (2012,
2013) incorporate templates from a shape database as (ge-
ometric) priors into a Bayesian framework. Similarly, a
database of 3D shapes is used by Pauly et al. (2005) for
completing 3D surface scans. For increased ﬂexibility com-
pared to static priors, Gal et al. (2007) use a context-
speciﬁc database of local shape priors, where the input
data is matched by (dynamically) combining multiple local
shape priors into a global prior. As pointed out by Berger
et al. (2014), both approaches described by Pauly et al.
(2005) and Gal et al. (2007) are limited by the assump-
tion that the point-clouds are assumed to be suﬃciently
dense.
A uniﬁed framework for repairing the geometry
and texture of meshes has been presented by Park et al.
(2006).
They employ context-based geometry ﬁlling for
ﬁlling holes in the surface, where available local patches
of the mesh are used to ﬁll its missing parts. The task
of obtaining high-resolution 3D meshes from low-quality
inputs is tackled by Shen et al. (2012) by dynamically as-
sembling object templates from a database of object parts.
The 3D shape completion methods discussed above have a
common focus on completing (mostly small) missing parts
of meshes obtained from 3D scans. However, our inter-
est lies primarily in methods that go beyond patching or
improving low-resolution input.
Blanz et al. (2004) have presented a closed-form solution
for SSM-based 3D surface reconstruction from a sparse set
of points, which relies, however, on the assumption that
such a set of points is already in correspondence to the
model. Albrecht et al. (2013) introduced posterior shape
models that have the objective to model the distribution of
a whole shape given only partial information. This method
assumes that the corresponding model points of the avail-
able partial data are known.
In their experiments this
issue is either solved manually or using the ICP method.
Similarly, for shape prediction from sparse observations,
Blanc and Sz´ekely (2012) use a variant of ICP that eval-
uates multiple initialisations. Anguelov et al. (2005) in-
troduced the shape completion and animation of people
(SCAPE) method, where one model for pose deformations
and one model for shape variations are learned separately.
The main objective of this method is the completion of
body shapes based on a small number of known positions
for some of the model points. Applied to bone models,
Rajamani et al. (2007) ﬁt an SSM to a small number
of anatomical landmarks that correspond to some of the
model points. Baka et al. (2010) ﬁt an SSM to sparse data
points that are in correspondence with the model, applied
to 2D heart datasets. By producing conﬁdence intervals as
output, their method is able to incorporate uncertainties
2


--- Page 3 ---

in the input data. Instead of using a trained SSM, Lu et al.
(2011) formulate a low-rank matrix recovery problem for
restoring missing parts of objects in archaeological stud-
ies. Considering that a set of (incomplete) objects of the
same class is available, and that correspondences between
common parts are known, their approach is based on the
assumption that all shapes are approximately linearly cor-
related. A shortcoming of the methods discussed so far is
that they all assume known correspondences. However, if
the objects do not exhibit a suﬃcient amount of distinct
features, the identiﬁcation of exact correspondences is very
diﬃcult or even infeasible in practice.
In the literature, there are several methods published
addressing this diﬃculty in (automatically) detecting the
correspondence between sparse data points and a model.
Due to its simplicity, the ICP algorithm, where correspon-
dences and transformations are estimated in an alternating
manner, is a very popular method for the registration of
two shapes represented as point sets. Numerous variants
of the originally-proposed method have since been devel-
oped, e.g.
by Rusinkiewicz and Levoy (2001); Granger
and Pennec (2002); Segal et al. (2009); Maier-Hein et al.
(2012); Bouaziz et al. (2013); Billings et al. (2015). For ex-
ample, Granger and Pennec (2002) propose the EM-ICP
algorithm where hidden variables are used to model un-
known correspondences in a surface registration problem.
Based on this work, Hufnagel et al. (2008) have proposed
a method for learning a PDM from unstructured point-
sets. For that, the authors establish probabilistic corre-
spondences ﬁrst, followed by the computation of the mean
shape and the modes of variability.
The surface recon-
struction method by Stoll et al. (2006) is able to deform a
given template to ﬁt point-cloud data. To do so, the user
deﬁnes initial correspondences between the template and a
set of points, which are then reﬁned iteratively in an ICP-
like manner. Along the lines of Stoll et al. (2006), for knee
surgery, Fleute and Lavall´ee (1998); Fleute et al. (1999)
have presented a methodology to ﬁnd pose and shape de-
formation parameters in order to ﬁt an SSM to very sparse
data points. Their approach resembles ICP due to the al-
ternating closest point estimation and pose/deformation
model parameter updates. Similarly, Chan et al. (2003) re-
construct 3D models of bones in orthopaedic surgery based
on a sparse set of points obtained from ultrasound. Here,
shapes are repeatedly instantiated using a PDM, where
each shape instance is used as input to ICP in order to
(rigidly) ﬁt the sparse points. This work has been extended
by Barratt et al. (2008), who use a PDM deﬁned on a reg-
ular grid instead of the shape surface. Zheng et al. (2006)
have proposed a three-stage procedure for sparse shape re-
construction in computer-assisted orthopaedic surgery. In
the ﬁrst stage, the pose of the sparse points is adapted us-
ing ICP so that they best ﬁt the mean shape. In the second
stage, the shape deformation parameters are estimated for
the given correspondences, which are eventually reﬁned
using a further deformation based on thin-plate splines.
Based on Gaussian Mixture Models with heteroscedas-
tic covariances, Zheng (2013) has proposed a method for
deformable shape registration. A promising approach of
ﬁtting a hand pose model to points that are suﬃciently
densely sampled from depth images is presented by Tay-
lor et al. (2014, 2016). The authors formulate a contin-
uous optimisation problem that solves simultaneously for
surface-based correspondences and model parameters us-
ing a nonlinear sum-of-squares objective.
In the work of Chang and Zwicker (2009), the registra-
tion of articulated shapes in two range scans is based on a
reduced deformation model deﬁned on a regular grid. In
this approach, the deformations are modelled by a convex
combination of rigid transformations, where the weights
are spatially varying. The registration is performed by al-
ternatingly updating the rigid transformations and their
weights, where closest point correspondences are recom-
puted after each step.
Despite the immense amount of literature on applica-
tions and variations of statistical shape models (for an
overview the interested reader is referred to the works
by Cootes and Taylor (1992); van Ginneken et al. (2002);
Heimann and Meinzer (2009)), there have been a limited
number of investigations into the use of SSMs for inter-
active segmentation. In the work of van Ginneken et al.
(2003), a user directly manipulates points of the PDM,
which has the disadvantage that the user needs to esti-
mate the (unknown) corresponding position of the con-
sidered model point in the image.
This is particularly
diﬃcult if the object does not exhibit distinct features.
In their slice-wise SSM-based segmentation of abdominal
aortic aneurysms, de Bruijne et al. (2004) initialise the
current slice’s PDM ﬁtting from the segmentation result
of the previous slice, with the option of manually correct-
ing segmentations on a per-slice basis.
Liu and Udupa
(2009) present Oriented Active Shape Models where the
semi-automatic live-wire technique is coupled with an Ac-
tive Shape Model.
Other authors present tools for the
posterior correction of model-based segmentations, such
as Timinger et al. (2003); Schwarz et al. (2008); Tan and
Acharya (2014). van den Hengel et al. (2007) present an in-
teractive procedure based on a set of rules and 2D sketches
for completing partial 3D models in structure from motion.
To summarise, existing approaches performing surface
reconstruction using SSMs either assume known corre-
spondences or estimate point-based correspondences in an
alternating manner. One exception is the work by Taylor
et al. (2016), where, for suﬃciently dense data, a surface-
based hand pose model ﬁtting is performed.
3. Background
In this section we introduce the notation and deﬁne
PDMs.
3.1. Notation
1p and 0p denote the p-dimensional column vectors con-
taining ones and zeros, respectively. Ip denotes the p × p
3


--- Page 4 ---

identity matrix and diag(x) is the p × p matrix with the
elements of the vector x ∈Rp on its diagonal. For a ma-
trix A, Ai,j is the (scalar) element in row i and column j.
The colon-notation is used to denote all rows or columns,
e.g. A:,j is the j-th column of A. For a matrix A, the
concatenation of all columns is given by vec(A).
By p(x) we denote the probability density function
(p.d.f.), or probability mass function in the discrete case,
where x can indicate both a random variable and a reali-
sation of it, depending on the context. N(x|µ, Σ) denotes
the Gaussian p.d.f. with mean µ and covariance matrix Σ.
3.2. Point Distribution Models
Statistical shape models based on PDMs (Cootes and
Taylor, 1992) are an established technique to capture non-
linear shape deformations of shapes in R3 from training
data by using a linear model in the higher-dimensional
shape space. Let {Xk : 1 ≤k ≤K} be the set of K train-
ing shapes, where each shape is represented by N points
(or vertices) in 3 dimensions given by the rows of the ma-
trix Xk ∈RN×3.
For processing multiple shapes in a
meaningful way, the N vertices of all K shapes have to
be in correspondence, i.e. the rows for all Xk are corre-
sponding to each other.
The PDM is obtained by ﬁnding an (aﬃne) subspace
close to the subspace spanned by the training shapes,
commonly performed by Principal Components Analysis
(PCA). For that, we deﬁne X = [x1, . . . , xK] ∈R3N×K,
with xk = vec(Xk) ∈R3N. This allows to compute the
modes of shape variability as the eigenvectors of the sam-
ple covariance matrix C =
1
K−1(X −¯x1T
K)(X −¯x1T
K)T ,
where ¯x = 1
K
PK
k=1 xk is the mean of all shapes in the 3N-
dimensional shape space. Let Φ ∈R3N×M be the matrix
of the ﬁrst M eigenvectors of C with largest eigenvalues.
For α being a variable in RM, the PDM y(α) : RM →R3N
is a vector-valued function deﬁned as
y(α) = ¯x + Φα ,
(1)
where α ∈RM is the shape deformation parameter. The
deformation of vertex i through α is denoted by
yi(α) = ¯xi + Φiα ,
(2)
where the three rows of vertex i are selected from ¯x and
Φ to obtain ¯xi ∈R3 and Φi ∈R3×M.
A common assumption is that α follows a zero-mean
Gaussian distribution, i.e. p(α) = N(α|0M, Λ), where Λ =
diag(λ1, . . . , λM) with λm being the m-th eigenvalue of C.
Thus, thanks to imposing a distribution upon α, we obtain
a distribution over shapes (Albrecht et al., 2013).
Usually, in addition to the PDM accounting for shape
deformations, a rigid transformation is employed in order
to account for the absolute pose of the shape y(α) with
respect to some reference (e.g. the image coordinate sys-
tem). Here we assume that the predominant part of the
pose has already been normalised. Minor pose variations
can be (approximately) captured implicitly in the PDM.
4. Problem Formulation
Given is a PDM that serves as prior for plausible shapes.
Also, we assume that (for ﬁxed α) the PDM points are
vertices of an oriented triangular surface mesh M. Addi-
tionally, we are given the set P = {pj : 1 ≤j ≤P} of
P surface points of the shape that is to be reconstructed,
where P is sparse in the sense that it only contains few
points pj lying on the object’s surface. The objective is
to ﬁnd the deformation parameter α such that y(α) “best
agrees” with the points in P.
4.1. A Generative Model
Our work is motivated by the widely used Coherent
Point Drift (CPD) approach for general point-set regis-
tration (Myronenko et al., 2007; Myronenko and Song,
2010).
It also resembles the approach by Zheng (2013)
for deformable shape registration, which however is based
on heteroscedastic GMMs with isotropic covariances, and
it is related to the EM-ICP algorithm (Granger and Pen-
nec, 2002) since we also make use of hidden variables to
model the unknown correspondences.
In the following we present the probabilistic model,
where a distribution is imposed over each vertex i by us-
ing a GMM. Given the set P and a PDM, we consider the
following assumptions:
1. For i = 1, . . . , N, each PDM vertex position yi(α) ∈
R3 is considered as the mean of a 3D Gaussian distri-
bution with covariance Σi.
2. Each point pj ∈P can be uniquely mapped to one
speciﬁc vertex index i, its generating component, from
whose distribution it is drawn.
As such, each point pj follows the distribution
p(pj|i, α, Σi) = N(pj|yi(α), Σi) ,
(3)
where all pj for j = 1, . . . , P are independent. The corre-
sponding graphical model is depicted in Fig. 1. Note that
all of the N Gaussian components are parametrised by α
and the covariances ˆΣ := {Σi}.
We assume that each
component is chosen with equal probability, i.e. p(i) = 1
N .
pj
↵
i
j=1,. . .,P
ˆ⌃
Figure 1: Graphical model of the generating process of P.
Our objective is to ﬁnd the parameters α and ˆΣ that
are most likely to have generated the points P. Since the
4


--- Page 5 ---

generating component ij of pj is unknown, the indices of
the generating components are treated as latent variables.
To incorporate this uncertainty, we consider a GMM for
the distribution of pj, leading to
p(pj|α, ˆΣ) =
N
X
i=1
p(i) p(pj|i, α, Σi)
(4)
= 1
N
N
X
i=1
N(pj|yi(α), Σi) .
(5)
Using Bayes’ theorem, one can derive the probability
that the i-th mixture component has generated the point
pj, given also α and ˆΣ, as
p(i|pj, α, ˆΣ) =
p(pj|i, α, ˆΣ)p(i)p(α, ˆΣ)
P
i′ p(pj|i′, α, ˆΣ)p(i′)p(α, ˆΣ)
(6)
=
p(pj|i, α, Σi)p(i)
P
i′ p(pj|i′, α, Σi′)p(i′)
(7)
=
N(pj|yi(α), Σi)
P
i′ N(pj|yi′(α), Σi′) .
(8)
4.2. Optimisation using EM
If the generating component ij of pj is unknown, ac-
cording to eq. (5) all points pj ∈P are independent and
identically distributed (i.i.d.). Thus, the log-likelihood as
a function of the model parameters θ := (α, ˆΣ) reads
L(θ) = ln p(P|θ) = ln
P
Y
j=1
N
X
i=1
p(i)p(pj|i, θ)
(9)
=
P
X
j=1
ln
N
X
i=1
p(i)p(pj|i, θ) .
The maximisation of L w.r.t. θ cannot be solved readily
due to the sum appearing inside the logarithm. Therefore,
a common approach is to employ an iterative method for
the maximisation. We denote the estimate of θ at iteration
n as θ(n) and rewrite eq. (9) as
L(θ, θ(n)) =
P
X
j=1
ln
N
X
i=1
p(i|pj, θ(n))p(i)p(pj|i, θ)
p(i|pj, θ(n))
.
(10)
Applying Jensen’s inequality (Jensen, 1906) leads to
L(θ, θ(n)) ≥
(11)
P
X
j=1
N
X
i=1
p(i|pj, θ(n)) ln p(i)p(pj|i, θ)
p(i|pj, θ(n))
=: Q(θ, θ(n)) .
As such, the right-hand side of eq. (11), which we denote
by Q(θ, θ(n)), is a lower bound for L(θ, θ(n)). Maximising
this lower bound is the key idea of the EM algorithm. In
the E-step, the probabilities p(i|pj, θ(n)) are evaluated for
ﬁxed θ(n) by using eq. (8).
Then, in the M-step, Q in
eq. (11) is maximised w.r.t. θ for the ﬁxed p(i|pj, θ(n))
computed before.
5. Methods
In section 5.1 below, we present the main novelty of
this paper, the anisotropic GMM-based ﬁtting approach
that employs covariance matrices that “oriented” accord-
ing to the surface normals at the PDM points. This al-
lows to move from a purely point-based matching (My-
ronenko et al., 2007; Myronenko and Song, 2010; Zheng,
2013; Bernard et al., 2015) to a more surface-based ﬁtting.
Subsequently, we describe a fast approximation of the
anisotropic GMM method. By using an extension to this
approximation, one can ensure that it is an instance of the
Generalised Expectation Maximisation (GEM) method
and thus the convergence is guaranteed.
5.1. Surface Reconstruction using an Anisotropic GMM
When using spherical covariances for each of the N
Gaussian components, a purely point-based ﬁtting is con-
ducted. However, in a vast amount of medical applications
of SSMs, the points of the PDM represent the vertices of
a surface mesh. This surface mesh is in general only an
approximation of a continuous surface. Whilst the sparse
points P are assumed to lie on this continuous surface,
in general they do not coincide with the PDM vertices.
Hence, matching the surface, depending on the PDM de-
formation parameter α, is more appropriate. A didactic
2D example is presented in Fig. 2.
5.1.1. Surface-aligned Covariance Matrices
We now formalise our surface-based ﬁtting method us-
ing a GMM with anisotropic covariance matrices. In the
GMM, the covariance matrix of each component i, i.e.
each vertex of the PDM, is deﬁned as
Σi(σ, α) := σ2Ci(α) .
(12)
The scalar parameter σ2 can be seen as a global scaling
factor, whereas the matrix Ci(α) models the anisotropy of
the surface structure locally by using a larger variance in
the directions of vectors lying in the tangent plane of the
PDM surface, compared to the variance along the PDM
normal direction (cf. Fig. 2 (c)).
Assuming that the surface mesh M of the underlying
shape of the PDM is given in the form of oriented triangles
(cf. section 4), with i2 and i3 we denote the index of the
“left” and “right” neighbour vertex of i, respectively. With
that, the surface normal at vertex i is given by
ni(α) =
(yi2(α) −yi(α)) × (yi3(α) −yi(α))
∥(yi2(α) −yi(α)) × (yi3(α) −yi(α))∥.
(13)
The matrix Ci is deﬁned as
Ci(α) := (1
η −1)ni(α)nT
i (α) + I3 ,
(14)
where the parameter η ≥1 weights the variances of vec-
tors along the normal direction compared to the tangen-
tial direction (note that we use the same value of η for
5


--- Page 6 ---

(a)
(b)
(c)
Figure 2: Anisotropic covariance matrices to achieve a surface-based ﬁtting. The sparse points P are shown in red, the PDM of a rectangle
is shown in green, where the green points deﬁne the PDM vertices (N = 12, M = 2, P = 12). The orientation of the covariance matrices is
shown as white ellipses. The objective is to deform the rectangle PDM such that it ﬁts the red points by adjusting α. The initialisation is
shown in (a). Since the red points are sampled between the PDM points (cf. shape approximation problem (Hill et al., 1995)), using spherical
covariance matrices results in a ﬁt that is even worse than the initialisation (b), whereas using anisotropic covariances results in a more
accurate ﬁt than the initialisation (c).
all points).
A motivation for eq. (14) can be found in
the work of Hill et al. (1995). For the covariance matrix
Σi(σ, α), the variance along the normal ni(α) is given by
σ2
η , and the variance in the direction of any vector in the
tangent plane is σ2. As such, for η = 1 one obtains an
isotropic GMM ﬁtting, in analogy to the CPD algorithm
(Myronenko et al., 2007; Myronenko and Song, 2010). We
refer to the isotropic method with η set to 1 as ISO. The
two main diﬀerences between CPD and our approach are
that in our case the transformations are parametrised by a
PDM, and that we also allow for anisotropic covariances.
Using transformations that are parametrised by a PDM
has also been done by Zheng (2013) for deformable shape
registration.
Choosing η > 1 achieves the desired be-
haviour of modelling a larger variance in the tangent plane.
Note that for η > 0, the matrix Ci(α) is symmetric and
positive deﬁnite with the spectrum { 1
η, 1, 1}. The inverse
of Ci, the precision matrix, is
Wi(α) := C−1
i (α) = (η −1)ni(α)nT
i (α) + I3 .
(15)
5.1.2. Maximum A Posteriori (MAP) Solution
We can cast the log-likelihood from eq. (9) into a
Bayesian view, leading to
Lposterior(θ) = ln[p(P|θ)p(θ)] ,
(16)
where additional knowledge in the form of the prior dis-
tribution p(θ) is incorporated.
By assuming p(α) =
N(α|0M, Λ) as described in section 3.2, and choosing a
uniform prior for p(σ), the prior distribution results in
p(θ) ∝p(α). The Q-function for the MAP solution reads
Q(α, σ, α(n), σ(n)) =
(17)
const −1
2αT Λ−1α −3P
2 ln σ2
−
1
2σ2
X
i,j
p(i|pj, α(n), σ(n))·
(pj −yi(α))T Wi(α)(pj −yi(α)) .
As already described, the E-step is solved by evaluating
eq. (8).
Then, the M-step comprises maximising Q in
eq. (17) w.r.t. α and σ. Due to the dependence of Wi
on α (for η ̸= 1), ﬁnding α that maximises Q does not
admit a simple closed-form solution. This is in contrast
to the isotropic case (η = 1), where α that maximises Q
can be found by solving a linear system of equations. In-
stead, α is now obtained using the BFGS quasi-Newton
method (Nocedal and Wright, 2006). The idea is to start
with the old value α(n), and then iteratively move along
directions that increase Q. Whilst the ordinary Newton
method requires the gradient of Q as well as its Hessian,
the BFGS quasi-Newton method uses an approximation of
the Hessian that is cheap to compute. A derivation of the
gradient ∇αQ of Q w.r.t. α can be found in Appendix A.
In order to obtain α, one option is to run the BFGS
quasi-Newton procedure until convergence, where one ob-
tains an α that (locally) maximises Q.
With that, the
updates of σ on α depend on each other and the pro-
cedure reverts to the ECM algorithm (Meng and Rubin,
1993). An alternative is to run only a single quasi-Newton
step in each M-step. With that, the obtained α is not a
local maximiser of Q; however, one still has the guarantee
that Q is non-decreasing. As such, this procedure reverts
to the GEM algorithm (Dempster et al., 1977).
Finally, the σ-update for ﬁxed α is given by
σ2 = 1
3P
X
i,j
p(i|pj, α(n), σ(n))·
(18)
(pj −yi(α))T Wi(α)(pj −yi(α)) .
The pseudocode of the anisotropic GMM ﬁtting procedure
is presented in Algorithm 1.
5.1.3. Fast Approximate Anisotropic GMM
Now we introduce an approximation of the α-update
that is a much faster alternative to the quasi-Newton
method.
The main idea is to use the previous value
α(n) instead of α for computing the anisotropic covari-
ance matrices Ci(α(n)) during the α-update in the M-
6


--- Page 7 ---

Input: ¯x, Φ, P, η, M
Output: α, σ2
Initialise: α = 0, σ2 =
1
3NP
P
i,j ∥pj −¯xi∥2, P ∈RP ×N,
y = ¯x + Φα
1 foreach i = 1, . . . , N do
// compute Wi
2
ni =
(yi2 −yi)×(yi3 −yi)
∥(yi2 −yi)×(yi3 −yi)∥
3
Wi = (η −1)ninT
i + I3
4 repeat
// E-step
5
foreach j = 1, . . . , P do
6
t = 0
7
foreach i = 1, . . . , N do
8
Pji = exp(−
1
2σ2 (pj −yi)T Wi(pj −yi))
9
t = t + Pji
10
Pj,: = 1
t Pj,:
// M-step
11
α = quasi-Newton(Q, ∇Q, α)
12
y = ¯x + Φα
13
foreach i = 1, . . . , N do
// compute Wi
14
ni =
(yi2 −yi)×(yi3 −yi)
∥(yi2 −yi)×(yi3 −yi)∥
15
Wi = (η −1)ninT
i + I3
16
σ2 =
1
3P
P
i,j Pji(pj −yi)T Wi(pj −yi)
17 until convergence
Algorithm 1: Pseudocode of the anisotropic GMM ﬁt-
ting method.
The notation “quasi-Newton(Q, ∇Q, α)”
denotes running the quasi-Newton method for maximis-
ing Q w.r.t. α, where ∇Q is its gradient and the third
argument is the initial value of α. If the GEM approach
is used, the quasi-Newton method is run only for a single
iteration. Note that the surface mesh M is used for the
normal computations.
step.
Our key assumption is that the PDM is well-
behaved in the sense that neighbouring vertices vary
smoothly during deformation; thus, locally the defor-
mation of an individual triangle is nearly a translation.
Since surface normals are invariant to translations it fol-
lows that ∥ni(α) −ni(α(n))∥is small, which implies that
∥Wi(α) −Wi(α(n))∥is also small.
The resulting Q-function using the proposed approxi-
mation is now given by
˜Q(α, σ, α(n), σ(n)) =
(19)
const −1
2αT Λ−1α −3P
2 ln σ2
−
1
2σ2
X
i,j
p(i|pj, α(n), σ(n))·
(pj −yi(α))T Wi(α(n))(pj −yi(α)) ,
where the diﬀerence to Q in eq. (17) is that the constant
Wi(α(n)) is now used in place of the function Wi(α). As
such, the α-update in the M-step is a quadratic concave
problem that can be maximised eﬃciently. The solution
for α is found by solving the linear system Aα = b, where
A ∈RM×M is given by
A = σ2Λ−1 +
X
i,j
p(i|pj, α(n), σ(n))ΦT
i Wi(α(n))Φi ,
(20)
and b ∈RM by
b =
X
i,j
p(i|pj, α(n), σ(n))ΦT
i Wi(α(n))(pj −¯xi) .
(21)
The pseudocode for this approximate method is similar to
Algorithm 1, except for line 11, where α is computed by
solving a linear system.
In order to guarantee that the approximate method con-
verges, it is necessary that in the M-step the value of
the exact Q in eq. (17) is non-decreasing, i.e.
the new
α = A−1b obtained using ˜Q must fulﬁl
Q(A−1b, σ(n), α(n), σ(n)) ≥Q(α(n), σ(n), α(n), σ(n)) .
(22)
For η = 1 the methods reverts to the isotropic case and
condition (22) vacuously holds. However, for η > 1 this
is not true in general. One way to ensure that Q is non-
decreasing is to evaluate the condition in eq. (22) in each
iteration, and, in the case of a violation, revert to one of
the quasi-Newton methods for the α-update. Speciﬁcally,
we consider a single quasi-Newton step for updating α. We
denote the approximate method without this convergence
check as ANISO, and the method with the convergence
check and the quasi-Newton step as fall-back as ANISOc.
In Fig. 3 we illustrate the behaviour of Q and compare
it with ˜Q for various choices of η. Note that for visualisa-
tion purposes we have chosen M = 2, whereas in higher-
dimensional cases the eﬀect of an increasing η on the non-
concavity can be expected to be more severe.
5.2. Performance Analysis
Table 1 summarises the computational complexity of the
presented methods. In Fig. 4 we plot the mean of the nor-
malised value of Q as a function of the processing time for
all four anisotropic ﬁtting methods. For each random run
we sample a shape instance by drawing α (cf. section 3.2),
select P points randomly from the mesh surface (cf. sec-
tion 6), and run the four methods. The obtained Q for
the four methods are then normalised such that in each
run the smallest Q corresponds to 0 and the largest Q
corresponds to 1 (normalisation w.r.t to all four methods
simultaneously). We have found that the single-step quasi-
Newton method (GEM) is faster compared to the full
quasi-Newton procedure (ECM). Moreover, compared to
both quasi-Newton methods, the approximate methods are
much faster. Since the ANISOc method makes use of ele-
ments both of the GEM and the ANISO method, the total
time complexity of the ANISOc method is the combined
time complexity for GEM and ANISO (cf. Table 1). Nev-
ertheless, in our simulations the ANISOc method comes
close to the ANISO method in terms of convergence speed.
This is because in the early stages of the iterative proce-
dure the ANISOc method satisﬁes condition (22) in most
cases. A violation of (22) happens more frequently in the
later stages. Since these results suggest that the ANISO
7


--- Page 8 ---

α1
α2
Q
η = 1
α1
α2
Q
η = 4
α1
α2
Q
η = 16
α1
α2
Q
η = 64
Figure 3: Illustration of the behaviour of Q for various η. The height and the colour of the surface both show the value of Q, eq. (17),
depending on α1 and α2. The red grid shows its concave approximation ˜Q as presented in eq. (19). The red dot denotes the value of Q at
α(n); at this position Q = ˜Q. The yellow dot indicates the maximum of Q. For the trivial case of η = 1 it can be seen that Q = ˜Q everywhere,
whereas an increasing η leads to a larger discrepancy between Q and ˜Q as well as to an “increased non-concavity” of Q.
method is faster and as good as the other methods, for our
experiments we use the ANISO method as representative
for the anisotropic ﬁtting methods.
6. Experiments
In this section we evaluate the proposed ﬁtting proce-
dures on ﬁve datasets with the parameters being shown in
Table 2. For the generation of the set of sparse points P,
we sample sparse points randomly on the shape surfaces.
To do so, we ﬁrst select a triangle from the surface mesh
with a probability proportional to its area. Then, we uni-
formly sample a point lying within the triangle according
to the procedure presented by Osada et al. (2002). More-
over, in order to evaluate how well our method is able to
cope with uncertainties in the given points, we considered
noisy versions of these points by adding spherical Gaus-
sian noise with covariance ˜σ2I3 to each point individually.
For each experiment we have taken the scale of the ob-
ject in the shape model into account for choosing ˜σ. The
considered values of ˜σ are presented in Table 2.
Table 2: Summary of parameters for the datasets. N is the number
of points in the PDM (“ds” denotes the downsampled PDM), K
the number of training shapes, M the number of modes of variation
for leave-all-in (LAI) and leave-one-out (LOO) experiments, η the
anisotropy parameter, and ˜σ the standard deviation of the noise
added to the points.
N
N (ds)
K
M (LAI)
M (LOO)
η
˜σ
brain
1792
371
17
16
96 (kPCA)
8
2 mm
femur
3800
759
60
59
58
4
5 mm
tibia
4701
814
60
59
58
4
5 mm
hip
5603
1120
48
47
46
4
5 mm
liver
4542
908
112
111
110
2
1 cm
In addition to the proposed probabilistic ﬁtting meth-
ods presented in this paper, we also evaluate two non-
probabilistic ICP approaches. The ﬁrst approach is the
regularised isotropic ICP method as outlined in Algo-
rithm 2, which we denote ICP in the experiments. The
second approach is an anisotropic version thereof, which
we denote AICP in the experiments. Similarly to Maier-
Hein et al. (2010), for the anisotropic ICP we compute
the nearest neighbours in the third line of Algorithm 2 by
Table 1: Computational complexity table. The complexity of the α-update for one iteration of the BFGS quasi-Newton methods is O(M2)
plus the complexity of the evaluation of Q and ∇Q (Nocedal and Wright, 2006) (we use n to denote the number of iterations of the quasi-
Newton method). The complexity of the α-update of the remaining methods comprises the computation of A and b, as well as solving a
linear system of equations of size M × M, for which we present the complexity O(M3) due to the matrix inversion involved. Note that in ⋆
we present the complexity for general Λ, for diagonal Λ the quadratic time complexity in M reduces to linear complexity.
anisotropic
isotropic
ECM
GEM
ANISOc
ANISO
ISO
update y
O(MN)
compute {Wi}
O(N)
-
E-step
O(NP )
α-update
evaluate Q, eq. (17)
O(MN + M 2 + NP )⋆
-
evaluate ∇Q, eq. (A.4)
O(M 2 + MNP )⋆
-
construct A
-
O(M 2N + NP )
construct b
-
O(MN + NP )
total α-update
O(n(M 2 + MNP ))
O(M 2 + MNP )
O(M 3 + MNP + M 2N)
O(M 3 + M 2N + NP )
σ-update
O(NP )
total
(per outer iteration)
O(n(M 2 + MNP ))
O(M 2 + MNP )
O(M 3 + MNP + M 2N)
O(M 3 + M 2N + NP )
8


--- Page 9 ---

ANISO
ANISOc
GEM
ECM
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 8
0 0.5 1 1.5 2
0.96
0.98
1
0
500
1,000
1,500
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 8
0
5
10
0.96
0.98
1
0
1,000
2,000
3,000
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 8
0
5
10
0.96
0.98
1
0
500
1,000
1,500
2,000
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 8
0
10
20
0.8
0.85
0.9
0.95
1
0
100
200
300
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 64
0
1,000
2,000
3,000
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 64
0
10
20
0.9
0.92
0.94
0.96
0.98
1
0
2,000
4,000
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 64
0
10
20
0.9
0.92
0.94
0.96
0.98
1
0
2,000
4,000
6,000
0
0.2
0.4
0.6
0.8
1
processing time [s]
proportion of best Q
⌘= 64
0
50
100
0.6
0.7
0.8
0.9
1
Figure 4: Proportion of best value of Q versus processing time averaged over 100 random runs for the four anisotropic methods for two choices
of η (in the rows). In each column a diﬀerent dataset has been used to produced the results, from left to right we show results produced by
the brain shapes dataset (N = 1792, M = 16, P = 30, cf. section 6.4), the femur dataset (N = 3800, M = 59, P = 30, cf. section 6.1), the
tibia dataset (N = 4071, M = 59, P = 30, cf. section 6.1), and the hip dataset (N = 5603, M = 47, P = 30, cf. section 6.3).
Input: ¯x, Φ, P, Λ
Output: α
Initialise: α = 0, p = vec([p1, . . . , pP ]) ∈R3P
1 repeat
2
y = ¯x + Φα
// Find nearest neighbours
3
N = ﬁndNearestNeighbourIndices(y, P)
// Solve linear system for α
4
A = ΦN ,:
5
b = p −¯xN
6
α = (AT A + Λ−1)−1AT b
// Tikhonov regularisation
7 until convergence
Algorithm 2: Pseudocode of the ICP baseline method.
The notation ΦN,: and ¯xN means selecting the appropri-
ate rows from Φ and ¯x according to the indices of the
nearest neighbours N.
taking the anisotropic covariance matrices (eq. (14)) into
account.
For both, ICP and AICP, the regulariser Λ cor-
responds to the covariance matrix of the PDM parameter
α (cf. section 3.2). Moreover, in our evaluation we com-
pare the ground truth data to the mean shape, i.e. in this
setting we do not run any ﬁtting procedure at all, which
amounts to setting α = 0.
The anisotropic method requires to set the parameter η
accounting for the amount of anisotropy. We have manu-
ally chosen the values of η for each dataset, as shown in
Table 2. We have empirically found that for an increasing
amount of uncertainty ˜σ in the sparse points, it is advan-
tageous to use a lower value of η.
We consider leave-all-in (LAI) and leave-one-out (LOO)
experiments. The LAI experiments measure the perfor-
mance of our method given a perfect model, whereas the
LOO experiments evaluate the generalisation ability to un-
seen data.
We use the Dice Similarity Coeﬃcient (DSC) as volu-
metric overlap measure, which is deﬁned as
DSC(Vx, Vy) = 2|Vx ∩Vy|
|Vx| + |Vy|
(23)
for the volumetric segmentations Vx and Vy. Additional
results considering surface-based measures for all datasets
are presented in the supplementary material.
6.1. Knee Bones: Femur and Tibia
For the femur and tibia datasets we assumed that the
pose has already been normalised and we worked directly
in the space of the SSM. In practice, this can for example
be tackled in a similar manner as by Seim et al. (2010),
who proposed an automated SSM-based knee bone seg-
mentation, where initially the model is positioned inside
the three dimensional CT or MR image via Generalised
Hough Transform (Ballard, 1981).
We present experiments for a PDM of the femur with
N = 3800 points (cf. Fig. 5 (a)) and a PDM of the tibia
with N = 4701 points (cf. Fig. 5 (c)).
Additionally,
we evaluated the ANISO method using the downsampled
PDMs, denoted ANISO-ds, where only a subset of the orig-
inal PDM vertices are used (cf. Fig. 5 (b) and Fig. 5 (d)).
Random sparse points are generated according to the pro-
cedure described in section 6, where for each training shape
10 instances of sparse points P are sampled. In Fig. 5 such
random instances of P are shown for the mean shapes of
both bones. Summaries of the results are shown in Fig. 6
for the femur and in Fig. 7 for the tibia.
It can be seen that for both bone PDMs if only P = 9
points are available, the ICP methods perform slightly
better than the ANISO method.
Once more points be-
come available, the ANISO method outperforms the ICP
method. Surprisingly, the ANISO-ds method, which uses
a downsampled PDM, outperforms the ANISO method for
P = 9. We assume that this is because the original PDMs,
comprising N = 3800 vertices for the femur and N = 4701
vertices for the tibia, contain ﬁne local details that lead to
an overﬁtting when reconstructing the surface from only
P = 9 points. In contrast, the downsampled PDM con-
tains less details that may impede the surface reconstruc-
9


--- Page 10 ---

(a)
(b)
(c)
(d)
Figure 5: Femur and tibia datasets.
(a) Femur mean shape with
N = 3800 vertices. (b) Downsampled mean shape with 759 vertices.
(c) Tibia mean shape with N = 4701 vertices. (d) Downsampled
mean shape with 814 vertices. For both bone models P = 36 sparse
points have been randomly drawn from the original surface according
to the procedure described in section 6.
tion. Moreover, the ANISO-ds method outperforms the
ISO-ds method, which conﬁrms our elaborations in Fig. 2
on real data. Moreover, for both bone PDMs, the AICP
method performs very similar to ICP.
6.2. Liver
We carried out LAI and LOO experiments using a liver
PDM with N = 4542 points (cf. Fig. 8 (a)). Figure 8 (b)
shows the downsampled PDM. For each training shape 10
instances of sparse points P are sampled. Summaries of
the results are shown in Fig. 9.
For both settings, LAI and LOO, without any noise, i.e.
˜σ = 0 cm, the anisotropic method outperforms both ICP
methods with respect to the mean DSC regardless of how
many random points have been sampled. Especially for
36 and 90 points the accuracy of the anisotropic method
becomes increasingly superior compared to ICP. Consid-
ICP (˜σ=0)
AICP (˜σ=0)
ANISO (˜σ=0)
ANISO-ds (˜σ=0)
ICP (˜σ=5)
mean shape
ANISO (˜σ=5)
ISO-ds (˜σ=0)
LAI
9
18
36
90
0.94
0.96
0.98
P
DSC (mean)
9
18
36
90
0.005
0.01
0.015
P
DSC (std)
9
18
36
90
0
10
20
P
runtime [s] (mean)
LOO
9
18
36
90
0.94
0.95
0.96
0.97
P
DSC (mean)
9
18
36
90
0.008
0.01
0.012
0.014
0.016
P
DSC (std)
9
18
36
90
0
5
10
15
P
runtime [s] (mean)
Figure 6: DSC and runtime for femur LAI and LOO results.
ICP (˜σ=0)
AICP (˜σ=0)
ANISO (˜σ=0)
ANISO-ds (˜σ=0)
ICP (˜σ=5)
mean shape
ANISO (˜σ=5)
ISO-ds (˜σ=0)
LAI
9
18
36
90
0.94
0.96
0.98
P
DSC (mean)
9
18
36
90
0.005
0.01
0.015
0.02
P
DSC (std)
9
18
36
90
0
5
10
15
20
P
runtime [s] (mean)
LOO
9
18
36
90
0.94
0.96
P
DSC (mean)
9
18
36
90
0.01
0.015
0.02
P
DSC (std)
9
18
36
90
0
10
20
P
runtime [s] (mean)
Figure 7: DSC and runtime for tibia LAI and LOO results.
10


--- Page 11 ---

(a)
(b)
Figure 8: Liver dataset. (a) Mean shape with N = 4542 vertices. (b)
Downsampled mean shape with 908 vertices. P = 36 sparse points
have been randomly drawn from the original surface according to the
procedure described in section 6.
ICP (˜σ=0)
AICP (˜σ=0)
ANISO (˜σ=0)
ANISO-ds (˜σ=0)
ICP (˜σ=1)
mean shape
ANISO (˜σ=1)
ISO-ds (˜σ=0)
LAI
9
18
36
90
0.8
0.85
0.9
0.95
P
DSC (mean)
9
18
36
90
0.02
0.04
P
DSC (std)
9
18
36
90
0
10
20
P
runtime [s] (mean)
LOO
9
18
36
90
0.8
0.85
0.9
P
DSC (mean)
9
18
36
90
0.02
0.03
0.04
0.05
P
DSC (std)
9
18
36
90
0
10
20
30
P
runtime [s] (mean)
Figure 9: DSC and runtime for liver LAI and LOO results.
ering the random points disturbed by Gaussian noise with
˜σ = 1 cm, the ICP method yields better results for 9 and
18 points. With noisy points, for the LAI and the LOO
experiments at least 36 points seem to be necessary for the
anisotropic method to achieve results similar to the ICP
methods with respect to the DSC. Similarly to the femur
and tibia cases, the AICP method is on par with the ICP
method.
6.3. Hip
We carried out LAI and LOO experiments using a hip
PDM with N = 5603 points (cf. Fig. 10 (a)). Figure 10 (b)
shows the downsampled PDM. For each training shape 10
instances of sparse points P are sampled. Summaries of
the results are shown in Fig. 11. For both settings, LAI
and LOO, without any Gaussian disturbance, i.e. ˜σ = 0
mm, the anisotropic method outperforms the ICP method
with respect to the mean Dice Similarity Coeﬃcient for
more than 9 points. Again, for the ANISO method the
accuracy is increasing with more sampled points.
Con-
sidering the random points disturbed by Gaussian noise
with ˜σ = 5mm, the ICP method yields better results for 9
points, whereas for 18 points or more the ANISO method
(a)
(b)
Figure 10: Hip dataset. (a) Mean shape with N = 5603 vertices. (b)
Downsampled mean shape with 1120 vertices. P = 36 sparse points
have been randomly drawn from the original surface according to the
procedure described in section 6.
11


--- Page 12 ---

performs better. Comparing the ICP and the AICP ap-
proach, both methods are similar with respect to the DSC.
ICP (˜σ=0)
AICP (˜σ=0)
ANISO (˜σ=0)
ANISO-ds (˜σ=0)
ICP (˜σ=5)
mean shape
ANISO (˜σ=5)
ISO-ds (˜σ=0)
LAI
9
18
36
90
0.7
0.8
0.9
P
DSC (mean)
9
18
36
90
0.02
0.04
0.06
P
DSC (std)
9
18
36
90
0
10
20
30
P
runtime [s] (mean)
LOO
9
18
36
90
0.75
0.8
0.85
0.9
P
DSC (mean)
9
18
36
90
0.02
0.04
0.06
P
DSC (std)
9
18
36
90
0
20
40
60
P
runtime [s] (mean)
Figure 11: DSC and runtime for hip LAI and LOO results.
6.4. Brain Structures
In this experiment we consider a multi-object PDM
that captures the inter-relation between multiple brain
structures, namely Substantia Nigra & Subthalamic Nu-
cleus (SN+STN, as compound object), Nucleus Ruber
(NR), Thalamus (Th) and Putamen & Globus Pallidus
(Put+GP, as compound object), where all structures are
considered bilaterally. The mean of the PDM is shown in
Fig. 12 (a).
The PDM is learned from multi-label segmentations
that are all represented in a common coordinate system,
the MNI ICBM 152 (Fonov et al., 2009) template space
in our case (more details on the manual annotation and
the establishment of correspondences can be found in our
previous work (Bernard et al., 2014, 2016b)). The align-
ment of the patient images into the MNI template space
is conducted using the rigid image registration method
FLIRT (Jenkinson and Smith, 2001).
Hence, thanks to
this alignment, the orientation and position are already
approximately normalised. Consequently, for a new pa-
tient image that is to be segmented, a registration to the
MNI template space is suﬃcient.
6.4.1. Interactive Segmentation
One interesting perspective of our presented shape-
aware surface reconstruction method might be its integra-
tion into an interactive segmentation setting. This could
be implemented by alternating between the user annotat-
ing object boundaries, and running our ﬁtting method in
order to reconstruct a surface from the user-input. As a
ﬁrst step into this direction, in addition to random sparse
points, for the brain structure dataset we also evaluate a
ﬁtting to partial contours. We decided to focus on partial
contours instead of full contours since for some of the struc-
tures some regions may be diﬃcult to delineate. In order
to perform this evaluation we synthetically generated con-
tours according to the procedure described in the supple-
mentary material. In Fig. 12 (c) we show the sparse points
that constitute these partial contours. The main idea of
the contour generation is as follows. First, we randomly
select a 2D slice of the binary 3D segmentation image for
a particular object. Next, from the 2D slice of the ground
truth segmentation a subcontour of the entire boundary is
randomly selected. Combining the chosen slice index with
the subcontour leads to a planar 3D contour.
Eventu-
ally, this contour is subsampled and the points are added
to P. Whilst our synthetic contour generation does not
substitute a proper study involving user-drawn contours,
we found that the resulting contours look plausible to be
drawn by a human operator. For the generation of the par-
tial contours we considered two settings, c1 and c2. For c1,
we have two contours in four of the eight brain structures,
as shown in Fig. 12 (c), where the number of points ranges
from 58 to 106, with a median of 80. For c2, we have a sin-
gle contour for each of the eight brain structures, where the
number of points ranges from 58 to 81, with a median of
68. Note that when considering partial contours, for each
pj ∈P we assume that it is known to which of the eight
brain structures it belongs, which is used to constrain the
E-step in our ﬁtting methods (and the nearest-neighbour
routine for the ICP methods).
In order to evaluate the robustness of our presented
method with respect to noisy inputs, we also created noisy
contours. For that, each partial contour is translated in
the image plane by a random vector that has a zero-mean
Gaussian distribution with covariance ˜σ2I2. Our motiva-
tion for using in-plane translations is that when the user
draws a contour in the image, the particular image plane
is ﬁxed and thus the only uncertainty occurs in-plane.
6.4.2. Results
For each of the K = 17 training shapes we sample 20
instances of sparse points P.
Following concepts intro-
duced by Cootes and Taylor (1995); Wang and Staib (1998,
2000), in the LOO experiments we increase the ﬂexibility
of the resulting shape model by extracting M = 96 eigen-
vectors of a modiﬁed covariance matrix. In our case, we
used the sum of the (scaled) covariance matrix C and a
Gaussian kernel with standard deviation of approximately
5mm. We refer the interested reader to our previous work
12


--- Page 13 ---

(a)
(b)
(c)
Figure 12: Brain shapes dataset. (a) Mean shape with N = 1792 vertices. (b) Downsampled mean shape with 371 vertices. In (a,b) P = 36
sparse points have been randomly drawn from the original surface according to the procedure described in section 6. (c) A shape instance
from the training set with partial contours.
for details (Bernard et al., 2016a), where the method is re-
ferred to as kPCA. We also demonstrated that this method
is able to improve the generalisation ability of the PDM
in this small training dataset comprising K = 17 shapes.
Summaries of the results are shown in Fig. 13.
ICP (˜σ=0)
AICP (˜σ=0)
ANISO (˜σ=0)
ANISO-ds (˜σ=0)
ICP (˜σ=2)
mean shape
ANISO (˜σ=2)
ISO-ds (˜σ=0)
LAI
9
18 36 90 c1
c2
0.4
0.6
0.8
1
P
DSC (mean)
9
18 36 90 c1
c2
0
0.1
0.2
P
DSC (std)
9 18 36 90 c1 c2
0
2
4
6
P
runtime [s] (mean)
LOO
9
18 36 90 c1
c2
0.4
0.5
0.6
0.7
0.8
P
DSC (mean)
9
18 36 90 c1
c2
0.05
0.1
0.15
0.2
P
DSC (std)
9 18 36 90 c1 c2
0
5
10
P
runtime [s] (mean)
Figure 13: DSC and runtime for brain shape LAI and LOO results.
As anticipated, the plots conﬁrm that with respect to
ﬁtting accuracy an increasing number of measurements P
improves the results. Moreover, it can be seen that the
ANISO method outperforms the ICP methods in all cases,
where the standard deviation of ICP is much larger. In all
cases, running any ﬁtting method is superior compared
to simply using the mean shape. Due to the simplicity
of the ICP method, its runtime is much lower compared
to the proposed ﬁtting methods. However, by using the
ANISO method with a downsampled PDM, the runtime
can be reduced compared to the ANISO method, whilst
still having superior ﬁtting accuracy compared to ICP.
When using the method for interactive segmentation,
the individual annotation of a moderate amount of random
points, e.g. P = 90, is rather tedious and time-demanding.
Thus, in the settings c1 and c2 we have evaluated the al-
ternative of using points that can be derived from a very
few number of partial contours. With that, one can ob-
tain a reasonable number of points, cf. section 6.4.1, with
much less eﬀort. Our results suggest that this is in general
preferable over using a small amount of random points, say
P ≤18. Nevertheless, the case of having P = 90 random
points outperformed the considered partial contours. We
believe the reason is that the random points contain more
diverse and scattered information compared to contours
containing many correlated points.
7. Conclusion and Outlook
In this paper we have presented a methodology for a
shape-aware surface reconstruction from sparse surface
points. The proposed methodology is superior compared
to the standard approach of ICP with respect to accu-
racy and robustness on a wide range of datasets. In this
method, the likely shape of the object that is to be recon-
structed is captured by a PDM associated with a surface
mesh. By interpreting the available sparse surface points
as samples drawn from a GMM, the surface reconstruction
task is cast as the maximisation of the posterior likelihood,
which we tackle by variants of the EM algorithm. In or-
der to achieve a surface-based ﬁtting, we use a GMM with
anisotropic covariance matrices, which are “oriented” by
the surface normals at the PDM points. However, this re-
13


--- Page 14 ---

sults in a non-concave optimisation problem that needs to
be solved in each M-step. We deal with this by maximis-
ing a concave approximation that considers the surface
normals of the PDM computed from the previous value
of the shape deformation parameter.
As stated before,
this approximation makes sense with the assumption that
neighbour PDM vertices vary smoothly and the fact that
surface normals are invariant to translations. We empiri-
cally demonstrated that ﬁnding a global maximum of this
approximation leads to better results compared to ﬁnding
a local optimum during the exact (non-concave) M-step.
Moreover, our proposed concave approximation results in
an algorithm that has the same time complexity as the
isotropic ﬁtting procedure.
The proposed surface reconstruction method deals ex-
clusively with shape deformations. Thus, the normalisa-
tion of the pose must be solved a-priori in an application-
dependent manner.
In the example of the multi-object
brain shape reconstruction we dealt with this issue by ﬁrst
performing rigid image registration in order to align the
data into a common coordinate system. Dealing with the
limitation of not explicitly considering a rigid transforma-
tion in order to model the pose of the object is the next
step for achieving an even broader applicability. Whilst
in principal one can formulate an analogous problem that
considers the pose, the resulting problem is much more
diﬃcult to solve. This is because a simultaneous maximi-
sation must be performed with respect to the rigid trans-
formation and the shape deformation parameter. This is
usually done iteratively, as in Active Shape Model search
(Cootes and Taylor, 1992).
With that, particular chal-
lenges to be dealt with are that the resulting surface re-
construction procedure would be much more sensitive to
unwanted local optima as well as much slower.
We have conducted an evaluation of the proposed al-
gorithmic tools on a wide range of datasets in order to
demonstrate their general applicability in the ﬁeld of med-
ical image analysis. For the evaluation we have considered
a general noise model, i.e. Gaussian noise that is indepen-
dent for each point. In addition, for the contour case we
also considered in-plane (Gaussian) noise. Since we focus
on demonstrating the general applicability, a detailed eval-
uation of certain application-speciﬁc aspects (e.g. speciﬁc
noise models) has not been studied in this paper.
One
interesting direction for future work is to consider out-
liers in the sparse points, which is relevant if the sparse
points are automatically generated (e.g. using feature ex-
traction methods). This could for example be tackled by
integrating an additional uniform component into the mix-
ture model (Myronenko and Song, 2010).
Another ap-
proach is to use a RANSAC-like procedure (Fischler and
Bolles, 1981). In order to encourage the integration of our
method into application-speciﬁc medical imaging work-
ﬂows, we make our method publicly available6. We expect
6https://github.com/fbernardpi/sparsePdmFitting
that the public availability of the method will stimulate
the commencement of interesting new research questions.
One such question may be which points are most useful
for the reconstruction of surfaces.
Acknowledgement
The authors gratefully acknowledge the ﬁnancial sup-
port by the Fonds National de la Recherche, Luxem-
bourg (6538106, 8864515, 9169303), by the German fed-
eral ministry of education and research (BMBF), grant no.
01EC1408B, and by the Einstein Center for Mathematics
(ECMath), Berlin.
Appendix A. Gradient of Q in (17)
In the following, we derive the gradient of Q w.r.t. α,
i.e.
∇αQ =
h
∂Q
∂αm
i
m .
(A.1)
For brevity, we write ∂· to denote the partial derivative
∂·
∂αm w.r.t. αm, where the dependence on m is implicit.
First, we note that the cross-product u × v of two vec-
tors u, v ∈R3 can be written as the matrix multiplication
[u]×v, where the operator [·]× : R3 →R3×3 creates a skew-
symmetric matrix from its input vector by
[


u1
u2
u3

]× :=


0
−u3
u2
u3
0
−u1
−u2
u1
0

.
(A.2)
Introducing
bi(α) := (yi2(α) −yi(α)) × (yi3(α) −yi(α)) ,
(A.3)
we can write ni(α) =
bi(α)
∥bi(α)∥. Now, by representing the
cross product in (A.3) as a matrix product with the nota-
tion from (A.2), and by using the product rule, the partial
derivative of bi(α) is given by
∂bi(α) = {∂[yi2(α) −yi(α)]×}(yi3(α) −yi(α))
(A.4)
+ [yi2(α) −yi(α)]×{∂(yi3(α) −yi(α))}
= [Φi2,m −Φi,m]×(yi3(α) −yi(α))
(A.5)
+ [yi2(α) −yi(α)]×(Φi3,m −Φi,m) .
Moreover,
∂∥bi(α)∥= bT
i (α){∂bi(α)}
∥bi(α)∥
.
(A.6)
By using the quotient rule, the partial derivative of ni(α)
is given by
∂ni(α) = ∥bi(α)∥{∂bi(α)} −bi(α){∂∥bi(α)∥}
∥bi(α)∥2
.
(A.7)
14


--- Page 15 ---

Using ∂ni(α), we can write
∂Wi(α) = (η−1)({∂ni(α)}nT
i (α) + ni(α){∂nT
i (α)}) .
(A.8)
Now, given the expression for ∂Wi(α), we can ﬁnally com-
pute the partial derivative of Q w.r.t. αm, which is
∂Q(α, σ, α(n), σ(n)) =
(A.9)
−(Λ−1)m,:α −
1
2σ2
X
i,j
p(i|pj, α(n), σ(n))·
[(pj −yi(α))T {∂Wi(α)}(pj −yi(α))−
2ΦT
i,mWi(α)(pj −yi(α))] .
References
Albrecht, T., L¨uthi, M., Gerig, T., Vetter, T., 2013. Posterior shape
models. Medical Image Analysis 17, 959–973.
Amenta, N., Bern, M., Kamvysselis, M., 1998. A new Voronoi-based
surface reconstruction algorithm, in: SIGGRAPH.
Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J.,
Davis, J., 2005. SCAPE: shape completion and animation of peo-
ple, in: SIGGRAPH, ACM. pp. 408–416.
Bajaj, C.L., Bernardini, F., Xu, G., 1995. Automatic reconstruction
of surfaces and scalar ﬁelds from 3D scans, in: SIGGRAPH.
Baka, N., de Bruijne, M., Reiber, J., 2010. Conﬁdence of model based
shape reconstruction from sparse data, in: Biomedical Imaging:
From Nano to Macro.
Ballard, D.H., 1981.
Generalizing the Hough transform to detect
arbitrary shapes. Pattern Recognition 13, 111–122.
Barratt, D.C., Chan, C.S., Edwards, P.J., Penney, G.P., Slom-
czykowski, M., Carter, T.J., Hawkes, D.J., 2008.
Instantiation
and registration of statistical shape models of the femur and pelvis
using 3D ultrasound imaging. Medical Image Analysis 12, 358–
374.
Berger, M., Tagliasacchi, A., Seversky, L., Alliez, P., 2014.
State
of the art in surface reconstruction from point clouds.
EURO-
GRAPHICS STAR .
Bernard, F., Gemmar, P., Hertel, F., Goncalves, J., Thunberg, J.,
2016a. Linear Shape Deformation Models with Local Support Us-
ing Graph-based Structured Matrix Factorisation, in: Computer
Vision and Pattern Recognition (CVPR), Las Vegas, NV.
Bernard, F., Gemmar, P., Husch, A., Saleh, C., Neb, H., Dooms,
G., Hertel, F., 2014. Improving the Consistency of Manual Deep
Brain Structure Segmentations by Combining Variational Interpo-
lation, Simultaneous Multi-Modality Visualisation and Histogram
Equilisation. Biomedical Engineering / Biomedizinische Technik
59, 131–134.
Bernard, F., Salamanca, L., Thunberg, J., Hertel, F., Goncalves, J.,
Gemmar, P., 2015. Shape-aware 3D Interpolation using Statistical
Shape Models, in: Proceedings of Shape Symposium, Delemont.
Bernard, F., Vlassis, N., Gemmar, P., Husch, A., Thunberg, J.,
Goncalves, J., Hertel, F., 2016b.
Fast correspondences for sta-
tistical shape models of brain structures, in: Proc. SPIE Medical
Imaging, San Diego.
Bernardini, F., Mittleman, J., Rushmeier, H., Silva, C., Taubin,
G., 1999. The ball-pivoting algorithm for surface reconstruction.
IEEE Transactions on Visualization and Computer Graphics 5,
349–359.
Besl, P.J., McKay, N.D., 1992.
A method for registration of 3-D
shapes. IEEE Transactions on Pattern Analysis and Machine In-
telligence 14, 239–256.
Billings, S.D., Boctor, E.M., Taylor, R.H., 2015.
Iterative most-
likely point registration (IMLP): a robust algorithm for computing
optimal shape alignment. PloS one 10.
Blanc, R., Sz´ekely, G., 2012. Conﬁdence regions for statistical model
based shape prediction from sparse observations. IEEE Transac-
tions on Medical Imaging 31, 1300–1310.
Blanz, V., Mehl, A., Vetter, T., Seidel, H.P., 2004.
A statistical
method for robust 3D surface reconstruction from sparse data, in:
3D Data Processing, Visualization and Transmission, pp. 293–300.
Bolle, R.M., Vemuri, B.C., 1991. On three-dimensional surface re-
construction methods.
IEEE Transactions on Pattern Analysis
and Machine Intelligence 13, 1–13.
Bouaziz, S., Tagliasacchi, A., Pauly, M., 2013. Sparse iterative closest
point. Computer Graphics Forum 32, 1–11.
de Bruijne, M., van Ginneken, B., Viergever, M.A., Niessen, W.J.,
2004. Interactive segmentation of abdominal aortic aneurysms in
CTA images. Medical Image Analysis 8, 127–138.
Chan, C.S., Edwards, P.J., Hawkes, D.J., 2003.
Integration of
ultrasound-based registration with statistical shape models for
computer-assisted orthopaedic surgery, in: SPIE Medical Imag-
ing, pp. 414–424.
Chang, W., Zwicker, M., 2009. Range Scan Registration Using Re-
duced Deformable Models. Computer Graphics Forum 28, 447–
456.
Cootes, T.F., Taylor, C.J., 1992.
Active Shape Models - Smart
Snakes, in: British Machine Vision Conference, pp. 266–275.
Cootes, T.F., Taylor, C.J., 1995. Combining point distribution mod-
els with shape models based on ﬁnite element analysis. Image and
Vision Computing 13, 403–409.
Dempster, A.P., Laird, N.M., Rubin, D.B., 1977. Maximum Like-
lihood from Incomplete Data via the EM Algorithm. Journal of
the Royal Statistical Society. Series B (Methodological) 39, 1–38.
Edelsbrunner, H., M¨ucke, E.P., 1994.
Three-dimensional alpha
shapes. ACM Transactions on Graphics 13, 43–72.
Fischler, M.A., Bolles, R.C., 1981.
Random sample consensus: a
paradigm for model ﬁtting with applications to image analysis
and automated cartography. Communications of the ACM 24.
Fleute, M., Lavall´ee, S., 1998. Building a complete surface model
from sparse data using statistical shape models: Application to
computer assisted knee surgery, in: International Conference on
Medical Image Computing and Computer-Assisted Intervention.
Fleute, M., Lavall´ee, S., Julliard, R., 1999.
Incorporating a sta-
tistically based shape model into a system for computer-assisted
anterior cruciate ligament surgery. Medical Image Analysis 3, 209–
222.
Fonov, V.S., Evans, A.C., McKinstry, R.C., Almli, C.R., Collins,
D.L., 2009.
Unbiased nonlinear average age-appropriate brain
templates from birth to adulthood. Neuroimage 47, S102.
Gal, R., Shamir, A., Hassner, T., Pauly, M., Cohen Or, D., 2007.
Surface reconstruction using local shape priors., in: Symposium
on Geometry Processing, pp. 253–262.
van Ginneken, B., de Bruijne, M., Loog, M., Viergever, M.A., 2003.
Interactive shape models, in: SPIE Medical Imaging, pp. 1206–
1216.
van Ginneken, B., Frangi, A.F., Frangi, R.F., Staal, J.J., Ter
Haar Romeny, B.M., Viergever, M.A., 2002. Active Shape Model
Segmentation with Optimal Features. IEEE Transactions on Med-
ical Imaging 21, 924–933.
Granger, S., Pennec, X., 2002.
Multi-scale EM-ICP: A fast and
robust approach for surface registration. European Conference on
Computer Vision .
Guo, G., Jiang, T., Wang, Y., Gao, W., 2012. Recovering Missing
Contours for Occluded Object Detection. IEEE Signal Processing
Letters 19, 463–466.
Guo, G., Jiang, T., Wang, Y., Gao, W., 2013. 2-D shape completion
with shape priors. Chinese Science Bulletin 58, 3430–3436.
Heckel, F., Konrad, O., Karl Hahn, H., Peitgen, H.O., 2011.
In-
teractive 3D medical image segmentation with energy-minimizing
implicit functions. Computers & Graphics 35, 275–287.
Heimann, T., Meinzer, H.P., 2009. Statistical shape models for 3D
medical image segmentation: A review. Medical Image Analysis
13, 543–563.
van den Hengel, A., Dick, A.R., Thorm¨ahlen, T., Ward, B., Torr,
P.H.S., 2007. Interactive 3D Model Completion, in: Digital Image
15


--- Page 16 ---

Computing Techniques and Applications, pp. 175–181.
Herman, G.T., Zheng, J., Bucholtz, C.A., 1992. Shape-based inter-
polation. IEEE Computer Graphics and Applications 12, 69–79.
Hill, A., Cootes, T.F., Taylor, C.J., 1995. Active Shape Models and
the Shape Approximation Problem. Image and Vision Computing
14, 601–607.
Hoppe, H., DeRose, T., Duchamp, T., McDonald, J.A., Stuetzle,
W., 1992. Surface reconstruction from unorganized points. SIG-
GRAPH , 71–78.
Horaud, R., Forbes, F., Yguel, M., Dewaele, G., Zhang, J., 2011.
Rigid and articulated point registration with expectation condi-
tional maximization. IEEE Transactions on Pattern Analysis and
Machine Intelligence 33, 587–602.
Hufnagel, H., Pennec, X., Ehrhardt, J., Ayache, N., Handels, H.,
2008. Generation of a statistical shape model with probabilistic
point correspondences and the expectation maximization-iterative
closest point algorithm. International Journal of Computer As-
sisted Radiology and Surgery 2, 265–273.
Jenkinson, M., Smith, S., 2001. A global optimisation method for
robust aﬃne registration of brain images. Medical Image Analysis
5, 143–156.
Jensen, J.L.W.V., 1906. Sur les fonctions convexes et les in´egalit´es
entre les valeurs moyennes. Acta mathematica 30, 175–193.
Kazhdan, M., Bolitho, M., Hoppe, H., 2006. Poisson surface recon-
struction, in: Eurographics Symposium on Geometry Processing.
Liu, J., Udupa, J.K., 2009.
Oriented active shape models.
IEEE
Transactions on Medical Imaging 28, 571–584.
Lu, M., Zheng, B., Takamatsu, J., Nishino, K., 2011.
3D shape
restoration via matrix recovery, in:
Computer Vision–ACCV
Workshops, pp. 306–315.
Maier-Hein, L., Franz, A.M., dos Santos, T.R., Schmidt, M.,
Fangerau, M., Meinzer, H.P., Fitzpatrick, J.M., 2012.
Conver-
gent iterative closest-point algorithm to accomodate anisotropic
and inhomogenous localization error. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence 34, 1520–1532.
Maier-Hein, L., Schmidt, M., Franz, A.M., dos Santos, T.R., Seitel,
A., J¨ahne, B., Fitzpatrick, J.M., Meinzer, H.P., 2010. Accounting
for anisotropic noise in ﬁne registration of time-of-ﬂight range data
with high-resolution surface data, in: International Conference on
Medical Image Computing and Computer-Assisted Intervention,
Springer. pp. 251–258.
Meng, X.L., Rubin, D.B., 1993. Maximum likelihood estimation via
the ECM algorithm: A general framework. Biometrika 80, 267–
278.
Myronenko, A., Song, X., 2010. Point Set Registration: Coherent
Point Drift. IEEE Transactions on Pattern Analysis and Machine
Intelligence 32, 2262–2275.
Myronenko, A., Song, X., Carreira-Perpin´an, M.A., 2007. Non-rigid
point set registration: Coherent Point Drift, in: Neural Informa-
tion Processing Systems, pp. 1009–1016.
Nocedal, J., Wright, S.J., 2006. Numerical Optimization. Springer.
Osada, R., Funkhouser, T., Chazelle, B., Dobkin, D., 2002. Shape
distributions. ACM Transactions on Graphics (TOG) 21, 807–832.
Park, S., Guo, X., Shin, H., Qin, H., 2006. Surface completion for
shape and appearance. The Visual Computer 22, 168–180.
Pauly, M., Mitra, N.J., Giesen, J., Gross, M.H., Guibas, L.J., 2005.
Example-Based 3D Scan Completion., in: Symposium on Geom-
etry Processing, pp. 23–32.
Rajamani, K.T., Styner, M.A., Talib, H., Zheng, G., Nolte, L.P.,
Ballester, M.A.G., 2007. Statistical deformable bone models for
robust 3D surface extrapolation from sparse data. Medical Image
Analysis 11, 99–109.
Raya, S.P., Udupa, J.K., 1990. Shape-based interpolation of multi-
dimensional objects. IEEE Transactions on Medical Imaging 9,
32–42.
Rusinkiewicz, S., Levoy, M., 2001.
Eﬃcient variants of the ICP
algorithm, in: 3-D Digital Imaging and Modeling.
Schroers, C., Setzer, S., Weickert, J., 2014. A Variational Taxonomy
for Surface Reconstruction from Oriented Points, in: Computer
Graphics Forum, pp. 195–204.
Schwarz, T., Heimann, T., Tetzlaﬀ, R., Rau, A.M., Wolf, I., Meinzer,
H.P., 2008. Interactive surface correction for 3D shape based seg-
mentation, in: SPIE Medical Imaging, pp. 69143O–69143O.
Segal, A., Haehnel, D., Thrun, S., 2009.
Generalized-icp, in:
Robotics: Science and Systems.
Seim, H., Kainm¨uller, D., Lamecker, H., Bindernagel, M., Mali-
nowski, J., Zachow, S., 2010. Model-based Auto-Segmentation of
Knee Bones and Cartilage in MRI Data. Proc. of Medical Image
Analysis for the Clinic: A Grand Challenge , 215–223.
Shen, C.H., Fu, H., Chen, K., Hu, S.M., 2012. Structure recovery by
part assembly, in: SIGGRAPH.
Stoll, C., Karni, Z., R¨ossl, C., Yamauchi, H., Seidel, H.P., 2006.
Template deformation for point cloud ﬁtting. SPBG , 27–35.
Tan, J.H., Acharya, U.R., 2014. Active spline model: A shape based
model—interactive segmentation.
Digital Signal Processing 35,
64–74.
Taylor, J., Bordeaux, L., Cashman, T., Corish, B., Keskin, C., Sharp,
T., Soto, E., Sweeney, D., Valentin, J., Luﬀ, B., 2016. Eﬃcient
and precise interactive hand tracking through joint, continuous
optimization of pose and correspondences. ACM Transactions on
Graphics (TOG) 35, 143.
Taylor, J., Stebbing, R., Ramakrishna, V., Keskin, C., Shotton, J.,
Izadi, S., Hertzmann, A., Fitzgibbon, A., 2014.
User-Speciﬁc
Hand Modeling from Monocular Depth Sequences, in: 2014 IEEE
Conference on Computer Vision and Pattern Recognition, IEEE.
pp. 644–651.
Timinger, H., Pekar, V., von Berg, J., Dietmayer, K., Kaus, M., 2003.
Integration of interactive corrections to model-based segmentation
algorithms, in: Bildverarbeitung f¨ur die Medizin 2003. Springer,
pp. 171–175.
Treece, G.M., Prager, R.W., Gee, A.H., 2000.
Surface interpola-
tion from sparse cross sections using region correspondence. IEEE
Transactions on Medical Imaging 11, 1106–1114.
Turk, G., O’Brien, J.F., 1999.
Shape transformation using varia-
tional implicit functions, in: SIGGRAPH, pp. 335–342.
Wang, Y., Staib, L.H., 1998. Boundary ﬁnding with correspondence
using statistical shape models, in: Computer Vision and Pattern
Recognition (CVPR), pp. 338–345.
Wang, Y., Staib, L.H., 2000. Boundary ﬁnding with prior shape and
smoothness models. IEEE Transactions on Pattern Analysis and
Machine Intelligence 22, 738–743.
Zheng, G., 2013. Expectation conditional maximization-based de-
formable shape registration, in: International Conference on Com-
puter Analysis of Images and Patterns, Springer. pp. 548–555.
Zheng, G., Rajamani, K.T., Nolte, L.P., 2006. Use of a Dense Sur-
face Point Distribution Model in a Three-Stage Anatomical Shape
Reconstruction from Sparse Information for Computer Assisted
Orthopaedic Surgery: A Preliminary Study. Computer Vision–
ACCV .
16
```

---

## Section 1.33: Sharp feature preserving MLS surface reconstruction based on local feature

Source File: Sharp feature preserving MLS surface reconstruction based on local feature.txt

### Paper Content:

```
# Sharp feature preserving MLS surface reconstruction based on local feature.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Sharp feature preserving MLS surface reconstruction based on local feature.pdf
# Output: ../layer2_completion/txt/Sharp feature preserving MLS surface reconstruction based on local feature.txt


--- Page 1 ---

Preprint (march 2012)
to appear in Graphical Models 2012
Sharp feature preserving MLS surface reconstruction based on local feature
line approximations
Christopher Weber a
Stefanie Hahmann b
Hans Hagen a
Georges-Pierre Bonneau b
aTechnische Universit¨at Kaiserslautern
bUniversit´e de Grenoble, Laboratoire Jean Kuntzmann, INRIA
Abstract
Sharp features in manufactured and designed objects require particular attention when reconstructing surfaces from unorganized
scan point sets using moving least squares (MLS) ﬁtting. It’s an inherent property of MLS ﬁtting that sharp features are smoothed
out. Instead of searching for appropriate new ﬁtting functions our approach computes a modiﬁed local point neighborhood so that
a standard MLS ﬁtting can be applied enhanced by sharp features reconstruction.
We present a two-stage algorithm. In a pre-processing step sharp feature points are marked ﬁrst. This algorithm is robust to noise
since it is based on Gauss map clustering. In the main phase, the selected feature points are used to locally approximate the feature
curve and to segment and enhance the local point neighborhood. The MLS projection thus leads to a piecewise smooth surface
preserving all sharp features. The method is simple to implement and able to preserve line-type features as well as corner-type
features during reconstruction.
Key words: MLS, sharp feature, surface reconstruction, Gauss map, clustering, point set surfaces
Fig. 1. Left: standard MLS surface. Middle: feature point detection
in point cloud. Right: sharp feature preserving MLS
1. Introduction
Point based surfaces have become a very popular and
attractive mesh less surface representation over the last
decade. They deﬁne a smooth surface using local moving
least squares (MLS) approximations of the data.
Since the initial approach [2] based on Levin’s [18] projec-
tion operator, many improvements in terms of eﬃciency
[3,28], rendering [12], stability [11], studies on properties
and limitations [3,4] and variants using implicit surfaces
[28,16,21] or specialized to non uniform, low dense sam-
pling [8] have been developed.
The main strengths of MLS ﬁtting include natural point
denoising due to local least squares approximation, which
can be seen as a local low pass ﬁlter. They generate smooth
surfaces even in the presence of noise. It is thus assumed
that the input data originates from a smooth surface.
Many scanned, manufactured or designed objects however
exhibit sharp features. Sharp features are an important
design element not only for mechanical parts. It is thus im-
portant to be capable to reconstruct those features not only
for reverse engineering applications such as surface recon-
struction, but also for quality control of a product, where


--- Page 2 ---

the scanned object is compared to the CAD-prototype.
Even though point based surfaces were initially developed
as an eﬃcient visualization tool for point sampled data,
they are nowadays used in a wide range of applications [10].
However, the requirements on point based surfaces to han-
dle uniform noise and to generate a smooth surface is in
contradiction to sharp feature preservation. It is thus not
surprising that for example Amenta and Kil [4] attest in-
stabilities near sharp features.
Although visual smoothness is one of the most required
properties of surfaces or shapes in general, for the men-
tioned applications in product design, reverse engineering
or quality control it is however indispensable to preserve
sharp features during reconstruction.
In this paper we address the problem of sharp feature
preserving surface reconstruction from arbitrary point sets
using moving least squares.
The output is a piecewise smooth point-based surface,
meaning a reﬁnable point cloud with normals. Feature
points are ﬁrst identiﬁed in the point cloud as part of a
pre-processing using an adaptive Gauss-map clustering
technique. Only neighborhoods containing some of the
feature points are modiﬁed. Inside these neighborhoods,
which are used by the projection operator, a local approx-
imating feature curve is computed and serves to make a
local decision in order to segment and enhance the point
neighborhood. Applying standard MLS ﬁtting [18] to the
modiﬁed neighborhoods automatically generates piecewise
smooth surfaces with sharp features since the points are
projected to only one smooth surface part.
All advantages of standard MLS ﬁtting (local smoothness,
robustness to noise) are preserved and augmented with the
ability to reconstruct sharp features.
Similar to RMLS [9] we identify individual neighborhoods
within the regions of identiﬁed feature points, but we don’t
rely on computational expensive robust statistics. Instead
we ﬁt local curves through the identiﬁed feature points in
order to slice the neighborhoods. Contributions and ad-
vantages of our technique in contrast to previous works
[9,11,25,21] can be listed as follows:
• Sharp features are automatically reconstructed, no man-
ual tagging [11,25] of feature lines is required.
• It is shown how local feature curve approximations can
be used to partition the neighborhoods used for MLS
projection.
• The method is much more simple to implement than sta-
tistical methods [9,6,7,21] where many parameters have
to be ﬁne-tuned.
• The quality of reconstructed sharp features is improved
with respect to previous MLS methods, e.g. our local
feature curve approximation and neighborhood modiﬁ-
cation reduces the appearance of jagged edges and pro-
duces smoother features.
• Feature point detection is a costly part here. MLS ﬁt-
ting with sharp features is then done in usual compu-
tation time. Performing the feature point detection in
a pre-computation has the advantage that MLS recon-
structions with diﬀerent smoothness parameters can be
performed multiple times. Our method speeds up such a
process signiﬁcantly.
The rest of the paper will be composed as follows: In Sec-
tion 2, we review related work in the ﬁeld of MLS recon-
struction. Section 3 sets basic notations of MLS ﬁtting.
Section 4 describes the pipeline from feature point extrac-
tion, neighborhood modiﬁcation and sharp feature surface
ﬁtting. Section 5 presents experimental results. Final re-
marks and future work close the paper.
2. Related Works
Moving least squares (MLS) surfaces are a very popular
mesh less surface reconstruction tool [1,2]. In contrast to
the common approach to generate a triangulated mesh,
MLS approximate an unorganized dense, possibly noisy,
set of points by an overall smooth point-based surface
[18]. Many variants have been published [1,9,11,16,25]
and several applications of point-based geometries can be
found in [10]. Preservation of sharp features is not an in-
herent property of MLS ﬁtting, any sharp feature will be
smoothed and appears rounded after the ﬁtting process.
So let us focus now only on MLS approaches with sharp
feature reconstruction in this section.
As one of the ﬁrst, Fleishman et al. [9] presented a MLS
approach that could reproduce sharp features. The so-
called robust moving least squares ﬁtting approach is
based on methods used in statistics to search for outliers
in the point set. It assumes, that the surface consists of
several smooth patches connected by sharp features. The
idea is, that a sample point lying on another smooth patch
will be identiﬁed as outlier during the statistical analysis.
An iterative reﬁtting procedure is necessary to add points
successively to the used neighborhood until an outlier is
detected. Several iterations are necessary. It reconstructs
the smooth surface parts, which in the end are connected
through a sharp feature. Although the resulting surface is
of a good quality, the outlier search and the reﬁtting of the
smooth surface parts are quite tricky to implement. The
method also has problems with jagged edges as pointed out
in [6,7] and needs a quite dense sampling. Our approach
is inspired by the partitioning of the local neighborhood
to reconstruct sharp features, although the way how we
achieve this is completely diﬀerent.
Guenebaud et al. [11] presented APSS, algebraic point set
surfaces. They use moving least squares ﬁtting of spheres
instead of planes. This leads to a good stability in under-
sampled datasets. The sharp feature extraction itself is
done manually by tagging of the point cloud, or automatic
and based on a statistical analysis analogous to [9].
¨Oztireli et al. [21] use a kernel regression technique to re-
construct sharp features. They called it RIMLS, Robust
Implicit MLS. Analogous to [9] they also use a statistics
approach to ﬁnd outliers belonging to diﬀerent smooth
2


--- Page 3 ---

patches on the surface. This technique has global param-
eters that can control the global visual sharpness of the
reconstruction. However the resulting surface remains al-
ways C2-continuous. So the reconstruction does not have a
tangent plane discontinuous sharp feature, but only gives
the visual eﬀect of a sharp feature during rendering. De-
pending on the application, this can be seen either as an
advantage or disadvantage.
An other interesting approach is the ERKPA by Reuter et
al. [25]. ERKPA stands for Enriched Reproducing Kernel
Particle Approximation. In this approach the user has to
tag the sharp features manually and the second step of the
MLS projection (the computation of the local polynomial
approximation) is modiﬁed. Instead of the normal projec-
tion, they add an enrichment function with discontinuous
derivatives to the approximation function. The enrichment
functions are compactly supported with a user speciﬁc
support size to control the inﬂuence of the sharp feature.
For n features, n enrichment functions are needed. Our
approach doesn’t require any manual user interaction.
Ohtake et al. [23] introduce the multi-level partition of
unity implicits. Piecewise quadratic functions are used to
capture the local surface and then use weighting functions
that blend together these local shapes of the surface. An
octree subdivision that adapts to variations in the com-
plexity of the shape is used. Sharp corners and edges can
be reconstructed by selecting appropriate shape functions.
Daniels et al. [6,7] build up on Fleishmans approach [9]
to project points onto the features of the dataset. A co-
variance analysis for smoothing and growing polylines of
the detected feature points leads to a global approxima-
tion of feature lines. Those feature lines are then used as
starting points for the construction of a triangle mesh via
an advancing-front algorithm. This approach is related
to ours only in the sense that they provide a pipeline for
global feature curve extraction. But they don’t apply it to
MLS surface ﬁtting.
Lipman et al. [19] also presented a MLS ﬁtting method
that is able to handle sharp features and can handle many
cases like line-type features quite well. But as they men-
tion themselves, their method needs careful ﬁne tuning in
presence of noise and has problems with structures like
the corner of a cube. Both situations are handled by our
approach as we will show in Sections 4.2.5 and 5.
In contrast to previous MLS methods for sharp feature
reconstruction that use statistics methods (forward search
combined with Levin’s MLS [9], or local kernel regression
combined with implicit MLS [21]), we derive our method
by enhancing basic MLS with both, an automatic adaptive
feature points identiﬁcation procedure and a neighborhood
segmentation in case a feature belongs to it.
There is also a large number of feature preserving sur-
face reconstruction techniques not based on MLS but
relying on surface meshes. To name a few we refer to
[14,5,13,15,29,31,26,27].
3. MLS basics and notations
The MLS surface of Levin [18] is a point based surface
representation approximating a given set of point samples.
Let P = {p1, · · · , pn} , pi ∈R3 be a set of unorganized
points sampled from a surface. Let Np denote the set of
k-nearest neighbors of p and Ip the index set of Np. Note
that k << n. The MLS surface SP is deﬁned implicitly by
a projection operator Ψ, that projects points from a vicin-
ity of the MLS surface onto the surface itself. SP is then
deﬁned as the set of points projecting onto themselves, i.e.
the ﬁx points of the operator.
The projection p 7→Ψ(p) of a point p is motivated by
the fact that the surface can be locally approximated by a
function. Its computation following Alexa et al. [2] is split
into three steps:
a) For a given point p and its local neighborhood Np com-
pute a local tangent reference plane H = {x| ⟨n, x⟩−D =
0, x ∈R3}, n ∈R3, ∥n∥= 1 by minimizing
X
i∈Ip
(⟨n, pi −p −tn⟩)2 w (∥pi −p −tn∥)
(1)
among all normal directions n and oﬀsets t. ⟨·, ·⟩denotes
the dot product in R3. w is a smooth monotone decreasing
weighting function
w(d) = e−d2
h2 ,
(2)
where h can be used as a parameter to adjust the smooth-
ing and interpolation behavior.
b) Find a bivariate polynomial approximation g : H 7→R
to the surface in a neighborhood Np of p by a weighted
least squares ﬁt similar to (1). Let qi be the orthogonal
projection of pi onto H, (xi, yi) its local coordinates, hi =
∥pi −qi∥the height of pi over H. The coeﬃcients of g are
computed by minimizing
X
i∈Ip
(g(xi, yi) −hi)2w (∥pi −p −tn∥).
(3)
c) The projection of p is ﬁnally given by
Ψ(p) := p + (t + g(0, 0))n.
(4)
Implementation details can be found in [2].
4. Sharp feature preserving MLS
Our method decomposes into 2 separate phases. First,
sharp feature points are marked in the point set. This can
be done as a pre-processing. In the second main phase,
the selected feature points serve as indicator to modify the
local neighborhood for a point being projected.
Performing the most diﬃcult and most time consuming
feature point detection part, in a separate (pre-processing)
3


--- Page 4 ---

phase has some advantages. First, once the feature points
are marked in a given point set, an important part of com-
putation time is gone. The user has thus the possibility to
recompute the MLS surface with diﬀerent sets of parame-
ters in order to ﬁne-tune the smoothness of the surface and
the sharpness of the features without having to recompute
the feature points again.
Second, both parts of the method rely on local neighbor-
hood computations. The k-nearest search is thus done only
once, stored and reused for MLS projection and neighbor-
hood modiﬁcation.
4.1. Marking the sharp feature points
Detection of sharp features in point clouds is not a trivial
task because of possible noise in the data set. Since we only
need the points belonging to a sharp feature to be marked,
any of the methods [30,6] seems appropriate. Several other
existent techniques for point clouds are dedicated to detect
points on any kind of characteristical feature, e.g. points
with high curvature value being not necessary a sharp fea-
ture [22,24,20]. These methods are however not appropri-
ate here, since they either produce a large band of feature
points or they reconstruct global feature lines using splines
which is quite costly. Let us just summarize the method we
use. All details can be found in [30].
The main idea here comes from the following observation
for piecewise smooth curves and surfaces: The Gauss map
of a particular point and its small neighborhood on a curve
in 2D for example behaves diﬀerent whether the point is a
sharp feature or not. In fact there are 3 typical clustering
behaviors to observe: one cluster when the point belongs
to a ﬂat region, a large fuzzy cluster for a point of a curved
region or 2 distinct clusters in case of a sharp feature point.
In 3D the clustering behavior is similar, except that more
than 2 clusters may appear for a corner-type feature.
In the present setting however the input is not a smooth
continuous surface, but an unstructured set of 3D points
sampled from a surface without any normal information
available. The expected output is a subset of points marked
as points belonging to sharp features of the geometry, the
so-called feature points. The algorithm ﬁrst computes local
neighborhoods for all data points using a kd-structure. For
each data point inside its neighborhood, a so-called discrete
Gauss-map clustering is computed to decide whether a data
point p belongs to a sharp feature or not. Since no normals
are available, all possible triangles ∆(p, xi, xj), formed of
p with two other points xi, xj ∈Np, i, j = 1, . . . , k are
computed. The Gauss map of these triangles is then clus-
tered using a geodesic distance criteria. The clustering be-
havior of the Gauss map of each point p can now be ana-
lyzed in order to decide if it’s a sharp feature point or not.
Note that for each triangle 2 opposite normal directions are
mapped to the unit sphere. Each cluster has thus an oppo-
site counter part on the sphere. In Figure 2 a 2D illustration
is given. 3 typical clustering behaviors are shown from left
to right corresponding to a point belonging to a ﬂat region,
to a curved region or to a sharp feature. In 3D the cluster-
ing behavior is similar, except that more clusters may ap-
pear for a corner-type feature. [30] presents ﬁnally an aug-
mented adaptive version of this basic idea. It is adaptive in
the sense that optimal local parameters are computed au-
tomatically so that very ﬂat as well as very acute features
can be recognized robustly even in the presence of noise.
Fig. 2. 2D examples of a case resulting from Gauss-map clustering.
Left: Clustering situation in ﬂat area; middle: the curved case shows
no clustering; right: in case of a sharp feature clearly distinguishable
clusters are formed
4.2. MLS surface computation
4.2.1. Overview
After applying the feature detection method, we know
for each point in the point set if it belongs to a sharp fea-
ture or not. This knowledge is exploited in the MLS ﬁtting
process in order to reconstruct sharp features as follows.
Let p be the point that is currently projected onto the
surface. A neighborhood Np of p is already available from
the previous feature detection step.
Our strategy for sharp feature reconstruction is not to
modify the mathematical formulation of the MLS method,
but to modify the neighborhoods and thus the projection
operator. A standard MLS projection is applied to all
points, except if their neighborhood contains sharp feature
points. Only then, the neighborhood is modiﬁed before
projection applies.
Basically the neighborhood modiﬁcation partitions Np
into two or three subsets, depending on the type of fea-
ture (line or corner) occurring inside. This partitioning is
performed by clipping the neighborhood points close to p
against a so-called local feature curve. This curve approxi-
mates the sharp feature locally inside Np. Finally, the curve
is up-sampled in order to smooth the sharp feature curve
and to ensure equal quality everywhere. p is then projected
on the surface part closest to p.
Each neighborhood subset deﬁnes in fact a diﬀerent
smooth surface. Together these surfaces meet with discon-
tinuous tangent planes creating thus the sharp feature.
The method is able to construct sharp features even in
the case of corner-type features. Let us ﬁrst treat the gen-
eral case of a line-type feature traversing the neighborhood
Np, meaning a it curved feature line. The case of a corner
4


--- Page 5 ---

Fig. 3. Local neighborhood modiﬁcation: a) the neighborhood of p (blue) is constructed. b) the feature points fi (red) are identiﬁed and the
local feature curve is constructed. c) the points xi ∈Np not belonging to the same surface part as p are removed from Np. d) the removed
points are replaced by samples from the feature curve.
and the way how to decide the type of feature will be han-
dled later in Sections 4.2.5 and 4.2.6.
4.2.2. Local feature curves
Fig. 4. a) Feature curve computation: red points are the feature
points fi. A cubic B´ezier curve F (green) approximates the feature
points. The B´ezier points b0 and b3 are set to the extremal feature
points. A simple heuristic determines b1, b2. Together they form the
control polygon on F (gray dotted); b) Situation with only a small
number of feature points fi in Np c) Situation of a poor sampled
dataset in the extreme case of non uniform sampling
Let us denote x1, x2, ..., xk ∈Np ⊂P the k-nearest
neighborhood of p, and fi the identiﬁed feature points
among the set Np. When a feature traverses Np, the points
xi deﬁne two diﬀerent surface parts.
We are interested only in the subset of points xi which
deﬁne the surface part closest to p. To this end a feature
curve is locally reconstructed as a cubic B´ezier curve F
deﬁned by
F(t) =
3
X
i=0
biB3
i (t),
bi ∈R3
(5)
and using the following heuristic to determine control
points bi.
Since the neighborhood Np represents only a very small
part of the surface (|Np| = k with k << n), we can assume
the traversing feature having a simple shape. Furthermore
fi being unsorted, we take the couple of feature points fi
and fj with largest distance to deﬁne the ﬁrst and last
B´ezier control points b0 := fi and b3 := fj. The remaining
fk are now sorted with respect to b0 and b3. If there are
exactly four feature points, b1, b2 are set accordingly. If
Fig. 5. Examples of computed local feature curves (black curve)
approximating the cyan feature points detected in the neighborhood
of p (blue point). Examples depicted from real surfaces in Figs.11
and 13.
there are more than four feature points, the set is divided
in two parts and an arbitrary representative of each subset
is chosen and set to be b1, b2. In Figure 4(a) an example of
feature curve computation from 5 feature points is shown.
This is a rough approximation, but suﬃcient when assum-
ing that Np represents a very small portion of the initial
point set. As one alternative, a more sophisticated curve
approximation would be costly without any extra beneﬁt.
Taking F as a simple straight line as another alternative
however would result in a poor quality feature curve.
Implementation issues:
(1) With a neighborhood size of k = 20 we generally get
4 or 5 feature points in Np. This corresponds in mean to
the number of points which can be aligned to a straight
line traversing a neighborhood in the middle of 20 points
which are placed regularly in a circle or quad. See Fig. 5
for some real data examples.
(2) In case the number of feature points is < 4 in Np we
don’t perform any neighborhood modiﬁcation since not
enough data is available for a reliable approximation of a
local feature. The situation may correspond to a feature
placed near the border of Np, so that the number of points
which would be eliminated by the feature curve is so small
that their inﬂuence during the projection is also small, see
Figure 4(b). Or it may correspond to errors in the feature
detection.
(3) In the case of very special non-uniform sampling of P
the method may fail, see Figure 4(c) for an example.
5


--- Page 6 ---

Fig. 6. Left: Gauss-map clustering. Right: Angle criterion. Both ap-
proaches can be used for neighborhood clipping in case of a line-type
feature.
4.2.3. Modiﬁcation of neighborhood: Clipping Np
The locally generated feature curve F will now serve to
divide the points xi ∈Np in two subsets and to keep the
subset which is ’close’ to p. Since the neighborhood Np
corresponds to a very small part of the point set, one can
assume the two surfaces being almost planar near the fea-
ture line (a similar assumption is done for the MLS surface
anyway). Two methods are possible:
(1) A Gauss-map clustering can be used again. For each
point xi, i = 1, . . . , k which is not a feature point a trian-
gle Ti = δ(xi, fj, fk) is formed together with two feature
points closest to xi. The corresponding normal vectors de-
ﬁne the Gauss-map to be clustered. The result are two clus-
ters, each cluster represents the points xi deﬁning one of
the two surface parts, Fig. 6-left.
(2) An alternative method for separating the neighbor-
hood points into two sets, is to compare the angles between
the projection vectors of each point xi onto the feature
curve F with the projection of p onto F. An angle greater
than a ﬁxed threshold value, would indicate that xi and p
belong to diﬀerent surface parts, see Fig. 6-right.
Discussion:
Note that the existence of two surface parts seperated by
a sharp feature is already acquired in this neighborhood.
We don’t need to decide whether there are two surface
parts, but only whether the points belong to one or to the
other part. Both methods can make proﬁt from the pre-
processing feature detection method and the angle thresh-
olds used there. In fact, to decide if a point is a sharp feature
point or not, a range of angles is computed there. Outside
this range, the point is assumed to belong to a smooth sur-
face part. So, in practice a range of [40◦, 140◦] is used to
deﬁne a sharp feature. The same range is used here to sep-
arate the points into two parts on either side of the feature
curve. It is thus guaranteed that the present sharp feature
reconstruction method works with the same precision than
the previous feature detection in particular in the presence
of noise. For the examples in this paper the ﬁrst method
has been used.
4.2.4. Up-sampling of the feature points
After clipping Np, the modiﬁed neighborhood is now re-
duced in size. In order to guarantee the modiﬁed MLS pro-
jection to work with the same precision everywhere, the
modiﬁed neighborhood has to be of same cardinal k as Np.
To this end additional points are sampled along the local
feature curve, see yellow points in Figure 3(d). This par-
ticular choice is motivated and justiﬁed by the fact that
increasing the number of local feature points
• strengthens the sharp feature reconstruction, and
• is smoothing the curve of the edge that is deﬁned by
the projection operator. A possible undersampling of the
data points along sharp features can thus be resolved.
Note that this local sampling step solves a problem that
has previously been raised as a limitation in [9] (end of
Sect.5) and left for future work there. It is indeed a draw-
back of statistic-based feature reconstruction methods that
the projection operator may not be smooth along sharp
features.
4.2.5. Special case: corner feature
Up to now, we have assumed that only a line-type feature
traverses the local neighborhood. It can happen that several
sharp features meet at a common vertex. The corner of a
cube is a typical example. If this situation happens inside a
neighborhood the approach presented for line-type features
in the previous sections would not work well, since a single
curve cannot ﬁt three or more feature curves meeting at a
common vertex.
One way to solve this problem would be to construct
several feature curves inside the neighborhood. It remain
the questions how to ensure that the independently con-
structed curves meet at a common corner point? and how
to chose this point? These questions are partially answered
in Daniels et al. [7]. However, the critical mass necessary
for [7] is not given here.
Fig. 7. Gauss-map clustering used for neighborhood clipping in case
of a corner-type feature.
The solution we propose is consistent to the rest of the
paper, since using again the Gauss-map clustering tech-
nique. It is composes of two steps:
Step 1: neighborhood clipping
What we need is a set of points belonging to the same
smooth surface part as p. Note that the feature points
separating the surfaces parts are already available. It’s
therefore suﬃcient to use the clipping technique (1) pro-
posed in Sect. 4.2.3: a Gauss-map for all neighborhood
points xi ∈Np is constructed by computing the normals of
6


--- Page 7 ---

all triangles ∆(p, fj, fk) and ∆(xi, fj, fk) (xi ̸= fj, fk),
i ∈Ip, where fj and fk can be chosen to be the two feature
points, e.g. closest to xi. Clustering of these normals re-
sults in three (cube’s corner) or more clusters. The cluster
representing p thus designates the subset of {xi} deﬁning
the requested surface part closest to p. All other points are
removed from Np, see Fig. 7.
Fig. 8. Result of the feature clustering near the corner of a cube
Step 2: neighborhood up-sampling
In order to increase the number of points inside Np and
simultaneously increase the precision of the sharp feature,
the set of feature points in Np is increased by up-sampling
some local feature curve. The computation of this feature
curve in the case of a corner-type feature requires to di-
vide the marked feature points into diﬀerent sets, each set
containing only the feature points belonging to one edge,
see the colored points in Fig.9a) illustrating the case of
a cube’s corner. For eﬃciency reasons we don’t compute
these groups of feature points for each p since there may
be many points in the point set close to p having the same
feature points inside their neighborhood. Instead, we mark
during the pre-processing (feature point detection) step
for each detected feature point f all feature points inside
Nf which belong to the same feature line as f. Figure
8 illustrates the result of this computation for a feature
point f (green dot) near a cube corner. The red dots in
the ﬁgure are all the features detected in the data set. Its
neighborhood is presented by the black circle. The yellow
dots are thus the marked neighbor features points inside
Nf and belonging to the same edge.
With these neighbor features marked for each feature
point in the data set, it is now an easy task to determine
for each point p the set of feature points closest to p (i.e.
containing the closest feature point). A cubic B´ezier feature
curve F is computed from this set and used for up-sampling
the modiﬁed neighborhood before projecting p, see Fig.9b).
4.2.6. Characterization of neighborhoods
One last issue remains: how to determine whether a point
neighborhood Np contains a line-type feature or a corner-
type feature? The number of Gauss-map clusters computed
for the neighborhood clipping (Fig. 6 and 7) clearly in-
dicates if the points belong to two diﬀerent surface parts
(line-type) or to more surface parts (corner-type).
Fig. 9. Local neighborhood modiﬁcation near corners: a) marking sets
of feature points belonging to diﬀerent features. b) selection of the
closest feature set closest to p, generation of the local feature curve,
clipping of local Neighborhood Np. c) up-sampling the neighborhood
with points on feature curve.
5. Results
We implemented and tested the sharp feature surface re-
construction method developed in the previous sections.
Let us report here on 4 types of input data
· academic models without any noise (cube with hole, bi-
linear surfaces)
· academic models with random noise added (cube)
· complex models (fan disk, trimstar, octﬂower)
· raw data output of a scanner (drill).
We focus in particular on the sharp edges, since classic MLS
projection is applied everywhere else on the model. Our
method is in total slower than classic MLS due to the fea-
ture point pre-processing and neighborhood modiﬁcation.
It oﬀers however the possibility to separate feature point
detection from the projection step. All feature point com-
putations including detection and the list of nearest con-
nected neighbors can be pre-processed. Timings are given
in [30]. Without these preprocessing steps, timings close to
the classical MLS can be reached, since only a few number
of the points (8% in the cube example of Fig.11) undergo
a neighborhood modiﬁcation with O(k) operations for fea-
ture curve computation, clipping and up-sampling (k =
|Np|). For that reason we don’t add timing tables here.
Figure 1 shows our sharp feature preserving reconstruc-
tion in comparison to Levin’s classical MLS.
Robustness to feature detection
The present technique depends on a prior feature points
detection method. The Gauss-map clustering method [30]
has been proven to be reliably enough.
It can be observed, for example that perfect (exact) input
data without noise leads to perfect recognition of feature
points and our resulting MLS surface is perfect as well, see
ﬁrst row of Figure 11. Any other feature detection method
can be used here instead.
It can be observed, furthermore that our ﬁtting method
is able to auto-correct the input data. In case where only
sparsely distributed feature points are detectable due to
noise in the input data for example, our method is able to
correct this failure up to a certain limit. Figure 10 shows the
octaﬂower data set. It is a noisy data set and it is not clean
7


--- Page 8 ---

Fig. 10. Noisy octaﬂower model. Left: feature points detected. Middle
and right: sharp reconstructions.
in the sense that data points are not exactly sampled on the
sharp features of the octaﬂower. Even though many feature
points are missing in the center and along the sharp curves
(Fig.10-left), the resulting surface only looses sharpness at
the center point, all other curves are sharply reconstructed.
Let us note also the smoothness of these curves. The reason
for this auto-correction property lies in the facts that on
one hand local and continuous smooth feature curves are
available and that on the other hand they are up-sampled
inside the local neighborhoods.
In Figure 12 another academic example is shown. It rep-
resents two bilinear surfaces with a common sharp edge.
The angle of the tangent planes varies along the edge from
acute (40◦) to obtuse (140◦). This case of varying angles
of a sharp feature is important in real life data sets and
could cause problems for feature detecting and reconstruct-
ing systems using global sets of parameters. The adaptive
feature detection method here ensures these cases to be
covered by our reconstruction method.
Robustness to noise
The beneﬁts of using local feature curves are also visible in
the presence of noise. Figure 11 shows an academic example
which has been perturbed. The noise is obtained by adding
to each point a random vector chosen in a ball whose radius
is 0.2% resp. 0.5% of the bounding box size of the model. In
Figure 11 we show the clean model without noise in the top
row, the 0.2% perturbed model in the middle row, and the
0.5% perturbed model in the bottom row. In each row the
performance of the feature detection, standard MLS and
our method are compared. As one can see, our sharp MLS
reconstruction uses the beneﬁt of the smoothing capabili-
ties of MLS in the ﬂat areas of the dataset while the sharp
edges are still reproduced.
Complex data sets
Further data sets have been tested. The fan disk model
in Figure 13, and the trim star model in Figure 1, are all
available through the AIM@SHAPE Shape repository 1 .
With the fan disk model, we would like to focus onto the
region where the sharp feature looses its sharpness. The
1 shapes.aimatshape.net
Fig. 11. Cube with hole. From left to right: original data set, smooth
MLS reconstruction, feature points detected, our method. First row:
clean data set without noise. Second row: data set perturbed with
0.2% random noise. Third row: data set perturbed with 0.5% random
noise.
Fig. 12. Two bilinear surfaces with a common sharp edge. The angle
of the tangent planes vary along the feature from acute (40◦) to
obtuse (140◦).
Fig. 13. Fandisk. Left: smooth standard MLS reconstruction. Middle:
feature points detected. Right: our method. Bottom: front view of a
smooth and sharp MLS reconstruction of the fandisk
transition behaves smoothly and natural.
The raw data set of a drill output from a CyberwareT M
scanner in Figure 14 shows from left to right the feature
points detection, shading of the original data, the classic
8


--- Page 9 ---

MLS reconstruction, the sharp feature preserving recon-
struction. This is a kind of worst case example. In the
closeup in Figure 15 the diﬀerence between the smooth and
the sharp reconstruction is clearly visible. The smooth re-
construction not only smooth the jagged edges of the orig-
inal data but also the whole cutting edge of the drill. The
sharp reconstruction is able to preserve the sharp cutting
edge.
Fig. 14. Drill raw data by CyberwareT M scanner. From left to right:
shading of the original data, the classic MLS reconstruction, feature
points detection, the sharp feature preserving reconstruction.
Fig. 15. Close up of the drill data; From left to right: shading of
the original data, the classic MLS reconstruction, the sharp feature
preserving reconstruction.
Comparison to known methods
We compared our method to other available feature sensi-
tive reconstruction methods. The cube with hole data set
was reconstructed with RIMLS [21] and APSS [11] using the
Meshlab 2 implementations. In Figure 16 the original data
set was taken. The same data set perturbed with 0.2% ran-
dom noise served in Figure 17. Figures 16(a),(b) show two
reconstructed surfaces by RIMLS and APSS. Naturally, in
these reconstructions the edges are smoothed out. In Fig-
ure 16(c), we show the surface that is reconstructed using
the new neighborhood modiﬁcation approach. The reason
is inherent to RIMLS characteristical behavior to produce
C2-continuous surfaces. In sharp regions curvature values
increase, but the surface remains curvature continuous. The
RIMLS reconstruction for the cube example took 170s, our
method took 190s. Hereby we used the parameter settings
proposed in Meshlab for sharp feature reconstruction. The
2 meshlab.sourceforge.net
APSS implementation in Meshlab unfortunately lacks the
ability to reconstruct sharp features. But it can be used as
an example to show the diﬀerence between a usual MLS
reconstruction method and reconstructions with sharp fea-
ture preserving.
Another issue is sensitivity to noise. Figure 17 compares
our method to others in the presence of noise. The noise is
much more visible in Figures 17(a) and (b). The reason for
that in RIMLS is that both eﬀects together (sharp feature
and smooth surface) can not be obtained. When setting the
parameters, one has to choose. We privileged sharp features
in Figure 17(a), but then the sensitivity to noise increased
and leads to this bumpy surface. A smoother surface can
be obtained, but then the features loose their sharpness
signiﬁcantly. Both Figures 16 and 17 clearly show the ad-
vantages of a sharp feature preserving method. The other
sharp feature reconstruction in [9] was not tested. An im-
plementation was not available and seems to be tricky.
Figure 18 is a further comparison. Here the angle of the
tangent planes along the sharp feature is continuously vary-
ing from acute to obtuse. The closeups give better insight.
Fig. 18. MLS reconstructions using RIMLS and our method, the
close up shows the diﬀerent behavior along sharp features
Limitations
In the presence of noise it is challenging to reconstruct
sharp features as well as smooth surfaces in general. The
present method is however able to enhance the sharpness
in comparison to other works, but to a limited extend.
Even though our method reduces the dependency from
parameter ﬁne tuning (most parameters for feature point
detection are adaptively computed) it still depends on the
appropriate choice of k. Similar to all MLS methods we
acknowledged this problem without being able to propose
a better solution.
6. Conclusion and future work
Our motivation was to show that it is possible to improve
MLS sharp feature reconstruction with an algorithm which
9


--- Page 10 ---

Fig. 16. MLS reconstructions using RIMLS — APSS — our method
Fig. 17. MLS reconstructions with low noise using RIMLS — APSS — our method
is not too tricky to implement. Simplicity and eﬃciency
characterizes our method. And what is more important, it
improves the smoothness of features curves of statistical
methods such as [9]. We presented a new augmented ver-
sion of the classic Moving Least Squares method to recon-
struct a surface from point cloud data with sharp features.
We not only segment the local neighborhood but modify it
before the moving least squares projection step to achieve
the sharp feature preservation. The method is capable of
handling sharp line-type features as well as corner features.
The resulting surface can be used in many applications such
as reverse engineering or quality control in a design process.
The parameters available for ﬁne-tuning of the result-
ing surface are the same as in the standard MLS approach.
The smoothness parameter h from the weight function in
(3) allows to adapt between interpolation and approxima-
tion. The size k of the local neighborhood governs about
the quality of the sharp feature. But the size of the neigh-
borhood is related to the computation time. In practice,
k > 20 is not reasonable.
It would ﬁnally be interesting to investigate the trade-oﬀ
between quality, simplicity and speed by comparing the
present method to methods using global optimization [9]
which are more robust but computationally more expen-
sive.
The use of global feature curves instead of local ones is an-
other interesting issue. We assume that computation time
will increase, but it would be interesting to pursue in the
future.
Another possible improvement of this present method could
be to guarantee C0-continuity. For this one has to guaran-
tee uniqueness of the local feature curve or to use the more
costly pre-computation of global feature curves. Here again
the question remains if the quality of the resulting surface
could be improved.
ACKNOWLEDGMENTS
The fandisk and trim-star are provided courtesy of MPII,
all by the AIM@SHAPE Shape Repository. The drill model
is courtesy of Sergei Azernikov, Siemens Corporate Re-
search, Princeton. This work was partially supported by
the IRTG 1131 of the DFG (German Research Founda-
tion), and by the Deutsche Forschungsgemeinschaft INST
248/72-1.
10


--- Page 11 ---

References
[1]
Alexa M., Behr J., Cohen-Or D., Fleishman S., Levin D.,
Silva T.: Point set surfaces. IEEE Visualization 2001 (2001).
[2]
Alexa, M., Behr, J., Cohen-or, D., Fleishman, S., Levin, D.,
and Silva, C. T. Computing and rendering point set surfaces.
IEEE Transactions on Visualization and Computer Graphics 9
(2003), 3–15.
[3]
Amenta, N. and Kil, Y.J. The Domain of a Point Set Surface.
Eurographics Symposium on Point-Based Graphics (2004).
[4]
Amenta, N. and Kil, Y.J. Deﬁning Point-Set Surfaces. In In
Proceedings SIGGRAPH 04 (2004).
[5]
Azernikov S., Miropolsky A., and Fischer A.
Surface
reconstruction of freeform objects based on multiresolution
volumetric method In ACM symposium on Solid modeling and
applications (SM ’03) (2003).
[6]
Daniels, J., Ha, L., Ochotta, T. and Silva, T. Robust Smooth
Feature Extraction from Point Clouds. In SMI (2005).
[7]
Daniels, J., Ha, L., Ochotta, T. and Silva, T.
Spline-
based feature curves from point-sampled geometry. The Visual
Computer 24(6), (2008).
[8]
Dey T. K. and Sun J.
An Adaptive MLS Surface for
Reconstruction with Guarantees.
Symposium on Geometry
Processing (SGP 2005).
[9]
Fleishman S., Cohen-Or D., Silva C.: Robust moving least-
squares ﬁtting with sharp features. ACM Trans. Graph. (2005),
37–49.
[10] Gross M., Pfister H-P. Point-Based Graphics. The Morgan
Kaufmann Series in Computer Graphics, (2007).
[11] Guennebaud G., Gross M.: Algebraic point set surfaces. In In
Proceedings SIGGRAPH 07 (2007).
[12] Guennebaud G., Germann M. and Gross M.:
Dynamic
Sampling
and
Rendering
of
Algebraic
Point
Set
Surfaces
Computer Graphics Forum 27, 2 (2008), 653–662.
[13] Hildebrand K., Polthier K., and Wardetzky K.
Smooth
feature lines on surface meshes. Proceedings of Symposium on
Geometric Processing, 2005.
[14] Hoppe H., DeRose T., Duchamp T., Halstead M., Jin H.,
McDonald J., Schweitzer J., Stuetzle W.: Piecewise Smooth
Surface Reconstruction ACM SIGGRAPH 1994 Proceedings ,
1994, 295-302.
[15] Hubeli A. and Gross M. Multiresolution feature extraction for
unstructured meshes. Proceedings of IEEE Visualization, pages
287–294, 2001.
[16] Kolluri R.: Provably good moving least squares. ACM SIAM
Symposium on Discrete Algorithms (2005).
[17] Levin D.: The approximation power of moving least-squares.
Mathematics of Computation 67 (1998), 1517–1531.
[18] Levin D.: Mesh-independent surface interpolation. Geometric
Modeling for Scientiﬁc Visualization (2003), 37–49.
[19] Lipman Y., Cohen-Or D., Levin D.:
Data-dependent MLS
for faithful surface approximation.
Symposium on Geometry
Processing (2008), 59–67.
[20] M´erigot, Q., Ovsjanikov, M., and Guibas, L. J.
Robust
voronoi-based curvature and feature estimation. In Symposium
on Solid and Physical Modeling (2009), W. F. Bronsvoort,
D. Gonsor, W. C. Regli, T. A. Grandine, J. H. Vandenbrande,
J. Gravesen, and J. Keyser, Eds., ACM, pp. 1–12.
[21] Oztireli C., Guennebaud G., Gross M.: Feature preserving
point
set
surfaces
based
on
non-linear
kernel
regression.
Computer Graphics Forum 28, 2 (2009).
[22] Gumhold, S., Wang, X., and McLeod, R. Feature extraction
from point clouds. Proceedings of 10th International Meshing
Roundtable V (2001).
[23] Ohtake Y., Belyaev A., Alexa M., Turk G. and Seidel H-
P.
Multi-level partition of unity implicits
In In Proceedings
SIGGRAPH 2003 (2003).
[24] Pauly, M., Keiser, R., and Gross, M.
Multi-scale feature
extraction on point-sampled surfaces.
Computer Graphics
Forum (2003).
[25] Reuter P., Joyot P., Trunzler J., Boubekeur T., Schlick
C.:
Surface reconstruction with enriched reproducing kernel
particle approximation.
Eurographics Symposium on Point-
Based Graphics (2005).
[26] Scheidegger, C.E., Fleishman, S., Silva, C.T. Triangulating
point set surfaces with bounded error. Symposium on Geometry
Processing (2005), 63-72.
[27] Schreiner,
J.,
Scheidegger,
C.,
Fleishman,
S.,
Silva,
C.
Direct
(re)meshing
for
eﬃcient
surface
processing.
Comput.Graph. Forum 25 (2006), 527536.
[28] Shen, C., O’Brien, J. F., and Shewchuk, J. R. Interpolating
and approximating implicit surfaces from polygon soup. ACM
Trans. Graph. 23 (August 2004), 896–904.
[29] Watanabe K. and Belyaev A. Detection of salient curvature
features on polygonal surfaces. Computer Graphics Forum, pages
385–392, 2001.
[30] Weber C., Hahmann S., Hagen H.: Sharp feature detection in
point clouds. In In Proceedings SMI 10 (2010).
[31] Weinkauf
T.
and
G¨unther
D.
Separatrix
Persistence:
Extraction of Salient Edges on Surfaces Using Topological
Methods.
Computer
Graphics
Forum
(Proc.
SGP
’09),
28(5):1519–1528, July 2009.
11
```

---

## Section 1.34: StarVector_ Generating Scalable Vector Graphics Code from Images and Text

Source File: StarVector_ Generating Scalable Vector Graphics Code from Images and Text.txt

### Paper Content:

```
# StarVector_ Generating Scalable Vector Graphics Code from Images and Text.pdf
# Converted: 2025-07-19 14:43:16
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/StarVector_ Generating Scalable Vector Graphics Code from Images and Text.pdf
# Output: /home/user/vekt/papers/pixel2physics/dot_txt/layer2_completion/StarVector_ Generating Scalable Vector Graphics Code from Images and Text.txt


--- Page 1 ---

arXiv:2312.11556v4  [cs.CV]  31 May 2025
StarVector: Generating Scalable Vector Graphics Code from Images and Text
Juan A. Rodriguez1,2,4 Abhay Puri1 Shubham Agarwal1, 2 Issam H. Laradji1, 5 Pau Rodriguez1
Sai Rajeswar1,2 David Vazquez1 Christopher Pal1,2,3 Marco Pedersoli1,4
1ServiceNow Research 2Mila - Quebec AI Institute 3Canada CIFAR AI Chair
4ÉTS, Montréal, Canada 5UBC, Vancouver, Canada
https://starvector.github.io/
Input Images to Vectorize
Generated SVG Code
Image Vectorization Output
rasterize
Text Instructions to SVG
A design of a storm cloud
An icon of a red cross inside a circle
An green and white emergency exit icon
    StarVector
Multimodal Language Model 
for SVG Generation
StarVector performs SVG generation from images or texts. 
It is trained on the large-scale       SVG-Stack Dataset,
and it is evaluated on      SVG-Bench
malware reverse engineering
netflow/enclave netflow
network protocol analysis
ssl/tls inspection
windows event logs
Network Intrusion Prevention
Network Segmentation
Network Segmentation
Network Intrusion Prevention
commonly used port
remote file copy
remote file copy
remote file copy
SHUTTLE
ARR 05-29
DEP 06-05
Ibiza Shuttle Bus
Ibiza Shuttle Bus
A
B
SHUTTLE
ARR 05-29
DEP 06-05
Ibiza Shuttle Bus
Ibiza Shuttle Bus
A
B
Text-Driven SVG Output
Figure 1. StarVector: A foundation model for SVG generation. StarVector’s multimodal architecture allows input from raster images
or text instructions. It converts a variety of raster visuals, including icons, logos, and technical diagrams, into vector graphics or generates
new SVGs from text. (Left) Inputs: raster images and text. (Right) Outputs: vectorized images (SVG)
Abstract
Scalable Vector Graphics (SVGs) are vital for modern im-
age rendering due to their scalability and versatility. Previ-
ous SVG generation methods have focused on curve-based
vectorization, lacking semantic understanding, often pro-
ducing artifacts, and struggling with SVG primitives beyond
path curves. To address these issues, we introduce StarVec-
tor, a multimodal large language model for SVG generation.
It performs image vectorization by understanding image
semantics and using SVG primitives for compact, precise
outputs. Unlike traditional methods, StarVector works di-
rectly in the SVG code space, leveraging visual understand-
ing to apply accurate SVG primitives. To train StarVector,
we create SVG-Stack, a diverse dataset of 2M samples that
enables generalization across vectorization tasks and pre-
cise use of primitives like ellipses, polygons, and text. We
address challenges in SVG evaluation, showing that pixel-
based metrics like MSE fail to capture the unique qualities
of vector graphics. We introduce SVG-Bench, a benchmark
across 10 datasets, and 3 tasks: Image-to-SVG, Text-to-
SVG generation, and diagram generation. Using this setup,
StarVector achieves state-of-the-art performance, produc-
ing more compact and semantically rich SVGs.
1. Introduction
Vector graphics represent an archetypal form of image rep-
resentation, where visual compositions are constituted by
scalable primitive shapes [33, 43, 50, 50]. For modern im-
age rendering, Scalable Vector Graphics (SVGs) [60] have
become the standard for representing vector graphics. The
SVG format [25] provides a comprehensive set of primi-
tives and styling options. At its core, the path represents
basic curves [60]. Combined with primitives like polygon
or ellipse, SVGs define complex designs precisely.
The task of image vectorization, i.e., converting pixel-
based raster images into SVGs, stands as a fundamen-
tal challenge in vector graphics. The main challenge lies
in developing methods that generalize across diverse do-
mains, from fonts and logos to complex illustrations and
diagrams [7, 8, 69, 70]. Traditional approaches often rely
on approximating images through multiple paths [43, 50,
51, 59, 87]. This strategy can be inefficient as shown in Fig.
2 (Right). For instance, a circle shape could be represented
as long path or, more precisely and compactly, as a sin-
gle <circle/> primitive. Similarly, text elements should
be vectorized as editable <text/> primitives to retain the
original textual content. This balance between curve-based
shape approximation and accurately recognizing primitives


--- Page 2 ---

Input Image
StarVector-8B
LIVE
VTracer
SVG Generation
Pixel Difference
DinoScore: 0.9969
MSE: 0.0024
SVG Generation
Pixel Difference
DinoScore: 0.9748
MSE: 0.0007
SVG Generation
Pixel Difference
DinoScore: 0.9248
MSE: 0.0063
SVG Generation
Pixel Difference
DinoScore: 0.9692
MSE: 0.0090
SVG Generation
Pixel Difference
DinoScore: 0.9186
MSE: 0.0012
SVG Generation
Pixel Difference
DinoScore: 0.9100
MSE: 0.0026
SVG Generation
Pixel Difference
DinoScore: 0.9969
MSE: 0.0024
SVG Generation
Pixel Difference
DinoScore: 0.9748
MSE: 0.0007
SVG Generation
Pixel Difference
DinoScore: 0.9248
MSE: 0.0063
SVG Generation
Pixel Difference
DinoScore: 0.9692
MSE: 0.0090
SVG Generation
Pixel Difference
DinoScore: 0.9186
MSE: 0.0012
SVG Generation
Pixel Difference
DinoScore: 0.9100
MSE: 0.0026
SVG Generation
Pixel Difference
DinoScore: 0.9969
MSE: 0.0024
SVG Generation
Pixel Difference
DinoScore: 0.9748
MSE: 0.0007
SVG Generation
Pixel Difference
DinoScore: 0.9248
MSE: 0.0063
SVG Generation
Pixel Difference
DinoScore: 0.9692
MSE: 0.0090
SVG Generation
Pixel Difference
DinoScore: 0.9186
MSE: 0.0012
SVG Generation
Pixel Difference
DinoScore: 0.9100
MSE: 0.0026
StarVector-8B
<svg width="300" height="300" ... version="1.1">
<rect x="110" y="10" width="160" height="230" fill="pink"
stroke="red"/>
<circle cx="160" cy="120" r="120" fill="tan" stroke="green"/>
<polygon points="110,20 280,125 175,280 5,180" fill="blue" />
</svg>
VTracer
<svg version="1.1" ... width="910" height="934">
<path d="M0 0 C300.3 0 600.6 0 910 0 ...
Z " fill="#928299"/>
<path d="M0 0 C1.484 0.99000001 ..." fill="#A95869"/>
<path d="M0 0 C1.320 0.659999999 2.6399 ..." fill="#A95869"/>
<path d="M0 0 C4.151643338387942 0 7 ..." fill="#A95869"/>
<path d="M0 0 C-0.6599999999999682 0 7 ..." fill="#A95869"/>
...
</svg>
Figure 2. (Left) Image Vectorization results using StarVector-8B, LIVE, and VTracer. Each row shows the input image, generated SVGs,
and pixel-wise difference maps to highlight accuracy. StarVector-8B better preserves shapes, color gradients, and text, despite minor
misplacements. Notably, MSE often misaligns with visual quality, e.g., regarding the ‘planet’ example, StarVector’s MSE (0.009) is higher
than LIVE’s (0.0012) and VTracer’s (0.0039), yet StarVector preserves the color gradient. For the ‘diagram’ example, StarVector preserves
the text. DinoScore better reflects these details, consistently favoring StarVector. (Right) Curve vs Primitive-based Vectorization. SVG
code generated by StarVector and VTracer for the given image. StarVector effectively leverages shape primitives, resulting in a compact
vectorization. VTracer decomposes the image into numerous paths, resulting in a more complex result with less semantic clarity.
has been previously unexplored and remains a core chal-
lenge in modern vectorization.
Vectorization approaches fall into two main categories:
traditional image processing and deep learning (DL)-based
methods. Image processing techniques [51, 59, 87] trace
vector curves via pixel-level analysis but often generate
overly complex representations with artifacts and lack se-
mantic understanding (Figure 2). DL methods [13, 14, 67,
89] improve vector modeling using latent variable mod-
els and differentiable rendering [43, 50] but struggle with
generalization and underutilize SVG primitives. This lim-
its their effectiveness for complex SVGs like scientific dia-
grams and hinders their application in modern multimodal
tasks such as text-driven SVG generation [24, 65, 72, 96].
Recent advancements in Multimodal Large Language
Models (MLLMs) [2, 45] have integrated visual under-
standing into transformer [84] architectures while demon-
strating strong code generation capabilities [1, 42, 49, 55].
Building on these developments, we introduce image vec-
torization as an inverse rendering and code generation task,
leveraging MLLMs to generate SVG code directly from in-
put images. This approach naturally encompasses the full
range of SVG primitives, enhancing both semantic under-
standing and generation capabilities (Table 6).
We introduce
StarVector, a foundational MLLM for
SVG generation.
StarVector processes both images and
text instructions to produce compilable SVG code, lever-
aging SVG primitives to accurately represent vector graph-
ics. We build upon the StarCoder works [42, 49] to connect
the code generation research with SVG generation. Fig-
ure 3 describes the model architecture. It integrates an im-
age encoder that projects images into visual tokens, and a
transformer language model for learning the relationships
between instructions, visual features, and SVG code se-
quences, to perform image vectorization (Image-to-SVG) or
text-driven SVG Generation (Text-to-SVG) tasks. StarVec-
tor performs primitive-aware vectorization through learned
semantic understanding, effectively leveraging SVG prim-
itives without explicit pixel reconstruction objectives. To
address the lack of large-scale SVG datasets for training
StarVector, we introduce SVG-Stack, with over 2M SVG
samples paired with rendered images and text descriptions.
Additionally, we find that conventional metrics like MSE
fail to adequately assess vector graphics quality, as demon-
strated in Figure 2. Instead, we propose DinoScore, a per-
ceptual similarity metric that better correlates with visual
human perception, and introduce SVG-Bench, an evalua-
tion framework spanning 10 datasets and 3 tasks: Image-to-
SVG, Text-to-SVG, and diagram generation.
Contributions
1. We introduce
StarVector, an MLLM capable of
image vectorization and text-driven SVG Generation,
uniquely preserving SVG primitives rather than produc-
ing multiple curves—a previously unexplored skill.
2. We create
SVG-Stack, a large-scale dataset with 2M
samples, supporting Image-to-SVG and Text-to-SVG.
3. We develop
SVG-Bench, an MLLM benchmark
with 10 datasets across 3 SVG tasks.
4. We conduct extensive experiments and evaluations, in-
cluding human assessments, demonstrating StarVector’s
advantages in primitive recognition and highlighting the
limitations of pixel-wise metrics for SVG evaluation.
2. Related Work
SVG Generation Methods. Early image vectorization ef-
forts [92] primarily relied on traditional techniques [21,
44, 95] such as segmentation and polynomial curve fit-
ting [43, 51, 59, 87]. With advancements in deep learn-


--- Page 3 ---

ing, new approaches have emerged. For instance, SVG-
VAE [47] employs a class-conditional Variational Autoen-
coder (VAE) [34] to predict a latent style vector and gener-
ate SVGs using an LSTM decoder [30]. DeepSVG [14] in-
troduces a hierarchical VAE architecture using transformers
for SVG path representation, while Im2Vec [67] converts
pixel images into latent representations that can be decoded
into paths with a recurrent neural network. However, these
methods are limited to basic primitives like paths, resulting
in a performance gap compared to traditional image pro-
cessing methods. Concurrently, BeyondPixels [99] utilizes
LLMs to predict command sequences in synthetic SVGs.
These trends highlight the potential of LLM models for im-
proved generalization in SVG generation.
Recent results in image generation using diffusion [29,
73] or autoregressive [23, 64, 96] models and the success
of text-to-image generation [6, 65, 72] have inspired the
research into text-conditioned SVG generation. VectorFu-
sion [33] leverages a strong text-to-image diffusion model
to find the SVG via iterative optimization. Some work has
focused on SVG editing from textual inputs [12].
CLI-
Passo [86] uses a CLIP distance loss to iteratively refine
SVG from sketches. These solutions are slow due to their it-
erative nature. IconShop [94] trains a BERT [20] model for
text-conditioned SVG generation of icons, but their method
is restricted to using paths.
In contrast, we train an MLLM on the inverse rendering
image vectorization task, leveraging visual understanding to
produce accurate SVG code with optimal use of primitives.
SVG Benchmarks and Datasets. Previous work on SVG
datasets and benchmarks has been limited. Existing Image-
to-SVG datasets mainly focus on fonts, icons, and emo-
jis [12–14, 67, 89, 94], offering limited variety for broader
SVG types and primitives. Recent text-driven SVG genera-
tion efforts [12, 16, 94] leverage large language models [56]
for synthetic captions, but limited dataset accessibility ham-
pers reproducibility.
Evaluation methods also face chal-
lenges. Existing pixel-based metrics like MSE [14, 47, 67]
fail to capture the fidelity and structure of SVGs, overlook-
ing aspects like line definition and primitive usage (Fig-
ure 2, illustrates how MSE can be misleading).
Recent
benchmarking efforts [54, 100] focus on caption-based gen-
eration or editing but remain limited in scope.
To address these gaps, we introduce SVG-Stack, a large-
scale dataset for diverse SVG generation tasks from both
images and text, and SVG-Bench, a unified benchmark with
datasets, tasks, and metrics tailored for SVGs. Together,
they provide a solid foundation for improving SVG model
training and evaluation.
Multimodal Language Models.
Large language mod-
els (LLMs) have achieved great success in natural lan-
guage processing (NLP) [10, 55, 61, 82–84], especially
in code generation tasks [9, 15, 17, 26, 32, 35, 42, 53].
Recent trends in Multimodal Large Language Modeling
(MLLM) [2, 45, 56, 88] have allowed infusing image un-
derstanding into text-only models, for tasks like visual-
question answering (VQA) [4] or image captioning [39–
41]. Current approaches define an image encoder for com-
puting visual tokens from images, that can be processed
by an LLM [2, 45, 56, 88]. Multiple works use a Vision
Transformer [22] backbone with pre-trained weights like
CLIP [62]. We get inspiration from these architectures for
training an MLLM on the tasks of image vectorization and
text-driven SVG Generation (also referred to as Image-to-
SVG and Text-to-SVG).
3.
SVG-Stack Dataset
To address the lack of large-scale SVG datasets for train-
ing foundational SVG models, we introduce SVG-Stack,
a dataset containing 2.1 million SVG samples for train-
ing, 108k for validation, and 5.7k for testing. Each SVG
is paired with its corresponding raster image and descrip-
tive text, making the dataset ideal for multi-modal learning.
SVG-Stack is sourced from The Stack [35], a diverse collec-
tion of code samples from various software languages. Our
selection builds upon the initial filtering and de-duplication
processes conducted in [3, 35, 42]. We perform additional
filtering to ensure non-duplicate SVG samples.
These samples come from publicly available reposito-
ries on GitHub, providing a rich variety of real-world SVGs
used across websites, graphic designs, and more. A key
advantage of our approach is the inclusion of SVGs with
diverse syntactic structures, varying templating approaches,
different header formats, framework-specific implementa-
tions, and full support for SVG primitives.
This diver-
sity significantly enhances model generalization capabili-
ties. This approach contrasts with previous datasets that
primarily focused on a narrow subset of SVG types [14,
47, 94].
SVG-Stack represents the first large-scale pre-
training dataset for SVG generation, with permissively li-
censed samples [35], bringing together a broad spectrum of
SVGs that closely mirror the diversity seen across the Web.
Table 5 and Figures [7, 8, 6] in Appendix 8 provide a com-
parison of available SVG datasets, showcasing the breadth
of test examples found in SVG-Stack.
Data Processing and Curation. Our data processing be-
gins with extracting SVG samples from TheStack [35],
followed by a comprehensive deduplication process based
on filenames, SVG code, and metadata.
We utilized
CairoSVG [36] for rasterization, removing samples that
produced completely white images. To optimize sequence
length and improve visualization, we eliminated comments
and XML headers from the SVG code.
Synthetic Generation of Text Instructions.
To enable


--- Page 4 ---

...
...
...
A folder icon with a 
white label on it.
Language Model (LLM) 
Input Text Instruction
SVG Code (ground truth)
Input Image
Tokenizer and  Embedder
Adapter
Image Encoder
Visual Tokens
Text Embeddings
SVG Embeddings
...
During training, the model observes sequences
of (text, svg) or (image, svg) pairs
During inference, the LLM predicts the
SVG code sequentially, conditioned on
visual tokens or text instructions
Generated SVG Code
Raster images
⊕
||
Linear
Vision Transformer
Adapter
Linear
Swish
LayerNorm
Linear
b) Computing Visual Tokens
a)       StarVector Architecture, Training and Inference 
Visual Tokens
Input Image (Patch Grid)
<svg xmlns="..." <path style="..." 
d="M 66.132812 0 L 15.234375..."/> 
</svg>
<svg xmlns="..."  
<path d="M 35.13 37.23..."/> 
</svg>
Figure 3. a) StarVector Architecture. Images are projected into visual tokens via an Image Encoder and Adapter, aligned with the
Language Model’s hidden space. Text conditioning uses the LLM’s tokenizer and embedder. ⊕denotes mutually exclusive addition
of image or text features to SVG tokens, while ∥indicates sequence concatenation. During training, the model maps token sequences
(visual or textual) to SVG code, and at inference, SVG code is generated sequentially. b) Visual Token Computation. We use a Vision
Transformer (ViT) to process image patches into hidden features, which are projected through a non-linear Adapter to form visual tokens.
Text-to-SVG generation, we augment our 2M samples with
synthetically generated textual captions describing the ras-
terized images. We employ open-source image captioning
models, specifically BLIP2 [41] and Llava [45], resulting
in a comprehensive dataset of 4 million paired textual cap-
tions and SVGs. Detailed information about the annotation
process is available in Appendix 8.4.
Data Augmentation. We implement several SVG-specific
augmentation techniques to enhance model robustness.
Rather than storing static-resolution raster images, we per-
form rasterization during data collation, enabling dynamic
SVG transformations without significant training overhead.
Our augmentation pipeline includes modifications to image
resolution, rotation, translation, scaling, and color proper-
ties. We leverage open-source libraries svgpathtools [74]
and bs4 [68] for SVG manipulation.
Ablation studies
demonstrate that these augmentation techniques signifi-
cantly improve model performance.
4.
StarVector
4.1. Architecture
StarVector is a foundational MLLM for SVG generation,
trained for Image-to-SVG and Text-to-SVG Generation.
It effectively uses image semantics to identify and utilize
shape primitives, producing precise and compact SVG out-
puts. This capability emerges as the model learns to predict
SVG code directly, while it is trained on a large SVG dataset
(SVG-Stack). We frame the task of image vectorization as
an inverse rendering and code generation problem, where
images are represented as visual tokens that precede the se-
quence of SVG code tokens. During generation, an image is
converted into visual tokens, prompting StarVector to pre-
dict the SVG code following a vectorization trigger token.
As depicted in Figure 3, our architecture employs a large
language model (LLM) fϕ and an image encoder eθ, pa-
rameterized by trainable parameters ϕ and θ, respectively.
Visual Tokens. For each input image xv, the Image En-
coder provides flattened grid features zv = eθ(xv). A Vi-
sion Transformer (ViT) [62] is utilized to define eθ(·). All
features from the last transformer layer are used, as we re-
quire high visual expressivity. LLM Adapter gφ is devoted
to projecting the visual features zv into the dimensionality
of the LLM, creating visual tokens hv:
hv = gφ(zv), where zv = eθ(xv)
(1)
As depicted in Figure 3-b, the LLM Adapter gφ performs
a non-linear projection of the image embeddings into the
LLM embedding space, producing a set of visual token
embeddings (or visual tokens). This transformation aligns
the image representations with the LLM, effectively bridg-
ing the visual and SVG code modalities. The Adapter is


--- Page 5 ---

composed of a sequence of fully connected (FC) layers
with Swish [63] activation function and Layer Normaliza-
tion [38]. We initialize the adapter parameters φ using the
normal distribution. We initialize the image encoder param-
eters θ using the public weights of CLIP ViT-L/14 [62].
gφ(zv) = LayerNorm(WL · Swish(Wh · zv))
(2)
Language Modeling. For each image xv, we define xt as
its corresponding textual caption and xs as the SVG code.
For each sample, we have a tuple (xv, xt, xs). Training
sequences are constructed by concatenation: (xv, xs) for
Image-to-SVG tasks and (xt, xs) for Text-to-SVG tasks. To
simplify, we use xc to represent the conditioning sequence,
which is either xv or xt, depending on the task.
As depicted in Figure 3, both textual xt and SVG xs
sequences are processed by a tokenizer and an embedder,
which converts text strings to tokens and tokens to embed-
dings. This embedding operation has trainable parameters.
We model the conditional probability of SVG sequences as:
p(xs | xc) =
L
Y
i=1
p(xs,i | xs,<i, xc),
(3)
where L is the length of the SVG sequence xs. This for-
mulation allows us to use a generative objective with next-
token cross-entropy over the SVG sequence. During infer-
ence, only the conditioning sequence xc is given as input,
and the SVG code is sampled autoregressively until the end-
ing <svg-end> token is reached.
StarVector Variants. We define two variants of StarVector
to explore its scaling behavior, varying in image resolution,
LLM parameter count, and context length.
1. StarVector-1B is initialized with a CLIP ViT-B/32 [62]
image encoder, processing images at a 224×224 reso-
lution to produce 257 visual tokens.
The LLM uses
StarCoder-1B [42] with a context length of 8192 tokens.
2. StarVector-8B
employs
a
SigLip
(siglip-so400m-
patch14-384) [97] image encoder, processing images at
a 384×384 resolution to yield 576 visual tokens. This
model utilizes StarCoder2-7B, offering an expanded
context length of 16k tokens. With a total of 8B param-
eters, we investigate how scaling can yield more precise
and compact SVGs due to higher image resolution and
enhanced LLM capacity.
Inference. During generation, StarVector processes an in-
put image or text, converting it into tokens, and then pre-
dicts subsequent tokens auto-regressively to produce SVG
code. This code is rasterized with CairoSVG [36], gener-
ating an image. A key challenge during generation is en-
suring both 1) syntactically valid SVGs and 2) SVGs opti-
mized for compactness and precision. Decoding introduces
stochasticity, and limited context length can result in incom-
plete SVGs. We find that StarVector is sensible to tem-
perature, length penalty, and logit bias, i.e., adding more
weight to certain tokens, like the <svg-end> token which
encourages valid SVG outputs (properly closed SVG). We
introduce them as inference hyperparameters. To further
improve quality, we generate k samples with varied param-
eters, ranking them based on DinoScore.
5.
SVG-Bench: Evaluation Suite for SVG
In response to the limited benchmarks for SVG evaluation
and to unify evaluation practices [14, 47, 67], we intro-
duce SVG-Bench. This benchmark assesses SVG methods
across Image-to-SVG, Text-to-SVG, and Diagram Gener-
ation tasks, encompassing various SVG types from simple
graphics like icons and fonts to complex diagrams with mul-
tiple primitives. Dataset statistics are presented in Table 5.
5.1. Tasks and Benchmarks
SVG-Bench focuses on the following tasks. For details on
dataset curation and visual examples, see Appendix 8.
1. Image-to-SVG: This task evaluates converting images
to SVGs across varying complexities.
We introduce
SVG-Fonts, SVG-Emoji, SVG-Icons, and SVG-Stack, in-
creasing in complexity. While prior works used simi-
lar datasets [14, 47, 67], access has often been unclear.
We provide standardized train, validation, and test splits,
along with simplified versions containing only paths for
compatibility with certain methods.
2. Text-to-SVG: We evaluate the model’s ability to gener-
ate SVGs from text instructions. This includes the SVG-
Stack test set, which provides two textual descriptions
per image, and the SVG-FIGR dataset, which is sourced
from FIGR-8-SVG [16, 94] dataset, enabling the gener-
ation of simpler (path-only) icons from text.
3. Diagram Generation: We assess the model’s perfor-
mance in generating diagrams, a specific type of SVG
that involves text, rectangles, and arrow primitives. For
this, we create the SVG-Diagrams test set by extracting
samples from SVG-Stack, including textual captions.
Evaluation Metrics.
To compute benchmark scores, we
define the following metrics: For image vectorization tasks,
we use Mean Squared Error (MSE), Structural Similarity
Index (SSIM) [90, 91], and Learned Perceptual Image Patch
Similarity (LPIPS) [98]. To address the limitations of pixel-
based metrics (see Figure 2), we propose DinoScore [57],
which computes L2 distance between DinoV2 features. To-
ken Length (Tokens) measures the size of the SVG samples.
We use the StarCoder [42] tokenizer to tokenize SVG code
and compute the average length. These metrics are also
used for Diagram Generation. For Text-to-SVG, we build


--- Page 6 ---

Table 1. Image Vectorization Results. Image processing methods (denoted by †) excel in pixel-based metrics (SSIM and MSE) while
StarVector models lead in semantic-based metrics (DinoScore and LPIPS). StarVector shows better performance in SVG-Stack, SVG-
Fonts, and SVG-Icons but underperforms in SVG-Emoji due to limited training data. We highlight the token lengths of generated SVGs
from different models, comparing them to the average token count in test examples (shown in gray below the “Tokens” header). Token
counts close to the actual number are marked in green, while the largest counts are highlighted in red. Notably, methods that perform well
on MSE tend to utilize a large number of tokens, whereas StarVector shows remarkable compression.
SVG-Stack
SVG-Fonts
SVG-Icons
SVG-Emoji
Method
Dino ↑
LPIPS ↓
SSIM ↑
MSE ↓
Tokens
2,822
Dino ↑
LPIPS ↓
SSIM ↑
MSE ↓
Tokens
3,136
Dino ↑
LPIPS ↓
SSIM ↑
MSE ↓
Tokens
3,305
Dino ↑
LPIPS ↓
SSIM ↑
MSE ↓
Tokens
5,618
AutoTrace†
0.942 0.063
0.930 0.009
59.1k 0.954 0.025
0.968 0.006
30.8k 0.946 0.053
0.937 0.014
56.7k 0.975 0.077
0.902 0.011
94.0k
Potrace†
0.898 0.139
0.856 0.036
7.5k
0.967 0.009
0.988 0.002
4.2k
0.972 0.023
0.973 0.004
12.0k
0.882 0.267
0.780 0.067
9.7k
VTracer†
0.954 0.062
0.883 0.010
9.7k
0.964 0.027
0.888 0.009
4.5k
0.940 0.062
0.914 0.017
20.0k
0.981 0.074
0.894 0.008
15.7k
Im2Vec
0.692 0.291
0.765 0.181
4.3k
0.733 0.140
0.837 0.135
4.3k
0.754 0.150
0.889 0.055
4.3k
0.732 0.465
0.774 0.126
3.8k
LIVE
0.934 0.059
0.953 0.003
18.3k
0.956 0.013
0.977 0.001
18.3k
0.959 0.035
0.973 0.004
18.2k
0.969 0.060
0.958 0.002
18.3k
DiffVG
0.810 0.156
0.856 0.019
19.7k
0.821 0.051
0.959 0.007
19.7k
0.952 0.056
0.956 0.015
19.8k
0.814 0.242
0.776 0.034
19.7k
GPT-4-V
0.852 0.317
0.711 0.195
443
0.842 0.198
0.749 0.197
279
0.848 0.238
0.755 0.144
524
0.850 0.344
0.712 0.170
672
StarVector-1B 0.926 0.149
0.840 0.078
3.7k
0.978 0.022
0.961 0.022
2.4k
0.975 0.040
0.931 0.026
3.5k
0.929 0.217
0.820 0.063
4.8k
StarVector-8B 0.966 0.058
0.947 0.026
5.3k
0.982 0.030
0.946 0.029
3.0k
0.984 0.035
0.975 0.012
2.8k
0.943 0.193
0.829 0.052
6.7k
SVG-Stacksim
SVG-Fontssim
SVG-Iconssim
SVG-Emojisim
AutoTrace†
0.945 0.063
0.922 0.018
74.1k 0.928 0.125
0.886 0.050
1.5k
0.915 0.111
0.901 0.044
1.3k
0.963 0.090
0.874 0.029
134.8k
Potrace†
0.970 0.022
0.968 0.006
12.2k
0.991 0.012
0.983 0.003
7.7k
0.983 0.025
0.976 0.004
10.4k
0.992 0.037
0.951 0.008
26.7k
VTracer†
0.935 0.061
0.914 0.020
16.0k
0.946 0.040
0.939 0.013
12.7k
0.945 0.043
0.946 0.012
11.9k
0.948 0.063
0.911 0.021
16.2k
Im2Vec
0.725 0.186
0.892 0.046
4.3k
0.857 0.184
0.833 0.096
284
0.860 0.207
0.792 0.129
453
0.695 0.179
0.898 0.045
3.7k
LIVE
0.963 0.039
0.974 0.005
18.3k
0.975 0.016
0.991 0.001
18.3k 0.961 0.030
0.978 0.003
18.2k 0.958 0.075
0.934 0.014
18.2k
DeepSVG
0.907 0.192
0.835 0.071
1.5k
0.928 0.125
0.886 0.050
1.5k
0.915 0.111
0.901 0.044
1.3k
0.822 0.209
0.841 0.074
1.8k
GPT-4 V
0.874 0.226
0.768 0.137
329
0.946 0.040
0.939 0.013
12.7k
0.945 0.043
0.946 0.012
11.9k
0.852 0.212
0.802 0.105
424
StarVector-1B 0.954 0.089
0.870 0.053
2.9k
-
-
-
-
-
-
-
-
-
-
0.977 0.073
0.897 0.043
3.0k
StarVector-8B 0.977 0.074
0.888 0.045
2.1k
0.993 0.012
0.970 0.009
1.3k
0.990 0.024
0.947 0.017
2.7k
0.903 0.163
0.791 0.091
3.2k
Table 2. Image Vectorization on SVG-Diagrams. StarVector
outperforms LIVE in DinoScore, LPIPS, and SSIM, while LIVE
ranks best in MSE. However, visual results (Fig. 2) confirm that
StarVector is the only effective method for SVG generation, under-
scoring the misalignment of MSE. Additionally, it remains com-
petitive in terms of token length.
SVG-Diagrams
Method
Dino ↑LPIPS ↓SSIM ↑MSE ↓Tokens ↓
Autotrace†
0.874
0.114
0.883
0.013
90.6k
Potrace†
0.875
0.153
0.862
0.026
22.6k
VTracer†
0.882
0.116
0.877
0.011
15.8k
LIVE
0.870
0.121
0.859
0.010
18.3k
DiffVG
0.822
0.170
0.859
0.019
19.8k
StarVector-1B
0.943
0.107
0.862
0.032
9.5k
StarVector-8B
0.959
0.093
0.890
0.027
-
on text-to-image literature [62, 66, 73, 81] and prior Text-
to-SVG methods [12, 94], using FID [81], FID-CLIP [94],
and CLIP Score [62] to measure image-text alignment.
6. Experiments and Results
We train StarVector (1B and 8B versions) on the inverse
rendering vectorization task using SVG-Stack dataset. We
Table 3. Usage of Paths and Inference Time. We ablate the use
of the path primitive across models. LIVE and DiffVG allow set-
ting the number of paths, while VTracer, Autotrace, and StarVec-
tor dynamically determine them. More paths generally improve
performance. LIVE achieves the best pixel metrics, but StarVector
excels in DinoScore. We also report average inference time per
sample, noting that LIVE is significantly slower. Results are aver-
aged across SVG-Bench datasets.
Method
# Paths Dino ↑LPIPS ↓SSIM ↑MSE ↓Time (s) ↓
LIVE
5
0.898
0.137
0.881
0.013
190
16
0.930
0.064
0.937
0.006
290
32
0.937
0.057
0.944
0.004
650
60
0.939
0.053
0.947
0.003
1,412
DiffVG
15
0.781
0.205
0.819
0.066
21
60
0.844
0.135
0.881
0.018
31
120
0.895
0.107
0.907
0.013
45
Vtracer
18
0.942
0.067
0.892
0.011
0.09
Potrace
-
0.937
0.109
0.897
0.024
10
AutoTrace
3k
0.951
0.065
0.924
0.010
1
StarVector-1B
8
0.952
0.107
0.883
0.044
41
StarVector-8B
10
0.963
0.085
0.911
0.031
74
then fine-tune on the other datasets mentioned in Section 5,
as well as for Text-to-SVG task. We evaluate StarVector and
other methods on SVG-Bench, focusing on quantitative and
qualitative performance, SVG primitive use, and compact-


--- Page 7 ---

Table 4.
Results on Text-to-SVG: We report FID, FID-CLIP
(FID-C), and CLIP Score (CLIP) on SVG-Stack and SVG-FIGR.
StarVector models outperform all previous baselines in all metrics.
We observe improvement when scaling StarVector from 1B to 8B.
Results for DeepSVG+GAN [14, 28] and Bert on SVG-FIGR are
extracted from [94], while StarVector is trained on the same data
and splits. Missing scores are due to limited model access.
SVG-FIGR
SVG-Stack
Method
FID ↓FID-C ↓CLIP ↑FID ↓FID-C ↓CLIP ↑
DeepSVG+GAN
-
12.011
21.783
-
-
-
Bert
-
35.104
22.035
-
-
-
IconShop
-
4.657
25.746
-
-
-
GPT-4
32.953
19.026
26.088
37.381
9.664
26.228
CodeLlama
29.002
22.536
26.227
34.777
11.152
25.532
StarVector-1B
15.263
3.834
26.342
28.374
6.482
29.372
StarVector-8B
10.067
1.308
27.366
25.828
4.645
31.307
ness. The following sections present the experimental setup
and results. Ablation studies on the architecture, data aug-
mentation, and generation can be found in Appendix 10.1.
Baselines. Baselines. For our comparisons, we consider
the following model baselines: For Image-to-SVG, we eval-
uate top image processing algorithms such as Potrace [59],
Vtracer [87], and Autotrace [51].
We also report on
deep learning methods including DeepSVG (<5M parame-
ters) [14], Im2Vec (<5M) [67], and MLLMs like GPT-4 Vi-
sion (>100B) [56]. For Text-to-SVG, we consider methods
like IconShop (>1B), DeepSVG+GAN (<5M), and BERT
(>1B), as outlined in [94]. Additionally, we evaluate LLMs
such as CodeLlama-70b [82] and GPT-4 (>100B) [55]. For
more details, see Appendix 9, and Table 6 summarizes the
SVG capabilities of all methods.
Training and Inference. We train StarVector-1B with a
batch size of 128 and StarVector-8B with 512, using a learn-
ing rate of 1e-5 and the AdamW optimizer. StarVector-1B
took 7 days on 8 A100 GPUs, while StarVector-8B took 10
days on 64 H100 GPUs, both completing 2 epochs. Full
training details are in Appendix 11.2. During inference,
we generate k = 5 SVG outputs with temperatures ranging
from 0 to 1, selecting the one with the highest DinoScore.
We set a logit bias of 10 for the <svg-end> token and apply
top-p nucleus sampling (0.9) with a length penalty of -0.5,
using beam search with a size of 1. We utilize vLLM [37]
as the backend to accelerate the generation process.
Experimental Setup. StarVector models are initialized as
described in Section 4 and all their weights are unfrozen
for the Image-to-SVG task using SVG-Stack. We then fine-
tune these models on the train sets of SVG-Emoji, SVG-
Fonts, and SVG-Icons, including their simplified versions.
For the Text-to-SVG task, the image encoder is disregarded.
Only the LLM is trained on SVG-Stack and FIRG-SVG.
We perform a comprehensive evaluation on SVG-Bench,
analyzing the performance of baseline models mentioned
above, alongside the StarVector models. We reproduce all
baselines by training them on the benchmark’s training sets,
within the limits of their availability.
6.1. Main Results
Image Vectorization. Table 1 presents vectorization scores
of models across 8 benchmarks. StarVector outperforms all
other models in terms of DinoScore, achieving the highest
score on six out of the eight benchmarks, thereby establish-
ing its dominance in this metric. Results for the other met-
rics—LPIPS, SSIM, and MSE—are more varied. However,
LIVE demonstrates superior performance on the SSIM and
MSE metrics across all datasets. It is important to note that
models that perform well on MSE tend to generate larger
SVG files, as indicated by the number of tokens in the SVG
code. This creates overly complex vectors with visible arti-
facts (see Fig. 4). Specifically, LIVE’s SVG outputs average
around 18k tokens, while VTracer varies between 4.5k and
20k tokens. In contrast, StarVector averages approximately
3k tokens, closely matching the ground truth token count.
This efficiency is primarily attributed to StarVector’s effec-
tive use of SVG primitives, as illustrated in Figure 2 (right).
Figure 4 presents qualitative results of models, on SVG-
Stack and SVG-Diagrams. In terms of visual quality, LIVE,
VTracer, and AutoTrace produce artifacts, especially when
dealing with small details. Potrace offers more sharp re-
sults, but it is monochromatic. StarVector-8B produces su-
perior results on shape preservation and definition. More re-
sults and qualitative samples can be found in Appendix 10.
Why is pixel-based MSE not well-suited?
Our results
reveal significant limitations of pixel-based metrics (MSE,
SSIM, LPIPS) for SVG quality assessment. While StarVec-
tor shows worse MSE scores compared to other methods
in Table 2, visual inspection of the results in Figures 4
and 14 (especially on diagrams) demonstrates StarVector’s
superior quality. This discrepancy is particularly evident in
the ‘planet’ example (Figure 2), where StarVector preserves
color gradients and line definition, yet receives poor MSE
scores due to minor pixel misalignments. Human evalu-
ation (Fig. 5) shows a preference for StarVector’s outputs
over other models, contradicting these metrics.
Pixel-based metrics have limitations due to (a) the preva-
lence of constant background colors (allowing even empty
SVGs to score reasonably), (b) sensitivity to small spatial
misalignments, and (c) inability to capture non-smooth arti-
facts at corners. DinoScore proves more reliable and aligns
consistently with visual quality, scoring StarVector higher
on well-formed samples while penalizing poorly formatted
ones, thanks to robust self-supervised training [57].


--- Page 8 ---

Input Image
StarVector-8B
LIVE
VTracer
PoTrace
AutoTrace
DinoScore: 0.9990
MSE: 0.0014
DinoScore: 0.9326
MSE: 0.0008
DinoScore: 0.9593
MSE: 0.0072
DinoScore: 0.7909
MSE: 0.1709
DinoScore: 0.9628
MSE: 0.0069
DinoScore: 0.9663
MSE: 0.0116
DinoScore: 0.8512
MSE: 0.0013
DinoScore: 0.9454
MSE: 0.0047
DinoScore: 0.7648
MSE: 0.1857
DinoScore: 0.9619
MSE: 0.0048
DinoScore: 0.9825
MSE: 0.0153
DinoScore: 0.9523
MSE: 0.0015
DinoScore: 0.9674
MSE: 0.0061
DinoScore: 0.8882
MSE: 0.0122
DinoScore: 0.9872
MSE: 0.0038
DinoScore: 0.9986
MSE: 0.0084
DinoScore: 0.8559
MSE: 0.0088
DinoScore: 0.9199
MSE: 0.0141
DinoScore: 0.9427
MSE: 0.0536
DinoScore: 0.9043
MSE: 0.0182
DinoScore: 0.9946
MSE: 0.0155
DinoScore: 0.9836
MSE: 0.0026
DinoScore: 0.8662
MSE: 0.0113
DinoScore: 0.9678
MSE: 0.0031
DinoScore: 0.8057
MSE: 0.0131
DinoScore: 0.9849
MSE: 0.0384
DinoScore: 0.9321
MSE: 0.0167
DinoScore: 0.8833
MSE: 0.0199
DinoScore: 0.9787
MSE: 0.0115
DinoScore: 0.9180
MSE: 0.0253
DinoScore: 0.9990
MSE: 0.0014
DinoScore: 0.9326
MSE: 0.0008
DinoScore: 0.9593
MSE: 0.0072
DinoScore: 0.7909
MSE: 0.1709
DinoScore: 0.9628
MSE: 0.0069
DinoScore: 0.9663
MSE: 0.0116
DinoScore: 0.8512
MSE: 0.0013
DinoScore: 0.9454
MSE: 0.0047
DinoScore: 0.7648
MSE: 0.1857
DinoScore: 0.9619
MSE: 0.0048
DinoScore: 0.9825
MSE: 0.0153
DinoScore: 0.9523
MSE: 0.0015
DinoScore: 0.9674
MSE: 0.0061
DinoScore: 0.8882
MSE: 0.0122
DinoScore: 0.9872
MSE: 0.0038
DinoScore: 0.9986
MSE: 0.0084
DinoScore: 0.8559
MSE: 0.0088
DinoScore: 0.9199
MSE: 0.0141
DinoScore: 0.9427
MSE: 0.0536
DinoScore: 0.9043
MSE: 0.0182
DinoScore: 0.9946
MSE: 0.0155
DinoScore: 0.9836
MSE: 0.0026
DinoScore: 0.8662
MSE: 0.0113
DinoScore: 0.9678
MSE: 0.0031
DinoScore: 0.8057
MSE: 0.0131
DinoScore: 0.9849
MSE: 0.0384
DinoScore: 0.9321
MSE: 0.0167
DinoScore: 0.8833
MSE: 0.0199
DinoScore: 0.9787
MSE: 0.0115
DinoScore: 0.9180
MSE: 0.0253
DinoScore: 0.9990
MSE: 0.0014
DinoScore: 0.9326
MSE: 0.0008
DinoScore: 0.9593
MSE: 0.0072
DinoScore: 0.7909
MSE: 0.1709
DinoScore: 0.9628
MSE: 0.0069
DinoScore: 0.9663
MSE: 0.0116
DinoScore: 0.8512
MSE: 0.0013
DinoScore: 0.9454
MSE: 0.0047
DinoScore: 0.7648
MSE: 0.1857
DinoScore: 0.9619
MSE: 0.0048
DinoScore: 0.9825
MSE: 0.0153
DinoScore: 0.9523
MSE: 0.0015
DinoScore: 0.9674
MSE: 0.0061
DinoScore: 0.8882
MSE: 0.0122
DinoScore: 0.9872
MSE: 0.0038
DinoScore: 0.9986
MSE: 0.0084
DinoScore: 0.8559
MSE: 0.0088
DinoScore: 0.9199
MSE: 0.0141
DinoScore: 0.9427
MSE: 0.0536
DinoScore: 0.9043
MSE: 0.0182
DinoScore: 0.9946
MSE: 0.0155
DinoScore: 0.9836
MSE: 0.0026
DinoScore: 0.8662
MSE: 0.0113
DinoScore: 0.9678
MSE: 0.0031
DinoScore: 0.8057
MSE: 0.0131
DinoScore: 0.9849
MSE: 0.0384
DinoScore: 0.9321
MSE: 0.0167
DinoScore: 0.8833
MSE: 0.0199
DinoScore: 0.9787
MSE: 0.0115
DinoScore: 0.9180
MSE: 0.0253
DinoScore: 0.9990
MSE: 0.0014
DinoScore: 0.9326
MSE: 0.0008
DinoScore: 0.9593
MSE: 0.0072
DinoScore: 0.7909
MSE: 0.1709
DinoScore: 0.9628
MSE: 0.0069
DinoScore: 0.9663
MSE: 0.0116
DinoScore: 0.8512
MSE: 0.0013
DinoScore: 0.9454
MSE: 0.0047
DinoScore: 0.7648
MSE: 0.1857
DinoScore: 0.9619
MSE: 0.0048
DinoScore: 0.9825
MSE: 0.0153
DinoScore: 0.9523
MSE: 0.0015
DinoScore: 0.9674
MSE: 0.0061
DinoScore: 0.8882
MSE: 0.0122
DinoScore: 0.9872
MSE: 0.0038
DinoScore: 0.9986
MSE: 0.0084
DinoScore: 0.8559
MSE: 0.0088
DinoScore: 0.9199
MSE: 0.0141
DinoScore: 0.9427
MSE: 0.0536
DinoScore: 0.9043
MSE: 0.0182
DinoScore: 0.9946
MSE: 0.0155
DinoScore: 0.9836
MSE: 0.0026
DinoScore: 0.8662
MSE: 0.0113
DinoScore: 0.9678
MSE: 0.0031
DinoScore: 0.8057
MSE: 0.0131
DinoScore: 0.9849
MSE: 0.0384
DinoScore: 0.9321
MSE: 0.0167
DinoScore: 0.8833
MSE: 0.0199
DinoScore: 0.9787
MSE: 0.0115
DinoScore: 0.9180
MSE: 0.0253
DinoScore: 0.9990
MSE: 0.0014
DinoScore: 0.9326
MSE: 0.0008
DinoScore: 0.9593
MSE: 0.0072
DinoScore: 0.7909
MSE: 0.1709
DinoScore: 0.9628
MSE: 0.0069
DinoScore: 0.9663
MSE: 0.0116
DinoScore: 0.8512
MSE: 0.0013
DinoScore: 0.9454
MSE: 0.0047
DinoScore: 0.7648
MSE: 0.1857
DinoScore: 0.9619
MSE: 0.0048
DinoScore: 0.9825
MSE: 0.0153
DinoScore: 0.9523
MSE: 0.0015
DinoScore: 0.9674
MSE: 0.0061
DinoScore: 0.8882
MSE: 0.0122
DinoScore: 0.9872
MSE: 0.0038
DinoScore: 0.9986
MSE: 0.0084
DinoScore: 0.8559
MSE: 0.0088
DinoScore: 0.9199
MSE: 0.0141
DinoScore: 0.9427
MSE: 0.0536
DinoScore: 0.9043
MSE: 0.0182
DinoScore: 0.9946
MSE: 0.0155
DinoScore: 0.9836
MSE: 0.0026
DinoScore: 0.8662
MSE: 0.0113
DinoScore: 0.9678
MSE: 0.0031
DinoScore: 0.8057
MSE: 0.0131
DinoScore: 0.9849
MSE: 0.0384
DinoScore: 0.9321
MSE: 0.0167
DinoScore: 0.8833
MSE: 0.0199
DinoScore: 0.9787
MSE: 0.0115
DinoScore: 0.9180
MSE: 0.0253
Figure 4.
Image vectorization results on SVG-Stack (first 3
rows) and SVG-Diagrams (last 3 rows) test sets.
Diagram Generation.
Table 2 shows results on SVG-
Diagrams, and Figures 4 and 14 provide visual examples.
Results highlight that StarVector is the only method ca-
pable of performing diagram generation, as it uniquely
applies the required primitives like rectangles, arrows, and
text, whereas other methods produce blobs and curves that
attempt to replicate structure and color. Metrics such as
DinoScore, LPIPS, capture this advantage, while MSE and
SSIM remain poorly aligned. Human evaluations further
confirmed the preference for StarVector’s outputs.
SVG Primitive Usage.
StarVector produces more com-
pact SVGs by optimally using SVG primitives. This inno-
vation combines visual semantic understanding and shape
composition with direct SVG code generation, enabling de-
composition into basic primitives. As shown in Figure 2
(right), StarVector efficiently represents shapes using prim-
itives, while VTracer relies on a large collection of paths.
See Appendix 10.4 for more examples (Figure 10) and SVG
tag distribution analysis (Figure 30).
Human Evaluation.
We conducted human evaluations
comparing StarVector-8B with baseline results, involving
participants from diverse backgrounds screened for con-
flicts of interest. Results in Figure 5 show a strong pref-
erence for StarVector-8B in all settings, particularly in the
SVG-Diagrams tasks, highlighting a disconnect between
pixel-based metrics (MSE, SSIM) and visual perception of
SVG. While baselines prioritize pixel-perfect reconstruc-
tion, humans prefer StarVector’s sharp, well-defined shapes
and effective use of primitives (Figure 30). Spearman cor-
relations between model metrics (MSE and DINO) and hu-
man evaluation show weak correlations for MSE (0.0596
StarVector-8B vs
Vtracer
StarVector-8B vs
AutoTrace
StarVector-8B vs
LIVE-32P
53.8%
63.4%
54.9%
12.9%
11.1%
14.2%
12.4%
10.7%
13.9%
20.8%
14.8%
17.1%
SVG-Stack
0
20
40
60
80
100
StarVector-8B vs
Vtracer
StarVector-8B vs
AutoTrace
StarVector-8B vs
LIVE-32P
58.6%
73.5%
71.1%
30.3%
15.9%
18.9%
8.5%
10.6%
7.8%
SVG-Diagrams
Percentage (%)
Win
Tie
Both Good
Lose
Figure 5.
Human evaluation.
StarVector-8B was evaluated
against top-performing baselines: LIVE, AutoTrace, and Vtracer.
Results consistently showed a strong human preference for SVG
outputs generated by StarVector-8B over the baselines. In total,
1,948 evaluations were collected from 30 unique participants.
and -0.1002), indicating it is not a strong predictor. In con-
trast, DinoScore exhibits stronger correlations, with values
of -0.6193 and 0.6214, and a strong correlation of 0.7577
between DINO differences and human evaluation differ-
ences, highlighting DINO as a more reliable metric.
Text-to-SVG. Table 4 shows StarVector outperforms base-
lines on SVG-FIGR and SVG-Stack. While qualitative re-
sults show potential, semantic accuracy often suffers due to
data quality (see Appendix 10.2).
7. Conclusions
We introduced StarVector, an MLLM that excels in Image-
to-SVG and Text-to-SVG generation, delivering superior
visual quality, precise line and shape rendering, and opti-
mal SVG primitive usage compared to baselines. To train
StarVector, we created SVG-Stack, a diverse dataset that en-
ables generalization across SVG types and primitives. Ad-
ditionally, we developed SVG-Bench, a unified benchmark
with tasks, datasets, and targeted metrics. Our study shows
that traditional metrics inadequately capture SVG quality,
leading us to propose more effective alternatives.
Limitations and Future Work.
StarVector is con-
strained by its 16k token context, which is inadequate for
complex SVGs. It relies primarily on code prediction with
minimal visual feedback. Additionally, generation speed is
limited by the LLM. Future work may investigate integrat-
ing pixel-based signals to overcome these challenges.
Acknowledgements. We sincerely thank Ghazwa Dar-
wiche, Christian Hudon, Fanny Rancourt, and Marie-Ève
Marchand for their invaluable administrative and technical
support. This work was supported by the Natural Sciences
and Engineering Research Council of Canada and Mitacs.
Chris Pal acknowledges the Canada CIFAR AI Chair. We
also appreciate the human verifiers for their contributions to
assessment and data verification.


--- Page 9 ---

References
[1] The claude 3 model family: Opus, sonnet, haiku. 2
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katherine Millican, Malcolm Reynolds, et al.
Flamingo: a visual language model for few-shot learning.
Advances in Neural Information Processing Systems, 35:
23716–23736, 2022. 2, 3, 28
[3] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Cheng-
hao Mou, Christopher Akiki, Carlos Munoz Ferrandis,
Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan
Dey, et al. Santacoder: don’t reach for the stars!
arXiv
preprint arXiv:2301.03988, 2023. 3
[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi
Parikh. Vqa: Visual question answering. In Proceedings
of the IEEE international conference on computer vision,
pages 2425–2433, 2015. 3
[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization.
arXiv preprint arXiv:1607.06450,
2016. 40
[6] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Ait-
tala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Kar-
ras, and Ming-Yu Liu.
ediff-i: Text-to-image diffusion
models with ensemble of expert denoisers. arXiv preprint
arXiv:2211.01324, 2022. 3
[7] Jonas Belouadi, Anne Lauscher, and Steffen Eger.
Au-
tomatikz: Text-guided synthesis of scientific vector graph-
ics with tikz. arXiv preprint arXiv:2310.00367, 2023. 1,
42
[8] Jonas Belouadi, Simone Paolo Ponzetto, and Steffen
Eger. Detikzify: Synthesizing graphics programs for sci-
entific figures and sketches with tikz.
arXiv preprint
arXiv:2405.15306, 2024. 1
[9] Ekaba Bisong and Ekaba Bisong. Google bigquery. Build-
ing Machine Learning and Deep Learning Models on
Google Cloud Platform: A Comprehensive Guide for Be-
ginners, pages 485–517, 2019. 3
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in neu-
ral information processing systems, 33:1877–1901, 2020.
3, 18
[11] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,
Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of
artificial general intelligence: Early experiments with gpt-
4. arXiv preprint arXiv:2303.12712, 2023. 18, 19
[12] Mu Cai, Zeyi Huang, Yuheng Li, Haohan Wang, and
Yong Jae Lee. Leveraging large language models for scal-
able vector graphics-driven image understanding.
arXiv
preprint arXiv:2306.06094, 2023. 3, 6, 40
[13] Defu Cao, Zhaowen Wang, Jose Echevarria, and Yan Liu.
Svgformer: Representation learning for continuous vec-
tor graphics using transformers.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10093–10102, 2023. 2, 13
[14] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and
Radu Timofte. Deepsvg: A hierarchical generative network
for vector graphics animation. Advances in Neural Informa-
tion Processing Systems, 33:16351–16361, 2020. 2, 3, 5, 7,
13, 14, 15, 18, 29
[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Ed-
wards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.
Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374, 2021. 3
[16] Louis Clouâtre and Marc Demers. Figr: Few-shot image
generation with reptile. ArXiv, abs/1901.02199, 2019. 3, 5
[17] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikan-
jam, Foutse Khomh, Michel C Desmarais, and Zhen
Ming Jack Jiang. Github copilot ai pair programmer: Asset
or liability? Journal of Systems and Software, 203:111734,
2023. 3
[18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo-
pher Ré. Flashattention: Fast and memory-efficient exact
attention with io-awareness. Advances in Neural Informa-
tion Processing Systems, 35:16344–16359, 2022. 41
[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In 2009 IEEE conference on computer vision
and pattern recognition, pages 248–255. Ieee, 2009. 40
[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.
Bert:
Pre-training of deep bidirectional
transformers for language understanding.
arXiv preprint
arXiv:1810.04805, 2018. 3
[21] James Richard Diebel. Bayesian Image Vectorization: the
probabilistic inversion of vector image rasterization. Stan-
ford University, 2008. 2
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al.
An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 3, 40
[23] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 12873–12883, 2021. 3, 26,
28, 39, 40
[24] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim
Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified
flow transformers for high-resolution image synthesis. In
Forty-first International Conference on Machine Learning,
2024. 2
[25] Jon Ferraiolo, Fujisawa Jun, and Dean Jackson. Scalable
vector graphics (SVG) 1.0 specification. iuniverse Bloom-
ington, 2000. 1
[26] Leo Gao, Stella Biderman, Sid Black, Laurence Golding,
Travis Hoppe, Charles Foster, Jason Phang, Horace He, An-
ish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset


--- Page 10 ---

of diverse text for language modeling.
arXiv preprint
arXiv:2101.00027, 2020. 3
[27] Xavier Glorot and Yoshua Bengio. Understanding the dif-
ficulty of training deep feedforward neural networks.
In
Proceedings of the thirteenth international conference on
artificial intelligence and statistics, pages 249–256. JMLR
Workshop and Conference Proceedings, 2010. 40
[28] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial networks. Com-
munications of the ACM, 63(11):139–144, 2020. 7
[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural informa-
tion processing systems, 33:6840–6851, 2020. 3
[30] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term
memory. Neural computation, 9(8):1735–1780, 1997. 3
[31] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
Choi. The curious case of neural text degeneration. arXiv
preprint arXiv:1904.09751, 2019. 27, 41
[32] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Al-
lamanis, and Marc Brockschmidt.
Codesearchnet chal-
lenge: Evaluating the state of semantic code search. arXiv
preprint arXiv:1909.09436, 2019. 3
[33] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion:
Text-to-svg by abstracting pixel-based diffusion models. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 1911–1920, 2023. 1,
3
[34] Diederik P Kingma and Max Welling. Auto-encoding vari-
ational bayes. arXiv preprint arXiv:1312.6114, 2013. 3
[35] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite,
Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The
stack: 3 tb of permissively licensed source code.
arXiv
preprint arXiv:2211.15533, 2022. 3, 14
[36] Kozea. Cairosvg. https://cairosvg.org/, 2023. 3,
5
[37] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng,
Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao
Zhang, and Ion Stoica.
Efficient memory management
for large language model serving with pagedattention. In
Proceedings of the 29th Symposium on Operating Systems
Principles, pages 611–626, 2023. 7
[38] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization.
ArXiv e-prints, pages arXiv–1607,
2016. 5
[39] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming
Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng
Cao, et al. mplug: Effective and efficient vision-language
learning by cross-modal skip-connections. arXiv preprint
arXiv:2205.12005, 2022. 3
[40] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation.
In
International Conference on Machine Learning, pages
12888–12900. PMLR, 2022.
[41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597, 2023. 3, 4, 14
[42] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muen-
nighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,
Christopher Akiki, Jia Li, Jenny Chim, et al.
Star-
coder:
may the source be with you!
arXiv preprint
arXiv:2305.06161, 2023. 2, 3, 5, 39, 41
[43] Tzu-Mao Li, Michal Lukáˇc, Michaël Gharbi, and Jonathan
Ragan-Kelley. Differentiable vector graphics rasterization
for editing and learning. ACM Transactions on Graphics
(TOG), 39(6):1–15, 2020. 1, 2, 17, 18, 29
[44] Zicheng Liao, Hugues Hoppe, David Forsyth, and Yizhou
Yu. A subdivision-based representation for vector image
editing. IEEE transactions on visualization and computer
graphics, 18(11):1858–1867, 2012. 2
[45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee.
Visual
instruction
tuning.
arXiv
preprint
arXiv:2304.08485, 2023. 2, 3, 4, 14, 28
[46] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Fe-
ichtenhofer, Trevor Darrell, and Saining Xie. A convnet
for the 2020s. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 11976–
11986, 2022. 26, 28, 39, 40
[47] Raphael Gontijo Lopes, David Ha, Douglas Eck, and
Jonathon Shlens. A learned representation for scalable vec-
tor graphics. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 7930–7939, 2019.
3, 5, 13, 14
[48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017. 41
[49] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,
Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder
2 and the stack v2: The next generation. arXiv preprint
arXiv:2402.19173, 2024. 2, 39
[50] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,
Nikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-
wise image vectorization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 16314–16323, 2022. 1, 2, 15, 17, 40
[51] Martin Weber.
Autotrace.
https://github.com/
autotrace/autotrace, 2024. 1, 2, 7, 16, 17
[52] Kenton Murray and David Chiang.
Correcting length
bias in neural machine translation.
arXiv preprint
arXiv:1808.10006, 2018. 41
[53] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,
Huan Wang, Yingbo Zhou, Silvio Savarese, and Caim-
ing Xiong. Codegen: An open large language model for
code with multi-turn program synthesis.
arXiv preprint
arXiv:2203.13474, 2022. 3, 39
[54] Kunato Nishina and Yusuke Matsui.
Svgeditbench: A
benchmark dataset for quantitative assessment of llm’s
svg editing capabilities. arXiv preprint arXiv:2404.13710,
2024. 3
[55] OpenAI. Gpt-4 technical report, 2023. 2, 3, 7, 19


--- Page 11 ---

[56] OpenAI. GPT-4V(ision) System Card. https://cdn.
openai.com/papers/GPTV_System_Card.pdf,
2023. Accessed: 2023-11-05. 3, 7, 16, 18, 29
[57] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervi-
sion. arXiv preprint arXiv:2304.07193, 2023. 5, 7
[58] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in PyTorch. In NeurIPS-W, 2017. 41
[59] Peter Selinger.
Potrace.
https://github.com/
tatarize/potrace, 2024. 1, 2, 7, 16, 17
[60] Antoine Quint. Scalable vector graphics. IEEE MultiMedia,
10(3):99–102, 2003. 1
[61] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. openAI, 2018. 3
[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International conference on machine learning,
pages 8748–8763. PMLR, 2021. 3, 4, 5, 6, 26, 28, 39, 40
[63] Prajit Ramachandran, Barret Zoph, and Quoc V Le.
Searching for activation functions.
arXiv preprint
arXiv:1710.05941, 2017. 5
[64] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. In Interna-
tional Conference on Machine Learning, pages 8821–8831.
PMLR, 2021. 3
[65] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation, 2021. 2, 3
[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen.
Hierarchical text-conditional
image generation with clip latents.
arXiv preprint
arXiv:2204.06125, 1(2):3, 2022. 6
[67] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and
Niloy J Mitra. Im2vec: Synthesizing vector graphics with-
out vector supervision. arXiv preprint arXiv:2102.02798,
2021. 2, 3, 5, 7, 15, 17, 29
[68] Leonard Richardson.
Beautifulsoup.
https://www.
crummy.com/software/BeautifulSoup/, 2023.
4
[69] Juan A Rodriguez, David Vazquez, Issam Laradji, Marco
Pedersoli, and Pau Rodriguez. Figgen: Text to scientific
figure generation. arXiv preprint arXiv:2306.00800, 2023.
1
[70] Juan A Rodriguez, David Vazquez, Issam Laradji, Marco
Pedersoli, and Pau Rodriguez. Ocr-vqgan: Taming text-
within-image generation. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision,
pages 3689–3698, 2023. 1
[71] Juan A Rodriguez, Nicholas Botzer, David Vazquez,
Christopher Pal, Marco Pedersoli, and Issam Laradji. In-
tentgpt:
Few-shot intent discovery with large language
models. arXiv preprint arXiv:2411.10670, 2024. 19
[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models, 2021. 2, 3
[73] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022. 3, 6, 26
[74] Andy S.
svgpathtools.
https://github.com/
mathandy/svgpathtools, 2023. 4
[75] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon,
Ross Wightman,
Mehdi Cherti,
Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for train-
ing next generation image-text models. Advances in Neural
Information Processing Systems, 35:25278–25294, 2022.
39, 40
[76] Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie,
Brian Strope, and Ray Kurzweil. Generating high-quality
and informative conversation responses with sequence-to-
sequence models. arXiv preprint arXiv:1701.03185, 2017.
41
[77] Noam Shazeer. Fast transformer decoding: One write-head
is all you need. arXiv preprint arXiv:1911.02150, 2019. 41
[78] Benjamin Spector and Chris Re. Accelerating llm infer-
ence with staged speculative decoding.
arXiv preprint
arXiv:2308.04623, 2023. 41
[79] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. The jour-
nal of machine learning research, 15(1):1929–1958, 2014.
40
[80] Chengjun Tang, Kun Zhang, Chunfang Xing, Yong Ding,
and Zengmin Xu. Perlin noise improve adversarial robust-
ness. arXiv preprint arXiv:2112.13408, 2021. 14
[81] Lucas Theis, Aäron van den Oord, and Matthias Bethge. A
note on the evaluation of generative models. arXiv preprint
arXiv:1511.01844, 2015. 6
[82] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-
tiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,
et al. Llama: Open and efficient foundation language mod-
els. arXiv preprint arXiv:2302.13971, 2023. 3, 7, 19
[83] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023. 19
[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances
in neural information processing systems, 30, 2017. 2, 3
[85] Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R
Selvaraju, Qing Sun, Stefan Lee, David Crandall, and


--- Page 12 ---

Dhruv Batra.
Diverse beam search: Decoding diverse
solutions from neural sequence models.
arXiv preprint
arXiv:1610.02424, 2016. 41
[86] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-
man Christian Bachmann, Amit Haim Bermano, Daniel
Cohen-Or, Amir Zamir, and Ariel Shamir.
Clipasso:
Semantically-aware object sketching. ACM Transactions
on Graphics (TOG), 41(4):1–11, 2022. 3
[87] Vision
Cortex.
VTracer.
https : / / www .
visioncortex.org/vtracer- docs, 2023.
1,
2, 7, 16, 17
[88] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao
Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang,
Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language
model’s perception of the world at any resolution. arXiv
preprint arXiv:2409.12191, 2024. 3
[89] Yizhi Wang and Zhouhui Lian. Deepvecfont: Synthesizing
high-quality vector fonts via dual-modality learning. ACM
Transactions on Graphics (TOG), 40(6):1–15, 2021. 2, 3
[90] Zhou Wang and Alan C Bovik. Mean squared error: Love
it or leave it? a new look at signal fidelity measures. IEEE
signal processing magazine, 26(1):98–117, 2009. 5
[91] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Image quality assessment: from error visibility
to structural similarity. IEEE transactions on image pro-
cessing, 13(4):600–612, 2004. 5
[92] Wikipedia.
Comparison of raster-to-vector conver-
sion software — Wikipedia,
the free encyclopedia.
http://en.wikipedia.org/w/index.php?
title = Comparison % 20of % 20raster - to -
vector % 20conversion % 20software & oldid =
1185354750, 2024. [Online; accessed 07-March-2024].
2
[93] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac,
Tim Rault, Rémi Louf, Morgan Funtowicz, et al.
Hug-
gingface’s transformers: State-of-the-art natural language
processing. arXiv preprint arXiv:1910.03771, 2019. 41
[94] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Icon-
shop: Text-based vector icon synthesis with autoregressive
transformers. arXiv preprint arXiv:2304.14400, 2023. 3, 5,
6, 7, 14, 19, 38
[95] Tian Xia, Binbin Liao, and Yizhou Yu. Patch-based image
vectorization with automatic curvilinear feature alignment.
ACM Transactions on Graphics (TOG), 28(5):1–10, 2009.
2
[96] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,
Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,
Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autore-
gressive models for content-rich text-to-image generation.
arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 2, 3
[97] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer. Sigmoid loss for language image pre-training.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 11975–11986, 2023. 5, 40
[98] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 5
[99] Tong Zhang, Haoyang Liu, Peiyan Zhang, Yuxuan Cheng,
and Haohan Wang.
Beyond pixels: Exploring human-
readable svg generation for simple images with vision lan-
guage models, 2023. 3
[100] Bocheng Zou, Mu Cai, Jianrui Zhang, and Yong Jae Lee.
Vgbench:
Evaluating large language models on vector
graphics understanding and generation.
arXiv preprint
arXiv:2407.10972, 2024. 3


--- Page 13 ---

StarVector: Generating Scalable Vector Graphics Code from Images and Text
Supplementary Material
In the following sections, we provide additional details
on the datasets used in this paper, present further experi-
ments, and describe our baselines in detail. We also dis-
cuss the StarVector architecture, its training process, and the
method for sampling SVG code from the model. Addition-
ally, we provide more insights into SVG-Bench, including
the proposed datasets and the different baselines within the
evaluation setup. Finally, qualitative results are presented to
showcase the strengths and limitations of our foundational
model.
8. SVG Datasets in SVG-Bench
Here we describe available SVG datasets in the recent lit-
erature. We extend our description of the datasets used for
training and evaluating StarVector and other baselines. Ear-
lier SVG datasets proposed in the literature (mainly datasets
of emojis and fonts) were not easily accessible due to bro-
ken URLs and no direct entry point. Therefore, we provide
them as part of SVGBench for easy reproducibility. We in-
troduce splits for train, validation, and testing. The train set
is used to optimize the parameter weights of the network.
The validation is used for tuning sampling hyperparameters,
and the test is used for evaluation. Our model can handle up
to 8k context tokens. Therefore, our datasets only consider
examples with up to 8,192 tokens. See Table 5 for a com-
plete description of the datasets. See Figures 6, 7, 8 for
ground truth examples of the test sets of SVG-Bench.
8.1. Datasets with Simplified SVGs.
We create simplified versions of our four main datasets, i.e.
emojis, icons, fonts, and SVG-Stack. This is done because
DeepSVG [14] requires a simplification of the SVG in its
input. The simplification consists of eliminating complex
primitives and using only vector paths.
Also, color and
shapes are abstracted only to use simple line strokes.
8.2. Creating the SVG-Fonts Dataset
To construct the SVG-Font dataset, we replicate the pro-
cedure described in SVG-VAE [47]1, which provides a list
of public URLs containing open font packages. We down-
load these packages, excluding any with broken URLs. The
TTF files are then converted to SFD format, and we fur-
ther use InkScape2 to convert them into SVG code. Sam-
ples from the test set are shown in Figure 7 (bottom-left).
These samples contain only path elements and represent a
1https://github.com/magenta/magenta/tree/main/
magenta/models/svg_vae
2https://inkscape.org/
Server_Response
MegaDatabase_Money query
MegaDatabase_Empty balance
Server_Request
List<Server_Response> query
SVG-Diagrams
wph
impbid
wff ph
wps
|- ( ph -> ( ps <-> ch ) )
wff ps
wch
wff ch
impcon4bid.1
|- ( ph -> ( ps -> ch ) )
con4d
wph
wff ph
wps
|- ( ph -> ( ch -> ps ) )
wff ps
wch
wff ch
impcon4bid.2
|- ( ph -> ( -. ps -> -. ch ) )
Space Used
101-250 GB (9.2%)
21-50 GB
(11.8%)
1-20 GB (58.0%)
51-100 GB (6.7%)
251-500 GB
(5.0%)
Others
(9.2%)
H3C
CH
CH3
H3C
CH
CH3
CH
CH3
H2N
H2N
N
N
N
HO
HO
OH
OH
OH
OH
OH
OH
O
OH
OH
H
H
H
H
H
«U»
ssas.model_json - U
databasename : (nvarchar(128))
«V»
ssas.model_json_10 - V
«V»
ssas.model_json_20 - V
«V»
ssas.model_json_32_relationships - V
databasename : (nvarchar(128))
relationships_name : (nvarchar(500))
The diagram is interactive and contains links.
Accordion title
Accordion content
Collapsed accordion
Collapsed accordion
Open Sans Semibold 16px
Open Sans Regular 16px
Open Sans Light Italic 16px
This is the first accordion
Additional information
parse_float
GPGGASentenceParser
::parse
GPRMCSentenceParser
::parse
PSTI030SentenceParser
::parse
PSTI032SentenceParser
::parse
Printer Vendor
Samsung Elect...
(6.8%)
Brother Indus...
(13.6%)
Canon
(18.2%)
Seiko Epson (4.5%)
Others (20.5%)
Hewlett-Packard
(36.4%)
06 months
2012
2014
2016
2018
70.0%
72.5%
75.0%
77.5%
80.0%
82.5%
Fully vaccinated six month olds
_mmheap::bubble_up
_mmheap::bubble_up_max
_mmheap::bubble_up_min
_mmheap::has_parent
_mmheap::min_level
_mmheap::parent
_mmheap::has_gparent
_mmheap::gparent
biosample
organismal entity
is_a
thing with taxon
uses
id
name
category
related to node property iri
full name
description
systematic synonym
has phenotype
in taxon
Figure 6. SVG Diagrams examples. These are ground truth SVG
examples from the test set. They are presented as SVG, showing
the challenge of understanding intricate structures and small texts,
with images of variate aspect ratios.
narrow range of images, each consisting of a single char-
acter per image, making them an ideal test case for SVG
generation [13, 14, 47].
8.3. Creating SVG-Diagrams
We introduce a novel SVG dataset, SVG-Diagrams, which
focuses exclusively on diagrams, graphs, workflows, and
other designs characterized by discrete elements such as
boxes, arrows, and text. To construct this dataset, we fil-
tered all SVGs containing the text element. Table 5 provides
detailed statistics about the dataset, and Figure 6 illustrates
test samples from SVG-Diagrams.
This new benchmark for diagram generation is highly
relevant, as it addresses a use case that cannot be tackled by
traditional image-processing models or methods limited to
the path primitive. Only approaches capable of leveraging
SVG code can fully exploit the use of primitives to generate
such structured designs effectively.
8.4. Generating Synthetic Captions on SVG-Stack
To generate textual instructions for vector images, we pro-
cess SVG-Stack images using visual captioning models.


--- Page 14 ---

Table 5. Summary of datasets. We offer a summary of statistics about the datasets used in our training and evaluation experiments. This
datasets are included in SVG-Bench. The subscript sim stands for the simplified version of the dataset, as required by some baselines.
Dataset
Train
Val
Test
Source
Token Length
SVG Primitives
Annotation
SVG-Stack
2,1M
108k
5,7k
TheStack [35]
1,822 ± 1,808
All
Caption
SVG-Stacksim
601k
30,k
1,5k
2k ± 918
Vector path
Caption
SVG-Diagrams
-
-
472
3,486 ± 1,918
All
Caption
SVG-Fonts
1,8M
91,5k
4,8k
Glypazzn [47]
2,121 ± 1,868
Vector path
Font type
SVG-Fontssim
1,4M
71,7k
3,7k
1,722 ± 723
Vector path
Font type
SVG-Emoji
8,7k
667
668
OpenMoji, NotoEmoji, TweMoji
2,551 ± 1,805
All
Class
SVG-Emojisim
580
57
96
2,448 ± 1,026
Vector Path
Class
SVG-Icons
80,4k
6,2k
2,4k
DeepSVG [14]
2,449 ± 1,543
Vector path
-
SVG-Iconssim
80,435
2,836
1,277
2,005 ± 824
Vector path
-
SVG-FIGR ‡
270k
27k
3k
IconShop [94]
5,342 ± 2,345
Vector path
Class, Caption
This approach provides a textual description for each im-
age, enabling us to fine-tune our model to follow textual
instructions. For this task, we leverage off-the-shelf AI cap-
tioners, specifically BLIP2 [41] and Llava [45].
Through prompt engineering, we guide these models in
performing the captioning task with reasonable quality. The
prompt we used is shared in Prompt 8.4. After automati-
cally captioning all SVG samples in SVG-Stack, we com-
pute the CLIP Score for the text-image pairs generated by
the two models—producing two captions per image. Using
a CLIP Score threshold of 30, we filter out text captions that
fall below this threshold.
Prompt 1. Utilized with BLIP2 and Llava for SVG
Captioning:
You are a helpful assistant. Your task
is to caption the input images with a concise and
clear description that represents what are the con-
tents of the image.
8.5. Data Augmentation for SVG
We introduce several data augmentation operations on
SVGs that aim to perform minor modifications to the SVG
code and rasterize it to get a new sample while training.
We include rotation, color, and curve noise. We evaluate
this setting on datasets with fewer samples, namely SVG-
Emoji and SVG-Icons, as the other two datasets are large
enough to do not overfit. Results are shown in Table 8.
Both datasets display improvements using these augmen-
tations. We see a substantial uplift for SVG-Emoji, which
has limited training data.
We introduce several augmentation operations to SVGs
to apply slight changes that help our model learn to generate
more precise results — for instance, being able to capture
exact colors from the image and encode them in hexadec-
imal code to insert it in the fill attribute of the SVG el-
ement. Applying rotations or adding noise to the curve’s
control points helps the model learn to precisely capture the
position of the edges or thickness of the stroke.
We perform random rotations in an angle range. We per-
form color changes by first parsing the element’s color using
the fill attribute and adding slight white Gaussian noise
to the RGB values. We propose curve noise by injecting
a small Perlin [80] noise into the control points in Bézier
curves. We also experimented with adding Gaussian noise,
which resulted in much less natural results. We apply this
noise by uniformly sampling a scalar from the interval be-
tween 0.01 and 0.05 and use it to scale the noise.
We apply these augmentations directly on the SVG code,
which involves parsing the XML code and accessing the at-
tributes and arguments of the primitives defined. We use
the libraries BeautifulSoup3 and SvgPathTools4.
Some primitives are simplified using our augmentations.
9. SVG Methods and Baselines
Here, we describe the previous methods and baselines
used to compare StarVector’s performance in Image-to-
SVG and Text-to-SVG generation tasks. We consider pre-
vious deep learning-based methods and image-processing
methods. We evaluate the baselines with publicly available
code in our proposed setup.
9.1. Image-to-SVG Baselines
We reproduce all previous approaches on our proposed
SVG-Bench benchmark, as the available results stem from
an unclear version of the fonts, emojis, and icons datasets.
For theImage-to-SVG task, we consider several baseline
3https://www.crummy.com/software/BeautifulSoup/
bs4/doc/
4https://github.com/mathandy/svgpathtools


--- Page 15 ---

SVG-Stack
SVG-Fonts
SVG-Emoji
SVG-Icons
RAM Size
4.01-8.0 GB
(16.9%)
8.01-16.0 GB
(18.0%)
16.01-24.0 GB (15.7%)
64.01-256.0 GB
(6.7%)
Others
(7.9%)
32.01-64.0 GB (34.8%)
C M
K
Y
Figure 7. Datasets in SVG-Bench. Ground truth test examples from the test sets of SVG-Stack, SVG-Emoji, SVG-Fonts, and SVG-
Icons. We show SVG images.
methods across deep learning and image processing ap-
proaches.
In the deep learning methods category, we start with
DeepSVG[14], Im2Vec[67], and LIVE[50], using the of-
ficial implementations with the hyperparameters proposed
by the authors, and applying their pre- and post-processing


--- Page 16 ---

SVG-Stack-Simple
SVG-Emoji-Simple
SVG-Fonts-simple
SVG-Icons-Simple
Figure 8. Simplified Datasets in SVG Bench. Ground truth test examples from the simplified test sets of SVG-Stack, SVG-Emoji,
SVG-Fonts, and SVG-Icons. We show SVG images.
code as required. Additionally, we incorporate the recent
GPT-4 Vision[56], which is capable of processing images
as input and generating corresponding SVG code as output.
For the image processing-based methods, which do not
rely on data-driven learning, we consider VTracer[87],
Autotracer[51], and Potrace [59], running them on the test
sets of SVG-Bench.


--- Page 17 ---

Table 6. Summary of SVG Methods. We compare SVG generation methods based on Image Processing, Latent Variable, Differentiable
Rendering, and Multimodal LLM, evaluating their performance in SVG Generalization and SVG Generation Tasks. The SVG Generalization
column shows whether a model generates diverse SVG types (e.g., icons, logos, complex shapes) with ✓or specializes in a subtype (e.g.,
emojis, fonts) with ✗. The SVG Primitive Coverage column indicates access to all SVG primitives. The table also evaluates Image
Vectorization, Text to SVG, and Diagram Generation, using ✓for support and ✗for limitations. ∗DeepSVG requires modifications for
image input.
SVG Coverage and Generalization
SVG Generation Tasks
Method Type
Model
Input
Train
Supervision
SVG
Generalization
SVG Primitive
Coverage
Image
Vectorization
Text to SVG
Diagram
Generation
Image Processing
Vtracer
Image
Image
✓
✗
✓
✗
✗
Autotrace
Image
Image
✓
✗
✓
✗
✗
Potrace
Image
Image
✓
✗
✓
✗
✗
Latent Variable
Im2Vec
Image
Image
✗
✗
✓
✗
✗
DeepSVG
SVG
Vector
✗
✗
✓∗
✗
✗
SVGFormer
SVG
Vector
✗
✗
✓
✗
✗
Diff. Rendering
DiffVG
Image
Image
✓
✗
✓
✗
✗
LIVE
Image
Image
✓
✗
✓
✗
✗
SAMVG
Image, Text
Image
✓
✗
✓
✗
✗
SVGDreamer
Image, Text
Image
✓
✗
✗
✓
✗
Multimodal LLM
GPT-4 V
Image, Text
SVG
✓
✓
✗
✓
✓
CodeLlama
Image
SVG
✓
✓
✗
✓
✓
IconShop
Text
SVG
✗
✗
✗
✓
✗
StarVector
Image, Text
SVG
✓
✓
✓
✓
✓
Autotrace5 [51] is a tool designed for converting images
to vector graphics, similar to Potrace. It supports various
input formats and can output to several vector formats. Au-
totrace’s key feature is its ability to transform pixelated im-
ages into smooth, scalable vectors, making it ideal for up-
grading images for various applications without losing de-
tail or clarity. Our experiments leverage the Python bind-
ings6 implementation of AutoTrace.
Potrace7 [59] is a utility designed to convert images into
refined, scalable vector graphics. It accepts input in vari-
ous bitmap formats and outputs to a selection of vector for-
mats. This functionality is particularly valuable for generat-
ing SVG of scanned imagery, such as logos and handwritten
documents. We employ a Python library8, which acts as a
wrapper around the original C implementation of Potrace.
VTracer9 [87] is an image processing algorithm to con-
vert images to SVGs. This 3-step pipeline algorithm relies
on the hierarchical clustering of images, which are traced
into vectors. First, pixels are converted into paths and then
simplified into polygons.
In the last step, polygons are
smoothened and approximated with a Bezier curve fitter.
We use the Python library10 for experiments, a wrapper over
the Rust implementation. Similar to Im2Vec, we scale down
5https://potrace.sourceforge.net/
6https://github.com/lemonyte/pyautotrace
7https://potrace.sourceforge.net/
8https://github.com/tatarize/potrace
9https://github.com/visioncortex/vtracer
10https://github.com/etjones/vtracer_py
all the images to 128X128 resolution. We use all the default
values for the image processing engine, which generates a
multi-colored SVG.
Im2Vec [67] uses an end-to-end VAE, trained using only
image supervision to produce vector graphics. The input
rasterized image is encoded to a ‘global’ latent vector and
passed to an RNN to produce latent code for each path. The
path decoder decodes these codes into Bezier paths to gen-
erate the output SVG. We used the publicly available code11
to report the results.
We scaled all the images to 128 × 128 resolution to be
compatible with the Im2Vec model. We used a learning rate
of 5×10−4 and a batch size of 8. We implemented a custom
post-processing operation for converting the vector parame-
ters obtained during Im2Vec inference to obtain compilable
SVG code.
LIVE, (Layer-wise Image Vectorization) [50] is a method
for progressively generating SVGs that closely fit a given
raster image by recursively adding and optimizing closed
vector paths. Using a differentiable renderer (based on Dif-
fVG [43]), LIVE enables direct optimization of paths under
raster image supervision while controlling shape complex-
ity by adjusting the number of path segments. It introduces
component-wise path initialization, identifying key visual
components to ensure efficient topology extraction and min-
imize redundant shapes. LIVE achieves high-quality recon-
structions with fewer paths, reducing SVG file size com-
11https://github.com/preddy5/Im2Vec


--- Page 18 ---

pared to other approaches. Nevertheless, its test time op-
timization approach makes it time-consuming during gen-
eration. We utilized their official open-source implementa-
tion 12 with the proposed hyperparameters. This method re-
quires to define a constant number of paths; the more paths
defined, the more accurate. We have performed an abla-
tion on the number of paths (see Table 3) and found that
paths=32 is an optimal value that brings good visual results.
However, it takes more than 10 minutes to generate a single
SVG, which makes it slow for a professional use case.
DiffVG [43] is a landmark in vector graphics research, pi-
oneering deep learning-based methods with the first differ-
entiable vector graphics rasterization pipeline. By leverag-
ing a combination of anti-aliasing techniques and gradient-
based optimization, DiffVG ensures differentiability. Un-
like methods relying on non-differentiable curve-to-mesh
conversions, DiffVG employs a forward-backward raster-
ization process, where the forward pass generates anti-
aliased images and the backward pass computes gradients
with respect to vector graphic parameters. Using the of-
ficial implementation13 and proposed hyperparameters, we
ablate the number of paths, finding paths=60 to be optimal.
DiffVG balances versatility and performance, achieving ap-
proximately 30 seconds per generation while excelling in
differentiable rendering tasks.
DeepSVG [14] was introduced as a hierarchical path-based
VAE encoder-decoder transformer architecture. Here, input
paths are encoded separately using a path encoder and ag-
gregated using a second encoder to produce a latent vector.
The decoder uses this latent vector to output the path rep-
resentations, which provide actual draw commands and ar-
guments. We used the open-source code14 to reproduce the
results on different datasets. However, since the DeepSVG
framework only allows simplified SVGs, we report results
on the ‘simplified’ test sets (see Table 1).
This model can only handle simplified SVGs composed
of simple line strokes and splines (see examples in Fig-
ure 9). Further, it can only process SVGs with eight groups
(i.e., groups of shapes or parent nodes) and vector paths of
at most 30 commands. To reproduce the DeepSVG base-
line, we use the original hyperparameters, including a learn-
ing rate of 1e −3 and a number of epochs of 50. We use
a batch size of 200, except for the smaller emoji dataset,
where we experiment with a batch size of 50.
12https://github.com/ma-xu/LIVE
13https://github.com/BachiLi/diffvg
14https://github.com/alexandre01/deepsvg
GPT-4 Vision. We use GPT-4V [56] by inserting an image
and zero-shot prompting to generate SVG code. Here, we
show how one can use prompt engineering [10, 11, 56] to
condition the model to generate executable SVG code rep-
resenting the given image. Prompt 9.2 was used for this
endeavor. We use the OpenAI library15.
Figure 9. Image-to-SVG results on simplified SVG-Icons and
SVG-Fonts test set.
Figure 10. Image-to-SVG results on SVG-Icons test set.
Figure 11. Image-to-SVG results on SVG-Stack test set. We show
cherry-picked failure examples of StarVector.
15https://platform.openai.com/docs/libraries


--- Page 19 ---

Figure 12. Image-to-SVG results on SVG-Emoji test set. We
show cherry-picked failure examples of StarVector.
9.2. Text-to-SVG Generation Baselines
For the Text-conditioned SVG generation task, we select
baselines based on works that contain reproducible method-
ologies in public datasets or public code repositories. We
reproduce baseline models from their official repositories,
respecting the proposed hyperparameters.
CodeLlama, [82, 83] has shown great success in general
coding benchmarks. To the best of our knowledge, CodeL-
lama has seen SVGs during training. Hence, it is reason-
able to consider it a strong baseline for text-conditioned
SVG generation. We use Anyscale endpoints16 to generate
CodeLlama results.
GPT 4, is a closed source LLM that shows state-of-the-art
results in many NLP sceneraios [11, 55, 71]. We evaluate
GPT-4’s 0-shot ability in generating SVGs when prompted
with text inputs. We use OpenAI API17 to generate results
for GPT-4 in the 0-shot setting. Prompt 9.2 was used for the
Text-to-SVG task.
Prompt 2.
Used on GPT4-V VLM for Image-
to-SVG Translation:
You are a helpful assistant.
Your task is to help researchers write SVG code to
reconstruct the provided image as accurately as pos-
sible. You should also provide a caption for the im-
age. You are dedicated to solving the task of Image-
to-SVG conversion for a robust system. Therefore,
you must always respond with the best SVG code
you can create. Feel free to use multiple paths to
generate a compliant SVG code within a maximum
of 8000 tokens. You should present the SVG code
that best reconstructs the input image enclosed in
triple quotes.
16https://app.endpoints.anyscale.com/
17https://platform.openai.com/docs/guides/gpt
Prompt 3.
Used on GPT4 and CodeLlama for
Text-to-SVG Generation:
You are a helpful assis-
tant assisting researchers in generating SVG code
from textual descriptions.
You will be provided
with details to guide your SVG creation. Your task
is to write SVG code that accurately represents the
given textual information to the fullest extent possi-
ble. You are committed to solving the task of SVG
generation for a robust system, so always strive to
produce the best SVG code you can. Feel free to
use multiple paths and any necessary shapes, colors,
or lines to generate compilable SVG code within a
maximum of 9000 tokens. The goal is to ensure
the resulting SVG, when rasterized, best represents
the described content. Respond only with the SVG
code, enclosed in triple quotes, that directly corre-
sponds to the provided textual description. Avoid
adding any explanation or commentary.
IconShop IconShop [94] uses a transformer-based archi-
tecture to encode path commands and learn to model SVG
path sequences autorregressively. It has shown excellent re-
sults in simplified icon scenarios and provides a good so-
lution to Text-to-SVG generation by extending the FIGR-
SVG dataset with captions. We have access to their dataset
and original splits and have trained our model on that data
using a pre-trained checkpoint (trained on SVG-Stack). We
have extracted the results from IconShop and included them
here to compare our method.
10. Additional Experiments and Results
10.0.1. Image-to-SVG Results
We show additional Image-to-SVG results from StarVector.
Figures [13 - 18] show substantial qualitative samples gen-
erated by StarVector on all the proposed datasets. All results
are computed in the test sets. We can observe the weak-
nesses and strengths of our model. Simplified datasets (Fig-
ures 16) are near-perfectly converted to SVG. In the case of
icons, in Figure 18, sometimes the model runs out of SVG
code tokens, and the image is incomplete. Results on SVG-
Emoji 17 show impressive performance in estimating the
shape’s color and semantics. However, it lacks fine-grained
and accurate positioning of objects, i.e., in some examples,
the model loses track of the coherent position and form of
shapes. These problems result from insufficient emoji sam-
ples, i.e., less than 10,000 training examples. This prob-
lem can be alleviated by scaling up the current model in the
number of parameters (currently 1.4 billion), training data
for pre-training, and computing resources.


--- Page 20 ---

Ground
truth
Generated
Ground
truth
Generated
Ground
truth
Generated
Ground
truth
Generated
Ground
truth
Generated
Ground
truth
Generated
Ground
truth
Generated
Ground
truth
Generated
Display Manager
SDDM (42.9%)
Unknown (51.2%)
LightDM
(3.6%)
GDM (2.4%)
Display Manager
SDDM (42.9%)
Unknown (51.2%)
LightDM
(3.7%)
GDM (2.4%)
Figure 13. Image-to-SVG Results on SVG-Stack. We present vectorizations of StarVector-1B on the test set of SVG-Stack. Left is input
raster image, right is the SVG image (in SVG format).


--- Page 21 ---

Input Image
StarVector-8B
LIVE
VTracer
PoTrace
AutoTrace
DinoScore: 0.9871
MSE: 0.0075
DinoScore: 0.8325
MSE: 0.0019
DinoScore: 0.8750
MSE: 0.0026
DinoScore: 0.7806
MSE: 0.0062
DinoScore: 0.8243
MSE: 0.0034
DinoScore: 0.9941
MSE: 0.0052
DinoScore: 0.7835
MSE: 0.0026
DinoScore: 0.8687
MSE: 0.0033
DinoScore: 0.9006
MSE: 0.0100
DinoScore: 0.7814
MSE: 0.0040
DinoScore: 0.9988
MSE: 0.0071
DinoScore: 0.8420
MSE: 0.0069
DinoScore: 0.9195
MSE: 0.0119
DinoScore: 0.9014
MSE: 0.0446
DinoScore: 0.9060
MSE: 0.0159
DinoScore: 0.9960
MSE: 0.0150
DinoScore: 0.8670
MSE: 0.0060
DinoScore: 0.8624
MSE: 0.0066
DinoScore: 0.8014
MSE: 0.0094
DinoScore: 0.7757
MSE: 0.0082
DinoScore: 0.9887
MSE: 0.0532
DinoScore: 0.8513
MSE: 0.0159
DinoScore: 0.7943
MSE: 0.0196
DinoScore: 0.8560
MSE: 0.0842
DinoScore: 0.7922
MSE: 0.0309
DinoScore: 0.9974
MSE: 0.0210
DinoScore: 0.7979
MSE: 0.0083
DinoScore: 0.8366
MSE: 0.0104
DinoScore: 0.8128
MSE: 0.0147
DinoScore: 0.8001
MSE: 0.0116
DinoScore: 0.9891
MSE: 0.0093
DinoScore: 0.8584
MSE: 0.0035
DinoScore: 0.8542
MSE: 0.0049
DinoScore: 0.8462
MSE: 0.0056
DinoScore: 0.8987
MSE: 0.0058
DinoScore: 0.9690
MSE: 0.0316
DinoScore: 0.8433
MSE: 0.0166
DinoScore: 0.8202
MSE: 0.0164
DinoScore: 0.7061
MSE: 0.0243
DinoScore: 0.8119
MSE: 0.0209
DinoScore: 0.9871
MSE: 0.0075
DinoScore: 0.8325
MSE: 0.0019
DinoScore: 0.8750
MSE: 0.0026
DinoScore: 0.7806
MSE: 0.0062
DinoScore: 0.8243
MSE: 0.0034
DinoScore: 0.9941
MSE: 0.0052
DinoScore: 0.7835
MSE: 0.0026
DinoScore: 0.8687
MSE: 0.0033
DinoScore: 0.9006
MSE: 0.0100
DinoScore: 0.7814
MSE: 0.0040
DinoScore: 0.9988
MSE: 0.0071
DinoScore: 0.8420
MSE: 0.0069
DinoScore: 0.9195
MSE: 0.0119
DinoScore: 0.9014
MSE: 0.0446
DinoScore: 0.9060
MSE: 0.0159
DinoScore: 0.9960
MSE: 0.0150
DinoScore: 0.8670
MSE: 0.0060
DinoScore: 0.8624
MSE: 0.0066
DinoScore: 0.8014
MSE: 0.0094
DinoScore: 0.7757
MSE: 0.0082
DinoScore: 0.9887
MSE: 0.0532
DinoScore: 0.8513
MSE: 0.0159
DinoScore: 0.7943
MSE: 0.0196
DinoScore: 0.8560
MSE: 0.0842
DinoScore: 0.7922
MSE: 0.0309
DinoScore: 0.9974
MSE: 0.0210
DinoScore: 0.7979
MSE: 0.0083
DinoScore: 0.8366
MSE: 0.0104
DinoScore: 0.8128
MSE: 0.0147
DinoScore: 0.8001
MSE: 0.0116
DinoScore: 0.9891
MSE: 0.0093
DinoScore: 0.8584
MSE: 0.0035
DinoScore: 0.8542
MSE: 0.0049
DinoScore: 0.8462
MSE: 0.0056
DinoScore: 0.8987
MSE: 0.0058
DinoScore: 0.9690
MSE: 0.0316
DinoScore: 0.8433
MSE: 0.0166
DinoScore: 0.8202
MSE: 0.0164
DinoScore: 0.7061
MSE: 0.0243
DinoScore: 0.8119
MSE: 0.0209
DinoScore: 0.9871
MSE: 0.0075
DinoScore: 0.8325
MSE: 0.0019
DinoScore: 0.8750
MSE: 0.0026
DinoScore: 0.7806
MSE: 0.0062
DinoScore: 0.8243
MSE: 0.0034
DinoScore: 0.9941
MSE: 0.0052
DinoScore: 0.7835
MSE: 0.0026
DinoScore: 0.8687
MSE: 0.0033
DinoScore: 0.9006
MSE: 0.0100
DinoScore: 0.7814
MSE: 0.0040
DinoScore: 0.9988
MSE: 0.0071
DinoScore: 0.8420
MSE: 0.0069
DinoScore: 0.9195
MSE: 0.0119
DinoScore: 0.9014
MSE: 0.0446
DinoScore: 0.9060
MSE: 0.0159
DinoScore: 0.9960
MSE: 0.0150
DinoScore: 0.8670
MSE: 0.0060
DinoScore: 0.8624
MSE: 0.0066
DinoScore: 0.8014
MSE: 0.0094
DinoScore: 0.7757
MSE: 0.0082
DinoScore: 0.9887
MSE: 0.0532
DinoScore: 0.8513
MSE: 0.0159
DinoScore: 0.7943
MSE: 0.0196
DinoScore: 0.8560
MSE: 0.0842
DinoScore: 0.7922
MSE: 0.0309
DinoScore: 0.9974
MSE: 0.0210
DinoScore: 0.7979
MSE: 0.0083
DinoScore: 0.8366
MSE: 0.0104
DinoScore: 0.8128
MSE: 0.0147
DinoScore: 0.8001
MSE: 0.0116
DinoScore: 0.9891
MSE: 0.0093
DinoScore: 0.8584
MSE: 0.0035
DinoScore: 0.8542
MSE: 0.0049
DinoScore: 0.8462
MSE: 0.0056
DinoScore: 0.8987
MSE: 0.0058
DinoScore: 0.9690
MSE: 0.0316
DinoScore: 0.8433
MSE: 0.0166
DinoScore: 0.8202
MSE: 0.0164
DinoScore: 0.7061
MSE: 0.0243
DinoScore: 0.8119
MSE: 0.0209
DinoScore: 0.9871
MSE: 0.0075
DinoScore: 0.8325
MSE: 0.0019
DinoScore: 0.8750
MSE: 0.0026
DinoScore: 0.7806
MSE: 0.0062
DinoScore: 0.8243
MSE: 0.0034
DinoScore: 0.9941
MSE: 0.0052
DinoScore: 0.7835
MSE: 0.0026
DinoScore: 0.8687
MSE: 0.0033
DinoScore: 0.9006
MSE: 0.0100
DinoScore: 0.7814
MSE: 0.0040
DinoScore: 0.9988
MSE: 0.0071
DinoScore: 0.8420
MSE: 0.0069
DinoScore: 0.9195
MSE: 0.0119
DinoScore: 0.9014
MSE: 0.0446
DinoScore: 0.9060
MSE: 0.0159
DinoScore: 0.9960
MSE: 0.0150
DinoScore: 0.8670
MSE: 0.0060
DinoScore: 0.8624
MSE: 0.0066
DinoScore: 0.8014
MSE: 0.0094
DinoScore: 0.7757
MSE: 0.0082
DinoScore: 0.9887
MSE: 0.0532
DinoScore: 0.8513
MSE: 0.0159
DinoScore: 0.7943
MSE: 0.0196
DinoScore: 0.8560
MSE: 0.0842
DinoScore: 0.7922
MSE: 0.0309
DinoScore: 0.9974
MSE: 0.0210
DinoScore: 0.7979
MSE: 0.0083
DinoScore: 0.8366
MSE: 0.0104
DinoScore: 0.8128
MSE: 0.0147
DinoScore: 0.8001
MSE: 0.0116
DinoScore: 0.9891
MSE: 0.0093
DinoScore: 0.8584
MSE: 0.0035
DinoScore: 0.8542
MSE: 0.0049
DinoScore: 0.8462
MSE: 0.0056
DinoScore: 0.8987
MSE: 0.0058
DinoScore: 0.9690
MSE: 0.0316
DinoScore: 0.8433
MSE: 0.0166
DinoScore: 0.8202
MSE: 0.0164
DinoScore: 0.7061
MSE: 0.0243
DinoScore: 0.8119
MSE: 0.0209
DinoScore: 0.9871
MSE: 0.0075
DinoScore: 0.8325
MSE: 0.0019
DinoScore: 0.8750
MSE: 0.0026
DinoScore: 0.7806
MSE: 0.0062
DinoScore: 0.8243
MSE: 0.0034
DinoScore: 0.9941
MSE: 0.0052
DinoScore: 0.7835
MSE: 0.0026
DinoScore: 0.8687
MSE: 0.0033
DinoScore: 0.9006
MSE: 0.0100
DinoScore: 0.7814
MSE: 0.0040
DinoScore: 0.9988
MSE: 0.0071
DinoScore: 0.8420
MSE: 0.0069
DinoScore: 0.9195
MSE: 0.0119
DinoScore: 0.9014
MSE: 0.0446
DinoScore: 0.9060
MSE: 0.0159
DinoScore: 0.9960
MSE: 0.0150
DinoScore: 0.8670
MSE: 0.0060
DinoScore: 0.8624
MSE: 0.0066
DinoScore: 0.8014
MSE: 0.0094
DinoScore: 0.7757
MSE: 0.0082
DinoScore: 0.9887
MSE: 0.0532
DinoScore: 0.8513
MSE: 0.0159
DinoScore: 0.7943
MSE: 0.0196
DinoScore: 0.8560
MSE: 0.0842
DinoScore: 0.7922
MSE: 0.0309
DinoScore: 0.9974
MSE: 0.0210
DinoScore: 0.7979
MSE: 0.0083
DinoScore: 0.8366
MSE: 0.0104
DinoScore: 0.8128
MSE: 0.0147
DinoScore: 0.8001
MSE: 0.0116
DinoScore: 0.9891
MSE: 0.0093
DinoScore: 0.8584
MSE: 0.0035
DinoScore: 0.8542
MSE: 0.0049
DinoScore: 0.8462
MSE: 0.0056
DinoScore: 0.8987
MSE: 0.0058
DinoScore: 0.9690
MSE: 0.0316
DinoScore: 0.8433
MSE: 0.0166
DinoScore: 0.8202
MSE: 0.0164
DinoScore: 0.7061
MSE: 0.0243
DinoScore: 0.8119
MSE: 0.0209
Figure 14. We compare the results from StarVector-8B with those from the most powerful baselines. Notably, StarVector is the only method
capable of producing acceptable results that preserve both structural integrity and textual content by utilizing a variety of SVG primitives.
In contrast, other methods tend to generate blobs and curves that merely attempt to fit the structure and color of the original image. We
present two metric scores for each sample: DinoScore and MSE. MSE consistently yields higher scores for other methods, as they focus
on fitting vectors to the image as accurately as possible. While StarVector may not achieve perfect reconstruction, its results are preferred
for their semantic fidelity. This highlights the limitations of MSE and the importance of DinoScore in capturing these aspects.


--- Page 22 ---

Figure 15. Image-to-SVG results on SVG-Fonts test set. Results are remarkably good, obtaining perfect font reconstructions. Intricate
details are preserved. This is because the dataset is very large, above 1M samples. This shows that if having access to a large dataset,
StarVector can learn high-quality SVG generation.


--- Page 23 ---

Figure 16. Image-to-SVG results on SVG-Fonts simplified test set.


--- Page 24 ---

Figure 17. Image-to-SVG results on SVG-Emoji test set. Results are mostly wrong in this benchmark, due to the small training dataset
of approximately 8k examples.


--- Page 25 ---

Figure 18. Image-to-SVG results on SVG-Icons test set.


--- Page 26 ---

Comparing parameter count of models
.
The num-
ber of learnable parameters in deep learning based models
often correlates with performance. Pre-LLM models like
Im2Vec and DeepSVG use significantly fewer parameters
(up to 5M) compared to StarVector and GPT-based mod-
els, which operate in the billions. While pre-LLM models
can produce accurate results, they lack the generalization
ability of LLM-based approaches. Comparing StarVector
with GPT models reveals that high-fidelity SVG genera-
tion is achievable with just 1B parameters, whereas GPT-
4V lacks specific training for this task. Future models will
likely incorporate SVG data in training, but current results
already demonstrate that LLM-based approaches offer su-
perior generalization and scalability for SVG generation, at
the cost of utilizing more parameters.
Context Length Limitation.
The model’s architecture
imposes a clear limitation on context length, which signifi-
cantly impacts training and testing data pipelines, as well as
the skills the model can learn. Our experiments show that
the model scales effectively with increasing context lengths,
from 8k to 16k, indicating that this limitation must be ad-
dressed with techniques for handling longer contexts—an
area LLMs are expected to improve. For fair comparisons,
we restricted our benchmark tests to a context length of 8k
and evaluated all baselines within this setting. However,
the benchmarks also provide versions with longer context
lengths to assess future models, as increased length gener-
ally correlates with more complex SVGs. We did not ob-
serve substantial differences in scores between the 4k and
10k token settings, primarily because the data in our bench-
mark can typically be represented using an average of 3k
tokens.
Limitations on Complex SVG Structures.
StarVector
encounters challenges with complex SVG structures, intri-
cate shapes, and detailed illustrations primarily due to limi-
tations in its architecture. Currently, the model’s image en-
coder handles images by simply padding and resizing them
to fixed dimensions of 224 or 384 pixels. This approach
may not adequately capture the nuances of complex dia-
grams. A potential improvement would be to implement a
dynamic image processing system akin to those found in
newer Vision-Language Models (VLMs), which could en-
hance the model’s ability to interpret and generate intricate
SVGs more effectively. Additionally, improving data clean-
ing processes is crucial, as the model sometimes produces
hallucinated information due to noise in the input data, such
as URLs or base64-encoded images. Addressing these is-
sues through architectural enhancements and more robust
data preprocessing could significantly improve StarVector’s
performance on complex SVG tasks.
Generalization to Non-Standard SVGs.
StarVector’s
ability to generalize to non-standard SVGs—those not rep-
resented in its main training distribution—poses a signifi-
cant challenge. While the model performs well on common
styles and primitives encountered during training, it strug-
gles with more unique or unconventional SVGs. This is pri-
marily due to the model’s training data, which tends to focus
on widely used shapes and designs. As a result, StarVector
may exhibit a bias towards these common styles, leading to
suboptimal performance when faced with SVGs that feature
unusual structures or less frequent elements.
To assess StarVector’s generalization capabilities, we
evaluated its performance on various datasets that in-
clude non-standard SVGs. The results indicate that while
the model can produce reasonable outputs for some non-
standard examples, it often falls short in accurately captur-
ing the intricacies of less familiar styles. This limitation
suggests that the model’s training set lacks sufficient diver-
sity to encompass the full range of potential SVG designs.
To address these concerns, future work should focus
on expanding the training dataset to include a wider vari-
ety of SVG styles and structures. Incorporating data from
niche applications and artistic domains could enhance the
model’s ability to generate SVGs across a broader spec-
trum of design elements. Additionally, techniques such as
domain adaptation and transfer learning could be explored
to improve generalization to non-standard SVGs, allowing
StarVector to adapt more effectively to unfamiliar inputs.
10.1. Ablation Studies
We performed ablations on the image encoder type, the data
augmentation pipeline, inference techniques, and genera-
tion parameters. Most of our ablations were performed on
the StarVector-1B model for faster iteration, and we empir-
ically find they work well on the larger StarVector-8B.
Image Encoder Ablation. The choice of image encoder
for the problem of Image-to-SVG is highly impactful, as
it determines how well visual information from raster im-
ages can be preserved in a representation suitable for pre-
cise reconstruction in the SVG space. We ablated the vi-
sual encoders by replacing them with VQGAN [23], Con-
vNext [46], and CLIP ViT-B/32 [62], in our StarVector-1B
proposed architecture. This setup evaluates three commonly
used approaches in visual representation learning [23, 62,
73]. In our experiments, CLIP consistently outperformed
across all metrics for various datasets (see Table 9). Figures
[19–22 further illustrate how VQGAN and ConvNext tend
to lose local details during generation, even while maintain-
ing semantic relevance.


--- Page 27 ---

SVG-Fonts
SVG-Emojis
SVG-Icons
SVG-Stack
Sampling technique
LPIPS ↓SSIM ↑MSE ↓LPIPS SSIM MSE LPIPS SSIM MSE LPIPS SSIM MSE
Greedy
0.019
0.969
0.013
0.251
0.731 0.071
0.059
0.912 0.028
0.157
0.797 0.067
+ Beam Search (B=5)
0.018
0.970
0.012
0.250
0.732 0.070
0.058
0.913 0.027
0.156
0.798 0.066
Nucleus Sampling (T=0.5)
0.013
0.976
0.008
0.202
0.778 0.051
0.043
0.923 0.022
0.153
0.785 0.072
Nucleus Sampling (T=1.0)
0.015
0.975
0.009
0.244
0.742 0.067
0.053
0.917 0.025
0.161
0.786 0.069
+ Beam-Search (B=5)
0.034
0.948
0.027
0.244
0.742 0.068
0.065
0.913 0.027
0.195
0.766 0.089
+ Beam-Search (B=10)
0.040
0.943
0.031
0.251
0.742 0.072
0.071
0.910 0.028
0.175
0.762 0.079
Table 7. Ablation study on sampling strategies. We experimented using greedy decoding and added a beam search with B=5. We
test nucleus sampling [31] using top p=0.9, with temperatures T=0.5 and T=1.0. The two final rows describe beam search with nucleus
sampling at T=1.0. See huggingface.com/blog/how-to-generate for reference on these sampling techniques.
Pre-training on SVG-Stack.
Pre-training on the SVG-
Stack is highly beneficial for the downstream datasets with
small data. Table 8 shows the uplift on all the metrics for
different datasets. Qualitatively, we can also see that pre-
training helps the model to identify the nuanced details from
the images. For the case of SVG-Emoji, pre-training is a vi-
tal requirement, as it overfits without it due to limited data.
Figure 17 shows that the model relies on colors and shapes
to generate the SVG.
Ablation on Generation Hyperparameters. We explore
the impact of different generation hyperparameters on the
StarVector-1B model. After an initial exploration to em-
pirically determine the most relevant hyperparameters, we
focus our ablation on these. We find that temperature and
the number of beams in beam search significantly affect per-
formance. The model is evaluated across various configu-
rations (see Table 7 and Figure 23). Our results show that
a beam search size of 5 achieves the best outcomes, albeit
with increased memory usage and runtime. Similarly, nu-
cleus sampling with a top-p of 0.9 and a temperature of 0.5
delivers the best overall performance.
10.2. Text-to-SVG Results
Figures [25 - 28] show additional qualitative results of
StarVector when performing the task of text-conditioned
SVG generation, performed on SVG-Stack and FIGR-SVG
test sets. Our samples show reasonable effectiveness at this
task, consistently grasping features like colors, shapes, and
semantic concepts. However, sometimes some details re-
quired in the prompt are lost, e.g., an exact number of cir-
cles, shapes inside other shapes, or the direction of arrows.
In some cases, some vector graphics shapes lose coherence,
which we attribute to our model’s current scale in terms of
model parameters and context length. We suspect that these
mistakes are due to the limited quality of the textual descrip-
tions, sometimes lacking precision and grounding on the
Figure 19. Ablation of Image Encoders Image vectorization re-
sults using different visual encoders on SVG-emoji test set. CLIP
is the image encoder that delivers the best results, whereas VQ-
GAN and ConvNet often miss relevant semantics of the image.
No SVG-Stack Pretrain (CLIP) refers to an ablation where we use
CLIP out of the box, without unfreezing its weights.
Figure 20. Ablation of Image Encoders Image vectorization re-
sults using different visual encoders on SVG-Stack test set. CLIP
offers the best results. VQ-GAN and ConvNet often miss relevant
semantics of the image.
SVG images. See Figure 24 for successful cases of Text-to-
SVG generation on SVG-Stack. Figure 29 highlights some
failure modes of StarVector-8B. These figures illustrate the
impact of different generation temperatures. We rank the
outputs generated at different temperatures based on their
CLIP Score in relation to the text instruction.
Nevertheless, the StarVector approach of using LLMs
for SVG code generation is the only method among base-
lines that allows us to create diverse vector graphics unre-


--- Page 28 ---

Figure 21. Ablation of Image Encoders Image vectorization re-
sults using different visual encoders on SVG-Icons test set. CLIP
brings the best visual results. VQ-GAN and ConvNext are not able
to capture correctly the details for correct vectorization. No SVG-
Stack Pretrain (CLIP) refers to an ablation where we use CLIP
out of the box, without unfreezing its weights. Notably, better re-
sults are obtained when training the CLIP image encoder on SVG-
Stack.
Figure 22. Ablation of Image Encoders Image vectorization re-
sults using different visual encoders on SVG-Fonts test set. As
in the other datasets tested, CLIP brings the best visual results, as
others are not able to provide perfect vector reconstruction when
intricate details are present. No SVG-Stack Pretrain (CLIP) refers
to an ablation where we use CLIP out of the box, without unfreez-
ing its weights. Notably, better results are obtained when training
the CLIP image encoder on SVG-Stack.
strainedly, paving the way for more challenging and intri-
cate designs.
10.3. Results on SVG-Diagrams
Figure 2 presents the results of StarVector-8B, along with
comparisons to LIVE, VTracer, Potrace, and AutoTrace.
StarVector-8B is the only approach that produces plausible
results, as it effectively leverages appropriate SVG primi-
tives. DinoScore aligns well with this visual assessment,
accurately reflecting the quality of StarVector-8B’s outputs.
In contrast, MSE consistently favors other baselines de-
spite their limitations. This is because MSE prioritizes exact
pixel matching, favoring models designed to fit curves and
colors to the input image. However, these baselines fail to
preserve the semantics of the original diagrams, resulting
in outputs where the meaning and structure are completely
lost.
Table 8. Results of SVG Data Augmentation. We ablate both our
data augmentation pipeline and the use of a pretraining stage with
SVG-Stack. These experiments are conducted on smaller datasets
that are more susceptible to overfitting, using the StarVector-1B
model. Vanilla refers to the StarVector model trained directly on
the given dataset without SVG-Stack pretraining. Next, we intro-
duce our data augmentation pipeline. Finally, we initialize train-
ing from an SVG-Stack pretrained checkpoint and fine-tune on the
given dataset. The “+” symbol indicates that the methods from the
previous rows are also included.
SVG-Emojis
SVG-Icons
Method
LPIPS ↓SSIM ↑MSE ↓LPIPS↓SSIM↑MSE↓
StarVector (vanilla)
0.355
0.683
0.108
0.104
0.845
0.047
+ Data Augmentation
0.329
0.706
0.097
0.057
0.905
0.029
+ SVG-Stack Pretrain
0.225
0.748
0.061
0.057
0.894
0.031
Table 9. Ablation of Image Encoders. We ablate different image
encoders with StarVector-1B, namely CLIP ViT-B/32 [62], VQ-
GAN [23], and ConvNext [46]. We experiment with training ex-
periments on SVG-Fonts and SVG-Emojis datasets. CLIP gives
the best results on all reconstruction metrics.
SVG-Fonts
SVG-Emojis
Encoder
LPIPS ↓SSIM ↑MSE ↓LPIPS ↓SSIM ↑MSE ↓
CLIP
0.026
0.955
0.021
0.202
0.778
0.051
VQGAN
0.092
0.854
0.072
0.345
0.688
0.099
ConvNext
0.085
0.854
0.073
0.311
0.708
0.088
10.4. Analysis of SVG Primitives
This section examines how StarVector leverages SVG prim-
itives to produce more compact and semantically accurate
SVGs.
In contrast to prior models constrained to using
only path primitives, StarVector effectively utilizes the en-
tire range of SVG primitives, including parametrically de-
fined shapes, gradients, and text elements.
This enhanced capability stems from its ability to operate
directly within the SVG code space, facilitated by its mul-
timodal, transformer-based architecture [2, 45], which inte-
grates visual and textual inputs. StarVector generates SVG
code that closely resembles the input raster image while
maintaining semantic awareness, enabling the use of sym-
metry, parametric shapes, and text. Prior methods, limited
to first-order path primitives, lack this semantic understand-
ing, resulting in less compact and less expressive SVG rep-
resentations.
Qualitative Analysis of SVG Primitives
Table 10
presents tests conducted on StarVector-8B and VTracer us-
ing simple designs composed of basic shapes such as cir-
cles, rectangles, and triangles, with variations in color,
transparency, and levels of overlap. StarVector-8B demon-
strates the ability to precisely identify the primitives that


--- Page 29 ---

Figure 23. Ablation study on sampling temperature. We tested the performance impact of StarVector-1B when changing the sampling
temperature. Results are computed for SVG-Emoji and SVG-Icons validation sets.
make up each design, producing visually accurate results
while maintaining compact, concise, and interpretable SVG
representations.
SVG Tag Distribution
Here, we show how StarVector
can generate complex SVGs using the full syntax of the
SVG language, in contrast with most of the literature meth-
ods, which are restricted to using only the path command.
Figure 30 displays the distribution of SVG tags in the SVGs
generated by StarVector along with the distribution in the
SVG-Stack dataset, showcasing the strength of our method
in using SVG tags and syntax in a similar way to the origi-
nal in-the-wild dataset. We have computed the exact statis-
tics on previous methods and found that they cannot come
close to StarVector in this metric, as they are limited to us-
ing paths and basic primitives. The effective usage of the
large array of SVG tags and syntax makes our method the
first SVG model to support these complexities.
10.5. Human Evaluation
We conducted a human evaluation to compare the outputs
of StarVector-8B, our best model, with those of the most
powerful baselines. Participants were selected from diverse
backgrounds and carefully screened for conflicts of interest,
with none of the key authors involved. The evaluation was
performed through a web interface (shown in Figure 31)
that provided anonymized outputs and randomized sample
presentations to prevent pattern recognition or bias.
The results, presented in Figure 5, demonstrate a strong
preference for StarVector-8B across all settings, especially
in SVG-Diagrams tasks. This highlights a disconnect be-
tween pixel-based metrics (e.g., MSE, SSIM) and human
visual perception of SVGs.
While baseline models of-
ten prioritize pixel-perfect reconstruction, human evaluators
preferred StarVector’s sharp, well-defined shapes and its ef-
fective use of primitives (Figure 30).
Spearman correlation analyses between model metrics
and human evaluations further emphasize this gap. MSE
shows weak correlations (0.0596 and -0.1002), indicating
its inadequacy as a predictor of human preferences. In con-
trast, DinoScore exhibits significantly stronger correlations,
with values of -0.6193 and 0.6214. Moreover, a robust cor-
relation of 0.7577 between differences in DinoScore and
human evaluation scores highlights DinoScore as a more
reliable metric for assessing SVG quality in alignment with
human judgment.
10.6. Comparing StarVector with Baselines
Here, we discuss the results of each baseline individually
and compare them to our proposed approach.
1. DeepSVG [14] is an elegant approach to learning a la-
tent variable model for SVG. It proves effective at learn-
ing the task for the simplified datasets (Figure 9). It can
accurately represent corners and edges. However, it only
works in simplified datasets. This limitation restricts it
from being a suitable solution in real applications.
2. Im2Vec [67] proposes a training procedure that does not
require having SVG ground truth. It uses only pixel-
based reconstruction loss with the input image, finding
the optimal SVG parameters using a differentiable ras-
terizer like DiffVG [43]. This framework is appealing,
as it aims to be used in images without SVG supervi-
sion. However, it requires hundreds of epochs with a
reduced dataset to overfit the model to those examples,
only working on modeling training examples, as seen
in [67]. Im2Vec results on the datasets presented SVG-
Bench are quite poor as it has bad generalization. There-
fore, qualitative samples are not presented.
3. GPT-4 Vision [56] excels at capturing the semantics
of images and generating captions that accurately de-
scribe them. The SVG generated from this description


--- Page 30 ---

Ground Truth
Temp = 0.0
Temp = 0.125
Temp = 0.5
Temp = 0.25
Temp = 0.375
Text Instruction
Generate an SVG of x
icon in black circle on
white background
Generate an SVG of a
green check mark in a
flower
Design an SVG of
pencil icon vector 
Design an SVG of a
colorful square logo
with a blue background.
Draw an SVG image of
four square black and
white icons.
Create an SVG of  a
large orange bell,
positioned in the center
of the image.
Generate an SVG of a
folder with a fan on it.
Draw an SVG picture of
a black and white icon
of a paper document
Draw an SVG image of
a blue and white train
icon.
Create an SVG for a
logo of a company
with a gold triangle
Figure 24. Text-to-SVG Results. We show successful Text-to-SVG results using StarVector-8B. We sample 5 different temperatures as an
ablation, showing the sensitivity of this parameter during generation. Results are presented in SVG (not raster images)


--- Page 31 ---

Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
A black and
white image of
a cross
A circle with
dots on it in
black and
white
Tv icon vector
A file icon
with a white
square on it
The japanese
flag is shown
in a white
background
Chinese font
design for the
word 'love'
A black and
white icon of a
pen and paper.
A black circle
with a play
button in the
center.
Memory size
pie chart
Memory Size
16384 (22.2%)
4096 (22.2%)
2048 (16.7%)
32768 (5.6%)
8192 (33.3%)
A knife is
shown on a
white
background
An i symbol in
a black circle
A pink light
bulb icon.
A purple and
white striped
logo
A white arrow
on an orange
background.
A white cross
on a white
background.
A square shape
is shown in the
image.
A gray circle
with a question
mark in the
center.
A green smiley
face with a
smiley face
Folder icon
with a person
icon
A fork and a
leaf icon on a
white
background
An eye icon
with a triangle
shape
A yellow star
is displayed on
a white
background.
Sun icon in a
circle on a
white
background
An open book
icon on a white
background
A black and
white
microphone
icon.
A black star on
a white
background
A black and
white icon of a
no entry sign
A green button
with a gray
arrow pointing
to it
A black and
white icon of
an envelope.
A black and
white sun
symbol.
A black and
white square
with a black
border.
A black and
white map
marker
symbol.
A green and
black circle
with a white
background
A black and
white image of
a game
controller
A video player
icon is
displayed in a
square.
The s logo in
black and
white
A black and
white headset
icon.
A graph of the
number of
people in a
class
A black and
white icon of a
trophy cup
A red square
with a white
letter A in the
center.
An eye icon in
a circle with a
black dot
A blue and
white icon of a
camera.
A pink pen on
a white
background
vector
Standard js
logo
JS
A black and
white logo of a
curved shape
A blue and
white paper
clip icon.
A black and
white
illustration of a
cell phone
A speech
bubble with an
x symbol on it
A blue and
white sign that
says "continue
setup".
A gray square
with four
squares on it
The facebook
logo, with a
white letter f
Apple pay logo
The flag of
france and the
flag of france
A yellow star
with a black
outline.
A black and
white triangle
shape with two
lines
A black and
white icon of a
lightning bolt
A yellow sign
with the
number 10 on
it.
A black and
white image of
a japanese
symbol
Chinese font
for the word
love
A pie chart
with the words
ethernet model
Ethernet Model
82579LM Gigab...
(5.6%)
Ethernet
Conn... (5.6%)
I211 Gigabit...
(5.6%)
RTL810xE PCI
... (11.1%)
Others (27.8%)
RTL8111/8168/... (44.4%)
A blue square
with a black
line in the
middle.
A white and
gray checkered
pattern
The icon for
the instagram
app
A black circle
is placed on a
white
background.
A cross symbol
with four lines
The facebook
logo is shown
on a white
background
A black and
white arrow
pointing to the
right
A red heart
with a
lightning bolt
through it.
A black cross
on a white
background.
A person
avatar icon on
a white
background
A black and
white airplane
icon.
A heart with a
cross in it
An envelope
icon on a white
background
The facebook
logo in black
and white
A white square
with a black
border
A black heart
shape on a
white
background.
A blue button
with the word
stata on it
The flag of
oman is shown
in red and
green
A black cross
on a white
background
A blue bar is
displayed on a
white
background.
A black and
white logo
with two
arrows
A black square
is placed on a
white
background.
A black circle
with a white
square in the
middle.
A clock icon is
displayed in a
white
background.
Figure 25. Text-to-SVG Generation results using StarVector-1B on SVG-Stack test set (i).


--- Page 32 ---

Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Square icon
vector
A red
emoticion with
an angry face
The letter v is
shown in black
and white
A pink speech
bubble on a
white
background
Whatsapp logo
A black and
white icon of a
sun
A black and
white image of
a large X.
Twitter logo in
a circle
A yellow and
black shield
with a rooster
on it.
A black star
with a white
outline.
A grey and
white image of
a lock and key.
A white X is
displayed on a
pink
background.
A yellow
triangle with a
black outline.
A black and
white logo
with a white
eye
A black and
white icon of a
document
Headphones
icon vector
A black and
white icon of a
radio wave.
A black and
white light
bulb icon.
A black and
white cloud
with a rain icon
A pixel style
image of a
brown box
A black and
white logo for
JPG.
A purple circle
with an
envelope icon
Pixel heart png
A black circle
with a smiling
face on it.
Three stacked
white boxes
with green
lines
The v logo
with a blue
arrow
Ethereum logo
with a blue
background
A blue cloud is
displayed on a
white
background.
A black and
white coffee
cup icon.
A black and
white YouTube
icon.
A letter a logo
with a red and
black letter
A purple circle
with a down
arrow in the
center.
A black and
white crescent
on a white
background
A black and
white logo of a
drop
A black and
white pizza
icon.
A phone icon
in a heart shape
The s logo in a
black and
white circle
A black circle
with a white
circle in the
middle.
A black and
white cloud
icon.
The c logo in a
green square
A black and
white logo of a
letter G.
A hand holding
a bag with the
letter r
A blue arrow
pointing up on
a white
background
The letter c in
black and
white
A phone icon
with a plus
sign
Youtube logo
with a play
button
A yellow
emoticion with
glasses on it
Chinese
character for
cross
A black and
white shopping
cart icon.
A black and
white
magnifying
glass icon.
A brown and
white battery
icon.
A black and
white clock
with a blue
hour hand.
Clipboard icon
with a square
and a square
The html5 logo
with the word
html5
A purple
square icon
with two
circles on it
A black and
white image of
a hamburger.
A black and
white film reel
icon
A green circle
with an arrow
pointing
upwards.
A black and
white sign with
the letter e
Sun icon
vector
A blue circle
with a white V
in the center.
Credit card
icon vector
A dollar sign
icon on a white
background
A black and
white square
with a white
border
A black and
white key
symbol.
A red triangle
with a white
background
Cross symbol
clip art
A black and
white icon of a
refrigerator.
A purple and
gray circle
with a crescent
moon
A white and
purple logo of
a letter C.
A purple circle
with a knife
icon
A black and
white image of
a question
mark.
A blank white
card with a
green border
A blue square
with an x in the
middle
A black and
white smiley
face icon
A black and
white icon of a
trash can.
A diagram of
the heartbeat
api
IStream
device_stream< IStream >
A red power
button with an
arrow pointing
up
Sun icon clip
art
Google plus
logo
Youtube logo
with a red play
button
A blue circle
with a white
center
A black and
white mouse
cursor icon
A black and
white icon of a
x
Figure 26. Text-to-SVG Generation results using StarVector-1B on SVG-Stack test set(ii).


--- Page 33 ---

Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
I love
Halloween skul
l-and-bones
emoji.
A necktie is a
formal
neckwear for a
uniform.
A square is a
quadrilateral
shape in
geometry.
The abstract
square is
distorted by a
third.
Map pin.
The lorry is a
delivery truck
for transport.
The rupee is
currency.
Weight
dumbbell.
Remove
groceries from
shopping
basket.
Cleaning
laundry with a
washing
machine.
The soup
spoon is
tableware.
A square and a
cube.
Hand-drawn
sad emoticon
upset and
angry.
Player
interface for
video play.
A good idea is
thinking about
a light bulb.
The up arrow
directs the
move.
The arrow sign
shows the
three-way
direction.
Wifi-router is
an electronic
device.
A frozen sweet
treat, popsicle
ice-cream.
Wifi is online
through w-lan.
Unlock your
smartphone
screen
notification.
Mustache.
Pinpoint your
music interests
on the map.
The gear
wheel's
configuration
is set by the
cog.
There is a
signpost with a
direction
arrow.
Search with a
magnifier to
explore and
view.
A square is a
shape.
Bell
notification.
A bar chart is a
type of chart.
A tank-top is a
type of shirt or
undershirt.
A necktie is an
accessory for
clothes.
The smiley
emoji
represents
happy
emotions.
The milk bottle
is a dairy
beverage.
The chef wore
a toque in the
kitchen.
Man has a
mustache.
The arrow
points down to
the heart with
love.
I like the social
like with a
thumbs-up.
The Holy
Cross is a
Christian
symbol.
Create new
cloud server
plus.
Upload bag
shopping
arrow.
The pin marks
the location
with a pointer.
A hexagon is a
shape with 6
sides.
The map pin
marks the
location.
Move in the
direction of the
sharp turn
arrow.
The sun is a
solar emoji
emoticon.
The user's
avatar is a map
pin on their
profile.
Right arrow.
The idea
invented
electricity's
light bulb.
Search using a
magnifying
glass.
Wifi.
Protect the
privacy of a
private
document/file.
Letter or email
message.
Data charted
on a graph
shows market
statistics.
I drive my
backpack with
flash storage.
Search and
find with a
magnifying
glass.
A medal,
award, badge,
prize, or star.
Admin shields
star.
The anchor
stopped the
heavy navy
ship.
A rhombus is a
symmetrical di
amond-shaped
figure.
Money is
currency or
cash, such as
the euro.
Chat using a
speech bubble.
The unhappy
face emoji.
I wore a
necktie.
Check the
checklist for a
checkmark.
The audio cord
connects to the
microphone.
A rupee coin is
currency used
for payment.
Aircraft,
missile, rocket,
spacecraft.
Web hosting
on the cloud
server.
Search with
magnifying
glass tool and
zoom.
A beer mug.
Apple is a fruit.
The pound
bulb is an idea
for a light bulb.
Usb flash
drives store
data in
memory.
Move the
arrow to
expand and
drag to drop.
Search with
magnifier.
One dice is
used in the
game of luck.
Favorite star.
The bus is a
vehicle for
transportation.
The arrow
indicates the
up direction
where.
Candle.
A trapezoid is
a shape.
Find location
with GPS map
pin.
Follow the
arrow for
direction.
The cassette
tape contains
music sound
on tape.
Figure 27. Text-to-SVG generation on FIGR-SVG test set (i).


--- Page 34 ---

Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Caption
Svg Gen.
Left arrow
points
backward.
A people group
network.
Women have a
female gender
symbol.
Spruce, pine,
and Christmas
trees are all
plants.
Download the
signpost arrow
direction.
Flat brush used
for painting
flat illustration.
Shop for a cart
while
shopping.
Sweep with a
broom to clean
the floor.
The bowl is a
food container
in the kitchen
set.
The tool has
options and
settings with
cogs.
The atom has a
proton link in
physics
science.
Boat or ship.
Map-pin marks
a location on a
map.
The map pin
shows the
location with
GPS.
The
erlenmeyer
flask is a
chemistry
equipment.
Bug is an
insect that can
have a virus or
error.
The modern
thick arrow
points right.
Male restroom
sign.
The pill bottle
contained
drugs for
health.
Snowflakes fall
in winter
weather.
The pointer
points to the
arrow right.
Tie.
Service bell.
Anchor.
Wifi is
wireless
connectivity to
a network.
The pharmacy
uses a mortar
and pestle.
The ace of
clubs is a poker
card.
Move in the
direction of the
right arrow
square.
Map-pin.
Minimal
trophy.
Watch TV on
monitor.
Connect to the
Wi-Fi.
Crown.
Lightning has
electricity and
high voltage.
Music sound
files contain
musical notes.
Find and focus
by zooming
with a
magnifier.
Media player
plays movie,
music, and
video.
Buy a home
online using a
laptop.
Record sound
with a
microphone for
voice audio.
Water in a
plastic bottle is
a beverage
drink.
Up arrow
indicates
direction.
Cylinder.
Boxer-briefs
are a type of
underwear.
Withdraw cash
from ATM for
euro money.
A smartwatch
displays the
time.
Calculate math
using a
calculator for
accounting.
Target in
crosshair.
A spoon is
silverware.
Arrow points
up at junction
for navigation.
The MKV file
extension is a
type of video
file.
Search for
glass with mag
nifying-glass
view.
Christmas ball
is a decoration.
Find location
using GPS
search and
pins.
Secure euro
payment with
money lock.
The up arrow
indicates the
top direction.
My favorite
documents are
starred on my
computer.
Drawers in
wardrobe.
Wifi is a
network for
internet
connection.
Erlenmeyer
flask and
beaker.
Play videos on
YouTube using
the video
player.
Place a map
pin.
The receiver
follows the
direction of the
arrow.
Buy cart or
basket from
market for
shopping.
The wise owl
has evil eyes.
Chat or
message when
talking in a
conversation.
Online video
on desktop PC.
Snowflakes
have
symmetry.
The mpg file is
a document
format.
Download the
pointer arrow
down using
technology.
I rate a half
star.
Euro.
The pitcher
holds a
beverage.
Ice cream is a
dessert food
popped in a
popsicle.
Christianity
revolves
around Jesus
and the cross.
Sort tiles in a
grid.
A file is a
document,
paper, or sheet
with pages.
A QR code is a
type of general
barcode.
Search and
find tools
magnify and
zoom.
Online
shopping
trolley/cart
finance.
Camera takes
photos with
lens for
multimedia.
The
magnifying
glass can
indicate an
enlargement.
The
thermometer
measures
temperature in
Celsius.
Picture camera
photography.
A Wi-Fi
network signal.
Figure 28. Text-to-SVG generation on FIGR-SVG test set (ii).


--- Page 35 ---

git
git
Design an SVG of a
black cat in tears.
Make an SVG of HC
logo in a white circle
Make an SVG of a
black arrow pointing
upwards.
Generate an SVG of a
black and white image
of a box with a square
shape.
Design an SVG of a
black and white image
of three dots
Draw an SVG image
of  a green recycling
symbol, three arrows
pointing inward. 
Design an SVG of the
tesla logo 
Make an SVG of a
black and white icon of
a computer program
called "GIT".
Generate an SVG of a
blue and gray square
with a blue arrow
pointing to it.
Ground Truth
Temp = 0.0
Temp = 0.125
Temp = 0.5
Temp = 0.25
Temp = 0.375
Text Instruction
Design an SVG of a
woman wearing a
yellow shirt and blue
jeans.
Figure 29. Text-to-SVG Results. We show failure Text-to-SVG results using StarVector-8B (cherry picked examples that show limita-
tions). We sample 5 different temperatures as an ablation, showing the sensitivity of this parameter during generation. Results are presented
in SVG (not raster images)


--- Page 36 ---

Table 10. Usage of SVG Primitives. Image vectorization results of StarVector and VTracer applied to images containing basic shapes,
such as circles, rectangles, and polygons, with varied colors and transparencies. The leftmost column shows the input images prompted
for vectorization, and other columns show the output SVG code, with the SVG primitives in red color. StarVector accurately identifies and
generates SVG code for each primitive, preserving their distinct characteristics. In contrast, VTracer relies on the path primitive, resulting
in SVG code that captures the input image in terms of pixels, with less fidelity to individual shapes. Due to the length of VTracer’s SVG
output, only the initial lines are shown. VTracer serves as a baseline model, representative of other baselines, which are omitted for space
but exhibit similar behavior, primarily using path without shape recognition.
Test Example
StarVector
VTracer
<svg width="150" height="150" xmlns="http://www.w3.org/2000/svg"
xmlns:xlink="http://www.w3.org/1999/xlink">
<rect x="35" y="0" width="114" height="132"
style="fill:rgb(255,50,50);stroke-width:1;stroke:rgb(0,0,0)"/>
<polygon points="56.25,49.5 112.5,147.75 0,147.75"
style="fill:rgb(200,0,200);stroke-width:1;stroke:rgb(0,0,0)"/>
</svg>
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="530"
height="460">
<path d="M0 0 C174.9 0 349.8 0 530 0 C530 151.8 530 303.6 530 460 C355.1
460 180.2 460 0 460 C0 308.2 0 156.39999999999998 0 0 Z "
fill="#FE3232" transform="translate(0,0)"/>
<path d="M0 0 C4.06429570258797 3.6330418563035494 6.775718865918918
7.966461775462079 9.71484375 12.50390625 C10.273576812744153
13.357300872802739 10.832309875488278 14.210695495605478..."
fill="#A95869" transform="translate(343,60)"/>
<path d="M0 0 C40.260000000000005 0 80.52000000000001 0 122 0
C123.16960912291928 66.0224184213402 123.16960912291928
66.0224184213402 123.416015625 93.876953125
C123.47276957931317..." fill="#A95869"
transform="translate(343,60)"/>
...
</svg>
<svg version="1.1" id="Layer_1"
xmlns="http://www.w3.org/2000/svg"
xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
width="64px" height="64px" viewBox="0 0 64 64"
enable-background="new 0 0 64 64" xml:space="preserve">
<ellipse fill="none" stroke="#CD2A2A" stroke-width="2.5"
stroke-miterlimit="10" cx="32.333" cy="32.083" rx="29.333"
ry="31.25"/>
<polygon fill="#CD2A2A" stroke="#CD2A2A" stroke-width="0.25"
stroke-miterlimit="10" points="32.333,3.833 55.167,46.333
9.5,46.333 "/>
<line fill="none" stroke="#000000" stroke-width="0.25"
stroke-miterlimit="10" x1="0" y1="0.167"x2="64"y2="0.167"/>
</svg>
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="1196"
height="1140">
<path d="M0 0 C394.68 0 789.36 0 1196 0 C1196 376.20000000000005 1196
752.4000000000001 1196 1140 C801.3199999999999 1140 406.64 1140 0
1140 C0 763.8 0 387.5999999999999 0 0 Z " fill="#FEFEFE"
transform="translate(0,0)"/>
<path d="M0 0 C1.8671875 0.24609375 1.8671875 0.24609375 4 1 C5.1328125
2.722656250000071 5.1328125 2.722656250000071 6.125 5.0625
C8.342476699064719 9.859072675207557 10.955923466443437
14.196311705833779 13.8125 18.625 C17.369173047863796 ..."
fill="#A95869" transform="translate(343,60)"/>
<path d="M0 0 C1.3488281421188617 2.6976562842377234 0.8033826481469646
4.151643338387942 0 7 C-0.6599999999999682 7.329999999999984
-1.3199999999999932 7.660000000000025 -2 8 C-2.6555511917654258
10.527332350510733 -2.6555511917654258 10.527332350510733 ..."
fill="#A95869" transform="translate(343,60)"/>
...
</svg>
<svg width="300" height="300"
xmlns="http://www.w3.org/2000/svg" version="1.1">
<rect x="110" y="10" width="160" height="230" fill="pink"
stroke="red"/>
<circle cx="160" cy="120" r="120" fill="tan" stroke="green"/>
<polygon points="110,20 280,125 175,280 5,180"
fill="blue" fill-opacity="0.25" stroke="blue" stroke-width="2" />
</svg>
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="910"
height="934">
<path d="M0 0 C300.3 0 600.6 0 910 0 C910 308.22 910 616.44 910 934
C609.7 934 309.4 934 0 934 C0 625.78 0 317.55999999999995 0 0 Z "
fill="#928299" transform="translate(0,0)"/>
<path d="M0 0 C1.4849999999999994 0.9900000000000091 1.4849999999999994
0.9900000000000091 3 2 C3 2.659999999999968 3 3.32000000000005 3 4
C3.7296093749999955 4.103124999999977 ..." fill="#A95869"
transform="translate(343,60)"/>
<path d="M0 0 C1.32000000000005 0.6599999999999966 2.6399999999999864
1.3200000000000003 4 2 C3.009999999999991 2 2.019999999999982 2 1
2 C1 2.6599999999999966 1 3.3200000000000003" fill="#A95869"
transform="translate(343,60)"/>
...
</svg>
<svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<circle cx="50.5" cy="39.5" r="40" fill="#60f0ff" />
<ellipse cx="50" cy="39" rx="50" ry="20" fill="#2810ff" />
<polygon points="10,100 50,42 90,100" fill="green" />
</svg>
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="510"
height="520">
<path d="M0 0 C168.3 0 336.6 0 510 0 C510 171.6 510 343.2 510 520 C341.7
520 173.39999999999998 520 0 520 C0 348.4 0 176.8 0 0 Z ..."
fill="#2E17FE" transform="translate(0,0)"/>
<path d="M0 0 C0.6599999999999682 0 1.3199999999999932 0 2 0
C3.1463165283203125 1.3810577392578693 3.1463165283203125
1.3810577392578693 4.428955078125 3.356689453125
C4.916970977783194 " fill="#2E17FE" transform="translate(0,0)"/>
<path d="M0 0 C1.5393356364577242 2.844848138263643 2.426629556693727
5.489600992013067 3.1875 8.625 C8.714349606195654
27.37779319117132 28.666518702768336 42.668449172855844 45 52
C77.0038243523029 ..." fill="#2E17FE" transform="translate(0,0)"/>
<path d="M0 0 C0.3300000000000409 0 0.6599999999999682 0 1 0 C1 101.31 1
202.62 1 307 C-16.159999999999968 307 -33.31999999999999 307 -51
307 C-69.10137393182612 285.06539394143425 -69.10137393182612
285.06539394143425 -73.55859375 275.48046875 ..." fill="#2E17FE"
transform="translate(0,0)"/>
...
</svg>
is valid and effectively incorporates semantic concepts
along with the accurate colors of the input image into
the SVG code (see Figures [9 - 12]). However, it falls
short in terms of reconstruction fidelity, as GPT-4V was
not specifically trained for reconstruction tasks, making
these results predictable.


--- Page 37 ---

Outer: Ground Truth
Middle: StarVector
Inner: Baseline (LIVE)
path (30.87%)
g (19.20%)
text (5.93%)
rect (5.64%)
title (4.31%)
polygon (4.07%)
stop (3.80%)
line (3.28%)
circle (2.66%)
defs (2.27%)
ellipse (2.10%)
linearGradient (1.71%)
use (1.46%)
style (1.02%)
tspan (0.88%)
a (0.80%)
div (0.68%)
metadata (0.63%)
animate (0.60%)
clipPath (0.55%)
RDF (0.48%)
Work (0.48%)
format (0.47%)
type (0.47%)
desc (0.47%)
namedview (0.42%)
polyline (0.37%)
filter (0.37%)
feGaussianBlur (0.33%)
feColorMatrix (0.29%)
switch (0.28%)
foreignObject (0.28%)
radialGradient (0.26%)
feOffset (0.21%)
feBlend (0.21%)
mask (0.21%)
marker (0.18%)
feFlood (0.11%)
feComposite (0.11%)
image (0.10%)
li (0.10%)
symbol (0.10%)
feMergeNode (0.10%)
permits (0.09%)
Agent (0.08%)
guide (0.08%)
grid (0.07%)
ud (0.07%)
p (0.06%)
perspective (0.06%)
path-effect (0.06%)
br (0.05%)
feMerge (0.05%)
creator (0.04%)
userDefs (0.04%)
license (0.04%)
font (0.04%)
pattern (0.03%)
Bag (0.03%)
feMorphology (0.03%)
date (0.03%)
subject (0.03%)
License (0.03%)
textBlock (0.03%)
textRect (0.03%)
paragraph (0.03%)
tabList (0.03%)
description (0.02%)
requires (0.02%)
path (34.07%)
g (27.19%)
use (16.94%)
text (2.81%)
rect (2.79%)
polygon (2.05%)
title (1.97%)
stop (1.94%)
symbol (1.76%)
line (1.40%)
circle (1.17%)
defs (1.01%)
ellipse (0.93%)
clipPath (0.81%)
linearGradient (0.63%)
style (0.48%)
a (0.42%)
tspan (0.29%)
animate (0.28%)
polyline (0.13%)
radialGradient (0.11%)
metadata (0.09%)
desc (0.09%)
mask (0.07%)
filter (0.07%)
RDF (0.06%)
guide (0.06%)
format (0.06%)
type (0.06%)
Work (0.06%)
namedview (0.05%)
feGaussianBlur (0.05%)
feColorMatrix (0.05%)
feOffset (0.03%)
div (0.03%)
feBlend (0.02%)
switch (0.02%)
foreignObject (0.02%)
path (96.76%)
defs (1.62%)
g (1.62%)
Figure 30. Distribution of SVG Primitives and SVG Tags. We show the frequency of SVG tags that appear in the ground truth of SVG-
Stack dataset (outer ring), compared to the frequency of tags generated by StarVector (middle ring), and compared to our most performant
baseline (LIVE). For visualization purposes, we apply a logarithmic scaling over the counts and show the base percentage in parentheses.
StarVector generates SVGs that contain tags with a similar distribution to the ground truth. The baseline is limited to paths or basic
primitives.
4. LIVE achieves the best results in terms of pixel-based
metrics like MSE, LPIPS, and SSIM (see Tables [1, 2,
3]). Using 32 paths, it effectively represents a wide range
of images, making it highly versatile for vectorization
tasks, including natural images, which StarVector cannot
handle due to its specialized training. However, LIVE
has notable limitations. It relies on a slow test-time opti-
mization process (approximately 10 minutes per sample,
using 32 paths, Table 3) to refine SVG outputs for good
MSE scores, often introducing unwanted visual artifacts.


--- Page 38 ---

Additionally, its exclusive use of path primitives results
in significantly larger SVG files (19k tokens), as shown
in Table1. In contrast, StarVector produces compact (3k
tokens) and professional-grade SVGs by leveraging a va-
riety of SVG primitives beyond paths, achieving higher
precision and efficiency.
Notably, we find that this method, and the broader fam-
ily of differentiable techniques it belongs to, is unsuit-
able for generating images that require specific primi-
tives, such as those in diagrams. Its performance on the
SVG-Diagrams benchmark is poor, as illustrated in Fig-
ure 14.
5. DiffVG offers comparable results as the other baselines
in terms of reconstruction metrics, and it can produce
suitable SVG image vectorization with 32 paths (same
as LIVE), as seen in Tables [1, 2, 3]. This tables also
show that DiffVG produces large SVGs as seen in the
number of tokens (Tokens column), approaximately 19k
tokens, similar to LIVE. This means that files are ex-
tremely large compared to the ones of StarVector. As
mentioned before, this is due to StarVector leveraging
understanding and SVG primitieves. Nevertheless, this
method is substantially faster than LIVE, requiring 30
seconds per sample.
6. Image Processing Methods: VTracer, Potrace, Auto-
trace Previous image processing methods for Image-to-
SVG are powerful, excelling at fitting vector images to
raster inputs with near-zero MSE while reliably captur-
ing the shapes and colors of the original image. How-
ever, we identify several shared limitations across
these methods: (1) lack of SVG file compression, as
they often generate excessively long paths (see the To-
kens column in Tables 1 and 2); (2) susceptibility to vi-
sual artifacts, especially with challenging patterns; and
(3) poor performance in vectorizing diagrams, as illus-
trated in Figure 14. On the SVG-Diagrams benchmark,
VTracer, Potrace, and AutoTrace struggle with pro-
ducing high-quality results.
These methods perform best on images that can be seg-
mented into distinct regions by color or texture but fail
with complex patterns, such as small, closely spaced
shapes or fine details. For example, in Figures 9 and 4,
small polygons and intricate text are inadequately vec-
torized, with details often lost. All three methods are
restricted to path primitives, limiting their ability to sup-
port features like optical character recognition or render-
ing text with the <text> tag. Although Potrace some-
times better preserves text compared to others, it still
cannot recognize or encode it semantically.
Despite these limitations, these methods excel in genera-
tion speed, making them highly efficient for many tasks.
As shown in Table 3, VTracer and AutoTrace can gen-
erate SVGs in under a second, while Potrace typically
takes around 10 seconds. In comparison, StarVector re-
quires over a minute, and other baselines can take any-
where from 10 to 20 minutes. While StarVector’s seman-
tic richness and compact outputs make it better suited
for certain applications, these image processing methods
demonstrate a clear advantage in scenarios where speed
is critical.
7. IconShop achieves remarkable results on the SVG-
FIGR dataset, as demonstrated in Table 4 and Figures
of their original work [94]. However, it is not designed
to handle SVG-Stack due to its restriction to modeling
only the path primitive. StarVector outperforms Icon-
Shop, as shown in Tables 4, across metrics such as FID,
FID CLIP, and CLIP Score. Qualitative examples further
highlight StarVector’s superior performance in Text-to-
SVG generation within the FIGR-SVG dataset.
8. LLMs for Code Generation. Methods utilizing LLMs
to directly generate SVG code present appealing advan-
tages. In our evaluation, we assessed GPT-4, GPT-4V,
CodeLlama, and our proposed StarVector approach. By
leveraging the code space, these models can utilize var-
ious SVG primitives based on their understanding of
raster images, including path shapes and higher-order
primitives like circles and text. This capability enables
applications in new domains, such as diagram genera-
tion, as demonstrated by the results in Figure 14. How-
ever, most LLMs have not been specifically trained for
the Image-to-SVG task, which limits their performance.
In contrast, StarVector outperforms other LLMs due to
its dedicated architecture and tailored training method-
ology, excelling in both image understanding and SVG
generation.
Upon reviewing the complete set of results, we conclude
that the StarVector approach is the only deep learning-
based Image-to-SVG model capable of achieving results
comparable to those of VTracer, Potrace, and AutoTrace.
Furthermore, StarVector paves the way for novel re-
search avenues in vector graphics generation, enabling
applications such as diagram generation, Text-to-SVG
generation, and potentially enhanced editing and under-
standing of vector images.


--- Page 39 ---

10.7. Human Evaluation
We conducted a human evaluation to compare the out-
puts of StarVector-8B, our best model, with those of
the most powerful baselines. Participants were selected
from diverse backgrounds and carefully screened for
conflicts of interest, with none of the key authors in-
volved. The evaluation was performed through a web
interface (shown in Figure 31) that provided anonymized
outputs and randomized sample presentations to prevent
pattern recognition or bias.
The results, presented in Figure 5, demonstrate a strong
preference for StarVector-8B across all settings, espe-
cially in SVG-Diagrams tasks.
This highlights a dis-
connect between pixel-based metrics (e.g., MSE, SSIM)
and human visual perception of SVGs.
While base-
line models often prioritize pixel-perfect reconstruction,
human evaluators preferred StarVector’s sharp, well-
defined shapes and its effective use of primitives (Fig-
ure 30).
Spearman correlation analyses between model metrics
and human evaluations further emphasize this gap. MSE
shows weak correlations (0.0596 and -0.1002), indicat-
ing its inadequacy as a predictor of human preferences.
In contrast, DinoScore exhibits significantly stronger
correlations, with values of -0.6193 and 0.6214. More-
over, a robust correlation of 0.7577 between differences
in DinoScore and human evaluation scores highlights
DinoScore as a more reliable metric for assessing SVG
quality in alignment with human judgment.
11. StarVector Method
Here, we provide details on the StarVector architecture,
training recipe, and generation process.
11.1. Architecture
11.1.1. Large Language Model
We consider several aspects when choosing the LLM to
handle the SVG code generation. First, we require an LLM
that can handle large token contexts during training, as SVG
code samples are typically of long lengths (i.e., between
1,000-4,000 tokens for the most common SVG datasets but
growing arbitrarily for much more complex vector graph-
ics). Second, we need fast decoding during the generation
of these large contexts. Finally, we would benefit from mod-
els that have been extensively pre-trained on general coding
tasks to avoid early training costs.
Some prior works offer open-source models that fit these
requirements.
We explored the open-source families of
models CodeGen [53], StarCoder [42] and StarCoder2 [49].
We empirically find the StarCoder family to be
the most suitable choice for our requirements.
Star-
Coder offers a pre-trained model with 1B parameters
(starcoderbase-1b) and a context length of 8k tokens,
making it ideal for smaller-scale experiments while main-
taining efficient generation speeds.
The model employs
Multi-Query Attention and a Fill-in-the-Middle objective,
trained on 1 trillion tokens, with a context window of 8192
tokens. Its compact size ensures compatibility with GPUs,
facilitating data parallelism during training—a crucial ben-
efit when fine-tuning all network parameters, including the
memory-intensive image encoder.
The second generation, StarCoder2, extends the context
length to 16,384 tokens, presenting an exciting avenue for
exploring training on SVGs with longer context require-
ments.
For this, we leverage the 7B parameter version
(starcoder2-7b), which incorporates a sliding window
attention mechanism of 4,096 tokens and was trained on
over 3.5 trillion tokens of code using the same Fill-in-the-
Middle objective [49]. This enhanced context capacity and
training scale make it a promising candidate for scaling
SVG-based experiments.
These two types of LLMs define the backbones of
our two StarVector variants.
StarVector-1B is based on
the starcoderbase-1b architecture and weights, while
StarVector-8B is based on the starcoder2-7b architec-
ture and weights.
11.1.2. Image Encoder
Our image encoding pipeline computes the images’ feature
representations using a backbone image encoder and aligns
them to the LLM via the Adapter module (see Figure 3).
State-of-the-art image encoders are typically focused on
natural images. However, our data contains images of lo-
gotypes, icons, fonts, or emojis, which usually contain no
background (which we set to white) and mostly constant
colors.
Note that the image encoder is used exclusively for the
Image-to-SVG task and is not employed during the Text-
to-SVG task. For Image-to-SVG, images must be projected
into a representation with the same dimensionality as that of
the LLM. We train the model to enable the LLM to ingest
these representations and generate SVG code sequentially.
To choose the best encoder, we draw inspiration from
the success of pre-trained encoder backbones in down-
stream computer vision tasks such as classification [62], re-
trieval, and generation [23], including both convolutional
and transformer-based models. Specifically, we experiment
with CLIP ViT-B/32 [62], ConvNext [46] (pre-trained on
LAION-2B [75]), and VQGAN [23], which we pre-train
on an image reconstruction task using raster images from
SVG-Stack. For CLIP, we have Lv = 257 embeddings,
including the CLS token. For VQGAN, we use the pre-
quantization layers and flatten them to obtain Lv = 196
embeddings. For ConvNext, we flatten the last activation
map to get Lv = 49 embeddings.
We explore several image encoders based on different


--- Page 40 ---

Figure 31. The web interface used during the human evaluation.
paradigms. VQGAN [23] is based on learning to project
images to discrete tokens.
First, we fine-tune an Ima-
genet [19]-pretrained VQGAN on the SVG-Stack dataset
with the VQ-adversarial reconstruction task. We find that
using the features before the quantization yields better re-
sults. ConvNext [46] is a convolutional backbone, which
we extract features before pooling. We start from a LAION-
2B [75]-pretrained checkpoint. Finally, ViT CLIP [62] is
based on the Visual Transformer (ViT) [22] and is well pre-
pared for autoregressive tasks. We extract all output repre-
sentations. We use a LAION-2B pre-trained model. During
the training of StarVector, all the parameters of the image
encoders are updated. We find that the best choice is using
CLIP. We consider that the gains in performance come from
CLIP using more visual tokens (257) than the other image
encoders.
The adapter first projects the features from the original
dimensionality Dv to a dimensionality Dv × 2, followed
by a Swish non-linear activation function and a linear pro-
jection to the LLM dimensionality Dl. Finally, we apply
a layer normalization [5]. We initialize the adapter param-
eters with Glorot [27]. Dropout [79] of 0.1 is applied at
the beginning. These hyperparameters were found using a
random search on SVG-Fonts.
Our results show that image resolution is essential to
capture fine-grained details like texts or high-frequency pat-
terns. As seen in the SVG-Diagrams dataset in Figure 6),
diagrams and figures are part of the SVG-Stack dataset
and present challenging horizontal or vertical aspect ratios.
When images have these aspect ratios, we make the image
fit in the 224×224 resolution, losing much detail, especially
for the OCR capabilities of reading rendered texts and ac-
curately displaying them.
Additional results comparing image encoders can be
found in Figures 19 and 22. These results show the boost
in precision obtained when using CLIP. VGQAN and Con-
vNext often fail to capture the image’s shape and the path’s
trajectory. We note that ConvNext performs better than VQ-
GAN. These differences are also due to the differences in
the number of parameters. The CLIP ViT-L/14 model that
we use consists of 290M parameters, VQGAN consists of
29M, and ConvNext consists of 179M parameters.
Generating SVGs from natural images is out of the scope
of this project.
However, future work should focus on
adapting this approach to natural images, drawing from [50]
and [12] to create a dataset of natural images and SVG pairs.
The selected image encoder architectures for StarVec-
tor include two variants: one with fewer parameters and
reduced image resolution, based on the CLIP ViT-B/32
model, which processes images at 224x224 pixels and
is utilized in StarVector-1B. The second variant, SigLip
(siglip-so400m302 patch14-384), has a larger num-
ber of parameters and processes images at a higher reso-
lution of 384x384 pixels, and is employed in StarVector-
8B. Given the positive results from the ViT architecture, we
chose the SigLip variant due to its demonstrated effective-
ness [97] and the enhanced resolution it provides.
11.2. Training
For training the StarVector model, we define the task of
Image-to-SVG as an inverse rendering problem that con-
verts a raster image (represented with visual tokens) into a


--- Page 41 ---

sequence of SVG code. This can be viewed as a sequence-
to-sequence problem that models the translation between
the image and SVG code domains.
As detailed in Sec-
tion 11, we utilize a CLIP ViT-B/32 for StarVector-1B and
SigLip for StarVector-8B as image encoders, along with a
non-linear adapter to generate a sequence of visual tokens.
The training process consists of two stages. In the first
stage, the Image-to-SVG training phase, we construct se-
quences of visual tokens (produced by the image encoder
and adapter) and SVG tokens, separated by a trigger to-
ken, <svg-start>.
We train the LLM to learn these
sequences on a large SVG-Stack dataset using a basic lan-
guage modeling loss that calculates the cross-entropy loss
in predicting the next token in a sequence based on the pre-
vious tokens.
This task enables the model to learn the concept of draw-
ing with SVG vectors that resemble the input image. Im-
portantly, this training can occur without supervision in the
image domain (i.e., without pixel loss), relying solely on
categorical cross-entropy loss for the LLM vocabulary in-
troduced by the next-token prediction task.
In the second stage, we fine-tune the checkpoint from
the first stage, which has learned SVG syntax through the
Image-to-SVG task, on the Text-to-SVG task. During this
phase, the image encoder is disregarded, as it becomes a
Text-to-text task where the text instructions and SVG codes
can be tokenized and processed directly by the LLM.
Training Details. We trained StarVector-1B on 1 node of 8
A100 80GB GPUs using Accelerate with DeepSpeed stage
2 and StarVector-8B on 8 nodes of 8 H100 80GB GPUs with
Fully Shared Data Parallel (FSDP). For Image-to-SVG,
we used total batch sizes of 128 and 512 for StarVector-
1B and -8B, respectively, a learning rate of 1e-5, and the
AdamW optimizer. To optimize memory and computation,
we employed bf16 precision, FlashAttention2, and gradient
checkpointing. StarVector-1B took 7 days to train, while
StarVector-8B took 10 days, with both models completing
2 epochs.
We use HuggingFace Transformers [93] and Py-
Torch [58] for the implementation. We use a batch size
of 2. Images are processed with a resolution of 224x224,
as defined by the pre-trained CLIP image encoder, and pro-
cess a maximum of 8192 tokens, considering the 257 visual
tokens and the rest for the SVG tokens. We use gradient
batch accumulation of 8 and train on a data parallel setup
with 4 A100 80GB GPUs, having an effective batch size of
64. The learning rate is set to 5 × 10−4 for training, us-
ing AdamW optimizer [48] for approximately five days of
training on the SVG-Stack dataset.
11.3. Generation
Here, we describe how to sample SVG code from our
model. As a decoder-only LLM [42], StarVector first com-
putes the key-value (KV) cache using the visual tokens from
the image and then produces the initial set of output logits.
This stage is often quick because the model can process the
entire visual token sequence simultaneously [78]. The se-
lected token from the output logits is then input back into
the model, which generates logits for the subsequent token.
This process is iteratively repeated until the model produces
the desired quantity of tokens. Our approach uses archi-
tectural improvements for fast decoding, such as FlashAt-
tention [18] and Multi-Query Attention [77]. We leverage
vLLM to improve inference speed.
We perform a grid search on SVG-Emoji and SVG-Icons
validation sets to select the correct sampling temperature.
The choice of temperature does not strongly impact the re-
sults. However, a 1-point increase in performance is ob-
served on CD for SVG-Emoji using temperatures close to
1.0.
We also present an ablation study of StarVector-1B pop-
ular decoding techniques [31, 52, 76, 85]. Specifically, we
experiment with greedy decoding, beam search, and nu-
cleus sampling with top-p. Results are shown in Table 7.
The use of nucleus sampling with top-p=0.9 and tempera-
ture T=0.5 (no beam search) shows to be the best option.
The beam search improves the greedy decoding baseline
but does not work well when combined with nucleus sam-
pling, increasing the inference time. In sum, we recommend
nucleus sampling [31] with top p=0.9 and temperature be-
tween 0.5 and 0.9 for the best performance.
Are the SVGs valid and compilable? A common issue
when generating SVGs with our approach is that the max-
imum token length of the LLM might not be sufficient to
complete the SVG code, leading to compilation errors. We
find that 85% of the generated SVG fit within the context
length and compile successfully. The remaining incomplete
samples are post-processed with cairosvg to produce a
complete and compilable SVG. However, in some cases,
parts of the image may be lost during this process. With
this technique, 100% of the generated SVGs are valid and
compilable.
Improving SVG Quality Through Sampling. The gener-
ation process is stochastic, meaning the outputs may some-
times take an incorrect path, leading to failed generations
or repetitive patterns. To address this, we propose a simple
baseline approach: generate k SVG outputs with varying
sampling parameters (e.g., by adjusting the temperature),


--- Page 42 ---

then compare the outputs with the ground truth using a vi-
sual metric (we propose DinoScore) to select the most ac-
curate result. In an ablation study conducted on SVG-Stack
using StarVector-8B with k = 1 and k = 5, we observe
a boost in DinoScore by 0.12. Empirically, after sampling
100 test samples, we find that 32% of the SVGs are more
accurate when using k = 5, though this increases the gener-
ation time by a factor of k. The use of vLLM helps mitigate
the slower sampling process, as it operates much faster. For
further improvement in code generation, previous work has
used MCTS techniques [7], which leverage visual feedback
more effectively to guide the generation and enhance the
stochastic sampling process.
```

---

## Section 1.35: Surface–Normal Estimation with Neighborhood Reorganization for 3D Reconstruction

Source File: Surface–Normal Estimation with Neighborhood Reorganization for 3D Reconstruction.txt

### Paper Content:

```
# Surface–Normal Estimation with Neighborhood Reorganization for 3D Reconstruction.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Surface–Normal Estimation with Neighborhood Reorganization for 3D Reconstruction.pdf
# Output: ../layer2_completion/txt/Surface–Normal Estimation with Neighborhood Reorganization for 3D Reconstruction.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220843031
Surface–Normal Estimation with Neighborhood Reorganization for 3D
Reconstruction
Conference Paper  in  Lecture Notes in Computer Science · November 2007
DOI: 10.1007/978-3-540-76725-1_34 · Source: DBLP
CITATIONS
20
READS
1,261
3 authors:
Felix Calderon
Universidad Michoacana de San Nicolás de Hidalgo
57 PUBLICATIONS   613 CITATIONS   
SEE PROFILE
Ubaldo Ruiz
Center for Scientific Research and Higher Education at Ensenada
33 PUBLICATIONS   287 CITATIONS   
SEE PROFILE
Mariano Rivera
Mathematics Research Center
149 PUBLICATIONS   2,262 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Ubaldo Ruiz on 05 June 2014.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

Surface–Normal Estimation with Neighborhood
Reorganization for 3D Reconstruction
Felix Calderon1, Ubaldo Ruiz1, and Mariano Rivera2
1 Universidad Michoacana de San Nicol´as de Hidalgo
Divisi´on de Estudios de Posgrado. Facultad de Ingenier´ıa El´ectrica
Santiago Tapia 403 Centro. Morelia, Michoac´an, M´exico. CP 58000
calderon@umich.mx ubaldo@fismat.umich.mx
2 Centro de Investigacion en Matematicas A.C.
Apdo. Postal 402, Guanajuato, Gto. Mexico. CP 36000
mrivera@cimat.mx
Abstract. Fastest three-dimensional (3D) surface reconstruction algo-
rithms, from point clouds, require of the knowledge of the surface–normals.
The accuracy, of state of the art methods, depends on the precision of esti-
mated surface–normals. Surface–normals are estimated by assuming that
the surface can be locally modelled by a plane as was proposed by Hoppe
et. al [1]. Thus, current methods for estimating surface–normals are prone
to introduce artifacts at the geometric edges or corners of the objects. In
this paper an algorithm for Normal Estimation with Neighborhood Reor-
ganization (NENR) is presented. Our proposal changes the characteristics
of the neighborhood in places with corners or edges by assuming a locally
plane piecewise surface. The results obtained by NENR improve the qual-
ity of the normal with respect to the state of the art algorithms. The new
neighborhood computed by NENR, use only those points that belong to
the same plane and they are the nearest neighbors. Experiments in syn-
thetic and real data shown an improvement on the geometric edges of 3D
reconstructed surfaces when our algorithm is used.
Keywords: Normal Estimation, Point Cloud, Surface Reconstruction.
1
Introduction
The computational representations of physical objects have large and wide ap-
plications in distinct areas like industrial design, computer simulations and
medicine, among others: an object digitalization can be easily studied, modi-
ﬁed or replicated. In a initial stage, a complex real object is detailed scanned, by
using of a proper device, for acquiring of a point cloud with thousand or millions
of points. In a second stage, a reconstruction algorithm is applied on the point
cloud for producing a, generally triangular, mesh that approximates the object
surface. Thus the resulting mesh is a suitable representation of the real object.
The reconstruction algorithms must be able to approximate, at a reasonable
computational time, the geometric features of the real object. Among the re-
construction algorithms reported in the literature, Multilevel Partition of Unity
L. Rueda, D. Mery, and J. Kittler (Eds.): CIARP 2007, LNCS 4756, pp. 321–330, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007


--- Page 3 ---

322
F. Calderon, U. Ruiz, and M. Rivera
Implicits (MPUI) [2] deserves our special attention. Currently, the MPUI is con-
sidered as one of the fastest and most up-to-date algorithms for surface recon-
struction, however, it is important to notice that the MPUI like the rest of the
volumetric methods [1,3,4] requires an estimation of the normal at each point.
Clearly, good normal estimations are necessary for a good surface reconstruction.
Some complex three–dimensional (3D) scanner devices estimate the surface
normals at the acquisition time. However, in order to eliminate any dependency
to those devices it is better to infer such normals from the point set. For in-
stance, Refs. [1,5] reported algorithms for addressing the surface normal prob-
lem. But those methods fail, to estimate correctly normals, at sites close to edges
or corners. In this paper we propose an edge preserving normal regularization
technique based in an adaptive rest condition spring system proposed by Rivera
and Marroquin in [6] that allows us to improve the normal-surface estimation at
points close to edges and, consequently, improving the quality of the ﬁnal surface
reconstruction.
The paper is organized as follows. In Sect. 2 we present a brief review of the
reconstruction algorithms that have been developed. In Sect. 3 we describe a
pioneering technique [1], that still, is widely used for normal estimation in point
clouds. In Sect. 4 we propose an algorithm for normal estimation that increase
the accuracy in regions with edges. In Sect. 5 we compare the performance of
the MPUI algorithm [2] when the surface–normal are estimated with the method
presented in [1] and with the method we propose here. Finally, some conclusions
are discussed in Sect. 6.
2
Surface Reconstruction
Currently, one can distinguish two main lines of research in the ﬁeld of surface
reconstruction from point clouds. The algorithms developed in the context of
Computational Geometry (CG) constitute a line of research [7]. In CG-based
methods the surface is reconstructed from the Delaunay tetrahedrization of the
point cloud [8,9,10]. Unfortunately, CG-based methods are very sensitive to noise
and computationally expensive; their computational complexity is of polynomial
order with respect the number of points. On the other hand, volumetric methods
constitute the alternative research line [1,2,3,4]. In this case, implicit functions
are ﬁtted to the point cloud and the surface is extracted (see [11,12,13]) as the
zero level set of the computed functions. These methods are used extensive be-
cause their better noise tolerance and because they are able to handle large point
clouds. However, it is necessary to supply reliable information about the normal
at each point. The current state of the art in this approach includes the MPUI
algorithm based on quadratic functions [2]. It is one of the few reconstruction
algorithms able to process sharp features and also one of the fastest techniques
available [2].
The MPUI algorithm works as follows. First it creates an octree-based subdi-
vision of a box that bounds the point cloud. At each cell of the octree the local
shape is approximated with a piecewise quadratic function. These functions work


--- Page 4 ---

Surface–Normal Estimation with Neighborhood Reorganization
323
like a signed distance function taking a positive value inside of the point cloud
and negative outside of it. The normals of the points are used to distinguish the
orientation locally. If the local approximation into a cell is not accurate, then
the cell is subdivided; such a procedure is repeated until a desirable accuracy
level is reached. The global implicit function describing the surface is given by
assembling the local approximations using local weights.
The MPUI algorithm has three types of quadratic functions that allows to
model a large variety of point set conﬁgurations. Additionally, MPUI provides
test–rule for choosing the appropriated quadratic form to ﬁt at each cell. These
test–rule give to the algorithm the ability to deal with surface edges. The MPUI
algorithm input are the point cloud and the respective surface–normals. In the
case were only the point cloud is given (surface normals are not provided), Ohtake
et al. [2] suggest to compute such surface–normals with the technique proposed
by Hoppe et al. in [1]. The surface-normal estimation method is described in
next section.
3
Standard Surface–Normal Estimation
The estimation of surface–normals from a point cloud is usually done in two
stages, as proposed Hoppe et al. in [1] (NE–Hoppe). In the ﬁrst stage the Tangent
Plane (TP) is estimated at each point. Thus the Orthogonal unitary vector to
the Tangent Plane (OTP) will be used as an approximation of the normal at
such point. In the second stage the orientation of the OTP, spatially coherent,
is computed.
Given a point set P = {p1, . . . , pN} and let be Vi the set of k nearest neighbors
(neighborhood) of the point pi. Then the TP at pi is obtained by ﬁtting a plane
to the points in Vi by using a least-squares procedure, then the surface–normal,
ni, is the normal to the TP. Hoppe et al. [1] proposed to compute, ni, as the
third eigenvector (associated with the smallest eigenvalue) of the local covariance
matrix:
Ci =

j∈Vi
(pj −ci) ⊗(pj −ci)
(1)
where ⊗denotes the outer product vector operator1. If λ1
i ≥λ2
i ≥λ3
i are the
eigenvalues of Ci, their associated eigenvectors v1
i , v2
i , v3
i , respectively, form an
orthonormal basis. Then ni is either v3
i or −v3
i . The neighborhood size is chosen
manually based on visual inspection of the resulting normals and it is the same
for each point in the set.
The Oriented OTP (OOTP) is computed such that nearby planes are consis-
tently oriented. The NE–Hoppe algorithm, proposed in [1], considered the state
of the art, is following described. First, an Euclidean Minimum Spanning Tree
is created over the TP centers {c1, . . . , cn} and it is enriched adding the edge
< i, j > if either ci ∈Vj, or cj ∈Vi. Then the edge cost is equaled to 1−|ni·nj|.
1 If v and u have components vi and uj respectively, then the matrix v ⊗u has uivj
as its ij-th entry.


--- Page 5 ---

324
F. Calderon, U. Ruiz, and M. Rivera
Next, the Minimum Spanning Tree of this graph is computed. The OTP whose
TP center has the largest z coordinate is forced to point towards +z axis. Root-
ing the tree at this node, then the tree is traversed in depth ﬁrst order, if during
traversal the current node i has been assigned the orientation ni and the node
j is the next node to be visited, then nj is replaced with −nj if ni · nj < 0.
Pauly et al. [5] noticed that nearby points in the neighborhood of a point
pi should have a stronger inﬂuence than distant points. Therefore, they assign
diﬀerent weights to elements in the neighborhood by depending on their distance
to pi. The weighting function is proposed to be the Gaussian: w(pj −pi) =
exp(−∥pj −pi∥2/(2σ2)), where σ is chosen as one third of the square distance
between pi and its farthest neighbor: σ2 = (1/6) maxpj∈Vi ∥pj −pi∥2.
Although the previous algorithms [1,5] work well in the presence of smooth
regions and moderate noise, they perform poorly in those regions near corners
or edges. If the neighborhood at each of the points has a ﬁxed size and it is
constructed using only the Euclidean distance then it is possible that points
considered as outliers for a certain region be used in the computation of the
normal. Hence, it is important to develop a new robust strategy that estimate
the local surface–normal by discarding neighbor points that lay beyond of a
surface edge or a corner. Figure 1 shows the normal estimation by diﬀerent
approaches for a step function in two dimensions. Figure 1(a) shows the ground
truth and Fig. 1(b) shows the computed normals using neighborhoods based
only in a proximity measure, note the eﬀect in corners of the step function. On
the other hand, Fig. 1(c) shows the results applying our approach (that will be
introduced in next section).
−2
0
2
4
6
8
10
12
−1
0
1
2
3
4
5
6
7
8
(a)
−2
0
2
4
6
8
10
12
−1
0
1
2
3
4
5
6
7
8
(b)
−2
0
2
4
6
8
10
12
−1
0
1
2
3
4
5
6
7
8
(c)
Fig. 1. Normal estimation comparison in two dimensions for a step function. (a)
Ground Truth, (b) NE–Hoppe algorithm and (c) NENR algorithm.
4
Normal Estimation with Neighborhood Reorganization
(NENR)
The normal estimation using the OOTP is equivalent to apply a low-pass band
ﬁlter to the point cloud, so the resulting normal, will have a smoothness degree
which is proportional to the neighborhood size. If OOTP is used, one is implicitly
assumed that the neighborhood around each point may be modelled by simple
plane. Such assumption is incorrect at points close to edges or corners. An al-
ternative approach could be to weight each point in the neighborhood; however


--- Page 6 ---

Surface–Normal Estimation with Neighborhood Reorganization
325
the underlying representation of each neighborhood still it is a unique plane. A
more appropriated model is to assume a plane–piecewise model. Half-quadratic
regularization (HQR) is an edge-preserving regularization technique for restor-
ing piecewise smooth signals [14,15,16,17]; the general HQR energy function is
given by
U(h, l) =

i∈Ω

(hi −gi)2 + α∥∇hi∥2 (1 −li)2 + αβl2
i

;
(2)
where g is a given signal, h is the ﬁltering signal with edge-preserving and α,
β are parameters which control the signal smoothness. li acts as an indicator
variable which disconnects the ∇hi terms with a huge contribution to the general
cost function. This technique is also applied by Calderon in [18,19], for Image
Registration.
In case of a piecewise constant surface, for a given neighborhood, we ap-
proximate this surface by TPs and it is desirable to get out points of the cloud
belonging to diﬀerent TPs. So a new neighborhood is computed considering only
the nearest neighbors who belong to the same TP. Figure 2(a) shows the stan-
dard plane estimation, for the case of three points on a corner, and Fig. 2(b)
shows the plane estimation when a point is rejected, a desirable condition in
those cases. We propose a HQR cost function for this purpose and the indicator
variable in our case, is used as non-membership term, lij = 1 means that the
j −th point, in the original neighborhood, it is not in the same TP that i −th
point. The OOTP smoothness is controlled with the parameters α and β. We
proposed to compute the surface-normal as the minimization of a constrained
HRQ cost function. The additional constraint imposes a unitary norm to the ﬂat
piecewise normal vector mi:
U(m, l) =

i∈Ω
⎧
⎨
⎩∥mi −ni∥2 + α

j∈Vi

∥mi −mj∥2 (1 −lij)2 + βl2
ij

⎫
⎬
⎭, (3)
s.t.
∥mi∥= 1,
∀i ∈Ω;
where ni is the normal vector computed with the NE–Hoppe algorithm described
in Sect. 3 for some neighborhood size. Then by using the Lagrange multipliers,
γ, we include the constraint in the Lagrangian:
L (m, l, γ) = U(m, l) −

i∈Ω
γi
 3

d=1
m2
i,d −1

(4)
Then the solution is computed solving the Karush-Khun-Tucker conditions:
∇mL (m, l, γ) = 0,
(5)
∇lL (m, l, γ) = 0,
(6)
∥mi∥= 1;
(7)
where ∇x denotes the partial gradient operator. We propose to use the Gauss-
Seidel algorithm, for solving this system of equation, due the fact that the system


--- Page 7 ---

326
F. Calderon, U. Ruiz, and M. Rivera
of equation has a banded, diagonally dominant and semi-positive deﬁned matrix,
so the t-th Gauss-Seidel Iteration is given by the equations (8, 9 and 10):
m(t)
i,d =
m(t)
i,d


k

m(t)
i,d
2
∀i ∈Ω,
(8)
l(t)
ij =

m(t)
i
−
m(t)
j

2
β +

m(t)
i
−
m(t)
j

2
∀< i, j >∈Ω2,
(9)
m(t+1)
i,d
=
ni,d + α 
j∈Vi m(t)
j,d

1 −l(t)
ij
2
1 + γi + α 
j∈Vi

1 −l(t)
ij
2 .
(10)
4
5
6
7
8
9
10
11
4
5
6
7
8
9
10
11
p
h 
p
i 
p
j 
(a)
4
5
6
7
8
9
10
11
4
5
6
7
8
9
10
11
p 
h 
p 
i 
p 
j 
(b)
Fig. 2. Diﬀerent plane estimations using three points on a corner. (a) Plane over the
three points and (b) plane over points pi and ph when pj is not included.
The restriction for the equation (3) is fulﬁll at each iteration according with
equation (8), so the Lagrange multiplier γ can take any value, because their
contribution to the constrained HRQ cost function will be zero at each iteration,
for simplicity we put γi = 0 in equation (10).
The NENR algorithm is resumed in the next steps:
1. Compute the OOTP ni for some neighborhood with size k
2. Compute the reorganized neighborhood and the ﬁlter normal mi doing
– (a) Set m(0)
i
= ni and t = 0
(b) Normalize the vectors m(t)
i
applying (8)
(c) Compute the memberships l(t)
ij using (9)
(d) Update the normal vectors m(t+1)
i
applying (10)
(e) Set t ←t + 1
(f) Repeat steps (b-e) until
m(t+1)
i
−m(t)
i
 < ε


--- Page 8 ---

Surface–Normal Estimation with Neighborhood Reorganization
327
3. Finally compute the robust OTP with the weighted covariance matrix:
Ci =

j∈Vi

1 −l∗
ij

[(pj −c∗
i ) ⊗(pj −c∗
i )]
(11)
with
c∗
i =

j∈Vi

1 −l∗
ij

pj

j∈Vi

1 −l∗
ij

(12)
using the ﬁnals (optimum) memberships l∗
ij, then compute the OOTP as was
described in Sect. 3.
5
Experimental Results
We perform experiments in both synthetic and real data, for comparing NE–
Hoppe and NENR algorithms. The synthetic data corresponds to a step function
and a 3D model with a ground truth while the real data corresponds to 3D models
widely used in the literature. All the 3D models were reconstructed using the
Ohtake’s MPUI implementation available at [20].
The results for the step function in two dimensions are presented in Fig. 1.
The normal vectors in Fig. 1(a) were assigned manually according with the step
function and the normal ﬁeld estimated by NE–Hoppe and NENR are shown
in Figs. 1(b) and 1(c), respectively. The neighborhood sizes for both algorithms
were k = 2 and the NENR parameters were α = 50000 and β = 0.001. Note the
similarity between the NENR (Fig. 1(c)) and the original normals (Fig. 1(a))
also note the problem presented by NE–Hoppe algorithm in corners of the step
function.
For the synthetic 3D model, the normals have been assigned manually accord-
ing to the characteristics of the surface object and its surface reconstruction is
shown in Fig. 3(a). The resulting surface is used as a reference for a qualitative
comparison. The MPUI parameters in this case were a grid size of 0.005, and
a max error of 0.005 at each cell. For the rest of the parameters we took the
default conﬁguration of Ohtake’s MPUI implementation. The neighborhood size
was taken equal to 15 for both normal estimation algorithms. Additionally, for
the NERN algorithm, we took α = 1000 and β = 0.01. The surface reconstructed
using the NE–Hoppe is presented in Fig. 3(b). Note that the edges of the re-
constructed surface are over–smoothed as a direct consequence of bad normal
estimation near these regions. Finally, Fig. 3(c) shows the surface reconstructed
using the NENR algorithm. We must notice that NENR algorithm increases the
quality of the reconstruction, shown sharped geometric edges without aﬀecting
the smooth areas, as you can see comparing Figs. 3(b) and 3(c).
For a quantitative comparison between NERN and NE-Hoppe algorithms, we
compute the angle between the ground truth surfaces normals and the estimated
normals using both algorithms. The mean angle and standard deviation are
shown in Table 1 for the step function (Fig. 1) and the synthetic 3D model
(Fig. 3). Note the better results for the normal estimation using NERN than
NE-Hoppe in both cases.


--- Page 9 ---

328
F. Calderon, U. Ruiz, and M. Rivera
(a) Ground Truth
(b) NE–Hoppe
(c) NENR
Fig. 3. Surface reconstruction using MPUI and diﬀerent Surface–Normal Estimations
Table 1. Normal estimation angle between ground truth normals and the estimated
normals of both methods
Algorithm
Step Function
Synthetic Model
Mean Angle Std. Deviation Mean Angle Std. Deviation
NE-Hoppe
5.7169◦
15.2029◦
7.5544◦
13.5544◦
NERN
0.1764◦
0.1852◦
0.9973◦
2.0968◦
(a) Bunny
(b) Golf club
Fig. 4. Surface reconstruction using MPUI and NENR for real models
Figures 4(a) and 4(b) show reconstructions, using the MPUI method with
NENR–computed normal, from a couple of real 3D models widely used in the
literature. The MPUI parameters for both models were, a grid size of 0.004, and
a max error of 0.002 at each cell. The rest of the parameters took the default
values setting by the Ohtake’s MPUI implementation. For the bunny model the
NERN parameters were k = 15, α = 100, β = 0.01 and in the case of the golf
club model were k = 12, α = 1000, β = 0.01.


--- Page 10 ---

Surface–Normal Estimation with Neighborhood Reorganization
329
6
Conclusions
In general, the normal estimation algorithms based on the covariance matrix
as NE–Hoppe approximate a neighborhood by a unique plane independently of
its local shape. Some algorithms, in order to improve the approximation by a
unique plane, reduce the neighborhood size or weight the covariance matrix,
nevertheless the approximation continues to be a unique plane.
The NENR algorithm produces a reduction in the neighborhood size, rejecting
the neighbors that have large diﬀerences. This condition warranties that the
neighbors have the same smoothness degree between them. The NERN algorithm
was tested using synthetic examples building with shape discontinuities. The
experiments presented better quantitative and qualitative results using NENR
than NE–Hoppe.
In cases of real models, a couple of experiments were done and the results were
very similar for both algorithm, therefore, using NENR in real smooth models
does not represent a lost in quality for surface reconstruction. In general, NENR
produces a better normal estimation than NE–Hoppe, in places located near
to edges, corners and geometric discontinuities. The NERN algorithm does not
have troubles with smooth regions or smooth models.
References
1. Hoppe, H., DeRose, T., Duchamp, T., McDonald, J., Stuetzle, W.: Surface recon-
struction from unorganized points. In: SIGGRAPH 1992: Proceedings of the 19th
annual conference on Computer graphics and interactive techniques, pp. 71–78.
ACM Press, New York (1992)
2. Ohtake, Y., Belyaev, A., Alexa, M., Turk, G., Seidel, H.P.: Multi-level partition of
unity implicits. In: SIGGRAPH 2003: ACM SIGGRAPH 2003 Papers, pp. 463–470.
ACM Press, New York (2003)
3. Bajaj, C.L., Bernardini, F., Xu, G.: Automatic reconstruction of surfaces and scalar
ﬁelds from 3d scans. In: SIGGRAPH 1995: Proceedings of the 22nd annual confer-
ence on Computer graphics and interactive techniques, pp. 109–118. ACM Press,
New York (1995)
4. Carr, J.C., Beatson, R.K., Cherrie, J.B., Mitchell, T.J., Fright, W.R., McCallum,
B.C., Evans, T.R.: Reconstruction and representation of 3d objects with radial
basis functions. In: SIGGRAPH 2001: Proceedings of the 28th annual conference
on Computer graphics and interactive techniques, pp. 67–76. ACM Press, New
York (2001)
5. Pauly, M., Keiser, R., Kobbelt, L.P., Gross, M.: Shape modeling with point-sampled
geometry. In: SIGGRAPH 2003: ACM SIGGRAPH 2003 Papers, pp. 641–650.
ACM Press, New York (2003)
6. Rivera, M., Marroquin, J.L.: The adaptive rest-condition spring system: An edge-
preserving regularization techique. In: ICIP-2000, vol. II, pp. 805–807. IEEE Signal
Processing Society, Vancouver, BC, Canada (2000)
7. O’Rourke, J.: Computational geometry in C. Cambridge University Press, New
York (2000)


--- Page 11 ---

330
F. Calderon, U. Ruiz, and M. Rivera
8. Amenta, N., Bern, M., Kamvysselis, M.: A new voronoi-based surface reconstruc-
tion algorithm. In: SIGGRAPH 1998: Proceedings of the 25th annual conference
on Computer graphics and interactive techniques, pp. 415–421. ACM Press, New
York (1998)
9. Amenta, N., Choi, S., Kolluri, R.K.: The power crust. In: SMA 2001: Proceedings
of the sixth ACM symposium on Solid modeling and applications, pp. 249–266.
ACM Press, New York (2001)
10. Dey, T.K., Goswami, S.: Tight cocone: a water-tight surface reconstructor. In: SM
2003: Proceedings of the eighth ACM symposium on Solid modeling and applica-
tions, pp. 127–134. ACM Press, New York (2003)
11. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface con-
struction algorithm. In: SIGGRAPH 1987: Proceedings of the 14th annual confer-
ence on Computer graphics and interactive techniques, pp. 163–169. ACM Press,
New York (1987)
12. Bloomenthal, J.: An implicit surface polygonizer, 324–349 (1994)
13. Hart, J.C.: Sphere tracing: A geometric method for the antialiased ray tracing of
implicit surfaces. The Visual Computer 12, 527–545 (1996)
14. Black, M., Rangarajan, A.: On the uniﬁcation of line processes, outlier rejection,
and robust statistics with applications in early vision. Int’l J. Computer Vision 19,
57–92 (1996)
15. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barluad, M.: Deterministic edge-
preserving regularization in computed imaging. IEEE Trans. Image Processing 6,
298–311 (1997)
16. Rivera, M., Marroquin, J.L.: Adaptive rest condition potentials: ﬁrst and second
order edge-preserving regularization. Journal of Computer Vision and Image Un-
derstanding 88, 76–93 (2002)
17. Rivera, M., Marroquin, J.: Half–quadratic cost functions with granularity control.
Image and Vision Computing 21, 345–357 (2003)
18. Calderon, F., Romero, L.: Non-parametric image registration as a way to obtain
an accurate camera calibration. In: Monroy, R., Arroyo-Figueroa, G., Sucar, L.E.,
Sossa, H. (eds.) MICAI 2004. LNCS (LNAI), vol. 2972, pp. 584–591. Springer,
Heidelberg (2004)
19. Calderon, F., Romero, L., Flores, J.: Ga-ssd-arc-nlm for parametric image registra-
tion. In: Mart´ınez-Trinidad, J.F., Carrasco Ochoa, J.A., Kittler, J. (eds.) CIARP
2006. LNCS, vol. 4225, pp. 227–236. Springer, Heidelberg (2006)
20. Ohtake,
Y.:
Mpui implementation
in
(2003),
http://www.mpi-inf.mpg.de/
~ohtake/mpu implicits/
View publication stats
```

---

## Section 1.36: Surface_Reconstruction_by_Voronoi_Filtering

Source File: Surface_Reconstruction_by_Voronoi_Filtering.txt

### Paper Content:

```
# Surface_Reconstruction_by_Voronoi_Filtering.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Surface_Reconstruction_by_Voronoi_Filtering.pdf
# Output: ../layer2_completion/txt/Surface_Reconstruction_by_Voronoi_Filtering.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/225544816
Surface Reconstruction by Voronoi Filtering
Article  in  Discrete & Computational Geometry · December 1999
DOI: 10.1007/PL00009475 · Source: CiteSeer
CITATIONS
881
READS
1,073
2 authors:
Nina Amenta
University of California, Davis
102 PUBLICATIONS   7,202 CITATIONS   
SEE PROFILE
Marshall Bern
Protein Metrics
221 PUBLICATIONS   11,931 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Nina Amenta on 27 September 2014.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

Surface
Reconstruction
b
y
V
oronoi
Filtering
Nina
Amen
ta

Marshall
Bern
y
August
,
		
Abstract
W
e
giv
e
a
simple
com
binatorial
algorithm
that
computes
a
piecewise-linear
appro
x-
imation
of
a
smo
oth
surface
from
a
nite
set
of
sample
p
oin
ts.
The
algorithm
uses
V
oronoi
v
ertices
to
remo
v
e
triangles
from
the
Delauna
y
triangulation.
W
e
pro
v
e
the
algorithm
correct
b
y
sho
wing
that
for
densely
sampled
surfaces,
where
densit
y
dep
ends
on
\lo
cal
feature
size",
the
output
is
top
ologically
v
alid
and
con
v
ergen
t
(b
oth
p
oin
t
wise
and
in
surface
normals)
to
the
original
surface.
W
e
describ
e
an
implemen
tation
of
the
algorithm
and
sho
w
example
outputs.

In
tro
duction
The
problem
of
reconstructing
a
surface
from
scattered
sample
p
oin
ts
arises
in
man
y
ap-
plications
suc
h
as
computer
graphics,
medical
imaging,
and
cartograph
y
.
In
this
pap
er
w
e
consider
the
sp
ecic
reconstruction
problem
in
whic
h
the
input
is
a
set
of
sample
p
oin
ts
S
dra
wn
from
a
smo
oth
t
w
o-dimensional
manifold
F
em
b
edded
in
three
dimensions,
and
the
desired
output
is
a
triangular
mesh
with
v
ertex
set
equal
to
S
that
faithfully
represen
ts
F
.
W
e
giv
e
a
\pro
v
ably
correct"
com
binatorial
algorithm
for
this
problem.
That
is,
w
e
giv
e
a
condition
on
the
input
sample
p
oin
ts,
suc
h
that
if
the
condition
is
met
the
algorithm
giv
es
guaran
teed
results:
a
triangular
mesh
of
the
same
top
ology
as
the
surface
F
,
with
p
osition
and
surface
normals
within
a
small
error
tolerance.
The
algorithm
relies
on
the
w
ell-kno
wn
constructions
of
the
Delauna
y
triangulation
and
the
V
oronoi
diagram.
This
pap
er
is
an
extension
of
previous
w
ork
b
y
Amen
ta,
Bern,
and
Eppstein
[
]
on
reconstructing
curv
es
in
t
w
o
dimensions.
Our
previous
w
ork
dened
a
planar
graph
on
the
sample
p
oin
ts
called
the
\crust".
The
crust
is
the
set
of
edges
in
the
Delauna
y
triangulation
of
the
sample
p
oin
ts
that
can
b
e
enclosed
b
y
circles
empt
y
not
only
of
sample
p
oin
ts,
but
also
of
V
oronoi
v
ertices.
The
crust
comes
with
a
guaran
tee:
if
the
curv
e
is
\w
ell-sampled",
then
the
crust
con
tains
exactly
the
edges
b
et
w
een
sample
p
oin
ts
adjacen
t
on
the
curv
e.
Our
notion
of
w
ell-sampled,
whic
h
in
v
olv
es
the
medial
axis
of
the
curv
e,
is
sensitiv
e
to
the
lo
cal
geometry
.
Hence
our
algorithm,
unlik
e
other
algorithms
for
this
problem,
allo
ws
highly
non
uniform
sampling,
dense
in
detailed
areas
y
et
sparse
in
featureless
areas.
An
y
pro
v
ably

Computer
Sciences,
Univ
ersit
y
of
T
exas,
Austin,
TX
.
W
ork
p
erformed
in
part
while
at
Xero
x
P
AR
C,
partially
supp
orted
b
y
NSF
gran
t
CCR-	0.
y
Xero
x
P
alo
Alto
Researc
h
Cen
ter,

Co
y
ote
Hill
Rd.,
P
alo
Alto,
CA
	0



--- Page 3 ---

correct
algorithm
m
ust
imp
ose
some
sampling
densit
y
requiremen
t,
similar
to
the
Nyquist
limit
in
sp
ectral
analysis.
The
extension
to
three
dimensions
in
this
pap
er
requires
b
oth
new
algorithmic
ideas
and
new
pro
of
tec
hniques.
Most
notably
the
algorithm
uses
only
a
subset
of
the
V
oronoi
v
er-
tices
to
remo
v
e
Delauna
y
triangles.
The
algorithm
pic
ks
only
t
w
o
V
oronoi
v
ertices|called
p
oles
|p
er
sample
p
oin
t:
the
farthest
v
ertices
of
the
p
oin
t's
cell
on
eac
h
side
of
the
surface.
With
this
mo
dication,
the
straigh
tforw
ard
generalization
of
our
t
w
o-dimensional
algorithm
no
w
w
orks.
Delauna
y
triangles
with
circumspheres
empt
y
of
p
oles
giv
e
a
piecewise-linear
surface
p
oin
t
wise
con
v
ergen
t
to
F
.
The
p
oles,
ho
w
ev
er,
also
enable
further
ltering
on
the
basis
of
triangle
normals.
Adding
this
ltering
giv
es
a
piecewise-linear
surface
that
con
v
erges
to
F
b
oth
p
oin
t
wise
and
in
surface
normals
(and
hence
in
area).
W
e
b
eliev
e
that
p
oles
ma
y
b
e
applicable
to
other
algorithms
as
w
ell,
p
erhaps
whenev
er
one
wishes
to
estimate
a
surface
normal
or
tangen
t
plane.
This
pap
er
is
organized
as
follo
ws.
Section

describ
es
previous
w
ork
on
surface
recon-
struction.
Section

giv
es
our
algorithm.
Section

states
our
theoretical
guaran
tees,
and
Section

sk
etc
hes
their
pro
ofs.
Section

sho
ws
some
example
outputs.

Previous
W
ork
Previous
w
ork
on
the
reconstruction
problem
falls
in
to
t
w
o
camps:
computer
graphics
and
computational
geometry
.
The
algorithms
in
use
in
computer
graphics
t
ypically
compute
an
appro
ximating
surface,
that
is,
a
surface
passing
close
b
y
,
rather
than
exactly
through,
the
original
sample
p
oin
ts.
The
algorithms
devised
b
y
computational
geometers
t
ypically
restrict
atten
tion
to
surfaces
on
the
original
sample
p
oin
ts,
usually
a
carefully
c
hosen
subset
of
the
Delauna
y
triangulation.
Only
recen
tly
ha
v
e
computational
geometers
started
pub-
lishing
algorithms
with
pro
v
able
prop
erties,
and
un
til
this
curren
t
pap
er
these
algorithms
with
guaran
tees
applied
only
to
reconstructing
curv
es
in
t
w
o
dimensions.
The
rst
and
most
widely
kno
wn
reconstruction
algorithm
in
the
computer
graphics
comm
unit
y
is
the
w
ork
of
Hopp
e
et
al.
[	
,
0
,

].
This
algorithm
estimates
a
tangen
t
plane
at
eac
h
sample
using
the
k
nearest
neigh
b
ors,
and
uses
the
distance
to
the
plane
of
the
closest
sample
p
oin
t
as
a
signed
distance
function.
The
zero
set
of
this
function
is
then
con
toured
b
y
a
con
tin
uous
piecewise-linear
surface
using
the
marc
hing
cub
es
algorithm.
A
later
algorithm
b
y
Curless
and
Lev
o
y
[
]
is
designed
for
data
samples
collected
b
y
a
laser
range
scanner.
This
algorithm
sums
anisotropically
w
eigh
ted
con
tributions
from
the
samples
to
compute
a
signed
distance
function,
whic
h
is
then
discretized
on
v
o
xels
to
eliminate
the
marc
hing
cub
es
step.
These
t
w
o
computer
graphics
algorithms
are
quite
successful
in
practice,
but
ha
v
e
no
pro
v
able
guaran
tees.
Indeed
there
exist
arbitrarily
dense
sets
of
samples,
for
example
ones
with
almost
collinear
nearest
neigh
b
or
sets,
for
whic
h
the
algorithm
of
Hopp
e
et
al.
w
ould
fail.
The
most
famous
computational
geometry
construction
for
asso
ciating
a
p
olyhedral
shap
e
with
an
unorganized
set
of
p
oin
ts
is
the
-shap
e
of
Edelsbrunner
et
al.
[
,

].
Lik
e
our
reconstructed
surface,
the
-shap
e
is
a
sub
complex
of
the
Delauna
y
triangulation.
A
Delauna
y
simplex
(edge,
face,
etc.)
b
elongs
to
the
-shap
e
of
S
if
its
circumsphere
has
radius
at
most
.
The
ma
jor
dra
wbac
k
of
using
-shap
es
for
surface
reconstruction
is
that
the
optimal
v
alue
of

dep
ends
on
the
sampling
densit
y
,
whic
h
often
v
aries
o
v
er
dieren
t



--- Page 4 ---

parts
of
the
surface.
F
or
uniformly
sampled
surfaces,
ho
w
ev
er,
-shap
es
are
w
ork
able.
Bernardini
et
al.
[
]
follo
w
-shap
e-based
reconstruction
with
a
clean-up
phase
to
resolv
e
sharp
dihedral
angles.
Edelsbrunner
and
Raindrop
Geomagic
[
]
are
con
tin
uing
to
dev
elop
-shap
e-based
reconstruction
along
with
proprietary
extensions.
An
early
algorithm
due
to
Boissonnat
[	
]
is
related
to
ours.
He
prop
osed
a
\sculpting"
heuristic
for
selecting
a
subset
of
Delauna
y
tetrahedra
to
represen
t
the
in
terior
of
an
ob
ject.
The
heuristic
is
motiv
ated
b
y
the
observ
ation
that
\t
ypical"
Delauna
y
tetrahedra
ha
v
e
circumspheres
appro
ximating
maximal
empt
y
balls
cen
tered
at
p
oin
ts
of
the
medial
axis;
our
algorithm
relies
on
this
same
observ
ation.
Boissonnat's
algorithm,
ho
w
ev
er,
o
v
erlo
oks
the
fact
that
ev
en
dense
sample
sets
can
giv
e
Delauna
y
tetrahedra
with
circumspheres
that
are
arbitrarily
far
from
the
medial
axis;
indeed
it
is
this
second
observ
ation
whic
h
motiv
ates
our
denition
of
p
oles.
Goldak,
Y
u,
Knigh
t
and
Dong
[
]
made
a
similar
o
v
ersigh
t,
asserting
incorrectly
that
the
V
oronoi
diagram
v
ertices
asymptotically
approac
h
the
medial
axis
as
the
sampling
densit
y
go
es
to
innit
y
.
Finally
,
for
the
t
w
o-dimensional
problem
there
are
a
few
recen
t
algorithms
with
pro
v-
able
guaran
tees.
Figueiredo
and
Miranda
Gomes
[
]
pro
v
e
that
the
Euclidean
minim
um
spanning
tree
can
b
e
used
to
reconstruct
uniformly
sampled
curv
es
in
the
plane.
Bernar-
dini
and
Ba
ja
j
[
]
pro
v
e
that
-shap
es
also
reconstruct
uniformly
sampled
curv
es
in
the
plane.
A
ttali
[
]
giv
es
y
et
another
pro
v
ably
correct
reconstruction
algorithm
for
uniformly
sampled
curv
es
in
the
plane,
using
a
subgraph
of
the
Delauna
y
triangulation
in
whic
h
eac
h
edge
is
included
or
excluded
according
to
the
angle
b
et
w
een
the
circumcircles
on
either
side.
Our
previous
pap
er
sho
w
ed
that
b
oth
the
crust
and
the

-sk
eleton
[
]
(another
empt
y-
region
planar
graph)
correctly
reconstruct
curv
es
ev
en
with
non
uniform
sampling.
Our
t
w
o-dimensional
results
[
]
are
in
this
w
a
y
strictly
stronger
than
those
of
the
other
authors.

Description
of
the
Algorithm
W
e
start
b
y
describing
the
algorithm
of
Amen
ta
et
al.
[
]
for
the
problem
of
reconstructing
curv
es
in
I
R

.
Let
F
b
e
a
smo
oth
(t
wice
dieren
tiable)
curv
e
em
b
edded
in
I
R

,
and
S
b
e
a
set
of
sample
p
oin
ts
from
F
.
Let
V
denote
the
v
ertices
of
the
V
oronoi
diagram
of
S
.
The
crust
of
S
con
tains
exactly
the
edges
of
the
Delauna
y
triangulation
of
S
[
V
with
b
oth
endp
oin
ts
from
S
.
Sa
ying
this
another
w
a
y
,
the
crust
con
tains
exactly
those
Delauna
y
edges
around
whic
h
it
is
p
ossible
to
dra
w
a
circle
empt
y
of
V
oronoi
v
ertices.
In
our
earlier
pap
er,
w
e
pro
v
ed
that
if
S
is
a
sucien
tly
dense
sample,
this
simple
algorithm
constructs
a
p
olygonal
appro
ximation
of
F
(Theorem

in
Section

b
elo
w).
The
straigh
tforw
ard
generalization
of
this
algorithm
fails
for
the
task
of
reconstructing
a
smo
oth
t
w
o-dimensional
manifold
em
b
edded
in
three
dimensions.
The
problem
is
that
v
ertices
of
the
V
oronoi
diagram
ma
y
fall
v
ery
close
to
the
surface,
thereb
y
punc
hing
holes
in
the
crust.
F
or
example,
the
V
oronoi
cen
ter
of
a
sliv
er
can
lie
arbitrarily
close
to
the
surface
F
.
A
sliver
is
a
tetrahedron
with
bad
asp
ect
ratio
y
et
a
reasonably
small
circumradius
to
shortest
edge
ratio,
suc
h
as
the
tetrahedron
formed
b
y
four
nearly
equally
spaced
v
ertices
around
the
equator
of
a
sphere.
The
x
is
to
consider
only
the
p
oles
.
The
p
oles
of
a
sample
p
oin
t
s
are
the
t
w
o
farthest
v
ertices
of
its
V
oronoi
cell,
one
on
eac
h
side
of
the
surface.
Since
the
algorithm
do
es
not
kno
w
the
surface,
only
the
sample
p
oin
ts,
it
c
ho
oses
the
p
oles
b
y
rst
c
ho
osing
the
farthest



--- Page 5 ---

.
Compute
the
V
oronoi
diagram
of
the
sample
p
oin
ts
S
.
.
F
or
eac
h
sample
p
oin
t
s:
(a)
If
s
do
es
not
lie
on
the
con
v
ex
h
ull
of
S
,
let
p
+
b
e
the
v
ertex
of
V
or
(s)
farthest
from
s.
(b)
If
s
do
es
lie
on
the
con
v
ex
h
ull
of
S
,
let
p
+
b
e
a
p
oin
t
at
\innite
distance"
outside
the
con
v
ex
h
ull
with
the
direction
of
sp
+
equal
to
the
a
v
erage
of
the
out
w
ard
normals
of
h
ull
faces
meeting
at
s.
(c)
Among
all
v
ertices
p
of
V
or
(s)
suc
h
that

p
+
sp
measures
more
than

=,
c
ho
ose
the
farthest
from
s
to
b
e
p
 .
.
Let
P
denote
all
p
oles
p
+
and
p
 ,
except
those
p
+
's
at
innite
distance.
Compute
the
Delauna
y
triangulation
of
S
[
P
.
.
(V
oronoi
Filtering)
Keep
only
those
triangles
in
whic
h
all
three
v
ertices
are
sample
p
oin
ts.
.
(Filtering
b
y
Normal)
Remo
v
e
eac
h
triangle
T
for
whic
h
the
normal
to
T
and
the
v
ector
to
the
p
+
p
ole
at
a
v
ertex
of
T
form
to
o
large
an
angle
(greater
than

for
the
largest-angle
v
ertex
of
T
,
greater
than

=
for
the
other
v
ertices
of
T
).
.
(T
rimming)
Orien
t
triangles
and
p
oles
(inside
and
outside)
consisten
tly
,
and
extract
a
piecewise-linear
manifold
without
b
oundary
.
Figure
.
The
surface
reconstruction
algorithm.
V
oronoi
v
ertex
regardless
of
direction
(or
a
ctional
p
ole
at
\innit
y"
in
the
case
of
an
un
b
ounded
V
oronoi
cell),
and
then
c
ho
osing
the
farthest
in
the
opp
osite
half-space.
See
step

in
Figure
.
Lemma

in
Section

sho
ws
that
this
metho
d
is
indeed
correct
for
w
ell-sampled
surfaces.
Denoting
the
p
oles
b
y
P
,
w
e
dene
the
crust
of
S
to
b
e
the
triangles
of
the
Delauna
y
triangulation
of
S
[
P
,
all
of
whose
v
ertices
are
mem
b
ers
of
S
.
Steps
{
compute
the
crust
(sometimes
called
the
r
aw
crust
to
distinguish
it
from
the
more
nished
v
ersions).
The
crust
has
a
relativ
ely
w
eak
theoretical
guaran
tee:
it
is
p
oin
t
wise
con
v
ergen
t
to
F
as
the
sampling
densit
y
increases.
Steps

and

are
\p
ostpro
cessing"
steps
that
pro
duce
an
output
with
a
stronger
guaran
tee:
con
v
ergence
b
oth
p
oin
t
wise
and
in
surface
normals,
and
top
ological
equiv
alence.
Step

remo
v
es
triangles
based
on
the
directions
of
their
surface
normals.
Let
T
b
e
a
triangle
of
the
crust
and
let
s
b
e
its
v
ertex
of
maxim
um
angle.
Step

remo
v
es
T
if
the
angle
b
et
w
een
the
normal
to
T
and
the
v
ector
from
an
y
one
of
T
's
v
ertices
to
its
rst-
c
hosen
p
ole
is
to
o
large.
The
denition
of
\to
o
large"
dep
ends
on
whic
h
v
ertex
of
T
is
under
consideration:
for
the
v
ertex
with
largest
angle,
to
o
large
means
greater
than
an
input
parameter

,
and
for
the
other
t
w
o
v
ertices
it
means
greater
than

=.
Angles
are
unsigned
angles
in
the
range
[0;

=].
As
stated
in
Theorem
,
the
c
hoice
of

is
connected
with
the
sampling
densit
y
.
If
the
user
of
our
algorithm
do
es
not
ha
v
e
an
estimate
of
the
sampling
densit
y
(the
parameter
r
in
Denition

b
elo
w),
then
the
user
can
slo
wly
decrease

,
bac
king
o
when
holes
start
to
app
ear
in
the
surface,
similar
to
c
ho
osing
a
surface
from
the
sp
ectrum
of
-shap
es
[
].
Step

ensures
that
the
reconstructed
surface
has
the
top
ology
of
the
original
surface;
b
efore
this
nal
step,
the
computed
surface
will
resem
ble
the
original
surface
geometrically
,



--- Page 6 ---

but
ma
y
ha
v
e
some
extra
triangles
enclosing
small
bubbles
and
p
o
c
k
ets.
The
problem
once
again
is
sliv
ers:
all
four
faces
of
a
at
sliv
er
ma
y
mak
e
it
past
steps

and
.
Step

rst
orien
ts
all
triangles.
Start
with
an
y
sample
p
oin
t
s
on
the
con
v
ex
h
ull
of
S
.
Call
the
direction
to
p
+
at
s
the
outside
and
the
direction
to
p
 the
inside
.
Pic
k
an
y
triangle
T
inciden
t
to
s,
and
dene
the
outside
side
of
T
to
b
e
the
one
visible
from
p
oin
ts
on
the
sp
+
ra
y
.
Orien
t
the
p
oles
of
the
other
v
ertices
of
T
to
agree
with
this
assignmen
t.
Orien
t
eac
h
triangle
sharing
a
v
ertex
with
T
so
that
they
agree
on
the
orien
tations
of
their
shared
p
oles,
and
con
tin
ue
b
y
breadth-rst
searc
h
un
til
all
p
oles
and
triangles
ha
v
e
b
een
orien
ted.
Our
Theorem
,
b
elo
w,
guaran
tees
that
this
orien
tation
is
consisten
t.
In
a
triangulated
piecewise-linear
t
w
o-dimensional
manifold,
t
w
o
triangles
meet
at
eac
h
edge,
with
outside
sides
together
and
inside
sides
together.
Dene
a
sharp
edge
to
b
e
an
edge
whic
h
has
a
dihedral
angle
greater
than

=
b
et
w
een
a
successiv
e
pair
of
inciden
t
triangles
in
the
cyclic
order
around
the
edge.
In
other
w
ords,
a
sharp
edge
has
all
its
triangles
within
a
small
w
edge.
W
e
consider
an
edge
b
ounding
only
one
triangle
to
ha
v
e
a
dihedral
of

,
so
suc
h
an
edge
is
necessarily
sharp.
Step

trims
o
p
o
c
k
ets
b
y
greedily
remo
ving
triangles
with
sharp
edges.
No
w
the
re-
maining
triangles
form
a
\quilted"
surface,
in
whic
h
eac
h
edge
b
ounds
at
least
t
w
o
triangles,
with
consisten
t
orien
tations.
Finally
,
Step

extracts
the
outside
of
this
quilted
surface
b
y
a
breadth-rst
searc
h
on
triangles.

Theoretical
Guaran
tees
What
sets
our
algorithm
apart
from
previous
algorithms
are
its
theoretical
guaran
tees.
W
e
b
egin
with
the
required
sampling
densit
y
,
whic
h
is
dened
with
resp
ect
to
the
medial
axis.
Denition
.
The
medial
axis
of
a
manifold
F
em
b
edded
in
I
R
d
is
the
closure
of
the
set
of
p
oin
ts
in
I
R
d
with
more
than
one
nearest
neigh
b
or
on
F
.
Figure

giv
es
an
example
of
the
medial
axis
in
I
R

;
in
I
R

,
the
medial
axis
is
generally
a
t
w
o-dimensional
surface.
Note
that
w
e
allo
w
the
surface
F
to
ha
v
e
more
than
one
connected
comp
onen
t.
Denition
.
The
lo
cal
feature
size
LFS
(p)
at
a
p
oin
t
p
on
F
is
the
Euclidean
distance
from
p
to
(the
nearest
p
oin
t
of
)
the
medial
axis.
Denition
.
Set
S

F
is
an
r
-sample
of
F
if
no
p
oin
t
p
on
F
is
farther
than
r

LFS
(p)
from
a
p
oin
t
of
S
.
Notice
that
the
notion
of
r
-sample
do
es
not
assume
an
y
global|or
ev
en
lo
cal|uniformit
y
.
F
urther
notice
that
to
pro
v
e
an
algorithm
correct,
w
e
m
ust
place
some
condition
on
the
set
of
sample
p
oin
ts
S
,
or
else
the
original
surface
could
b
e
an
y
surface
passing
through
S
.
Our
pap
er
on
curv
e
reconstruction
[
]
pro
v
ed
the
follo
wing
theoretical
guaran
tee.
Theorem

(Amen
ta
et
al.
[
]).
If
S
is
an
r
-sample
of
a
curv
e
in
I
R

for
r

:0,
then
the
crust
includes
all
the
edges
b
et
w
een
pairs
of
sample
p
oin
ts
adjacen
t
along
F
.
If
S
is
an
r
-sample
for
r

:,
then
the
crust
includes
exactly
those
edges.



--- Page 7 ---

Figure
.
The
medial
axis
of
a
smo
oth
curv
e.
T
o
state
our
results
for
the
three-dimensional
problem,
w
e
m
ust
dene
a
generalization
of
adjacency
.
Consider
the
V
oronoi
diagram
of
the
sample
p
oin
ts
S
.
This
V
oronoi
dia-
gram
induces
a
cell
decomp
osition
on
surface
F
called
the
r
estricte
d
V
or
onoi
diagr
am
:
the
b
oundaries
of
the
cells
on
F
are
simply
the
in
tersections
of
F
with
the
three-dimensional
V
oronoi
cell
b
oundaries.
W
e
call
a
triangle
with
v
ertices
from
S
a
go
o
d
triangle
if
it
is
dual
to
a
v
ertex
of
the
restricted
V
oronoi
diagram;
go
o
d
triangles
are
necessarily
Delauna
y
triangles.
Our
rst
three-dimensional
result
sho
ws
that
go
o
d
triangles
deserv
e
their
name.
T
o
our
kno
wledge,
our
pro
of
of
this
result
is
the
rst
pro
of
that
the
three-dimensional
De-
launa
y
triangulation
of
a
sucien
tly
dense
set
of
samples
con
tains
a
piecewise-linear
surface
homeomorphic
to
F
.
Theorem
.
If
S
is
an
r
-sample
of
F
for
r

:,
then
the
go
o
d
triangles
form
a
p
olyhedron
homeomorphic
to
F
.
Our
next
t
w
o
theorems
state
the
theoretical
guaran
tees
for
the
three-dimensional
(ra
w)
crust.
Theorem
.
If
S
is
an
r
-sample
for
r

:,
then
the
crust
includes
all
the
go
o
d
triangles.
Theorem
.
If
S
is
an
r
-sample
for
r

:0,
then
the
crust
lies
within
a
fattened
surface
formed
b
y
placing
a
ball
of
radius
r
LFS
(q
)
around
eac
h
p
oin
t
q

F
.
Step

adds
another
guaran
tee:
con
v
ergence
in
surface
normals.
The
ra
w
crust
some-
times
includes
small
skinn
y
triangles
with
surface
normals
that
deviate
signican
tly
from
the
surface
normals.
F
or
example,
the
insides
of
the
sausages
sho
wn
on
the
left
in
Fig-
ure

ha
v
e
a
sort
of
\w
ash
b
oard"
texture.
Con
v
ergence
in
surface
normal
ensures
that
the
area
of
the
trimmed

-crust
con
v
erges
to
that
of
the
surface,
and
w
e
use
it
in
the
pro
of
of
Theorem
.
Theorem
.
Assume
S
is
an
r
-sample
and
set

=
r
.
Let
T
b
e
a
triangle
of
the

-crust
and
t
a
p
oin
t
on
T
.
The
angle
b
et
w
een
the
normal
to
T
and
the
normal
to
F
at
the
p
oin
t
p

F
closest
to
t
measures
O
(
p
r
)
radians.



--- Page 8 ---

Finally
,
the
trimming
or
\manifold
extraction"
step,
Step
,
adds
the
guaran
tee
of
top
ological
equiv
alence.
Theorem
.
Assume
S
is
an
r
-sample
and
set

=
r
.
F
or
sucien
tly
small
r
,
the
trimmed

-crust
is
homeomorphic
to
F
.

Pro
ofs
In
this
section
w
e
giv
e
the
pro
ofs
of
the
theoretical
guaran
tees.
W
e
b
egin
with
some
de-
nitions.
A
t
eac
h
p
oin
t
p

F
,
there
are
t
w
o
tangen
t
me
dial
b
al
ls
cen
tered
at
p
oin
ts
of
the
medial
axis.
The
v
ectors
from
p
to
the
cen
ters
of
its
medial
balls
are
normal
to
F
,
and
F
do
es
not
in
tersect
the
in
teriors
of
the
medial
balls.
Since
LFS
(p)
is
at
most
the
radius
of
the
smaller
medial
ball,
F
is
also
conned
b
et
w
een
the
t
w
o
tangen
t
balls
of
radius
LFS
(p).
W
e
call
these
the
big
tangent
b
al
ls
at
p
(this
is
somewhat
misleading
since
in
general
the
medial
balls
at
p
are
bigger);
w
e
will
use
the
big
tangen
t
balls
to
b
ound
the
curv
ature
of
F
in
terms
of
LFS
(p).
W
e
call
a
maximal
empt
y
ball
cen
tered
at
a
V
oronoi
v
ertex
a
V
or
onoi
b
al
l
,
and
the
V
oronoi
ball
cen
tered
at
a
p
ole
a
p
olar
b
al
l
.
Our
rst
lemma
is
rather
basic:
a
Lipsc
hitz
condition
for
the
LFS
(p)
function.
W
e
use
d(p;
q
)
to
denote
the
Euclidean
distance
from
p
to
q
.
Angles
are
measured
in
radians.
Lemma
.
F
or
an
y
t
w
o
p
oin
ts
p
and
q
on
F
,
jLFS
(p)
 LFS
(q
)j

d(p;
q
).
Pro
of:
LFS
(p)

LFS
(q
)
 d(p;
q
),
since
the
ball
of
radius
LFS
(q
)
around
q
con
tains
the
ball
of
radius
LFS
(q
)
 d(p;
q
)
around
p
and
con
tains
no
p
oin
t
of
the
medial
axis.
Similarly
,
LFS
(q
)

LFS
(p)
 d(p;
q
).
Our
second
lemma
is
a
sort
of
Lipsc
hitz
condition
for
the
direction
of
surface
normals,
whic
h
can
b
e
regarded
as
a
function
from
F
to
the
t
w
o-dimensional
sphere.
Lemma
.
F
or
an
y
t
w
o
p
oin
ts
p
and
q
on
F
with
d(p;
q
)


min
fLFS
(p);
LFS
(q
)g,
for
an
y

<
=,
the
angle
b
et
w
een
the
normals
to
F
at
p
and
q
is
at
most
=(
 ).
Pro
of:
Let
us
parameterize
the
line
segmen
t
pq
b
y
length.
Let
p(t)
denote
the
p
oin
t
on
pq
with
parameter
v
alue
t
and
let
f
(t)
denote
the
nearest
p
oin
t
to
p(t)
on
the
surface
F
.
In
other
w
ords,
f
(t)
is
the
p
oin
t
at
whic
h
an
expanding
sphere
cen
tered
at
p(t)
rst
touc
hes
F
.
P
oin
t
f
(t)
is
unique,
b
ecause
otherwise
p(t)
w
ould
b
e
a
p
oin
t
of
the
medial
axis,
con
tradicting
d(p;
q
)

LFS
(p).
Let
n(t)
denote
the
unit
normal
to
F
at
f
(t),
and
jn
0
(t)j
the
magnitude
of
the
deriv
ativ
e
with
resp
ect
to
t,
that
is,
the
rate
at
whic
h
the
normal
turns
as
t
gro
ws.
The
c
hange
in
normal
b
et
w
een
p
and
q
is
at
most
R
pq
jn
0
(t)jdt,
whic
h
is
at
most
d(p;
q
)
max
t
jn
0
(t)j.
The
surface
F
passes
b
et
w
een
the
big
tangen
t
balls
of
radius
LFS
(f
(t))
at
f
(t),
so
the
greater
of
the
t
w
o
principal
curv
atures
at
f
(t)
is
no
more
than
the
curv
ature
of
these
tangen
t
balls.
The
rate
at
whic
h
the
normal
c
hanges
with
f
(t)
is
at
most
the
greater
principal
curv
ature,
and
hence
jn
0
(t)j
is
at
most
the
rate
at
whic
h
the
normal
turns
(as
a
function
of
t)
on
one
of
these
tangen
t
balls.
Referring
to
Figure
,
w
e
see
that
dt

(LFS
(f
(t))
 d(f
(t);
p(t)))

sin

:



--- Page 9 ---

θ
dt
f(t)
p(t)
q
p
F
Figure
.
Bounding
jn
0
(t)j
in
terms
of
the
radius
LFS
(f
(t))
and
d(f
(t);
p(t)).
No
w
sin

approac
hes

as

go
es
to
zero,
so
jn
0
(t)j
=
lim

!0

=dt

=(LFS
(f
(t))
 d(f
(t);
p(t))):
W
e
ha
v
e
that
d(f
(t);
p(t))

d(p(t);
p)

LFS
(p)
and
d(f
(t);
p)

d(f
(t);
p(t))
+
d(p(t);
p)

LFS
(p);
so
b
y
Lemma
,
LFS
(f
(t))

(
 )LFS
(p):
Altogether
w
e
obtain
max
t
jn
0
(t)j

=((
 )LFS
(p)),
whic
h
yields
the
lemma.
W
e
next
sho
w
that
the
cells
of
the
V
oronoi
diagram
of
S
are
long
and
skinn
y
.
W
e
let
V
or
(s)
denote
the
closure
of
the
V
oronoi
cell
of
s,
that
is,
all
p
oin
ts
at
least
as
close
to
s
as
to
an
y
other
sample
p
oin
t.
W
e
ignore
the
degenerate
case
that
V
or
(s)
is
un
b
ounded
on
b
oth
sides
of
F
.
Lemma
.
Let
s
b
e
a
sample
p
oin
t
from
an
r
-sample
S
.
(a)
On
either
side
of
F
at
s,
some
p
oin
t
of
V
or
(s)
has
distance
at
least
LFS
(s)
from
s.
(b)
The
in
tersection
of
V
or
(s)
and
F
is
con
tained
in
a
ball
of
radius
r
 r
LFS
(s)
ab
out
s.
Pro
of:
On
either
side
of
F
at
s,
the
cen
ter
c
of
the
big
tangen
t
ball
of
radius
LFS
(s)
lies
within
V
or
(s),
and
hence
(a)
holds.
F
or
part
(b),
let
p

V
or
(s)
\
F
.
Since
s
is
the
closest
sample
p
oin
t
to
p,
d(p;
s)

r
LF
S
(p)

r
(LF
S
(s)
+
d(p;
s))
b
y
Lemma
.
So
d(p;
s)

r
 r
LFS
(s).
The
next
lemma
mak
es
precise
the
idea
that
these
long
skinn
y
V
oronoi
cells
are
p
erp
en-
dicular
to
the
surface.
Lemma
.
Let
s
b
e
a
sample
p
oin
t
from
an
r
-sample
S
.
Let
v
b
e
an
y
p
oin
t
in
V
or
(s)
suc
h
that
d(v
;
s)


LFS
(s)
for

>
0.
The
angle
at
s
b
et
w
een
the
v
ector
to
v
and
the
normal
to
the
surface
(orien
ted
in
the
same
direction)
is
at
most
arcsin
r

( r
)
+
arcsin
r
 r
.



--- Page 10 ---

F
α
β
p
m
v
B
B
B
v
m
p
s
φ
q
Figure
.
The
v
ector
from
s
to
a
distan
t
V
oronoi
v
ertex
suc
h
as
a
p
ole
m
ust
b
e
nearly
normal
to
the
surface.
Pro
of:
Let
B
v
b
e
the
V
oronoi
ball
cen
tered
on
v
.
Let
B
m
b
e
the
medial
ball
touc
hing
s
on
the
same
side
of
the
surface
F
,
and
let
m
b
e
its
cen
ter.
Let

b
e
the
angle
b
et
w
een
the
segmen
ts
sv
and
sm,
that
is,
the
angle
referred
to
in
the
lemma.
Let
B
p
b
e
the
ball
of
radius
LFS
(s),
tangen
t
to
F
at
s,
but
lying
on
the
opp
osite
side
of
F
from
B
m
;
let
p
b
e
the
cen
ter
of
B
p
.
The
surface
F
passes
b
et
w
een
B
m
and
B
p
at
s,
and
do
es
not
in
tersect
the
in
terior
of
either
of
them,
as
sho
wn
in
Figure
.
Since
p
and
v
lie
on
opp
osite
sides
of
F
,
line
segmen
t
pv
m
ust
in
tersect
F
at
least
once.
Let
q
b
e
the
in
tersection
p
oin
t
closest
to
p.
No
sample
p
oin
t
can
lie
in
either
B
p
or
B
v
,
so
the
nearest
sample
p
oin
t
to
q
m
ust
b
e
s.
Since
B
p
has
radius
LFS
(s),
d(q
;
s)

sin
()LFS
(s),
where

is
the
angle

spq
.
W
e
are
in
terested
in
angle

v
sm,
whic
h
is

=

+

.
Since
B
v
has
radius
at
least

LFS
(s),
d(q
;
s)


sin
(
)LFS
(s),
where

is
the
angle

sv
q
.
Since
S
is
an
r
-sample,
d(q
;
s)
m
ust
b
e
less
than
r
 r
LFS
(s).
Com
bining
the
inequalities,
w
e
obtain


arcsin
r
 r
and


arcsin
r

( r
)
,
whic
h
together
giv
e
the
b
ound
on
.
T
ogether
Lemmas
(a)
and

sho
w
that
the
v
ector
from
a
sample
p
oin
t
to
its
rst
p
ole
p
+
is
a
go
o
d
appro
ximation
to
the
surface
normal.
This
observ
ation
ma
y
ha
v
e
wider
applicabilit
y
than
to
our
o
wn
surface
reconstruction
algorithm;
for
example,
the
V
oronoi
diagram
and
the
p
oles
could
b
e
used
to
obtain
pro
v
ably
reliable
estimates
of
tangen
t
planes
in
the
algorithm
of
Hopp
e
et
al.
Our
next
lemma
sho
ws
that
w
e
do
indeed
correctly
select
the
second
p
ole
p
 .
Recall
that
p
 is
dened
to
b
e
the
farthest
V
oronoi
v
ertex
from
s
on
the
opp
osite
side
of
the
surface
from
p
+
.
Lemma
.
Let
s
b
e
a
sample
p
oin
t
from
an
r
-sample
S
with
r

=.
The
second
p
ole
p
 of
s
is
the
farthest
V
oronoi
v
ertex
v
of
s
suc
h
that
the
v
ector
sv
has
negativ
e
dot
pro
duct
with
sp
+
.
Pro
of:
By
Lemma
(a),
d(s;
p
 )

LFS
(s),
so
b
y
Lemma

the
angle
b
et
w
een
sp
+
and
sp
 is
at
least

 
arcsin(r
=(
 r
)),
so
sp
 
sp
+
<
0.
Lemma

also
sho
ws
that
for
an
y
V
oronoi
v
ertex
v
on
the
same
side
of
F
as
p
+
,
with
d(s;
v
)

LFS
(s),
the
angle
b
et
w
een
	


--- Page 11 ---

sv
and
sp
+
is
at
most

arcsin
r
 r


=.
Hence
an
y
v
farther
from
s
than
p
 m
ust
ha
v
e
sv

sp
+
>
0.
Our
next
lemma
b
ounds
the
angle
b
et
w
een
the
normal
to
a
go
o
d
triangle
and
the
surface
normals
at
its
v
ertices.
Lemma
.
Let
T
b
e
a
go
o
d
triangle
and
s
a
v
ertex
of
T
with
angle
at
least

=,
and
c
ho
ose
r
<
=.
(a)
The
angle
b
et
w
een
the
normal
to
T
and
the
normal
to
F
at
s
is
at
most
arcsin
(
p
r
=(
 r
)).
(b)
The
angle
b
et
w
een
the
normal
to
T
and
the
normal
to
F
at
an
y
other
v
ertex
of
T
is
at
most
r
=(
 r
)
+
arcsin
(
p
r
=(
 r
)).
Pro
of:
F
or
part
(a),
let
C
b
e
the
circumcircle
of
T
and
let

C
b
e
its
radius.
Consider
the
balls
of
radius
LFS
(s)
tangen
t
to
F
at
s
on
either
side
of
F
.
These
balls
in
tersect
the
plane
of
T
in
\t
win"
disks
of
common
radius

B
,
tangen
t
at
p
oin
t
s,
as
sho
wn
in
Figure
.
Our
rst
aim
is
to
b
ound

B
in
terms
of

C
.
Since
the
balls
of
radius
LFS
(s)
are
empt
y
of
sample
p
oin
ts,
the
t
win
disks
cannot
con
tain
v
ertices
of
T
.
In
order
to
maximize

B
relativ
e
to

C
,
w
e
assume
that
the
t
win
disks
pass
through
the
v
ertices
of
T
and
that
the
angle
at
s
measures
exactly

=.
No
w
it
is
not
hard
to
sho
w
that

B
is
maximized
exactly
when
T
is
equilateral:
if
w
e
mo
v
e
s
a
w
a
y
from
the
midp
oin
t
of
the
arc
co
v
ered
b
y
the
t
win
disks,
k
eeping
the
t
win
disks
passing
through
the
v
ertices
of
T
,
the
radius

B
decreases,
un
til
s
reac
hes
one
of
the
other
v
ertices
of
T
and

B
=

C
.
Since
the
w
orst-case
conguration
is
equilateral
T
,
w
e
can
conclude
that

B

p

C
.
W
e
can
b
ound
these
radii
in
terms
of
LFS
(s).
Let
u
denote
the
restricted
V
oronoi
diagram
v
ertex
dual
to
T
.
Since
u
lies
on
the
line
through
the
cen
ter
of
C
normal
to
the
plane
of
C
,

C

d(u;
s).
By
Lemma
(b),
d(u;
s)

r
 r
LFS
(s),
so
altogether

B

p
r
LFS
(s)=(
 r
).
No
w
to
nd
the
angle
b
et
w
een
the
normal
to
T
and
the
normal
to
F
at
s,
w
e
consider
one
of
the
big
tangen
t
balls
B
at
s.
Let
m
denote
the
cen
ter
of
B
and
v
denote
the
cen
ter
of
the
disk
of
radius

B
that
is
the
in
tersection
of
B
with
the
plane
of
T
,
as
sho
wn
in
Figure
.
The
segmen
t
sm
is
normal
to
F
at
s
and
the
segmen
t
mv
is
normal
to
T
,
so
the
angle
w
e
w
ould
lik
e
to
b
ound
is

smv
.
The
triangle
smv
is
righ
t,
with
h
yp
oten
use
of
length
LFS
(s)
and
leg
opp
osite

smv
of
length

B

p
r
LFS
(s)=(
 r
).
Hence

smv
measures
at
most
arcsin
(
p

r
=(
 r
)).
F
or
part
(b),
let
s
0
b
e
one
of
the
other
v
ertices
of
T
.
Since
T
is
a
go
o
d
triangle,
s
and
s
0
are
neigh
b
ors
in
the
restricted
V
oronoi
diagram.
Let
p
b
e
a
p
oin
t
on
the
b
oundary
of
b
oth
restricted
V
oronoi
diagram
cells.
Then
d(p;
s)

r
LFS
(p)

r

 r
min
fLFS
(s);
LFS
(s
0
)g:
So
d(s;
s
0
)

r
 r
min
fLFS
(s);
LFS
(s
0
)g.
By
Lemma
,
the
angle
b
et
w
een
the
normals
to
F
at
s
and
s
0
is
at
most
r
=(
 r
)
for
r
<
=.
W
e
need
one
more
lemma
for
the
pro
of
of
Theorem
.
This
lemma
is
a
top
ological
result
concerning
the
medial
axis
that
ma
y
b
e
indep
enden
tly
useful.
Lemma
.
If
a
ball
B
in
tersects
surface
F
in
more
than
one
connected
comp
onen
t,
then
B
con
tains
a
p
oin
t
of
the
medial
axis
of
F
.
0


--- Page 12 ---

ρC
B
ρ
m
s
C
v
T
u
Figure
.
Bounding
the
angle
b
et
w
een
the
normal
to
the
triangle
and
the
normal
to
the
surface
at
s.
Pro
of:
Assume
B
\
F
has
more
than
one
connected
comp
onen
t.
Let
c
b
e
the
cen
ter
of
B
and
p
the
nearest
p
oin
t
on
F
to
c.
If
p
is
not
unique,
then
c
is
a
p
oin
t
of
the
medial
axis
and
w
e
are
done.
Let
q
b
e
the
nearest
p
oin
t
to
c
in
a
connected
comp
onen
t
of
B
\
F
that
do
es
not
con
tain
p.
Imagine
a
p
oin
t
c
0
mo
ving
from
c
to
w
ards
q
along
segmen
t
cq
.
Throughout
this
journey
,
c
0
is
closer
to
q
than
to
an
y
p
oin
t
outside
B
,
so
the
closest
p
oin
t
on
F
to
c
0
m
ust
b
e
some
p
oin
t
of
B
\
F
.
A
t
the
b
eginning
of
the
journey
,
the
closest
p
oin
t
to
c
0
is
p
and
at
the
end
it
is
q
,
so
at
some
critical
c
0
the
closest
p
oin
t
m
ust
c
hange
connected
comp
onen
ts.
Suc
h
a
c
0
is
a
p
oin
t
of
the
medial
axis.
W
e
no
w
giv
e
the
pro
of
of
Theorem
:
the
go
o
d
triangles
form
a
p
olyhedron
homeomor-
phic
to
F
.
The
pro
of
relies
on
the
lemmas
ab
o
v
e
along
with
a
result
of
Edelsbrunner
and
Shah
[
].
Pro
of
of
Theorem
:
The
theorem
of
Edelsbrunner
and
Shah
tells
us
that
it
suces
to
sho
w
that
S
has
the
follo
wing
close
d-b
al
l
pr
op
erty
:
the
closure
of
eac
h
k
-dimensional
face,


k

,
of
the
V
oronoi
diagram
of
S
in
tersects
F
in
either
the
empt
y
set
or
in
a
closed
(k
 )-dimensional
top
ological
ball.
Let
s
b
e
a
sample
p
oin
t
and
V
or
(s)
its
V
oronoi
cell.
Let
the
direction
of
the
normal
to
F
at
s
b
e
v
ertical.
Lemma
(b)
sho
ws
that
V
or
(s)
\
F
is
small,
tting
inside
a
ball
B
around
s
of
radius
r
 r
LFS
(s).
No
w
Lemma

sho
ws
that
F
\
B
has
a
single
connected
comp
onen
t,
and
Lemma

with

=
r
=
 r
sho
ws
that
F
\
B
is
nearly
horizon
tal,
more
precisely
,
the
normal
to
F
\
B
is
no
where
farther
than
r
=(
 r
)

=
radians
from
v
ertical,
assuming
r

:.
These
statemen
ts
in
turn
imply
that
F
\
B
is
a
top
ological
disk;
it
cannot
ha
v
e
a
handle
since
it
is
ev
erywhere
nearly
horizon
tal,
and
it
cannot
ha
v
e
a
hole
b
ecause
its
b
oundary
is
conned
to
the
\lo
w
latitudes"
of
B
.
First
consider
an
edge
e
of
V
or
(s),
that
is,
the
case
k
=
.
If
e
has
nonempt
y
in
ter-
section
with
F
,
then
e
is
normal
to
the
go
o
d
triangle
T
dual
to
its
in
tersection
p
oin
t.
By
Lemma
(b),
e
m
ust
b
e
within
r
=(
 r
)
+
arcsin
(
p

r
=(
 r
))
radians
from
the
normal
to
F
at
s.
F
or
r

:,
this
expression
is
less
than
:	,
so
e
is
within
:	
radians
from
v
ertical,
and
consequen
tly
can
in
tersect
F
only
once
within
B
.
Next
consider
a
face
f
of
V
or
(s),
that
is,
the
case
k
=
.
F
ace
f
is
con
tained
in
a
plane
h,
the
p
erp
endicular
bisector
of
s
and
another
sample
p
oin
t
s
0
,
where
ss
0
is
an
edge
of
a
go
o
d
triangle.
Plane
h
m
ust
con
tain
a
v
ector
parallel
to
the
normal
of
T
,
so
again



--- Page 13 ---

Lemma
(b)
establishes
that
the
angle
b
et
w
een
h
and
the
surface
normal
at
s,
and
hence
b
et
w
een
f
and
the
surface
normal
at
s,
is
at
most
:	
radians
when
r

:.
Consider
a
single
connected
comp
onen
t
C
of
f
\
F
,
a
nearly
horizon
tal
curv
e
dra
wn
across
the
face
f
.
Let
H
b
e
the
set
of
p
oin
ts
p
in
B
n
C
suc
h
that
the
line
segmen
t
from
p
to
its
closest
p
oin
t
on
C
forms
an
angle
smaller
than
.
radians
with
horizon
tal.
(Set
H
is
a
union
of
w
edges
with
v
ertices
on
the
curv
e
C
.)
W
e
assert
that
all
p
oin
ts
of
(F
\
B
)
n
C
lie
in
H
.
W
e
pro
v
e
this
assertion
b
y
sho
wing
that
F
\
B
cannot
cross
the
b
oundary
of
H
.
Assume
(F
\
B
)
n
C
do
es
con
tain
a
p
oin
t
p
on
the
b
oundary
of
H
.
Let
q
b
e
the
closest
p
oin
t
of
C
to
p.
The
v
ertical
plane
P
through
p
and
q
in
tersects
F
\
B
in
a
curv
e.
By
the
Mean
V
alue
Theorem
there
m
ust
b
e
a
p
oin
t
along
this
curv
e
at
whic
h
the
tangen
t
forms
an
angle
greater
than
.
radians
with
horizon
tal;
the
normal
to
F
at
this
p
oin
t
m
ust
b
e
at
least
.
radians
from
v
ertical,
a
con
tradiction.
W
e
further
assert
that
all
p
oin
ts
of
f
\
B
lie
outside
of
H
.
F
ace
f
lies
in
a
plane
within
:	
radians
of
v
ertical,
and
within
a
strip
on
this
plane
b
ounded
b
y
lines
within
:	
radians
of
v
ertical.
All
shortest
segmen
ts
from
p
oin
ts
of
f
to
C
lie
within
this
strip,
and
hence
are
within
:	
radians
of
v
ertical.
Since
f
lies
outside
H
and
F
inside
H
within
B
,
C
m
ust
b
e
the
only
connected
comp
onen
t
of
f
\
F
,
so
f
\
F
is
a
top
ological
-ball.
Finally
consider
V
or
(s)
itself,
the
case
k
=
.
Consider
an
y
connected
comp
onen
t
C
of
the
in
tersection
of
F
\
B
and
the
V
oronoi
cell.
As
in
case
of
k
=
,
let
H
con
tain
eac
h
p
oin
t
that
can
b
e
connected
to
its
closest
p
oin
t
of
C
b
y
a
line
segmen
t
forming
an
angle
smaller
than
.
radians
with
horizon
tal.
The
same
argumen
t
as
ab
o
v
e
sho
ws
that
F
\
B
cannot
cross
the
b
oundary
of
H
.
Since
eac
h
p
oin
t
along
a
face
of
V
or
(s)
in
tersecting
F
can
b
e
connected
to
its
closest
p
oin
t
of
C
b
y
a
segmen
t
within
:	
radians
of
v
ertical,
the
same
is
true
of
an
in
terior
p
oin
t
of
V
or
(s
)
.
Since
F
\
B
is
conned
to
one
piece
of
B
n
C
and
V
or
(S
)
to
another,
w
e
can
conclude
that
C
is
the
only
connected
comp
onen
t
of
F
\
V
or
(S
).
Aiming
for
a
con
tradiction,
assume
that
C
is
a
top
ological
disk
with
holes.
Consider
an
y
v
ertical
plane
P
that
meets
t
w
o
comp
onen
ts
of
the
b
oundary
of
C
at
angles
at
least

=
 =.
(T
o
nd
suc
h
a
plane,
w
e
could
pro
ject
the
t
w
o
b
oundary
comp
onen
ts
on
to
a
horizon
tal
plane,
and
then
sw
eep
around
a
normal
to
one
closed
curv
e
in
order
to
nd
a
line
meeting
eac
h
closed
curv
e
p
erp
endicularly
.)
As
sho
wn
in
Figure
,
within
plane
P
the
b
oundary
of
V
or
(s)
meets
F
at
an
angle
larger
than
:0,
extends
some
distance
on
the
other
side
of
F
,
and
then
recrosses
F
again
at
:0
from
v
ertical.
(Wh
y
:0?
The
face
of
V
or
(s)
is
within
:	
of
v
ertical
as
ab
o
v
e,
and
:
>
=
is
added
for
P
's
deviation
from
p
erp
endicularit
y
with
the
face.)
Since
the
tangen
t
to
F
\
P
is
ev
erywhere
within
=
radians
of
horizon
tal,
if
F
recrossed
V
or
(s)
within
P
,
then
P
\
V
or
(s)
w
ould
b
e
noncon
v
ex,
a
con
tradiction.
Finally
C
cannot
ha
v
e
a
handle
b
ecause
it
is
a
piece
of
the
top
ological
disk
F
\
B
.
Hence
C
m
ust
itself
b
e
a
top
ological
disk
and
w
e
are
done.
Next
w
e
giv
e
a
pro
of
of
Theorem
:
the
ra
w
crust
con
tains
all
the
go
o
d
triangles.
The
in
tuition
b
ehind
this
pro
of
is
that
restricted
V
oronoi
cells
are
small
and
p
oles
are
far
a
w
a
y
,
so
that
the
ball
cen
tered
at
a
v
ertex
u
of
the
restricted
V
oronoi
diagram,
passing
through
the
three
sample
p
oin
ts
whose
cells
meet
at
u,
m
ust
b
e
empt
y
of
p
oles.
Pro
of
of
Theorem
:
Let
T
b
e
a
triangle
dual
to
a
v
ertex
u
of
the
restricted
V
oronoi
diagram.
Consider
the
ball
B
u
cen
tered
on
u
with
b
oundary
passing
through
the
v
ertices



--- Page 14 ---

Vor(s)
F
1.05 rad
Figure
.
A
v
ertical
cross-section
of
F
\
V
or
(s)
sho
ws
the
imp
ossibilit
y
of
a
disk
with
a
hole.
of
T
.
Since
T
is
a
Delauna
y
triangle,
B
u
con
tains
no
p
oin
t
of
S
in
its
in
terior.
Since
S
is
an
r
-sample
of
F
for
r
<
,
the
radius
of
B
u
is
less
than
r
LFS
(u).
By
the
denition
of
LFS
,
ev
en
the
larger
ball
B
0
u
of
radius
LFS
(u)
cen
tered
on
u
cannot
con
tain
a
p
oin
t
of
the
medial
axis.
No
w
assume
that
B
u
con
tains
a
p
ole
v
of
a
sample
p
oin
t
s.
W
e
will
sho
w
that
under
this
assumption,
rst,
that
B
v
m
ust
con
tain
a
p
oin
t
of
the
medial
axis,
and
second,
that
the
p
olar
ball
B
v
m
ust
b
e
con
tained
in
B
0
u
,
thereb
y
giving
a
con
tradiction.
In
particular,
B
v
m
ust
con
tain
the
cen
ter
m
of
the
medial
ball
B
m
at
s
that
is
on
the
same
side
of
F
as
v
.
Notice
that
m
necessarily
lies
in
V
or
(s)
and
ball
B
m
has
radius
at
least
LFS
(s),
while
the
radius
of
B
v
is
at
least
that
of
B
m
(b
y
Lemma
).
By
Lemma
,

msv
measures
at
most

arcsin
r
 r
,
whic
h
is
less
than
:
for
r

:.
A
calculation
sho
ws
that
B
v
m
ust
con
tain
the
medial
axis
p
oin
t
m.
Since
v
lies
in
B
u
,
the
radius
of
B
v
is
no
greater
than
the
distance
from
v
to
the
nearest
v
ertex
of
T
,
whic
h
is
at
most
r
LFS
(u)
since
S
is
an
r
-sample.
Since
d(u;
v
)

r
LFS
(u),
ball
B
v
lies
en
tirely
within
B
0
u
since
r
LFS
(u)

LFS
(u).
W
e
no
w
mo
v
e
on
to
the
pro
of
of
Theorem
.
Let
s
b
e
a
sample
p
oin
t
and
v
a
p
ole
of
s.
W
e
shall
dene
a
forbidden
region
inside
p
olar
ball
B
v
,
whic
h
cannot
b
e
p
enetrated
b
y
large
crust
triangles.
Let
B
+
m
b
e
the
big
tangen
t
ball
at
s,
on
the
same
side
of
F
as
v
,
and
let
B
 m
b
e
the
big
tangen
t
ball
on
the
other
side,
with
F
passing
b
et
w
een
them.
Let
B
b
e
the
ball
concen
tric
with
B
 m
with
radius
(
 a)LFS
(s),
as
sho
wn
in
Figure
(a);
a
is
a
constan
t
that
will
b
e
c
hosen
later.
Notice
that
Lemma
(a)
sho
ws
that
the
radius
of
B
v
is
at
least
that
of
B
.
Denition
.
The
reection
of
a
p
oin
t
t
through
B
v
is
the
p
oin
t
t
0
along
ra
y
v
t
suc
h
that
line
segmen
t
tt
0
is
divided
in
to
equal
halv
es
b
y
the
b
oundary
of
B
v
.
The
spindle
of
s
is
f
t

B
v
j
segmen
t
tt
0
in
tersects
B
g,
that
is,
all
p
oin
ts
in
B
v
whose
reection
lies
in
or
b
ey
ond
B
.
The
spindle
is
shaded
in
Figure
(a).
Our
plan
is
to
conne
large
crust
triangles
b
et
w
een
the
union
of
spindles
on
eac
h
side
of
F
as
sho
wn
in
Figure
(b).
(Small
crust
triangles
lie
within
the
fattened
surface
simply
due
to
their
size.)
W
e
start
b
y
pro
ving
t
w
o
lemmas
ab
out
spindles:
they
are
indeed
forbidden
regions,
and
they
ha
v
e
relativ
ely
\at"
b
ottoms,
meaning
that
their
width
do
es
not
shrink
with
shrinking
r
.
Lemma
.
No
crust
triangle
T
whose
Delauna
y
ball
B
T
has
radius
greater
than
r
LFS
(s)
can
p
enetrate
the
spindle
of
s.



--- Page 15 ---

m
T
B
s
B
F
v
B
-
B
t
-
t’
T
v
Figure
.
(a)
The
Delauna
y
ball
B
T
of
a
triangle
in
tersecting
the
spindle
m
ust
con
tain
a
big
patc
h
of
surface
F
.
(b)
Spindles
of
sample
p
oin
ts
fuse
so
that
all
triangles
m
ust
lie
close
to
F
.
Pro
of:
Assume
t
is
a
p
oin
t
inside
B
v
on
a
crust
triangle
T
with
Delauna
y
ball
B
T
.
W
e
rst
assert
that
B
T
con
tains
the
reection
p
oin
t
t
0
.
Let
H
b
e
the
plane
con
taining
the
in
tersection
of
the
b
oundaries
of
B
v
and
B
T
.
Since
the
v
ertices
of
T
lie
on
B
T
outside
B
v
,
T
m
ust
b
e
con
tained
in
the
closed
halfspace
b
ounded
b
y
H
not
con
taining
v
.
It
suces
to
pro
v
e
the
lemma
for
the
case
in
whic
h
t
lies
righ
t
on
H
,
as
the
reection
of
an
y
t
in
the
in
terior
of
the
halfspace
lies
b
et
w
een
H
and
a
reection
of
a
p
oin
t
on
H
.
W
e
ma
y
also
assume
that
ball
B
T
passes
through
v
,
since
if
w
e
replace
B
T
with
the
ball
that
touc
hes
v
and
has
the
same
in
tersection
with
H
,
the
part
of
B
T
outside
B
v
shrinks
(making
things
harder
for
our
lemma).
No
w
consider
an
y
plane
con
taining
line
v
t.
Balls
B
v
and
B
T
in
tersect
this
plane
in
circles
and
plane
H
in
tersects
in
a
line
con
taining
the
m
utual
c
hord
of
these
circles.
See
Figure
(a).
Assume
w.l.o.g.
that
the
cross-section
of
B
v
is
the
unit
circle
with
cen
ter
v
=
(0;
).
Let
t
=
(0;
y
t
).
Denote
the
cen
ter
and
radius
of
B
T
's
cross-section
b
y
(x;
y
)
and
.
Since
t
lies
along
the
m
utual
c
hord,
it
has
equal
\p
o
w
er
distance"
to
(0;
)
and
(x;
y
):
(
 y
t
)

 
=
x

+
(y
 y
t
)

 

:
Substituting
(
 y
)

for


 x

,
w
e
obtain
y
t

 y
t
=
(y
 y
t
)

 (
 y
)

;
whic
h
simplies
to
y
=
(
 y
t
)=(
 y
t
).
Th
us
the
cen
ters
of
all
p
ossible
B
T
circles
lie
on
the
same
horizon
tal
line,
as
sho
wn
in
Figure
(b).
An
y
B
T
passes
through
the
reection
of
(0;
)
across
the
horizon
tal
line,
the
p
oin
t
(0;
(
 y
t
)=(
 y
t
)
 ).
F
or
an
y
v
alue
of
y
t
<
,
(
 y
t
)=(
 y
t
)
 
<
 y
t
,
so
B
T
con
tains
t
0
=
(0;
 y
t
).



--- Page 16 ---

T
H
t
t’
v
(x,y)
ρ
B
v
B
Figure
.
(a)
B
T
m
ust
con
tain
reection
p
oin
t
t
0
.
(b)
The
family
of
p
ossible
B
T
circles.
Th
us
if
the
original
p
oin
t
t
lies
within
the
spindle
of
s,
then
B
T
m
ust
in
tersect
B
,
the
ball
concen
tric
with
B
 m
.
Aiming
for
a
con
tradiction,
assume
that
t
do
es
indeed
lie
within
the
spindle
of
s.
Then
B
T
p
enetrates
eac
h
of
B
v
and
B
 m
\deeply",
at
least
r
LFS
(s)
in
to
eac
h
of
these
balls.
Consider
the
disk
D
m
b
ounded
b
y
the
circle
that
is
the
in
tersection
of
the
b
oundaries
of
B
T
and
B
 m
.
Using
the
facts
that
the
radius
of
B
T
is
at
least
r
LFS
(s),
the
radius
of
B
 m
at
least
LFS
(s)

r
LFS
(s),
and
the
fact
that
B
T
cuts
at
least
r
LFS
(s)
in
to
B
 m
,
w
e
can
calculate
that
D
m
has
radius
at
least
:r
LFS
(s).
There
is
an
analogous
disk
D
v
,
b
ounded
the
in
tersection
of
the
b
oundaries
of
B
v
and
B
 m
,
with
radius
at
least
:r
LFS
(s).
W
e
no
w
assert
that
there
exists
a
p
oin
t
c

F
\
(B
T
[
B
v
),
with
d(c;
s)

p
LFS
(s),
suc
h
that
the
ball
of
radius
:r
LFS
(s)
around
c
con
tains
no
sample
p
oin
ts.
Surface
F
is
conned
b
et
w
een
B
 m
and
B
+
m
,
and
hence
m
ust
cross
B
T
[
B
v
\deeply",
meaning
that
some
p
oin
t
of
F
inside
B
T
[
B
v
m
ust
b
e
at
least
distance
:r
LFS
(s)
from
the
b
oundary
of
B
T
[
B
v
.
Moreo
v
er,
there
is
a
deep
p
oin
t
no
farther
than
p
LFS
(s)
from
s,
since
B
T
in
tersects
b
oth
shrunk
en
ball
B
and
the
spindle
of
s.
(If
w
e
tak
e
s
to
b
e
the
north
p
ole
of
B
 m
,
then
the
w
orst
case
w
ould
b
e
a
v
ery
large
B
T
with
deep
p
oin
t
nearest
the
equator
of
B
 m
.)
No
w
since
d(c;
s)

p
LFS
(s),
LFS
(c)

(
+
p
)LFS
(s).
W
e
ha
v
e
obtained
a
con
tra-
diction
to
F
b
eing
r
-sampled.
The
next
lemma
sho
ws
that
spindles
ha
v
e
at
b
ottoms.
In
this
lemma
w
e
assume
that
B
and
B
v
ha
v
e
equal
radius.
It
is
not
hard
to
conrm
that
this
assumption
is
w
orst
case:
a
larger
B
v
just
giv
es
a
larger,
atter
spindle.
Lemma
	.
Assume
that
B
and
B
v
are
unit
balls,
and
that
the
distance
b
et
w
een
them
is
at
most


:0.
Let
t
b
e
a
p
oin
t
outside
B
and
outside
the
spindle
induced
b
y
B
in
B
v
.
Let
p
b
e
the
closest
p
oin
t
on
B
to
t.
If
j

ompj,
the
measure
of

omp
in
radians,
is
less
than
:0,
then
d(t;
p)


+
j

ompj.
Pro
of:
Assume
v
has
co
ordinates
(0;
).
The
w
orst
case
for
the
lemma
o
ccurs
when

assumes
its
maxim
um
v
alue,
as
larger

means
a
higher
and
narro
w
er
spindle,
thereb
y
maximizing
d(t;
p)
relativ
e
to

+
j

ompj.
So
assume
m
has
co
ordinates
(0;
 :0).



--- Page 17 ---

B
v
B
t’
t
o
p
x
.2 rad
.32 rad
Figure
	.
The
spindle
curv
es
gradually
,
so
t
m
ust
b
e
close
to
B
.
Dra
w
the
:0-radian
ra
y
with
origin
m
and
the
:-radian
ra
y
with
origin
v
as
sho
wn
in
Figure
	.
The
ra
ys
in
tersect
at
a
p
oin
t
x
with
co
ordinates
ab
out
(:	;
:).
By
computing
the
distances
to
the
b
oundaries
of
B
v
and
B
along
ra
y
v
x,
w
e
can
conrm
that
x
lies
inside
the
spindle.
Th
us
the
b
oundary
of
the
spindle
lies
b
elo
w
x
on
the
:0-radian
ra
y
with
origin
m.
Assume
t
and
p
are
at
the
extremal
p
ositions
allo
w
ed
b
y
the
lemma,
so
that
t
is
on
the
b
oundary
of
the
spindle
and
j

ompj
=
:0.
The
distance
from
x
to
m
is
less
than
:,
so
d(t;
p)
 

:	

j

ompj.
Since
d(t;
p)
increases
ev
er
more
rapidly
as
j

ompj
increases,
this
inequalit
y
also
applies
to
p
oin
ts
t
and
p
suc
h
that
j

ompj
<
:0
as
w
ell.
W
e
are
no
w
in
a
p
osition
to
nish
the
pro
of
of
the
theorem:
all
crust
triangles
lie
within
the
fattened
surface
formed
b
y
placing
a
ball
of
radius
r
LFS
(q
)
around
eac
h
p
oin
t
q

F
.
Pro
of
of
Theorem
:
Let
B
T
b
e
the
Delauna
y
ball
of
the
crust
triangle
con
taining
p
oin
t
t.
Let
s
b
e
the
sample
p
oin
t
nearest
t.
If
B
T
has
radius
less
than
r
LFS
(s),
then
there
is
nothing
to
pro
v
e,
since
s
itself
could
b
e
the
q
of
the
theorem.
So
assume
B
T
has
radius
at
least
r
LFS
(s).
Let
B
v
,
B
 m
,
and
B
b
e
the
p
olar
ball
of
s,
the
tangen
t
ball
of
radius
LFS
(s)
on
the
opp
osite
side
of
F
,
and
the
concen
tric
ball
with
radius
reduced
b
y
r
LFS
(s)
as
in
Figure
0.
Let
o
and
o
0
b
e
the
p
oin
ts
of
lune
B
 m
\
B
v
closest
to
the
cen
ters
of
B
 m
and
B
v
,
resp
ectiv
ely
.
Surface
F
could
pass
through
the
p
oin
t
o
0
,
and
if
it
did,
s
w
ould
necessarily
b
e
the
closest
sample
p
oin
t
to
o
0
,
since
B
 m
and
B
v
are
b
oth
empt
y
.
Hence
b
y
Lemma
(b),
d(s;
o
0
)

r
LFS
(s)=(
 r
).
Since
B
v
has
radius
at
least
that
of
B
 m
,
d(s;
o)

d(s;
o
0
).
Let
p
and
p
0
b
e
the
closest
p
oin
ts
to
t
on
B
and
B
 m
,
resp
ectiv
ely
,
and
let
q
b
e
the
p
oin
t
of
F
on
line
pt
closest
to
t.
Hence
d(t;
q
)

d(p;
t).
By
an
argumen
t
analogous
to
that
used
for
o
0
,
d(s;
p
0
)

r
LFS
(s)=(
 r
),
and
so
b
y
the
triangle
inequalit
y
,
d(o;
p
0
)

r
LFS
(s)=(
 r
).
So

omp
0


arcsin(r
=( r
)),
whic
h
for
r

:0,
is
less
than
:0
radians.
The
set-up
satises
the
h
yp
otheses
of
Lemma
	,
only
with
radii
scaled
b
y
(
 r
)LFS
(s).
By
Lemma
,
t
m
ust
lie
b
et
w
een
the
spindle
and
B
m
.
Applying
Lemma
	,
d(t;
p)

r
LFS
(s)
+
j

ompj(
 r
)LFS
(s):
W
e
no
w
use
the
fact
that
j

ompj


arcsin(r
=(
 r
))

r
,
to
obtain
d(t;
p)

r
LFS
(s)
+
r
(
 r
)LFS
(s)

r
LFS
(s):



--- Page 18 ---

v
B
F
s
t
-
q
o
Bm
B
p
o’
p’
-
Figure
0.
Crust
p
oin
t
t
m
ust
b
e
near
surface
p
oin
t
q
.
Finally
,
d(s;
q
)

r
LFS
(s)=(
 r
),
so
b
y
Lemma
,
LFS
(q
)

(
 r
)LFS
(s)=(
 r
),
and
hence
r
LFS
(q
)

d(t;
p)

d(t;
q
).
Let
T
b
e
a
triangle
of
the

-crust
with

=
r
,
t
b
e
a
p
oin
t
on
T
,
and
p
b
e
the
closest
p
oin
t
to
t
on
F
.
Theorem

states
that
the
angle
b
et
w
een
the
normal
to
T
and
the
normal
to
F
at
p
measures
O
(
p
r
)
radians.
Pro
of
of
Theorem
:
First,
w
e
establish
the
easier
claim
that
at
eac
h
sample
p
oin
t
s,
the
normals
to
inciden
t

-crust
triangles
do
not
deviate
b
y
more
than
O
(r
)
radians
from
the
normal
to
F
.
This
statemen
t
follo
ws
from
the
fact
that
Step

of
the
algorithm
remo
v
es
eac
h
triangle
around
s
whose
normal
forms
an
angle
larger
than
r
with
the
v
ector
to
the
p
ole.
By
Lemma
,
the
p
ole
v
ector
deviates
from
the
normal
to
F
b
y
at
most
 
=

arcsin(r
=(
 r
)),
so
that
 

:r
for
r

:0.
No
w
let
t
b
e
an
y
p
oin
t
on
a

-crust
triangle
T
,
and
let
p
b
e
the
closest
p
oin
t
on
F
to
t.
By
Theorem
,
d(t;
p)

r
LFS
(p).
Let
s
denote
the
closest
v
ertex
of
T
to
t,
C
the
radius
of
T
's
circumcircle,
and

the
radius
of
T
's
Delauna
y
ball
B
T
.
If
C

p
r
LFS
(s)
then
d(s;
p)
is
O
(
p
r
),
and
Theorem

follo
ws
from
Lemma

and
the
b
ound
on
 
.
So
assume
C
and
hence

is
at
least
p
r
LFS
(s).
Let

denote
the
angle
b
et
w
een
the
normal
to
F
at
s
and
the
v
ector
from
s
to
the
cen
ter
v
of
B
T
.
Lemma

with

=
p
r
implies
that



p
r
=(
 r
)
radians.
Next
let

denote
the
angle
b
et
w
een
the
normal
to
T
at
s
and
the
v
ector
from
s
to
v
,
as
sho
wn
in
Figure
.
Angle



+
 
,
where
 
,
as
ab
o
v
e,
is
the
angle
b
et
w
een
the
normal
to
the
surface
at
s
and
the
normal
to
T
.
Since
 
=
O
(r
),
w
e
can
conclude
that



p
r
for
small
enough
r
.
No
w
C
=

sin

,
so

=
C
=
sin


LFS
(s)=.
Th
us
the
assumption
that
C
is
large
(at
least
p
r
LFS
(s))
sho
ws
that

m
ust
b
e
v
ery
large
(at
least
LFS
(s)=).
W
e
can
no
w
return
to
Lemma

with

=
=.
This
time
w
e
obtain
an
upp
er
b
ound
of
O
(r
)
on

and

,
and
a
lo
w
er
b
ound
of

(LFS
(s)=
p
r
)
on
.
(Sadly
,
w
e
cannot
rep
eat
this
tric
k
to
inate

indenitely
,
since
 
remains
O
(r
).)
Notice
that
since

is
O
(r
),
the
plane
con
taining
T
cuts
a
small
spherical
cap
on
B
T
,
one
subtending
solid
angle
of
only
O
(r
).
This
means
that
T
itself
is
small
with
resp
ect
to
B
T
;
the
p
oin
t
t

T
can
b
e
at
most
O
(r
)
from
a
v
ertex
s,
b
ounding
(b
y
Lemma
)



--- Page 19 ---

s
BT
φ
δ
δ
F
t
p
v
ρ
C
T
Figure
.
Rep
eated
use
of
Lemma

sho
ws
that
if
triangle
T
is
large
B
T
m
ust
b
e
enormous.
LFS
(t)

O
(r
)
+
LFS
(s),
whic
h
is
O
(
p
r
).
And
since
t
is
within
r
LFS
(p)
of
p,
LFS
(p)
is
O
(
p
r
)
as
w
ell.
No
w
assume
that
the
normal
to
F
at
p
deviates
from
the
normal
to
T
b
y

(
p
r
),
and
con-
sider
the
big
tangen
t
balls
of
radius
LFS
(p)
at
p.
The
p
oin
t
p
is
close
{
within
O
(r
LFS
(p))
{
to
the
surface
of
B
T
,
while
the
radius
of
B
T
is
m
uc
h
larger
{

=
O
(LFS
(p)=
p
r
)
{
than
the
radius
of
the
big
tangen
t
balls
at
p.
F
or
some
small
enough
v
alue
of
r
,
the
big
tangen
t
balls
in
tersect
B
T
in
circular
patc
hes
of
radius

(
p
r
)LFS
(p).
As
in
the
pro
of
of
Lemma
,
F
is
conned
b
et
w
een
these
t
w
o
balls,
so
there
m
ust
b
e
a
similar-size
patc
h
of
F
inside
B
T
,
and
hence
empt
y
of
sample
p
oin
ts,
whic
h
giv
es
a
con
tradiction
to
S
b
eing
an
r
-sample.
This
con
tradiction
establishes
Theorem
.
Finally
,
Theorem

states
that
for
sucien
tly
small
r
,
the
trimmed

-crust
is
homeo-
morphic
to
F
.
Pro
of
of
Theorem
:
W
e
rst
pro
v
e
that
the
(un
trimmed)

-crust
still
con
tains
all
the
go
o
d
triangles.
Since
Theorem

sho
ws
that
the
ra
w
crust
con
tains
all
the
go
o
d
triangles,
w
e
only
need
to
sho
w
that
eac
h
go
o
d
triangle
passes
the
ltering-b
y-normal
step.
Let
T
b
e
a
go
o
d
triangle
and
s
its
v
ertex
of
maxim
um
angle.
By
Lemma
(a),
the
angle
b
et
w
een
the
normal
to
T
and
the
normal
to
F
at
s
measures
at
most
arcsin
(
p
r
=(
 r
))
radians.
By
Lemma
,
the
angle
b
et
w
een
the
p
ole
v
ector
at
s
and
the
normal
to
F
at
s
measures
at
most

arcsin(r
=(
 r
)).
Com
bining
these
t
w
o
b
ounds,
the
angle
b
et
w
een
the
normal
to
T
and
either
p
ole
v
ector
at
s
m
ust
b
e
less
than
r
=

.
Similarly
,
Lemmas
(b)
and

com
bine
to
sho
w
that
the
angle
b
et
w
een
the
normal
to
T
and
the
p
ole
angle
at
an
y
other
v
ertex
of
T
is
at
most

arcsin(r
=(
 r
))
+
r
=(
 r
)
+
arcsin
(
p

r
=(
 r
))
radians,
whic
h,
for
small
enough
r
,
is
less
than
r
=

=.
W
e
m
ust
no
w
sho
w
that
the
trimming
op
eration
(Step
)
pro
duces
a
set
of
triangles
with
the
same
top
ology
as
the
go
o
d
triangles.
Let
s
b
e
a
sample
p
oin
t,
and
assume
the
normal
to
F
at
s
is
v
ertical.
Step

ensures
that
for
r

:0,
all
triangles
around
s
remaining
after
Step

ha
v
e
normals
within
:
radians
of
v
ertical.
By
Lemma
,
the
v
ector
from
s
to
one
of
its
p
oles
is
within
:
radians
of
v
ertical.
Since
:
+
:
<

=,
the
v
ertex-to-triangle
breadth-rst-searc
h
in
Step

orien
ts
triangles
consisten
tly:
the
orien
tations
do
not
dep
end
on
the
actual
searc
h
order,
and
at
eac
h
v
ertex
they
agree
with
an
orien
tation
of
F
.



--- Page 20 ---

Figure
.
A
reconstructed
minimal
surface
along
with
the
p
oles
of
sample
p
oin
ts.
The
crust
con
tains
exactly
the
original
triangles.
(Sample
p
oin
ts
courtesy
of
Hugues
Hopp
e)
After
all
triangles
with
sharp
edges
ha
v
e
b
een
remo
v
ed,
all
w
alks
along
the
remaining
set
of
triangles,
that
do
not
pierce
a
triangle,
m
ust
run
along
either
only
inside
or
only
outside
sides
of
triangles.
Go
o
d
triangles
cannot
ha
v
e
sharp
edges,
since
the
dihedral
b
et
w
een
adjacen
t
go
o
d
triangles
is
less
than

=,
and
hence
are
nev
er
remo
v
ed.
Consider
the
mapping
that
tak
es
eac
h
p
oin
t
of
space
to
its
closest
p
oin
t
on
F
.
W
e
claim
that
the
restriction
of
this
mapping
to
the
trimmed

-crust
is
a
homeomorphism.
Since
the
go
o
d
triangles
surviv
ed
up
un
til
the
nal
breadth-rst-searc
h,
the
trimmed

-crust
con
tains
a
set
of
triangles
homeomorphic
to
F
and
at
least
one
p
oin
t
of
the
trimmed

-crust
is
mapp
ed
to
eac
h
p
oin
t
of
F
.
By
Theorem

eac
h
triangle
is
nearly
parallel
to
F
,
so
the
map
is
one-to-one
on
eac
h
triangle.
And
b
ecause
the
triangles
are
consisten
tly
orien
ted,
p
oin
ts
on
t
w
o
dieren
t
triangles
cannot
map
to
the
same
p
oin
t
on
F
.

Implemen
tation
and
Examples
Manolis
Kam
vysselis,
an
undergraduate
from
MIT,
implemen
ted
steps
{
of
the
crust
algorithm
during
a
summer
at
Xero
x
P
AR
C.
W
e
used
Clarkson's
Hul
l
program
[
]
for
Delauna
y
triangulation,
and
Ge
omview
[
]
to
visualize
and
prin
t
the
results.
W
e
used
v
ertices
from
pre-existing
p
olyhedral
mo
dels
as
inputs,
in
order
to
compare
our
results
with
\ground
truth".
A
companion
pap
er
[]
rep
orts
on
our
exp
erimen
tal
ndings.
The
only
tric
ky
part
of
the
implemen
tation
w
as
the
handling
of
degeneracies
and
near
degeneracies.
Our
test
examples,
man
y
of
whic
h
started
from
appro
ximately
gridded
sample
p
oin
ts,
included
n
umerous
quadruples
of
p
oin
ts
supp
orting
sliv
ers.
Kam
vysselis
incorp
o-
rated
an
explicit
tolerance
parameter
;
the
circumcen
ter
of
quadruples
within

of
co
circu-
	


--- Page 21 ---

Figure
.
The
ra
w
crust
con
tains
some
extra
triangles
linking
the
sausages;
this
defect
is
corrected
b
y
step
.
(Sample
p
oin
ts
courtesy
of
P
aul
Hec
kb
ert)
larit
y
w
as
computed
b
y
simply
computing
the
circumcen
ter
of
a
subset
of
three.
This
\hac
k"
did
not
aect
the
o
v
erall
algorithm,
as
these
cen
ters
w
ere
nev
er
p
oles.
Running
time
w
as
only
a
little
more
than
the
time
for
t
w
o
three-dimensional
Delauna
y
triangulations.
Notice
that
the
Delauna
y
triangulation
in
step

in
v
olv
es
at
most
three
times
the
original
n
um
b
er
of
v
ertices.
Figure
.
(a)
The
pig
sample
set
con
tains

p
oin
ts.
(b)
A
close-up
of
the
fron
t
feet
sho
ws
an
eect
of
undersampling.
(Sample
p
oin
ts
courtesy
of
Tim
Bak
er)
Figure

sho
ws
an
esp
ecially
adv
an
tageous
example
for
our
algorithm,
a
w
ell-spaced
p
oin
t
set
on
a
smo
oth
surface.
Ev
en
though
our
algorithm
is
not
designed
for
surfaces
with
b
oundary
,
it
ac
hiev
es
p
erfect
reconstruction
on
this
example.
Of
course,
the
trimming
step
should
not
b
e
used
in
reconstructing
a
surface
with
b
oundary
.
Figure

sho
ws
an
eect
of
undersampling.
(W
e
sa
y
w
e
ha
v
e
undersample
d
if
the
0


--- Page 22 ---

sample
set
is
not
an
r
-sample
for
a
sucien
tly
small
r
.)
In
this
example,
the
ra
w
crust
con
tains
all
the
go
o
d
triangles,
along
with
some
extra
triangles.
The
extra
triangles
turn
separated
sausages
in
to
link
sausages,
and
as
men
tioned
ab
o
v
e
roughen
the
inside
surfaces
of
the
sausages.
Both
of
these
defects
are
corrected
b
y
step
,
ltering
b
y
normals.
Figure

sho
ws
another
eect
of
undersampling:
missing
triangles
around
the
c
hest
and
ho
o
v
es.
Some
sample
p
oin
ts
are
not
\opp
osed"
b
y
samples
on
the
other
side
of
these
roughly
cylindrical
surfaces;
hence
V
oronoi
cells
extend
to
o
far
and
p
oles
lter
out
some
go
o
d
triangles.
An
r
-sample
for
a
sucien
tly
small
r
w
ould
b
e
v
ery
dense
near
the
ho
o
v
es,
whic
h
include
some
rather
sharp
corners.

Conclusions
and
F
uture
W
ork
In
this
pap
er
w
e
ha
v
e
giv
en
an
algorithm
for
reconstructing
an
in
terp
olating
surface
from
sample
p
oin
ts
in
three
dimensions.
The
algorithm
is
simple
enough
to
analyze,
easy
enough
to
implemen
t,
and
practical
enough
for
actual
use.
Our
previous
pap
er
[
]
ga
v
e
t
w
o
pro
v
ably
go
o
d
algorithms
for
reconstructing
curv
es
in
t
w
o
dimensions,
one
using
V
oronoi
ltering
as
in
this
pap
er,
and
the
other
using
the

-
sk
eleton.
It
is
in
teresting
to
ask
whether
the

-sk
eleton
can
b
e
generalized
to
the
problem
of
surface
reconstruction.
(W
e
kno
w
that
the
most
straigh
tforw
ard
generalization
of
the

-sk
eleton
do
es
not
w
ork.)
Another
in
teresting
question
concerns
the
generalization
of
V
oronoi
ltering
to
higher
dimensions.
Manifold
le
arning
is
the
problem
of
reconstructing
a
smo
oth
k
-dimensional
manifold
em
b
edded
in
I
R
d
.
This
problem
arises
in
mo
deling
unkno
wn
dynamical
systems
from
exp
erimen
tal
observ
ations
[0
].
Ev
en
if
V
oronoi
ltering
can
b
e
generalized
to
this
problem,
its
running
time
for
the
imp
ortan
t
case
in
whic
h
k

d
w
ould
not
b
e
comp
etitiv
e
with
algorithms
that
compute
triangulations
only
in
k
-dimensional
subspaces
[0
],
rather
than
in
I
R
d
.
Along
with
the
t
w
o
theoretical
op
en
questions
outlined
ab
o
v
e,
there
are
sev
eral
quite
practical
directions
for
further
researc
h
on
our
algorithms.
What
is
the
empirical
maxim
um
v
alue
of
r
for
whic
h
our
algorithm
giv
es
reliable
results?
W
e
b
eliev
e
that
the
v
alue
of
r

:0
in
Theorem

is
m
uc
h
smaller
than
necessary
.
Is
the
crust
useful
in
simplication
and
compression
of
p
olyhedra?
Can
the
crust
b
e
extended
to
inputs
with
creases
and
corners,
suc
h
as
mac
hine
parts?
Can
the
crust
b
e
mo
died
for
the
problem
of
reconstruction
from
cross-sections,
in
whic
h
the
input
is
more
structured
than
scattered
p
oin
ts?
Ac
kno
wledgemen
ts
W
e
w
ould
lik
e
to
thank
Alan
Cline,
Bob
Connelly
,
T
amal
Dey
,
Herb
ert
Edelsbrunner,
Da
vid
Eppstein,
Da
vid
Goldb
erg,
and
Manolis
Kam
vysselis
for
helpful
con
v
ersations,
and
Ken
Clarkson
and
The
Geometry
Cen
ter
for
making
their
soft
w
are
a
v
ailable.



--- Page 23 ---

References
[]
N.
Amen
ta,
M.
Bern,
and
D.
Eppstein.
The
crust
and
the

-sk
eleton:
com
binatorial
curv
e
reconstruction.
T
o
app
ear
in
Gr
aphic
al
Mo
dels
and
Image
Pr
o
c
essing
.
[]
N.
Amen
ta,
M.
Bern,
and
M.
Kam
vysselis.
A
new
V
oronoi-based
surface
reconstruction
algorithm.
T
o
app
ear
in
Siggr
aph
		.
[]
D.
A
ttali.
r
-Regular
shap
e
reconstruction
from
unorganized
p
oin
ts.
In
Pr
o
c.
th
A
CM
Symp.
Computational
Ge
ometry
,
		,
{.
[]
G.
Barequet.
Piecewise-linear
in
terp
olation
b
et
w
een
p
olygonal
slices.
In
Pr
o
c.
0th
A
CM
Symp.
Computational
Ge
ometry
,
		,
	{0.
[]
C.
Ba
ja
j,
F.
Bernardini,
and
G.
Xu.
Automatic
reconstruction
of
surfaces
and
scalar
elds
from
D
scans.
Pr
o
c.
SIGGRAPH
'	
,
		,
0	{.
[]
F.
Bernardini
and
C.
Ba
ja
j.
Sampling
and
reconstructing
manifolds
using
-shap
es,
	th
Canadian
Confer
enc
e
on
Computational
Ge
ometry
,
		,
	{	.
[]
F.
Bernardini,
C.
Ba
ja
j,
J.
Chen
and
D.
Sc
hik
ore.
Automatic
reconstruction
of
D
CAD
mo
dels
from
digital
scans.
T
ec
hnical
rep
ort
CSD-	-0,
Purdue
Univ
ersit
y
(		).
[]
F.
Bernardini,
C.
Ba
ja
j,
J.
Chen,
D.
Sc
hik
ore.
A
triangulation-based
ob
ject
reconstruc-
tion
metho
d.
Pr
o
c.
th
A
CM
Symp.
Computational
Ge
ometry
,
		,
{.
[	]
J-D.
Boissonnat.
Geometric
structures
for
three-dimensional
shap
e
reconstruction.
A
CM
T
r
ans.
Gr
aphics

(	)
{.
[0]
C.
Bregler
and
S.
M.
Omoh
undro.
Nonlinear
manifold
learning
for
visual
sp
eec
h
recog-
nition.
Pr
o
c.
th
International
Conf.
on
Computer
Vision
,
		,
	{		.
[]
K.
Clarkson.
Hul
l
:
a
program
for
con
v
ex
h
ulls.
h
ttp://cm.b
ell-labs.com/netlib/v
oronoi/h
ull.h
tml.
[]
B.
Curless
and
M.
Lev
o
y
.
A
v
olumetric
metho
d
for
building
complex
mo
dels
from
range
images.
Pr
o
c.
SIGGRAPH
'	
,
		,
0{.
[]
H.
Edelsbrunner.
Surface
reconstruction
b
y
wrapping
nite
sets
in
space.
T
ec
h.
Rept.
	-00,
Raindrop
Geomagic,
Inc.,
		.
[]
H.
Edelsbrunner,
D.G.
Kirkpatric
k,
and
R.
Seidel.
On
the
shap
e
of
a
set
of
p
oin
ts
in
the
plane.
IEEE
T
r
ans.
on
Information
The
ory
	
(	),
-	.
[]
H.
Edelsbrunner
and
E.
P
.
M

uc
k
e.
Three-dimensional
alpha
shap
es.
A
CM
T
r
ans.
Gr
aphics

(		)
{.
[]
H.
Edelsbrunner
and
N.
Shah.
T
riangulating
top
ological
spaces.
Pr
o
c.
0th
A
CM
Symp.
Computational
Ge
ometry
,
		,
{	.
[]
L.
H.
de
Figueiredo
and
J.
de
Miranda
Gomes.
Computational
morphology
of
curv
es.
Visual
Computer

(		)
0{.
[]
J.
Goldak,
X.
Y
u,
A.
Knigh
t,
and
L.
Dong.
Constructing
discrete
medial
axis
of
-D
ob
jects.
Int.
J.
Computational
Ge
ometry
and
its
Applic
ations

(		)
{	.



--- Page 24 ---

[	]
H.
Hopp
e.
Surfac
e
R
e
c
onstruction
fr
om
Unor
ganize
d
Points
.
Ph.D.
Thesis,
Computer
Science
and
Engineering,
U.
of
W
ashington,
		.
h
ttp://www.researc
h.microsoft.com
/researc
h/graphics/hopp
e/thesis/thesis.h
tml
[0]
H.
Hopp
e,
T.
DeRose,
T.
Duc
hamp,
J.
McDonald,
and
W.
Stuetzle.
Surface
recon-
struction
from
unorganized
p
oin
ts.
Pr
o
c.
SIGGRAPH
'	
,
		,
{.
[]
H.
Hopp
e,
T.
DeRose,
T.
Duc
hamp,
H.
Jin,
J.
McDonald,
and
W.
Stuetzle.
Piecewise
smo
oth
surface
reconstruction.
Pr
o
c.
SIGGRAPH
'	
,
		,
	{.
[]
D.
G.
Kirkpatric
k,
J.
D.
and
Radk
e.
A
framew
ork
for
computational
morphology
.
Computational
Ge
ometry,
G.
T
oussain
t,
ed.,
Elsevier,
pp.
-.
[]
S.
Levy
,
T.
Munzner,
and
M.
Phillips.
Geom
view.
h
ttp://www.geom.umn.edu/soft
w
are/do
wnload/
geom
view.h
tml
[]
S.
Lo
dha.
Scattered
Data
T
ec
hniques
for
Surfaces.
T
o
app
ear
in
Ge
ometry
Dete
ction,
Estimation
and
Synthesis
for
Scientic
Visualization
,
Academic
Press.
[]
S.
Mann,
C.
Lo
op,
M.
Lounsb
ery
,
D.
Mey
ers,
J.
P
ain
ter,
T.
DeRose,
and
K.
Sloan.
A
surv
ey
of
parametric
scattered
data
tting
using
triangular
in
terp
olan
ts.
Curve
and
Surfac
e
Design
,
H.
Hagen,
ed.,
SIAM,
		,
{.
[]
R.
C.
V
eltk
amp.
Close
d
obje
ct
b
oundaries
fr
om
sc
atter
e
d
p
oints.
LNCS
V
ol.
,
Springer,
		.

View publication stats
```

---

## Section 1.37: Surface_Reconstruction_from_Point_Clouds

Source File: Surface_Reconstruction_from_Point_Clouds.txt

### Paper Content:

```
# Surface_Reconstruction_from_Point_Clouds.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Surface_Reconstruction_from_Point_Clouds.pdf
# Output: ../layer2_completion/txt/Surface_Reconstruction_from_Point_Clouds.txt


--- Page 1 ---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221511837
Surface Reconstruction from Point Clouds
Conference Paper  in  IFIP Advances in Information and Communication Technology · January 1998
DOI: 10.1007/978-0-387-35392-0_18 · Source: DBLP
CITATIONS
10
READS
626
2 authors:
Bill Toll
Taylor University
3 PUBLICATIONS   13 CITATIONS   
SEE PROFILE
Fuhua Cheng
University of Kentucky
66 PUBLICATIONS   1,070 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Fuhua Cheng on 01 May 2014.
The user has requested enhancement of the downloaded file.


--- Page 2 ---

Chapter #
Surface Reconstruction from Point Clouds
Bill Toll (btoll@css.tayloru.edu)
Computing and System Sciences, Taylor University, Upland, IN
Fuhua (Frank) Cheng (cheng@cs.engr.uky.edu)
Computer Science, University of Kentucky, Lexington, KY
Key words:
surface reconstruction, point clouds, feature points, reverse engineering
Abstract:
Surface reconstruction from point clouds is an important element of reverse
engineering as used to create solid models of existing objects.  The research
presented in this paper assumes no knowledge of relationships among the
elements of the point cloud.  The approach to initial mesh creation is based on
extraction of points, called “feature points”, from the cloud based on geometric
relationships with nearby points.  A neighborhood is established for each data
point, the points in the neighborhood are ordered, and feature points are
identified based on relationships among the neighbors.  Feature points within a
neighborhood are linked to construct a polygonal mesh that is then
triangulated.
1. 
INTRODUCTION
Solid models of surfaces are desired for many purposes including input
of existing physical parts to CAD/CAM systems for modification, automatic
creation of machine tool paths, error inspection, and input to finite element
systems.  If such models do not exist, they may be created from point clouds
sampled from the surface by technologies such as laser scanners.
The creation of a model from a point cloud typically begins with the
reconstruction of the surface in the form of a piecewise linear mesh surface.
Although some situations allow the use of known properties of the data, the
abstract problem was first stated by Hoppe, et al1.  Their formulation views


--- Page 3 ---

the problem as the process of converting an input set of 3D points, sampled
from or near a surface, into a polygonal mesh.
One general approach to the problem creates a dense initial mesh that is
simplified by one of several techniques including application of an energy
function that measures deviation from the original (constructed) mesh2.
Another technique3 uses bounded approximation to construct a surface mesh
with each vertex within a prescribed distance from a vertex in the original
mesh.  The new mesh is constructed by merging faces of the original mesh.
Another technique4 merges faces by triangle decimation (iterative removal of
vertices).
Other approaches begin with an initial mesh and extract a surface
representation directly from the mesh.  The work of Hoppe’s group1, for
example, creates a mesh of all data points and extracts a surface from the
mesh by contour tracing a signed distance function to approximate tangent
planes.  The vertices of the resulting triangulation are not necessarily part of
the original data set, but may be created points.  This triangulation, however,
requires simplification as presented in following work by the same group
where the mesh is optimized2 and a piecewise smooth (tangent plane
continuous) surface is constructed by subdivision from the optimized
triangular mesh5.
An alternate approach6 is to construct a three-dimensional Delaunay
triangulation of volume elements, smooth the surface triangulation, and
compute smooth Bernstien-Bezier patches to approximate the triangulated
surface.  A very recent approach7 constructs an output mesh that interpolates,
rather than approximates, the input points by use of 3D Voronoi diagrams.
This technique functions best when it is possible to control the sampling
density over the surface.
2. 
FEATURE POINT BASED RECONSTRUCTION
The focus of this work is to prepare an initial mesh that is smaller than a
dense initial mesh, but is an accurate representation of the surface and at
least comparable to reduced meshes produced by other techniques.  The
mesh that is created can be a representation of the surface or the starting
point for a mesh optimization or surface fitting algorithm.  One of the
characteristics that differentiates the feature point approach is that it is based
on the identification of points in the original data set that represent the
surface topology and shape.  Therefore, all the points in the resulting mesh
are from the original data set and the surface, at this point in the process,
approximates the surface as a well-chosen subset.


--- Page 4 ---

Conceptually, the approach attempts to identify “feature curves” that are
natural edges of the surface.  These curves are constructed by connecting
feature points. The feature points are selected by a point-driven algorithm
without previous mesh construction.  The algorithm consists of pre-
processing, feature point extraction, and mesh generation.
3. 
ALGORITHM
The first step is pre-processing where the data is input, organized, and
partially analyzed to prepare for the remaining operations.  Pre-processing
includes data input, sorting, determination of a neighborhood for each point,
and computation of an approximate normal vector to the surface at each
point.  The second step is extracting feature points from the data.  This
process is accomplished by detecting areas where there is an obvious edge
based on the distribution of points in the neighborhood or areas where there
is significant variation in approximate surface normals within the
neighborhood.  The final step of mesh generation consists of extraction of
additional points as needed, generation of edges, generation of polygons
from the edges, and triangulation of the polygons.
3.1 
Neighborhoods
Since extraction of feature points from a data set is based on examination
of the local variation of the surface in the vicinity of the point, a
neighborhood is established for each point. A neighborhood of a point
consists of data points from the original data set which are “near” the given
point.
An ideal definition of neighborhood for the purpose of feature point
extraction would be that a neighborhood includes only those data points that
describe the surface at the given point but includes all points necessary for
measurement of the variation of the surface around the given point.
Due to the above considerations and the potentially varying density of
data points, a neighborhood is chosen for each point that includes all points
within a given distance of the point in combination with a limit on the
maximum number of points in a neighborhood.
3.2 
Normal Vectors
Normal vectors are defined in a statistical sense for each data point based
on the points in its neighborhood.  The method used to find the normal
vector is similar to that used by other surface reconstruction algorithms that


--- Page 5 ---

make use of the tangent plane as an approximation to the surface in the
neighborhood.  The normal is calculated by applying principle component
analysis on the covariance matrix of the neighbor points around the point
being investigated.  The normal vector is used to establish a tangent plane,
but this plane is used only for sorting neighboring points rather than surface
approximation.
Some surface reconstruction techniques1 replace the data point with the
centroid of the neighborhood and have the additional requirement that the
surface normals for all neighborhoods must be consistently oriented in that
they all point “out”.  This problem has been shown to be NP-complete8.  The
feature point technique uses the normal and tangent plane only for sorting
and, thus, does not require a globally consistent orientation.
3.3 
Feature Point Extraction
Feature points are extracted from the data set based on variation among
the points in a neighborhood.  Therefore, neighboring points must be
compared with each other.  There are two types of comparisons used, both of
which compare “adjacent” points.  Therefore, the concept of adjacent points
in a 3D neighborhood must be defined.  The approach used is to project the
points to the tangent plane for the neighborhood where the tangent plane is
defined to be perpendicular to the normal vector.  The projected points are
then sorted by angle in the plane so adjacency relationships can be
determined.  Figure 1 illustrates this process.
There are three basic types of feature points and three corresponding
extraction mechanisms.
Boundary feature points are data points that are selected because analysis
of the points in the neighborhood indicate that there is a boundary, or natural
edge, of the surface near the point.
Variation feature points are selected by a mechanism that indicates that
the surface changes shape near the point.  The extraction is based on the
amount of variation of approximate surface normals around the data point.
Finally, there may be regions of a surface where no "interesting" shape
transitions occur.  The surface is smooth and uniform, but not flat.  For such
surfaces, it is common that no boundary or variation feature points would be
extracted.  Some points need to be selected from this surface so that the
region is not represented as a flat region bounded by boundary and variation
feature points.  Such points are labeled complementary feature points.


--- Page 6 ---

Figure 1.  Data points and projection (a) data points, (b) neighborhood, (c) projection plane,
(d) points projected to plane, (e) projected points
3.4 
Boundary Feature Points
Boundary points are feature points that are identified, or extracted, from
the original data set, based on the property that points in its neighborhood
indicate that an edge of the actual surface is present in the neighborhood.
Furthermore, the identified point is, itself, a part of this edge.
Such points are identified based on the spatial distribution of projected
neighbor points. It is assumed that if a natural edge exists that passes through
the data point, then there will be points in the neighborhood that lie along the
natural edge.  If there is a line formed in the projection plane, that line is
assumed to indicate the presence of an edge of the surface.  Line in this
context means two adjacent neighbor points which when connected through
the prospective feature point form an approximately straight line.  Thus, if
there are adjacent points in the neighborhood which have an angle between
( a )
( b )
( c )
( d )
( e )


--- Page 7 ---

them greater than a given value, typically 160o, then the point is labeled a
feature point.  Figure 2 illustrates a boundary feature point determination.
Figure 2. Testing for boundary feature point
3.5 
Variation Feature Points
Variation feature points are extracted based on variation in surface
normals within a neighborhood around a point.  However, the normals
calculated for each point are not used.  Rather, a series of triangles are
formed by using the central point as one vertex of each triangle and using
two adjacent neighbor points as the other vertices for a given triangle.  The
surface normal for each triangle is calculated with the cross product of the
vectors from the central point to the two adjacent neighbors.  Note that the
3D neighbor points are used as the projected points all lie in the same plane.
Figure 3 illustrates this process by displaying the normal vectors for the
triangles in a neighborhood.
If there is sufficient variation between normals for adjacent triangles, the
central point is labeled a variation feature point.  While this technique will
identify some points that would have been previously extracted as boundary
feature points, boundary points from a flat or nearly flat surface would not
be detected with the variation approach.
Figure 3. Testing for variation feature point


--- Page 8 ---

3.6 
Complementary Feature Points
Regions of a surface where no boundary or variation feature points exist
may be flat and could be represented by large triangles in a mesh.  However,
since there is no prior knowledge of the surface, it is not known if the
surface is flat or is smoothly curved such that no variation feature points are
detected.  An example of such a surface is the side of a cylinder.
A point is labeled a complementary feature point if it is not a feature
point and there are no feature points in its neighborhood.
It is also important that all feature points have at least two feature points
in their neighborhood so that the mesh generated by connecting feature
points forms closed polygons.  If a feature point does not have at least two
feature point neighbors, additional points must be identified.  These added
points can also be viewed as complementary feature points.
3.7 
Mesh Generation
The mesh is created from edges formed by connecting feature points.  A
feature point should be connected to all feature points in its neighborhood.  It
is not necessary that all feature points in a common neighborhood are
connected to each other.  Indeed, if all such edges are formed, there will be
intersecting edges.  Before the edge data structure is built, intersecting edges
are disallowed by removing feature points from neighborhoods that cause
intersecting edges.  Additional complementary points may need to be labeled
so that closed polygons are formed.
Edges are generated by connecting each feature point to all feature points
remaining in its neighborhood.  The edges are traversed in each direction to
verify that closed polygons are formed and the polygons are triangulated to
form a triangular mesh.
As polygon edge E is traversed from point P to point Q, the next edge of
the polygon is the next edge counterclockwise in order around point Q.
However, if the normals for the neighborhoods are not consistent, the
definition of clockwise varies between the neighborhoods.  Therefore, the
dot product of the normal vectors is used to determine if the next edge
should be chosen in a clockwise direction.  If so, this direction is maintained
in the traversal of the given polygon until another such transition occurs.
4. 
RESULTS
While further investigation is needed to better understand the role of
various parameters of the algorithm and locally adaptive approaches should


--- Page 9 ---

be investigated for neighborhood determination, encouraging results have
been obtained with the technique.
The following figures illustrate the results of the algorithm on a
distributor cap data set found in Hoppe’s work1.  Figure 4 is the data set
consisting of 12,745 points.  Figures 5-8 illustrate the feature point
extraction process with Figure 5 displaying 594 boundary feature points,
Figure 6 displaying 3,675 feature points after the extraction of variation
feature points, Figure 7 displaying 4,101 feature points including
complementary points, and Figure 8 displaying the final feature point set
after edge extension consisting of 4,347 points.
Figure 9 is the final triangulation that contains 12,635 edges.
Figure 4. Distributor cap data set


--- Page 10 ---

Figure 5. Boundary feature points
Figure 6. Variation and boundary feature points


--- Page 11 ---

Figure 7. Complementary, variation, and boundary feature points
Figure 8. Final feature point set


--- Page 12 ---

Figure 9. Triangulation of feature points
REFERENCES
1 
Hoppe, Hugues, DeRose, Tony, Duchamp, Tom, McDonald, John and Stuetzle, Werner.
Surface Reconstruction from Unorganized Points. Computer Graphics (Proceedings of
Siggraph) 26(2), July 1992, 71-78.
2 
Hoppe, Hugues, DeRose, Tony, Duchamp, Tom, McDonald, John, and Stuetzle, Werner.
Mesh Optimization. Computer Graphics (Proceedings of Siggraph), 1993, 19-26.
3 
Kalvin, Alvin D. and Taylor, Russell H.. Superfaces: Polygonal Mesh Simplification
with Bounded Error. IEEE Computer Graphics and Applications, May 1996, 64-77.
4 
Schroeder, W. J., Zarge, J. A., and Lorensen, W. E.. Decimation of Triangle Meshes.
Computer Graphics (Proceedings of Siggraph), 26(2), July 1992, 65-70.
5 
Hoppe, Hugues, DeRose, Tony, Dechamp, Tom, Halstead, Mark, Jin, Hubert,
McDonald, John, Schweitzer, Jean, and Steutzle, Werner.  Piecewise Smooth Surface
Reconstruction, Computer Graphics (Proceedings of Siggraph), 1994, 295-302.
6 
Bajaj, Chandrajit L., Bernardini, Fausto, and Xu, Guoliang.  Automatic Reconstruction
of Surfaces and Scalar Fields from 3D Scans.  Computer Graphics (Proceedings of
Siggraph), 1995, 109-118.
7 
Amenta, Nina, Bern, Marshall, and Kamvysselis, Manolis.  A New Voronoi-Based
Surface Reconstruction Algorithm, Computer Graphics (Proceedings of Siggraph), 1998,
415-421.
8 
Garey, Michael R. and Johnson, David S.. Computers and Intractibility, W. H. Freeman
and Company, 1979.
View publication stats
```

---

## Section 1.38: Systematic_Comparison_of_Vectorization_Methods_in_

Source File: Systematic_Comparison_of_Vectorization_Methods_in_.txt

### Paper Content:

```
# Systematic_Comparison_of_Vectorization_Methods_in_.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Systematic_Comparison_of_Vectorization_Methods_in_.pdf
# Output: ../layer2_completion/txt/Systematic_Comparison_of_Vectorization_Methods_in_.txt


--- Page 1 ---



Citation: Krzeszewska, U.;
Poniszewska-Mara´nda, A.;
Ochelska-Mierzejewska, J. Systematic
Comparison of Vectorization
Methods in Classiﬁcation Context.
Appl. Sci. 2022, 12, 5119. https://
doi.org/10.3390/app12105119
Academic Editor: Valentino Santucci
Received: 18 April 2022
Accepted: 17 May 2022
Published: 19 May 2022
Publisher’s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright:
© 2022 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
applied  
sciences
Article
Systematic Comparison of Vectorization Methods in
Classiﬁcation Context
Urszula Krzeszewska
, Aneta Poniszewska-Mara´nda *
and Joanna Ochelska-Mierzejewska
Institute of Information Technology, Lodz University of Technology, 93-590 Lodz, Poland;
urszula.krzeszewska@dokt.p.lodz.pl (U.K.); joanna.ochelska-mierzejewska@p.lodz.pl (J.O.-M.)
* Correspondence: aneta.poniszewska-maranda@p.lodz.pl
Abstract: Natural language processing has been the subject of numerous studies in the last decade.
These have focused on the various stages of text processing, from text preparation to vectorization
to ﬁnal text comprehension. The goal of vector space modeling is to project words in a language
corpus into a vector space in such a way that words that are similar in meaning are close to each
other. Currently, there are two commonly used approaches to the topic of vectorization. The ﬁrst
focuses on creating word vectors taking into account the entire linguistic context, while the second
focuses on creating document vectors in the context of the linguistic corpus of the analyzed texts. The
paper presents the comparison of different existing text vectorization methods in natural language
processing, especially in Text Mining. The comparison of text vectorization methods is possible by
checking the accuracy of classiﬁcation; we used the methods NBC and k-NN, as they are some of
the simplest methods. They were used for the classiﬁcation in order to avoid the inﬂuence of the
choice of the method itself on the ﬁnal result. The conducted experiments provide a basis for further
research for better automatic text analysis.
Keywords: natural language processing; text vectorization; Continuous Bag of Words; skip-gram;
k-nearest neighbors; Naive Bayesian Classiﬁer
1. Introduction
Natural language processing has been the subject of numerous studies in the last
decade. These have focused on the various stages of text processing, from text preparation
to vectorization to ﬁnal text comprehension. The goal of vector space modeling is to project
words in a language corpus into a vector space in such a way that words that are similar
in meaning are close to each other. Currently, there are two approaches to the topic of
vectorization. The ﬁrst one focuses on placing words in the context of the language, while
the second one deals with full documents and the representation of single sentences and
paragraphs in the context of the documents under consideration.
Automatic text processing is a rapidly growing ﬁeld of science. The area of knowledge
related to this topic is mainly concerned with data mining, which is an extremely popular
and important ﬁeld of computer science these days. In many languages from around
the world, starting with English, research has already gone very far. Each stage of text
processing is analyzed in turn, starting from how the training data are selected to the choice
of the appropriate method for analyzing the text [1,2].
The classiﬁcation of written statements is one of the operations that allow us to talk
about intelligent text analysis, and at the same time, it is so simple that it is ideal as a marker
for the operation of other stages of text processing. Any written statement can be analyzed,
including casual speech, a message from an information service, as well as a scientiﬁc
publication. Each type of text poses different challenges for automatic text analysis tools.
This paper collects the most popular text vectorization methods from both approaches
to modeling the vector space of texts, explains the deep machine learning mechanisms
Appl. Sci. 2022, 12, 5119. https://doi.org/10.3390/app12105119
https://www.mdpi.com/journal/applsci


--- Page 2 ---

Appl. Sci. 2022, 12, 5119
2 of 14
behind them, and creates a summary of them based on a study of the classiﬁcation perfor-
mance of texts previously vectorized by the methods studied. In this paper, the main focus
of the analyses is on the ﬁnal task of the prepared text vectors, namely, to understand them.
Therefore, it was decided to use classiﬁcation as a determinant of the effectiveness of the
used methods. What is more, the simplest of the methods was used for classiﬁcation to
avoid the inﬂuence of the choice of the method itself on the ﬁnal result. The experiments
conducted provide a basis for further research for better automatic text analysis.
The main objective of this paper is to compare different existing text vectorization
methods in natural language processing. The comparison of text vectorization methods is
possible by checking the accuracy of classiﬁcation.
The paper is structured as follows: Section 2 focuses on selected technical issues related
to automatic text processing. Section 3 describes the dataset on which the experiments
were performed—it includes information such as set counts and text length histograms,
and describes the preparation of the data for further automated processing steps. Section 4
presents the results of the conducted research, while Section 5 compares these results with
the available literature.
2. Natural Language Processing in Artiﬁcial Intelligence
Natural language processing (NLP) is a ﬁeld of artiﬁcial intelligence, as well as lin-
guistics, designed to make computers understand statements or written words in natural
language used by humans [3]. It is commonly possible to encounter applications of NLP
within autocorrect of written text, correctness checking, including grammar as well as
spelling, automatic completion of a written word or sentence, spam ﬁlter in an email sys-
tem, chatbots that are part of a customer service system, linguistic summaries of statements,
or databases.
Due to the enormous potential that NLP brings, current work in this topic focuses
on the following topics [4]: natural language understanding at spoken and written levels,
preparing summaries, commentaries and summaries of texts, extracting information from
texts (e.g., search engines), conducting dialogue in natural language, automatic analysis of
the meaning and grammar of texts, foreign language teaching, and the translation of texts.
There are three basic steps in automatic text processing: (1) text preparation, (2) vec-
torization, and (3) ﬁnal processing, such as classiﬁcation.
Any type of text analysis requires the transformation of unstructured documents into
structured data. In machine learning, for supervised learning, it is necessary to create
feature vectors in a speciﬁed feature space. The process in which we store previously
prepared text in a speciﬁed vector space is called vectorization. Within natural language
processing, from the point of view of data structures, two main types of vectorization can
be distinguished:
•
mapping texts to vectors of strings—direct representation of words or sentences,
•
mapping texts to vectors of numbers—indirect representation of words or sentences.
The ﬁrst one is not used often because of the high computational complexity of
comparing words stored as strings. However, it has the advantage of being understandable
by humans, which makes it easier to catch potential errors in the algorithm or to perform
manual analysis of cases for which the algorithm fails.
The second one refers to all methods that allow words to be converted to numbers, in-
cluding language modeling and feature learning, referred to as word embedding. Mapping
to vectors of numbers is possible only after tokenization. It is one of the basic transforma-
tions of the text by dividing it into smaller parts, which will then be analyzed. These parts
are called tokens. In the case of text analysis, tokens are most often single words, but due to
the nature of the analysis performed, they can also be punctuation marks or sets of words.
Within the vectorization of text by mapping it to real numbers, speciﬁc methods
are known, such as: (1) Continuous Bag of Words (CBOW), (2) Skip-gram bag of words,
(3) Term Frequency–Inverse Document Frequency (TF-IDF), and (4) Distributed Memory of
Paragraph Vector (DM-PV) [5–9].


--- Page 3 ---

Appl. Sci. 2022, 12, 5119
3 of 14
Word2Vec is one of the largest and most widely used word vectorization methods in
natural language processing. This model was created by Google in 2013. It is a predictive
deep learning model based on computing and generating high-quality, distributed and
continuous, consistent vector representations of words to capture the contextual and
semantic similarity of words. These are typically unsupervised models that can take large
text corpora, create dictionaries for words, and generate consistent word embeddings in
vector space [5,6].
There are two different model architectures that can be used within Word2Vec to create
representations of embedded words [5,6]: Continuous Bag of Words (CBOW) Model and
Skip-gram Model.
The Continuous Bag of Words (CBOW) model learns to predict the target word using
all the words in its environment. The sum of context vectors is used to predict the target
word, which means that the target word vector is created based on the surrounding words.
The number of neighboring words considered is determined by the deﬁned size of the
window surrounding the target word.
The Skip-gram model, or Skip-gram bag of words, learns to predict a word based on
a neighboring word. That is, the algorithm creates a vector of a given word with respect
to its context. It is a symmetric algorithm in structure and operation with respect to the
CBOW algorithm.
The CBOW model predicts the target word according to its context, which is repre-
sented as a bag of the words contained in a ﬁxed size window around the target word. This
model takes all the words in the surrounding window and uses the sum of their vectors to
predict the target. On the other hand, the Skip-gram model learns to predict a target word
thanks to a nearby word and tries to predict the target using a random close-by word.
Term Frequency–Inverse Document Frequency (TF-IFD) is a statistical method to calcu-
late weights for individual words found in a text document. The TF-IFD weighting consists
of two basic terms. The ﬁrst calculates the normalized term frequency. Term frequency
measures how often a word (term) appears in a document. Because each document is
different in length, it is possible that a given word appears much more frequently in longer
texts than in shorter ones [10]. The second is the inverse document frequency. It measures
how important a term is globally. When calculating TF, all terms are considered equally
important. However, it is known that some terms may appear many times but have little
importance to the whole text [11,12].
The extension of the Word2Vec method is the distributed memory of paragraphs, often
referred to by its analogy as Doc2Vec. It follows the analogy that just as a ﬁxed-length
vector can represent a single word, a separate ﬁxed-length vector can represent an entire
paragraph [5,13,14]. The operation scheme of the basic DM-PV method is based on the
CBOW model with the difference that for a given text, the selected context window of size
n represents the n words drawn from the text, and in addition to these context words, the
entire document is also given as an input.
This model is almost identical to the model of the CBOW method—the learning
process is the same as in the case of the CBOW or Skip-gram algorithms—a simple neural
network with a single hidden layer is used. This is the only method in the list above that
takes the entire document (as a context), not just individual words from that document, to
create vectors of words that are then used to represent the text.
3. Methodology
The entire calculation was performed on a dataset derived from a news database on the
BBC website of the ﬁve most popular news categories from 2004 to 2005 [15]. This collection
contains 2225 documents, each of which was assigned to one of the categories: “business”,
“entertainment”, “politics”, “sports”, or “tech”. Each document is a separate (*.txt) text ﬁle
assigned to a directory structure that deﬁnes the category membership. Pre-processing of
these documents allowed us to extract 12,162 texts forming the basis for the calculations.
All of the extracted texts were used in the comparison. These data are much more than


--- Page 4 ---

Appl. Sci. 2022, 12, 5119
4 of 14
the text documents due to the fact that a single document contained several unrelated
messages separated by multiple newline characters. One example text document is shown
in Figure 1.
Figure 1. Example of the initial text ﬁle used for the calculation.
Data from all of the text ﬁles were compiled into a single *.csv ﬁle containing the
following columns (Figure 2):
•
no—ordinal number, allowing the unique identity of the text,
•
category—single category to which the text belongs,
•
text— basic data,
•
length—the length of the text (it was helpful in the collection of further statistics on
the texts, and also allowed the detection of initial errors in the script for dividing text
documents into individual texts).
Figure 2. Extract of *.csv ﬁle obtained from individual text ﬁles used for the calculation.
In order to better describe the structure of the texts for each category and for all texts,
the following measurements were made: number of texts, mean text length (in words),
median text length (in words), maximum text length (in words), and minimum text length
(in words).


--- Page 5 ---

Appl. Sci. 2022, 12, 5119
5 of 14
3.1. Aggregate Data Analysis
Summary statistics for the data in all the texts examined are shown in Table 1 and in
Figure 3.
Table 1. Summary statistics of the data in all of the studied texts by category.
All
Business
Entertainment
Politics
Sport
Tech
Number of texts
12,162
2624
2053
2538
2459
2488
Average length
70
64
62
73
69
81
Quarterly deviation
45.5
42
39
45.5
49.5
54
Median length
56
54
53
65
39
60
Maximum length
445
283
273
292
445
423
Minimum length
5
5
5
5
5
5
Figure 3. Summary statistics of data in studied texts—length of texts for all categories.
As can be seen on the histogram, the structure of text length is not homogeneous.
Short texts (up to 50 words) predominate, while there are very few texts longer than
200 words. The consequence of this structure of all texts is the faster processing of data in
the learning process. However, it may result in the shortest texts between categories to be
hardly distinguishable.
3.2. Data Analysis for Single Categories
The data from each category were then analyzed against each other. The basic statistics
by category are presented in Table 1. It shows that all categories have a similar number of
texts, about 2500 texts. The dataset for the “entertainment” category is the least numerous.
While the most numerous was for the “business” category. The average text length between
categories varies between 62 for the “entertainment” category and 81 for the “tech” category,
while the median is lower, ranging between 39 for the “sports” category and 65 for the
“politics” category. The longest of the texts belongs to the “sports” category, while the
shortest text for all categories is only 5 words.
All collections of texts by category have a similar structure. Most are short texts,
under 50 words, while very few are second texts, over 200 words. For each category, the
length distribution is similar to the summary distribution, and also the distributions are
similar between categories. This makes it possible to simply conclude that the length of
the texts does not affect the results because it is not a characteristic of any category of
the texts. It would be different if the training set took a category with only long texts
and another with only short texts. In that case, it could have a negative impact on the
results and the subsequent classiﬁcation of the texts, thus undermining the reliability of the
research conducted.


--- Page 6 ---

Appl. Sci. 2022, 12, 5119
6 of 14
3.3. Text Preparation
Performing the learning process on text data requires the appropriate prior preparation.
Each document is transformed in the same way so that it can be represented in vector form,
then placed in the appropriate analysis space. We refer to the basic operations that a text
document can undergo shallow text analysis and the following transformations, performed
in the following order:
1.
Removal of special characters—most electronic texts contain characters related to text
formatting, whose removal is important for optimal processing.
2.
Uniform size of letters—since all text processing algorithms are case-sensitive, it is
necessary to force all letters to be the same size.
3.
Remove punctuation and quotation marks—punctuation and quotation marks in sen-
tences play a communicative role. In automatic natural language analysis, it is
necessary to decide whether the value added by punctuation is large enough. Leaving
punctuation unmodiﬁed increases the dataset size to be processed and further com-
plicates the computation. In the case of classifying the topic of an utterance, it is not
so important. It is different when we analyze the emotional coloring of an utterance
(whether it is positive or negative). Then, the punctuation, especially emotional signs
(exclamation marks, question marks, ellipsis), may prove crucial for the interpretation
of the text.
4.
Remove possessive pronouns—they indicate belonging to someone or something. In the
framework of automatic text analysis, especially the classiﬁcation of the subject of the
text, the information about afﬁliation is not necessary, but as in the case of punctuation
marks, each case of automatic text analysis should be considered individually in terms
of the usefulness of this information.
5.
Lemmatization—is the process by which we replace words with their base form or
lemma. The use of lemmatization is important because the same word can take differ-
ent forms, each of which carries the same information, and unfortunately, the machine
treats them as completely different words. In the case of English, we lemmatize only
the verbs. In order to determine what part of speech a given word is and what its
lemma is, dictionary methods are most often used. They consist in storing in mem-
ory the entire dictionary for a given language. In practice, dictionary methods are
often imperfect and may not contain a complete dictionary due to the high dynamics
of language change; this may be especially important when analyzing informal or
youth texts.
6.
Removal of “stop words"—stop words are words that occur relatively often in the
studied set of documents and, at the same time, are irrelevant because they do not
carry information. For each language, there are commonly available sets of such words.
Removing them from the text allows the statistical models needed for classiﬁcation to
work correctly but may completely change the meaning of the text.
The step of removing the “stop words” in preprocessing the text can be as follows:
stop_words = list(stopwords.words(’english’))
df[’text_6’] = df[’text_5’]
for stop_word in stop_words:
regex_stopword = r"\b" + stop_word + r"\b"
df[’text_6’] = df[’text_6’].str.replace(regex_stopword, ’’)
The last step is to re-save the result of all performed operations to *.csv ﬁles—the ﬁrst
ﬁle contains the texts together with all data preprocessing steps, while the second contains
only category information, original text, and the result of all processing.
3.4. Vectorization
The implementation of vectorization methods to compare classiﬁcation results used
the gensim library of Python, which provides ready-made Word2Vec and Doc2Vec models.
In order to create a CBOW model, the Word2Vec method is used as follows:


--- Page 7 ---

Appl. Sci. 2022, 12, 5119
7 of 14
model = Word2Vec(documents, vector_size=100, window=5,
min_count=2, workers=8, sg=0)
The parameters successively accepted by the method that creates the model are:
•
document, which is the object containing the tagged texts,
•
vector_size, which is the size of the resulting vectors, in this case, as in the other
methods, it is 100,
•
window, which is the maximum distance between the current word and the predicted
word in the sentence, in this case, 5,
•
min_count, a number deciding which words will be ignored when creating vectors—if
a given word occurs less than the given number, it will not be taken into account
during vectorization,
•
workers, or the number of auxiliary threads used in training the model, this value
should be adjusted to the number of processors on which the learning process is
performed.
The unique parameter for the Word2Vec method is the sg parameter—it is the parame-
ter that determines whether the CBOW or Skip-gram method will be used. When it is 0, it
is CBOW, and when it is 1, it is Skip-gram.
The vectors created in this way for words should be converted into features of texts
allowing their fundamental classiﬁcation. The simplest way is to average the word vectors
for all words in the text. For this purpose, we built a transformer initialized with a word:
class MeanEmbeddingVectorizer(object):
def __init__(self, word2vec):
self.word2vec = word2vec
# if a text is empty we should return a vector
of zeros
# with the same dimensionality as all the other
vectors
self.dim = len(word2vec.itervalues().next())
def fit(self, X, y):
return self
def transform(self, X):
return np.array([
np.mean([self.word2vec[w] for w in words if w in
self.word2vec]
or [np.zeros(self.dim)], axis=0)
for words in X
])
A whole-text-based method, DM-PV, or Doc2Vec, was used for comparison. The
process of creating this model is analogous to the process of creating the Word2Vec model
as follows:
model = Doc2Vec(documents, vector_size=100, window=5,
min_count=2, workers=8)
Each model was learned in exactly the same way, for a predeﬁned number of epochs
max_epochs equal to 200.
3.5. Data Split
For adequate validation, it was necessary to split the dataset according to cross-
validation recommendations. A 10-fold cross validation was used. The script to split the
dataset into training and testing looks as follows:


--- Page 8 ---

Appl. Sci. 2022, 12, 5119
8 of 14
k = 10
cv = model_selection.StratifiedKFold(n_splits=k)
fold = 0
for train, test in cv.split(documents):
print "Fold %i (%i w TS, %i w VS)" % (fold, len(train),
len(test))
fold += 1
Initially, a constant k for k-fold cross-validation was declared, which is the target
number of splits. Then the cv split object is initialized with this parameter. Finally, the loop
iterates over the cv object and displays the sizes of the split sets: train and test.
3.6. Classiﬁcation Task
Two methods, the k-nearest neighbors and Naive Bayesian Classiﬁer, were used for
the classiﬁcation task. Both of these methods are very simple. This is very important
because classiﬁcation has been chosen as the determinant of the overall performance of a
text mining method. More than one method was chosen to see if the trends for the selected
vectorization methods were the same for different text classiﬁcation methods.
The methods used for classiﬁcation are from the sklearn library. The k-nearest neigh-
bour method, where n is equal to 5, is as follows:
#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier
#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=5)
The Naive Bayesian Classiﬁer (NBC) method is as follows:
#Import Naive Bayes Classifier
from sklearn.naive_bayes import GaussianNB
#Create KNN Classifier
nbc = GaussianNB()
4. Results of Experiments and Their Analysis
The realized experiments contained 10 tests. Each of them used one of ﬁve imple-
mented text vectorization methods in combination with one of the two text classiﬁcation
methods. This yielded the following test cases:
•
vectorization by the CBOW method with word vector averaging for texts, classiﬁcation
by the k-nearest neighbors method, denoted in the results as CBOW k-NN,
•
Skip-gram vectorization with word vector averaging for texts, classiﬁcation by the
k-nearest neighbors method, denoted in the results as Skip-gram k-NN,
•
CBOW vectorization with the tf-idf method, k-nearest neighbors classiﬁcation, denoted
in the results as CBOW tf-idf k-NN,
•
Skip-gram vectorization using the tf-idf method, k-nearest neighbors classiﬁcation,
denoted in the results as Skip-gram tf-idf k-NN,
•
DM-PV vectorization, k-nearest neighbor classiﬁcation, doc2vec k-NN,
•
CBOW vectorization with word vector averaging for texts, Naive Bayesian Classiﬁer
classiﬁcation, denoted as CBOW NBC in the results,
•
Skip-gram vectorization with word vector averaging for texts, Naive Bayesian Classi-
ﬁer classiﬁcation, denoted as Skip-gram NBC in the results,
•
CBOW vectorization with tf-idf, Naive Bayesian Classiﬁer, denoted as CBOW tf-idf
NBC in the results,
•
Skip-gram vectorization using the tf-idf method, Naive Bayesian Classiﬁer, denoted
in the results as Skip-gram tf-idf NBC,


--- Page 9 ---

Appl. Sci. 2022, 12, 5119
9 of 14
•
vectorization with the DM-PV method, Naïve Bayesian Classiﬁer, denoted as doc2vec
NBC in the results.
The results for the tests performed were compared under the classiﬁcation method.
The test results for the k-nearest neighbor method are shown in Tables 2 and 3 and Figure 4.
Table 2. Accuracy of k-NN classiﬁcation for different vectorization methods.
Accuracy
Epoch
CBOW k-NN
0.941
144
Skip-gram k-NN
0.926
152
CBOW tf-idf k-NN
0.945
137
Skip-gram tf-idf k-NN
0.931
166
Dov2vec k-NN
0.939
112
Table 3. Precision, recall, and F1 scores for all vectorization methods with k-NN classiﬁcation by
speciﬁc topics.
CBOW
SkipGram
CBOW TF-TDF
SkipGram TF-TDF
Doc2vec
Business precision
0.96
1
0.95
0.92
0.95
Business recall
0.94
0.93
0.95
0.93
0.94
Business F1
0.95
0.96
0.95
0.93
0.94
Entertainment precision
0.86
0.87
0.93
0.9
0.91
Entertainment recall
0.94
0.93
0.94
0.93
0.94
Entertainment F1
0.9
0.9
0.94
0.91
0.92
Politics precision
0.96
0.92
0.96
0.97
0.96
Politics recall
0.94
0.93
0.94
0.93
0.94
Politics F1
0.95
0.92
0.95
0.95
0.95
Sport precision
0.96
0.92
0.94
0.94
0.94
Sport recall
0.94
0.93
0.94
0.93
0.94
Sport F1
0.95
0.92
0.94
0.93
0.94
Tech precision
0.96
0.92
0.95
0.92
0.94
Tech recall
0.94
0.93
0.94
0.93
0.94
Tech F1
0.95
0.92
0.95
0.93
0.94
Figure 4. Accuracy of k-NN classiﬁcation for selected text vectorization methods by epoch.
The test results for the Naive Bayesian Classiﬁer are shown in Tables 4 and 5 and
Figure 5.


--- Page 10 ---

Appl. Sci. 2022, 12, 5119
10 of 14
Table 4. Accuracy of Naive Bayesian Classiﬁer classiﬁcation for different vectorization methods.
Accuracy
Epoch
CBOW NBC
0.951
153
Skip-gram NBC
0.947
157
CBOW tf-idf NBC
0.955
146
Skip-gram tf-idf k-NN
0.949
179
Dov2vec k-NN
0.950
125
Table 5. Precision, recall, and F1 scores for all vectorization methods with Naive Bayesian Classiﬁer
classiﬁcation by speciﬁc topics.
CBOW
SkipGram
CBOW TF-TDF
SkipGram TF-TDF
Doc2vec
Business precision
0.95
0.98
0.9
0.97
0.91
Business recall
0.97
0.91
0.9
0.9
0.9
Business F1
0.96
0.94
0.9
0.94
0.91
Entertainment precision
0.99
0.91
0.93
0.93
0.95
Entertainment recall
0.97
0.97
0.95
0.96
0.99
Entertainment F1
0.98
0.94
0.94
0.95
0.97
Politics precision
0.93
0.96
0.99
0.93
0.97
Politics recall
0.84
0.97
0.97
0.96
0.96
Politics F1
0.89
0.96
0.98
0.94
0.97
Sport precision
0.97
0.94
0.97
0.97
0.96
Sport recall
0.99
0.95
0.98
0.96
0.96
Sport F1
0.98
0.95
0.97
0.96
0.96
Tech precision
0.93
0.94
0.98
0.95
0.95
Tech recall
0.99
0.95
0.98
0.97
0.95
Tech F1
0.96
0.94
0.98
0.96
0.95
Figure 5. Accuracy of Naive Bayesian Classiﬁer classiﬁcation for selected text vectorization methods
by epoch.
As can be seen from the data obtained, none of the methods stood out in terms of
efﬁciency in the case studied. For the k-nearest neighbor method, the ﬁnal results are in
the range of [0.92; 0.95], while for the Naive Bayesian Classiﬁer, they are in the range of
[0.94; 0.96]. The results obtained using NBC show much smaller differences than the k-NN
method. At the same time, it should be noted that the classiﬁcation performance of this
method is better than the classiﬁcation via the much simpler k-NN method.
Regardless of the classiﬁcation method chosen, the Continuous Bag of Words method
with feature extraction using Term Frequency—Inverse Document Frequency performed
best. In the case of classiﬁcation with the k-NN method, an accuracy score of 0.945 was
obtained, while using the Naïve Bayesian Classiﬁer, it was 0.955.
However, for the studied case of document topic classiﬁcation, the lowest performance
results were obtained using the Skip-gram method when averaging the word vectors. The


--- Page 11 ---

Appl. Sci. 2022, 12, 5119
11 of 14
classiﬁcation of previously vectorized text with this method using k-NN yielded a score of
0.926 while using NBC yielded a score of 0.947.
The method that took the longest time for the obtained performance result to be invari-
ant across subsequent iterations was the Skip-gram method using tf-idf feature extraction.
For the k-NN method, it was 166 epochs, while for the Bayesian classiﬁer, it was as long as
179. This long time may be due to the complexity of the vectors obtained from this method.
Despite the fact that the model is actually a mirror image of the CBOW method, it ultimately
creates many more dependencies between words. These dependencies, which are often not
obvious, can have a major impact on the time required to obtain invariant performance.
On the other hand, the method that needed the least number of epochs to obtain an
invariant efﬁciency result is the Distributed Memory—Paragraphs Vectors method. It takes
only 112 epochs for the k-NN method and 125 epochs for the Naive Bayesian Classiﬁer.
This short time, compared to the Skip-gram tf-idf method mentioned earlier, may be due to
directly creating vectors based on whole texts rather than ﬁrst creating vectors of words
and then creating vectors of texts based on those vectors. This approach allows the machine
to detect relationships within whole texts earlier, which are not as obvious for word-based
methods. In addition, this method obtained performance results signiﬁcantly higher than
the other tested methods from the very beginning. Hence the relative value of the obtained
performance was lower with a similar absolute result.
Another aspect worth noting is the course of effectiveness values depending on the
epoch. Irrespective of the method of text classiﬁcation, the course of these functions was
similar. In the early epochs, the accuracy values increase faster, while already around the
100th epoch, the effectiveness differences in subsequent epochs are not so large. Interest-
ingly, the method with the worst performance also starts with the lowest score. However,
what the initial classiﬁcation score is is in no way an indicator of whether the method
will work. This can be seen in the CBOW method. In the initial epochs, the accuracy
score using this method is second-worst, while after getting its best accuracy score, it is
the second-best. Consequently, this means that it is impossible to unambiguously decide
whether the method will work in the case under study (cannot decide on the effectiveness
of the method by analyzing the initial results).
It is also worth emphasizing the fact that all methods of text vectorization gave a
similar course of the accuracy function in subsequent epochs regardless of the chosen
method of classiﬁcation.
5. Comparison with Results of Studies Available in the Literature
Exactly the same results are not available in the literature for all the methods addressed
in this paper. Therefore, the comparison made was focused on the works described
in [16–18].
The obtained results of classiﬁcation accuracy are consistent with those available in
the literature. The effectiveness of the methods tested in [16] ranges from 0.8 to 0.95. A
comparison of the obtained effectiveness for the analysis of news texts from the [16] and
our work is given in Table 6.
Table 6. Comparison of classiﬁcation performance results for CBOW and Skip-gram methods with
the work [16].
Effectiveness [16]
Epoch [16]
Effectiveness
Epoch
CBOW
0.9341
137
0.941
144
Skip-gram
0.9147
197
0.926
152
As can be seen, the efﬁciency value is practically the same. The number of epochs
needed to train the CBOW method is also similar. In the case of the Skip-gram method,
the number of epochs is even higher than the research conducted in this paper. This is an
additional conﬁrmation that the Skip-gram method, due to the creation of complex vectors,
needs more time to achieve satisfactory performance.


--- Page 12 ---

Appl. Sci. 2022, 12, 5119
12 of 14
In addition, the authors of [16] have included graphs showing the variation of accuracy
depending on the epoch. These results are similar to those obtained in our work, except
that in [16], as early as around epoch 50, the effectiveness is close to the ﬁnal effectiveness,
while in the case of our work, only for epoch 130, the results were close. This behavior may
be due to the choice of the test set for the methods. The more difﬁcult it is, the more time
will be needed to train the same effective method.
In contrast, the results in [17] are different. The summary includes a comparison of a
class of texts represented by many documents and by few documents. A comparison of
these data with the present work is shown in Table 7.
Table 7. Comparison of classiﬁcation performance results for CBOW, doc2vec and tf-idf methods
with the work [17].
Effectiveness [17]
Effectiveness
CBOW
0.941
0.815
doc2vec
0.939
0.310
tf-idf
0.931
0.842
The largest discrepancy is seen here for the doc2vec method. The authors of [17] point
out that the behavior of this method in the tests they performed is different from those
available in [13]. This result may be inﬂuenced by the authors’ use of Chinese language
texts in the context of public ﬁnancial documents for classiﬁcation.
The same methods in many publications have been tested on different datasets, and it
is the selection of the dataset that determines their effectiveness. For comparison, in [19],
CBOW was compared for several datasets and the respective effectiveness was 61.8% for
MR [19], 77.9% for SST [20], and 87.1% for IMDB [21]. For the same datasets, tests were also
conducted for the Skip-gram method; the corresponding MR was 63.5%, SST was 77.3%,
and IMDB was 89.1%. Which clearly allows us to conclude that it is the complexity of
the data to be created and then classiﬁed that determines the effectiveness of the method.
Hence, different methods for the same datasets have similar effectiveness in terms of values.
It is also worth noting that the effectiveness of the Skip-gram method within the study [18]
is higher than CBOW, contrary to what came out in this study.
6. Discussion and Conclusions
The result of our work is a summary of text vectorization methods in the context of
the obtained classiﬁcation performance for two word-based methods (Continuous Bag
of Words, Skip-gram) in two variants of feature extraction (by averaging and by the tf-
idf method) and for one method based on the whole document (Distributed Memory
Paragraph Vectors). This summary has been prepared for two very simple classiﬁcation
methods (k-nearest neighbor method and Naive Bayesian Classiﬁer). These results give
grounds to consider the choice of the text vectorization method as a factor inﬂuencing the
classiﬁcation performance.
The obtained results can easily be considered reliable. All experiments were performed
on a relatively large dataset (more than 12 thousand texts with an average length of about
70 words). This means that the size of the training set had no negative impact on the
performance of the vectorization methods. In addition, the variety of categories for which
it was possible to perform classiﬁcation within a single text type (web news content)
allowed for a good description of the relationship between the vectorization methods and
classiﬁcation performance.
It is also worth emphasizing the fact that the course of the effectiveness function in
successive epochs is independent of the classiﬁcation method. As was already mentioned,
this independence allows the choice of the method of text vectorization to be considered
as an important factor determining the accuracy of the classiﬁcation. Going further, it
allows us to hypothesize that the creation of a text vectorization method combining the
advantages of currently known methods or types of methods could further increase the


--- Page 13 ---

Appl. Sci. 2022, 12, 5119
13 of 14
classiﬁcation efﬁciency. Which, as a consequence, would mean an increase in the efﬁciency
and accuracy of the processing or understanding of texts by machines. The combination
of these issues, together with methods that analyze speech and create their transcriptions,
offers great opportunities for the development of AI.
Worth noting is the relationship between learning speed and the type of text vector-
ization method. Word-based methods are slower than those based on vectorization of
the entire text. At the same time, both types of vectorization methods produce similar
classiﬁcation performance results. It is worth considering a combination of word-based
methods with simultaneous global context (relative to all words in the language) and whole-
text-based methods with local context (only the texts on which the vectorization methods
have been trained). This combination could allow for even better results while optimizing
the number of learning epochs. Research is already underway on how text vectors should
be created to best allow them to be represented. One example of a new approach to text
vectorization is the use of an attention mechanism in the learning process [22].
Regarding some limitations of this study, among the most important are:
•
comparison of only word and sentence embeddings methods, future research should
also focus on other methods, such as those based on transformers architecture,
•
the use of classiﬁcation as a determinant of the effectiveness of the methods—there
are many other methods to test the effectiveness of the methods, for example, by
comparing word vectors or similarity between words, but in this study, classiﬁcation
was deliberately chosen,
•
using the classiﬁcation of only the topics of texts—instead, it is worth considering
options more difﬁcult than topic recognition, such as the classiﬁcation of sentiment or
emotion in texts,
•
only one dataset was used to test the methods—within topic recognition, it is possible
to conduct tests on many other datasets, including those whose structure is more
difﬁcult of texts are much longer,
•
testing only for English—in English the research on NLP issues is the most advanced.
Therefore, the results of the methods may be the best in this language, it is worth
analyzing the vectorization methods in comparison with other languages in the future,
also from other language groups, such as Arabic or Chinese.
To conclude, it can be emphasized that the current implementations of text vector-
ization methods achieve similar efﬁciency results regardless of the method chosen. All
methods achieved efﬁciencies above 90%, which, for the current natural language process-
ing applications, such as spam ﬁltering or detecting offensive comments on the Internet, is
more than sufﬁcient. Hence, further research and the optimization of methods used within
NLP should focus on the development of artiﬁcial intelligence and human imitation in the
understanding of written and spoken text.
The conducted research allows for further development of issues related to the creation
of vector spaces in natural language processing:
•
Classiﬁcation of texts not only in terms of the topic of speech but also other factors, such
as emotional color. Each type of classiﬁcation poses new challenges for vectorization
methods, such as ﬁnding other characteristics of the same texts classiﬁed ﬁrst in terms
of topic and then in terms of emotional color.
•
Analysis of texts other than Internet news. Statements from colloquial, ofﬁcial, and
scientiﬁc language are characterized by different phrases, often also by a different
structure of the utterance. At present, we do not know a method that would allow for
the simultaneous classiﬁcation of texts of different types. Hence, it is worth verifying
whether the use of existing methods to classify different types of texts at the same time
will be effective. Another option is to try to create a method that can cope with the
classiﬁcation of these kinds of texts.
Author Contributions: Conceptualization, U.K. and J.O.-M.; methodology, U.K. and J.O.-M.; valida-
tion, A.P.-M. and J.O.-M.; formal analysis, U.K., A.P.-M., and J.O.-M.; investigation, U.K.; resources,


--- Page 14 ---

Appl. Sci. 2022, 12, 5119
14 of 14
U.K.; data curation, U.K.; writing—original draft preparation, U.K., A.P.-M., and J.O.-M.; writing—
review and editing, A.P.-M.; visualization, A.P.-M. All authors have read and agreed to the published
version of the manuscript.
Funding: This research received no external funding.
Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
References
1.
Tixier, A.J.-P.; Hallowell, M.R.; Rajagopalan, B.; Bowman, D. Automated content analysis for construction safety: A natural
language processing system to extract precursors and outcomes from unstructured injury reports. Autom. Constr. 2016, 62, 45–56.
[CrossRef]
2.
Zou, Y.; Kiviniemi, A.; Jones, S.W. Retrieving similar cases for construction project risk management using Natural Language
Processing techniques. Autom. Constr. 2017, 80, 66–76. [CrossRef]
3.
Jain, A.; Kulkarni, G.; Shah, V. Natural Language Processing. Int. J. Comput. Sci. Eng. 2018, 6, 161–167. [CrossRef]
4.
Khurana, D.; Koli, A.; Khatter, K.; Singh, S. Natural Language Processing: State of The Art. Current Trends and Challenges. arXiv
2017, arXiv:1708.05148.
5.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.S.; Dean, J. Distributed Representations of Words and Phrases and their
Compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems, Lake Tahoe,
NV, USA, 5–10 December 2013; Volume 2, pp. 3111–3119.
6.
Mikolov, T.; Chen, K.; Corrado, G.; Dean, J. Efﬁcient Estimation of Word Representations in Vector Space.
arXiv 2013,
arXiv:1301.3781.
7.
Qun, L.; Weiran, X.; Jun, G. A Study on the CBOW Model’s Overﬁtting and Stability. In Proceedings of the International
Conference on Information and Knowledge Management, Shanghai, China, 3 November 2014; pp. 9–12. [CrossRef]
8.
Yan, S.; Shuming, S.; Jing, L.; Haisong, Z. Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word
Embeddings. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 2 (Short Papers); Association for Computational Linguistics: New Orleans, LA, USA, 2018;
pp. 175–180. [CrossRef]
9.
Salton, G.; Wong, A.; Yang, C.S. A vector space model for automatic indexing. Commun. ACM 1975, 18, 613–620. [CrossRef]
10.
Shahzad, Q.; Ramsha, A. Text Mining: Use of TF-IDF to Examine the Relevance of Words to Documents. Int. J. Comput. Appl.
2018, 181. [CrossRef]
11.
Stephen, R. Understanding Inverse Document Frequency: On Theoretical Arguments for IDF. J. Doc. 2004, 60, 503–520. [CrossRef]
12.
Havrlanta, L.; Kreinovich, V. A Simple Probabilistic Explanation of Term Frequency-InverseDocument Frequency (tf-idf ) Heuristic
(and Variations Motivatedby This Explanation). Int. J. Gen. Syst. 2015, 46, 27–36. [CrossRef]
13.
Le, Q.V.; Mikolov, T. Distributed Representations of Sentences and Documents. arXiv 2014, arXiv:1405.4053.
14.
Douzi, S.; Amar, M.; El Ouahidi, B.; Laanaya, H. Towards A new Spam Filter Based on PV-DM (Paragraph Vector-Distributed
Memory Approach). Procedia Comput. Sci. 2017, 110, 486–491. [CrossRef]
15.
Green, D.; Cunningham, P. Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering. In
Proceedings of the ICML 2006, Pittsburgh, PA, USA, 25–29 June 2006.
16.
Jang, B.; Kim, I.; Kim, J. Word2vec convolutional neural networks for classiﬁcation of news articles and tweets. PLoS ONE 2019,
14, e0220976. [CrossRef]
17.
Wang, Y.; Zhou, Z.; Jin, S.; Liu, D.; Lu, M. Comparisons and Selections of Features and Classiﬁers for Short Text Classiﬁcation. Iop
Conf. Ser. Mater. Sci. Eng. 2017, 261, 012018. [CrossRef]
18.
Lei, X.; Cai, Y.; Xu, J.; Ren, D.; Li, Q.; Leung, H.-F. Incorporating Task-Oriented Representation in Text Classiﬁcation. Available
online: https://openreview.net/forum?id=LYknk8R-Bht (accessed on 2 October 2021).
19.
Database for Sentiment Analysis. Available online: https://www.cs.jhu.edu/mdredze/datasets/sentiment/unprocessed.tar.gz
(accessed on 2 November 2021).
20.
Movie review data for Sentiment Analysis. Available online: https://www.cs.cornell.edu/people/pabo/movie-review-data/
(accessed on 3 November 2021).
21.
Deeply Moving: Deep Learning for Sentiment Analysis. Available online: http://nlp.stanford.edu/sentiment (accessed on
2 November 2021).
22.
Linden, J.; Forsstrom, S.; Zhang, T. Evaluating Combinations of Classiﬁcation Algorithms and Paragraph Vectors for News Article
Classiﬁcation. In Proceedings of the 2018 Federated Conference on Computer Science and Information Systems, Pozna´n, Poland,
9–12 September 2018; pp. 489–495. [CrossRef]
```

---

## Section 1.39: The O(N) Vector Model

Source File: The O(N) Vector Model.txt

### Paper Content:

```
# The O(N) Vector Model.pdf
# Converted: 2025-07-19 12:45:28
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/The O(N) Vector Model.pdf
# Output: ../layer2_completion/txt/The O(N) Vector Model.txt


--- Page 1 ---

The O(N) Vector Model
paul.romatschke@colorado.edu
Fall 2020
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
1 / 13


--- Page 2 ---

Review
In lecture 22, we discussed the QFT of a complex scalar ﬁeld φ
Separating φ into real and imaginary parts φ =
1
√
2 (φ1 + iφ2) we
found
Z =
Z
Dφ1Dφ2e−SE [φ1,φ2] ,
(26.1)
for the QFT partition function
In this lecture, we will consider an N-component scalar ﬁeld
⃗φ =




φ1
φ2
. . .
φN




(26.2)
as a generalization of the complex scalar ﬁeld
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
2 / 13


--- Page 3 ---

The Action for the Complex Scalar Field
For a single complex scalar ﬁeld, we had in (22.4) the Euclidean action
SE =
Z
s
h
∂aφ∂aφ∗+ m2φφ∗+ 4λ (φφ∗)2i
.
(26.3)
We found (26.3) has an additional symmetry: it is invariant under the
transformation
φ(x) →eiαφ(x) ,
 φ∗(x) →e−iαφ∗(x)

,
(26.4)
with arbitrary (but constant) α
This is called a U(1) transformation, for a unitary 1x1 matrix
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
3 / 13


--- Page 4 ---

The Action for the Complex Scalar Field
In components φ =
1
√
2 (φ1 + iφ2) we had in (22.7):
SE =
Z
x
1
2∂aφ1∂aφ1 + 1
2∂aφ2∂aφ2 + m2
2 φ2
1 + m2
2 φ2
2 + λ
 φ2
1 + φ2
2
2

.
(26.5)
The U(1) symmetry (26.4) now becomes
 φ1(x)
φ2(x)

=

cos α
sin α
−sin α
cos α
  φ1(x)
φ2(x)

,
(26.6)
where we can think of φ1, φ2 as the coordinates of a vector in a plane
With this interpretation, (26.6) is the rotation of the vector in the
plane, also called an SO(2) transformation, for a special (unit
determinant) orthogonal 2x2 matrix
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
4 / 13


--- Page 5 ---

The O(N) Vector Model
Let us now consider a generalization of the 2-component vector
 φ1(x)
φ2(x)

to a vector with N scalar ﬁeld components
⃗φ =




φ1(x)
φ2(x)
. . .
φN(x)



.
(26.7)
By analogy with the complex scalar ﬁeld, we expect the Euclidean
action to be invariant under an SO(N) symmetry (in addition to the
usual Lorentz invariance)
In 3+1 dimensions, one such action that generalizes (26.5) is
SE =
Z
x
1
2∂a⃗φ · ∂a⃗φ + m2
2
⃗φ · ⃗φ + 2λ
N

⃗φ · ⃗φ
2
.
(26.8)
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
5 / 13


--- Page 6 ---

The O(N) Vector Model
The QFT that arises from the classical action (26.8) is called the
O(N)-vector model
The partition function for the O(N) vector model is given by
Z =
Z
D⃗φe−SE .
(26.9)
For N=2, the O(N) vector model partition function is identical to Z
for the complex scalar ﬁeld
For N=1, the O(N) vector model partition function is identical to Z
for the real scalar ﬁeld, with double the coupling constant
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
6 / 13


--- Page 7 ---

Solving the O(N) Vector Model
We can treat the interaction term λ

⃗φ · ⃗φ
2
in perturbation theory
just like for the real scalar ﬁeld
However, we have a huge advantage in the O(N) model over a real
scalar ﬁeld QFT: we can solve the theory exactly in the limit N ≫1
This is a rare case where one does not need perturbation theory to
study a QFT
I’ll be covering the basics here, but advanced students may ﬁnd the
following reference useful: https://arxiv.org/pdf/1905.09290.pdf
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
7 / 13


--- Page 8 ---

Solving the O(N) Vector Model
To solve the O(N) model in the large N limit, ﬁrst insert unity in the
path integral for the partition function:
Z =
Z
D⃗φe−SE × 1 .
(26.10)
Next, write unity as a (path-) integral over a δ function
1 =
Z
Dσδ
 
σ −
⃗φ · ⃗φ
N
!
(26.11)
Use the δ function to replace the quartic term in the action by σ2
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
8 / 13


--- Page 9 ---

Solving the O(N) Vector Model
Next, write the δ function in integral representation as
δ
 
σ −
⃗φ · ⃗φ
N
!
=
Z
Dζei
R
x ζ

σ−
⃗φ·⃗φ
N

(26.12)
We get for Z:
Z =
Z
D⃗φDσDζe−1
2
R
x ⃗φ[−∂2
a+m2+ 2iζ
N ]⃗φ−2λN
R
x σ2+i
R
x ζσ .
(26.13)
The path integral over σ is Gaussian, we can integrate out σ to ﬁnd
Z =
Z
D⃗φDζe−1
2
R
x ⃗φ[−∂2
a+m2+ 2iζ
N ]⃗φ−
1
8λN
R
x ζ2 .
(26.14)
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
9 / 13


--- Page 10 ---

Solving the O(N) Vector Model
Letting ζ →N × ζ gives
Z =
Z
D⃗φDζe−1
2
R
x ⃗φ[−∂2
a+m2+2iζ]⃗φ−N
8λ
R
x ζ2 .
(26.15)
Separating ζ now into a “mean-ﬁeld” part and ﬂuctuations
ζ(x) = ¯ζ + ζ′(x) as in lecture 19 gives
Z =
Z
d ¯ζ
Z
D⃗φDζ′e−1
2
R
x ⃗φ[−∂2
a+m2+2i(¯ζ+ζ′)]⃗φ−NβV
8λ ¯ζ2−N
8λ
R
x ζ′2 .
(26.16)
So far everything is exact for all N
Now let’s consider the limit N →∞
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
10 / 13


--- Page 11 ---

Solving the O(N) Vector Model
For N →∞, the path integral over ζ′ gives a contribution of order
eln N to Z
But the mean-ﬁeld term is eN ≫eln N in the large N limit
So in the large N limit, neglecting the path integral over ζ′ becomes
exact and we get
lim
N≫1 Z =
Z
d ¯ζ
Z
D⃗φe−1
2
R
x ⃗φ[−∂2
a+m2+2i ¯ζ]⃗φ−NβV
8λ ¯ζ2 .
(26.17)
The remaining path integral over the O(N) vector ﬁeld ⃗φ is Gaussian,
and is given by N-copies of the real scalar ﬁeld partition function,
lim
N≫1 Z =
Z
d ¯ζeN ln Zfree(T,√
m2+2i ¯ζ)−NβV
8λ ¯ζ2 ,
(26.18)
where the “mass” of the real scalar ﬁeld is
p
m2 + 2i ¯ζ
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
11 / 13


--- Page 12 ---

Solving the O(N) Vector Model
The remaining integral over ¯ζ can be evaluated from the saddle point
of the integral
For N →∞, the saddle point approximation is not an approximation,
but becomes exact
Denoting the position of the saddle as ¯ζ = ˜ζ, we have
lim
N≫1 Z = eN ln Zfree(T,√
m2+2i ˜ζ)−NβV
8λ ˜ζ2 .
(26.19)
Using the thermodynamic relation p = ln Z
βV this can be written as
lim
N≫1 Z = e
NβV

pfree(T,√
m2+2i ˜ζ)−
˜ζ2
8λ

.
(26.20)
or
p(T, m, λ) = N
"
pfree(T,
q
m2 + 2i ˜ζ) −
˜ζ2
8λ
#
.
(26.21)
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
12 / 13


--- Page 13 ---

Solving the O(N) Vector Model
The exact result for the QFT pressure of the O(N) model depends
on the coupling explicitly as well a implicitly through the saddle point
condition
∂
∂˜ζ
pfree(T,
q
m2 + 2i ˜ζ) −
˜ζ
4λ = 0 .
(26.22)
The free pressure for a single scalar ﬁeld in 3+1 dimensions is
divergent – we will discuss nonperturbative renormalization of the
theory in the next lecture
We will discuss how to evaluate the solution (26.21) in face of the
condition (26.22) in the next lectures
paul.romatschke@colorado.edu
Lecture 26
Fall 2020
13 / 13
```

---

## Section 1.40: Topology-Aware Surface Reconstruction for Point Clouds

Source File: Topology-Aware Surface Reconstruction for Point Clouds.txt

### Paper Content:

```
# Topology-Aware Surface Reconstruction for Point Clouds.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Topology-Aware Surface Reconstruction for Point Clouds.pdf
# Output: ../layer2_completion/txt/Topology-Aware Surface Reconstruction for Point Clouds.txt


--- Page 1 ---

Eurographics Symposium on Geometry Processing 2020
Q. Huang and A. Jacobson
(Guest Editors)
Volume 39 (2020), Number 5
Topology-Aware Surface Reconstruction for Point Clouds
Rickard Brüel-Gabrielsson1,2 Vignesh Ganapathi-Subramanian1 Primoz Skraba3,4 Leonidas J. Guibas1
1Stanford University 2Unbox AI 3Queen Mary University of London 4Jožef Stefan Institute
rbg@cs.stanford.edu, vigansub@stanford.edu, p.skraba@qmul.ac.uk, guibas@cs.stanford.edu
Figure 1: Reconstruction of example point clouds from the McGill dataset [SZM*08]. First row: Input Point cloud. Second row: Our
reconstruction of the shape in the ﬁrst row. Third row: Poisson surface reconstruction [KBH06] of the shape in the ﬁrst row. Shapes showcased
(left to right) are two chairs, cup, octopus, snake, dolphin, teddy and table respectively.
Abstract
We present an approach to incorporate topological priors in the reconstruction of a surface from a point scan. We base the
reconstruction on basis functions which are optimized to provide a good ﬁt to the point scan while satisfying predeﬁned topo-
logical constraints. We optimize the parameters of a model to obtain a likelihood function over the reconstruction domain.
The topological constraints are captured by persistence diagrams which are incorporated within the optimization algorithm to
promote the correct topology. The result is a novel topology-aware technique which can (i) weed out topological noise from
point scans, and (ii) capture certain nuanced properties of the underlying shape which could otherwise be lost while performing
surface reconstruction. We show results reconstructing shapes with multiple potential topologies, compare to other classical
surface construction techniques, and show the completion of real scan data.
CCS Concepts
• Theory of computation →Computational geometry; • Computing methodologies →Shape modeling; • Mathematics of
computing →Algebraic topology;
1. Introduction
Shapes are characterized by a number of markers that are repre-
sentational and explain their different properties. These could be
geometric, structural, topological, functional, or stylistic. These
properties add to our understanding of shape collections and can
be instrumental in solving important problems in 3D vision, ge-
ometry processing, and computer graphics. Applications include
shape alignment, shape correspondences, surface reconstruction,
and shape synthesis. Of these different considerations, the topolog-
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John
Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.
arXiv:1811.12543v3  [cs.CG]  15 Sep 2021


--- Page 2 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
ical invariance of shapes is an oft-observed trait across collections
that is seldom explicitly exploited. It is an important property of
many collections, especially those where the shapes have common
structure. This phenomenon can be observed in human body parts.
For example, the topology of a healthy human heart is always the
same, whether it comes from a child or an adult. One of the rea-
sons this has not been explicitly addressed is that most shape pro-
cessing problems, in particular surface reconstruction techniques,
involve choices that are both continuous and combinatorial. Con-
tinuous choices involve parameter regression, where a certain set
of parameters can be regressed from within a continuous set. This
could include learning chair leg lengths or the radius of a spheri-
cal object. The more computationally difﬁcult choices are combi-
natorial or discrete and often involve making decisions about shape
classiﬁcations; topological information is of this type. For example,
this appears in regression of shape grammars, where an example of
a combinatorial choice is that of having an armrest or not. Combi-
natorial choices are extremely challenging to make and prove to be
a bottleneck in many shape processing tasks.
To make topological information more amenable to optimiza-
tion, we use persistent homology. This is a tool that provides topo-
logical markers, called persistence diagrams, that capture these
combinatorial choices through a continuous proxy, describing the
topology of point clouds over multiple scales. Surface reconstruc-
tion from a point scan usually involves being oblivious to the scale
of the point scan which can lead to topologically incorrect recon-
structions. We present a technique to extract a surface from a point
scan while preserving the predeﬁned topology of the shape. Since
persistence-based tools range across different scales, we manage to
ﬁlter out a reliable reconstruction of the point scan which respects
the requisite topology.
Most surface reconstruction techniques manage topology in a
post-processing step, such as removing spurious components in the
reconstruction or ensuring that an extracted surface is watertight.
However, topological information can aid in completing sparse
point scans reliably, as well as resolve topological ambiguities, es-
pecially in the case of non-intersecting close regions on the sur-
face of the shape. We highlight the advantages of topology-aware
shape completion with multiple examples in Section 8. Our tech-
nique constructs likelihood functions that take high values on points
that likely lie on or near the surface. These functions are informed
by backpropagation from the persistence diagrams of candidate sur-
faces, aiding to obtain one of requisite topology. These diagrams act
as an intermediary between continuous scale information around
the points of the scan and the combinatorial nature of topology. A
summary of our contributions is:
• A novel topology-aware likelihood function, optimized on a
point cloud, and based on persistent homology measures of any
dimension.
• An automated surface reconstruction algorithm that may pre-
serve or even create the requisite topology.
• A new measure of topological ﬁdelity for reconstruction along
with a comparison of our approach with existing methods.
2. Related Work
Reconstructing surfaces from point clouds is a difﬁcult problem
which has received extensive attention. We ﬁrst describe related
work in surface reconstruction, followed by an overview of related
topological techniques.
2.1. Surface Reconstruction from Point Clouds
The problem of surface reconstruction has multiple facets and
many interesting sub-problems, we direct the reader to the survey
by Berger et. al. [BTS*14] for further reading. Surface reconstruc-
tion from point clouds appears in numerous scenarios including ur-
ban reconstruction [MWA*13] and completing partial surfaces or
point scans [ACK13]. The specﬁc problem we consider is interpo-
lating a point cloud to generate a surface [HDD*92; GG07; Dey06].
Most surface reconstruction techniques model the surface to be
characterized by the zero-level set of a function that is deﬁned
over space [KBH06; CBC*01; SAAY06]. These works approxi-
mate the signed Euclidean distance from the underlying surface,
and essentially interpolate values between the input points, extract-
ing the underlying surface as a zero set of this interpolated distance
ﬁeld. Since there is no additional information in this sort of tech-
nique, there is nothing to prevent points far from the surface to have
extremely low values. Therefore, many techniques use an inside-
outside points approach where points inside the surface and outside
it are provided so as to guide the function sign at various regions in
space. Moreover, while this is an extremely elegant representation
of a shape, it is also restrictive, with the optimization step including
multiple equality constraints. The work by Poranne at al. [PGK10]
takes a minor detour from this, where they do not necessarily force
the surface value to be zero. Instead, they obtain the surface by
applying an algorithm based on the watershed transform [RM00]
which extracts the low-level set of the function values as the sur-
face. In this work, we build a likelihood function where points on
the surface are given high values, with no value constraints. This
way, the surface values can develop organically. A simple surface
reconstruction technique from existing function values can then be
used to obtain the actual surface.
Topological control in surface reconstruction has been per-
formed by user interaction [SLS*07; YHZ*], ﬁxed template
optimization [BP05; GSDP*18; ZSCP08], removing topologi-
cal errors from an existing surface [JZH07; WHDS04], and
optimization-based surface reconstruction [SLS*06; HZCJ17;
ZCL14; LDK*18]. In this work, we approach the topology-guided
reconstruction along the lines of the optimization-based technique,
locating parameters that maximize the function values at surface
points, while preserving the topology, using ideas from persistent
homology as discussed below.
2.2. Persistent Homology
Persistent homology, or simply persistence, is a well-established
tool in applied and computational topology. Topological simpliﬁ-
cation based on function optimization is present in some of the ear-
liest work on persistence [ELZ00]. More generally, it been used
as a tool to satisfy topological criteria in a variety of geometry pro-
cessing applications including shape matching [CZCG05], optimal
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 3 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
pose-matching [DLL*10], and shape segmentation [SOCG10]. The
problem of topological simpliﬁcation of shapes while preserving
persistent features in given data has traditionally been performed
in the context of function denoising [AGH*09; BLW12]. The in-
clusion of topological information in optimization has appeared in
the work of Gameiro et al. [GHO16] where the authors attempt to
perform point cloud continuation for dynamical systems based ap-
plications. Recent work by Poulenard et al. [PSO18] use the idea
of function optimization for the purpose of shape matching. The
work in this paper mirrors that of Poulenard et al. by performing
function optimization on basis coefﬁcients, which are used as the
building blocks of a given function, which in our case is repre-
sentative of the shape surface. This general approach has received
a large amount of attention recently including theoretical analy-
sis [LOT19] as well as numerous applications [BGND*20]. We re-
fer the reader to [LOT19] for a recent overview of applications.
Overview: The rest of the paper is arranged as follows: we begin
with a discussion of classical surface reconstruction in Section 3,
followed by the required topological preliminaries in Section 4. The
two ideas are combined to perform topology-aware surface recon-
struction. In Section 5, we discuss the optimization to ﬁnd locally
optimal parameters to a topology-aware likelihood function. Once
the likelihood function has been computed, we extract a surface us-
ing the technique described in Section 6. Section 8 showcases the
technique in practical example cases and evaluates it against other
state-of-the-art reconstruction techniques. Section 9 concludes the
paper with a brief discussion on future work.
3. Implicit Surface Representation
Using level sets of functions to represent surfaces reconstructed
from point scans is a well-established technique
[KBH06;
CBC*01]. At a high level, previous techniques optimize the func-
tion so that the level set and hence the extracted surface closely
approximate the point cloud. We perform the optimization while
promoting the correct topology of the extracted surface.
Classical surface reconstruction techniques use different ap-
proaches to build a function to ﬁt a surface to a point scan. Poisson
reconstruction constructs a function which evaluates to 0 on the
surface. It requires normal information to label points as inside and
outside the surface, assigning +1 to points outside the surface and
−1 inside the surface. This can be used to then reduce the problem
to solving the Poisson equation [KBH06]. Alternatively, radial ba-
sis functions can be used, again evaluating to 0 on the surface and a
small value at nearby points not on the surface [CBC*01]. To avoid
confusion, we note that in our approach, we attempt to build the
surface to be higher-valued as opposed to lower-valued as in the
above two techniques. We construct multivariate Gaussians cen-
tered around the points in the scan, and then minimize a topology-
aware loss function using stochastic gradient descent to optimize
the covariance matrices of the Gaussians. We then use a topology-
aware surface reconstruction technique to obtain the surface. More
generally, we consider basis functions ϕp that are deﬁned for every
point in the point cloud, p ∈P. Each function is parameterized by a
vector αp. The likelihood function of the surface including a point
Figure 2: An example of a persistence diagram for the sum of three
Gaussians in 2D. (a) Heat map of the function, and (b) The cor-
responding persistence diagram. The critical points of the height
function p, q, and s are local maxima, r, t, and u are saddles, and v
is a local minimum. Each of the local maxima creates a component
(the red points on the right represent 0D homology classes where
the heights are the birth times). Saddles r and t merge components
and correspond to the death times of two of the red points. The sad-
dle u creates a ring (1D homology class), which is closed when the
minimum at v is reached, corresponding to the black point in (b).
x ∈Rd is a linear combination of these basis functions:
f(x,αP) = ∑
p∈P
ϕp(x,αp)
(1)
where αP refers to the collection of parameters αp for all points
p ∈P. The choice of basis functions ϕp are a design choice, and
are ideally functions whose maxima are at the point p around which
they are centered.
As mentioned above, we use multivariate Gaussians as our basis
functions. A Gaussian centered at a point µ is given by
G(x;µ,Σ) =
q
(2π)−3|Σ|−1exp(−(x−µ)T Σ−1(x−µ))
where Σ is a symmetric covariance matrix. We deﬁne the Gaus-
sian basis-function around point p of P to be ϕp(x;αp) =
G(x; p,αTpαp). The covariance matrix is a symmetric positive def-
inite matrix and hence can be decomposed as M = AT A for some
matrix A. The singular value decomposition (SVD) of M is given
by M = UΛUT , deﬁning A = UΛ1/2UT . If Λ = diag(λ1,λ2,λ3),
then Λ1/2 = diag(
p
λ1,
p
λ2,
p
λ3). We deﬁne our parameters to
be the symmetric square roots of the covariance matrix, i.e. αp = A.
The parameters of the likelihood functions, that is the covariance
matrices, are initialized such that the derivative of the Gaussian
is maximized at the average distance between points. To improve
computational efﬁciency we only evaluate the likelihood function
on a grid, which we denote by X.
In the rest of the paper, we use the notation p ∈P to denote
points in the point cloud and x ∈X to denote the grid points where
the function is evaluated. The resolution of the grid ensures non-
trivial distances between points in the point cloud and the grid
points. Rather than use a uniform grid, we enlarge X by treating
the likelihood function as a probability distribution over a much
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 4 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
ﬁner grid from which we sample additional points. This allows us
to have a higher grid-resolution around the points P and so capture
more detail without signiﬁcantly increasing computational costs.
In the following sections, we build on the framework from
[PSO18] to derive a backpropogation function to optimize the like-
lihood function with respect to topological constraints.
4. Topological Preliminaries
Our main tool to promote certain topological structures is persistent
homology. Here we review the relevant topological notions which
will be used in later sections. For readers who are unfamiliar with
persistence we provide some intuition; however, for a more com-
plete introduction to the underlying theory, we refer the reader to
[EH10]. We focus on our speciﬁc setting, although many of the
techniques readily generalize.
The idea of the likelihood function, f(·,αP), is that it should
evaluate to larger values close to surface. The super-level set
f −1([t,∞),αP) should therefore contain the surface with the cor-
rect topology for some known t. Hence our goal is to optimize f
so that super-level sets of f both ﬁt the points and have the correct
topology.
To study the topology of a space, we ﬁrst construct a combi-
natorial representation of that space. This is done by building a
triangulation of the space in the form of an embedded simplicial
complex K where the vertices are the points X. Recall that a k-
dimensional simplex is the convex combination of k + 1 vertices
(or equivalently, k+1 points in X). We restrict ourselves to R2 and
R3, and so we only need to consider vertices, edges, triangles, and
tetrahedra. As previously mentioned, for computational reasons we
only compute f on a set of points x ∈X. We initialize the vertices
of K as the points of X and construct a Delaunay triangulation to
obtain the higher dimensional simplices. The function f on X can
be extended to K in a piecewise-linear fashion. In the following,
we do not distinguish between the PL-approximation and the true
function, but we address this at the end of this section.
To describe the topology, we use homology, which captures cer-
tain aspects of connectedness of a space. For completeness, we de-
ﬁne homology as we refer to it at the end of this section. For a
simplicial complex K, we can consider the vector spaces gener-
ated by the k-simplices, with one vector space per dimension, de-
noted by Ck(K), i.e. the chain groups constructed with ﬁeld coefﬁ-
cients. One can deﬁne a boundary operator, which is a linear map
∂k : Ck(K) →Ck−1(K) such that ∂k ◦∂k+1 = 0. The k-dimensional
homology is deﬁned as
Hk(K) = ker ∂k
im ∂k+1
where elements of ker ∂k are called k-cycles and elements of
im ∂k+1 are called k-boundaries. Two cycles are homologous if their
difference can be written as a linear combination of boundaries, and
the set of all cycles that are homologous to a given cycle is called a
homology class. The rank of the k-th homology counts the number
of k-dimensional features, i.e. 0-dimensional features are connected
components, 1-dimensional features are holes, and 2-dimensional
features are voids. We cannot optimize directly for the correct ho-
mology as the rank or number of holes is a discrete quantity. Rather,
we use persistent homology which tracks how homological features
appear and disappear over a ﬁltration, i.e. a sequence of spaces re-
lated by inclusion. In our setting, we consider the ﬁltration induced
by the super-level sets of the likelihood function f : K →R. Deﬁn-
ing,
Kα = {σ ∈K | ∀v ∈σ, f(v) ≥α}
This deﬁnes a ﬁltration since Ka ⊆Kb for all a ≥b. Surprisingly,
the homology of ﬁltrations can be fully described by the appearance
and disappearance of features, called births and deaths. The set of
the pairs of births and deaths are called a persistence diagram. We
omit the formal algebraic deﬁnition, but for our purposes we can
deﬁne a persistence diagram as a map from a space and a function
to a set of points in R2.
PDf (k) : (K, f) →{bi,di}i∈Ik
where PD f (k) refers to the births and deaths of k-dimensional ho-
mological features, and we drop subscript f when the function f
is clear from the context. We deﬁne PD f = ∪kPDf (k). We often
refer to the collection of points as the persistence diagram, with the
implication that K and f are ﬁxed; see Figures 2 and 3 for ex-
amples in 1D and 2D (where PD(0) and PD(1) are superimposed)
respectively.
Crucially, it is possible to deﬁne meaningful distances between
diagrams. This makes it possible to talk about how far a given space
is from the desired topology, which in turn allows for the optimiza-
tion of topology over persistence diagrams. There is a large litera-
ture on distances between diagrams and their respective properties
[CSEH07; CSEHM10; Bub15]. However, we do not use these dis-
tances directly; instead we introduce speciﬁc cost functions used in
Sections 5 and 8.
A key ingredient is the existence of an inverse map from the
points of the persistence diagram back to K
π′
f : {bi,di}i∈Ik →(σ,τ)
where σ,τ ∈K. This map is formally deﬁned and used in [PSO18;
LOT19]. Intuitively, this map can be understood algorithmically.
For each homological feature, there is one simplex which creates it
and one simplex which bounds it (or kills it); π′
f is simply this cor-
respondence. In the setting of a super-level set ﬁltration, the func-
tion value of a simplex is given by a vertex value:
f(σ) = min
v∈σ f(v)
Hence, we can reﬁne the inverse map to
π f : {bi,di}i∈Ik →(xb,i,xd,i)
where xb,i,xd,i ∈X are the extremal vertices in corresponding sim-
plices, see Figure 3.
The ﬁnal concept we deﬁne is the cycle representative of a ho-
mology class which is our initial candidate for the reconstructed
surface. For simplicity, we assume homology with Z2 coefﬁcients
since we use the support of the cycle, i.e. all simplices with non-
zero coefﬁcients. A cycle representative of a homology class is a
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 5 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
Figure 3: A one dimensional example of a persistence diagram
and the inverse map π. The function on the left has critical points
at points p, r, and q. The local maxima create components in the
super-level sets and so represent birth times (y-axis), while the min-
imum kills one of the components (the younger one) and so is a
death time (x-axis). The inverse map for a point in the diagram re-
turns the corresponding critical points/simplices.
cycle which represents the entire homology class. This is precisely
equivalent to choosing a basis element for a vector space.
In computing persistent homology, most algorithms compute a
cycle basis†. This cycle is canonical under some genericity as-
sumptions as it equivalent to a minimum weight basis [STY17].
In practice, the cycle is stable to small perturbations although it is
not difﬁcult to construct cases where it is not. We refer this cycle as
the Persistent Cycle Representative (PCR) of a point in the persis-
tence diagram. This cycle most often serves as our initial estimate
of the surface (see Section 6). This works well when the desired
surface is a manifold and so there is a unique top-dimensional ho-
mology class which captures the surface. When there are multiple
top-dimensional classes, as in the wedge of two circles as in Fig-
ure 7, we return the union of the PCRs corresponding to the top-
dimensional classes. In cases where there is no top dimensional
class as in Figure 12, we output the super-level set itself as a thick-
ened surface – see Section 6 for a further details.
We conclude this section with the following remarks. For com-
putational efﬁciency, we use a PL-approximation of the likelihood
rather than the true function. For this approximation, there is an
equivalence between super-level set and upper-star ﬁltrations which
we use implicitly when deﬁning the inverse map. Furthermore, us-
ing the stability of persistence diagrams, it is possible to rigorously
bound the error in the persistence diagrams introduced by this ap-
proximation in terms of the chosen grid spacing. We omit this as the
derivation and proof would introduce additional technicalities with-
out adding much substance to our results, as there are no stability
results for the inverse maps and cycle representatives. We note the
work of [EFR13], which showed that there do exist speciﬁc conﬁg-
urations where the error could be non-negligible, but that these do
not occur in practice. Finally, we do not discuss the algorithms for
computing persistence diagrams as it has been extensively studied,
with a number of efﬁcient implementations available. We direct the
reader to [OPT*17] for a recent survey.
† Some algorithms only output the barcode or use cohomology, however
the standard algorithm [EH10] returns a cycle bases.
5. Topology-aware Optimization
Our optimization procedure assumes that we have prior informa-
tion about the topology of our surface. Determining the topology
from a point cloud is a related but separate problem, which we do
not address here. We build on the framework from [PSO18] to opti-
mize the parameters of the likelihood function αP. Speciﬁcally, we
derive a backpropogation function to compute the gradient with re-
spect to a topological prior. In this section, we consider the parame-
terized function f(αP) : K →R with the associated persistence di-
agram PDf(αP) and inverse mapping πf(αP) : (bi,di) →(xb,i,xd.i)
where x ∈X.
We deﬁne a topological prior as a functional on the space of
diagrams:
E(PDf ) : {(bi,di)}I →R
(2)
In general, the functional can take in persistence diagrams for all
dimensions but we often restrict to a single ﬁxed dimension.
From this point on, we assume that the index Ik = {1,...,Nk}
is sorted by decreasing persistence. In other words, |di −bi| >
|dj −bj| for i < j. We also note that Ik is not static throughout the
optimization process but recomputed for every new changed func-
tion f. For a reconstruction, a topological prior is expressed via
Betti numbers. That is, how many components we would like the
reconstruction to have, how many holes, or voids. In principle, we
could include higher dimensional information, but here we focus on
surfaces in R3. Hence, if we would like the reconstruction to have
ℓk-dimensional features, the functional we use emphasizes the ℓ-
most persistent k-dimensional features (and de-emphasizes the less
peristent features, i.e. tries to send them to the diagonal) in the re-
construction is:
E(PDf(αP)(k)) = −((dℓ−bℓ)2 −(dℓ+1 −bℓ+1)2),
(3)
Or in other words, it maximizes the difference between the lifetime
of the ℓ-th most persistent feature and the (ℓ+1)-th most persistent
feature. This does not guarantee convergence to a "reasonable" so-
lution, i.e. we have no guarantees about the resulting features, but
in practice it converges to what one would expect.
There are many factors to consider when creating topological
priors and we describe precise functionals in Section 8. Here we
derive the general formula, assuming the functional is of the form
given in Equation 2. Using the chain rule,
∂E
∂αP
= ∑
i∈I
∂E
∂bi
∂bi
∂αP
+ ∂E
∂di
∂di
∂αP
= ∑
i∈I
∂E
∂bi
∂f(xb,i)
∂αP
+ ∂E
∂di
∂f(xd,i)
∂αP
(4)
where we use πf(αP)(bi,di) = (xb,i,xd,i). In our setting,
f(x,αP) = ∑
p∈P
G(x; p,αT
pαp),
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 6 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
Figure 4: Steps of the Surface Reconstruction algorithm. (a) Sim-
plicial complex (b) Generator/Proposed surface (c) Enumerating
intersecting cofaces (d) 1D surface output
and hence
∂f(x,αP)
∂αP
= ∑
p∈P
f(x,αp)

−1
2

(αT
pαp)−1
−(αT
pαp)−1(x−p)(x−p)T (αT
pαp)−1 ∂αTpαp
∂αp
!
where
∂αTpαp
∂αp
= αT
pJij +J jiαp such that (Jij)kl = δikδjl
This can be evaluated at xb,i and xd,i and substituted into Equa-
tion 4. It remains to compute the derivative of the functional to
complete the formula. For example given the functional in Equa-
tion 3, we obtain
∂E
∂αP
= ∑
p∈P
(αT
pαp)−1 D(αp, p,xb,ℓ,xd,ℓ,bℓ,dℓ)
−D(αp, p,xb,ℓ+1,xd,ℓ+1,bℓ+1,dℓ+1)

(αT
pαp)−1 ∂αTpαp
∂αp
with
D(αp, p,xb,i,xd,i,bi,di) = −(di −bi)2(αT
pαp)
+
 (xd,i −p)(xd,i −p)T di −(xb,i −p)(xb,i −p)T bi

(di −bi)
This was implemented as a PyTorch module making experimen-
tation with different cost functions straightforward. As the back-
propagation is separated for the diagrams, the pipeline only re-
quires an implementation of the derivative of the functional. Note
that the cost function above is an illustrative example and the dif-
ferentiable functions used to generate the examples are given in
Section 8.
6. Surface Reconstruction
The optimization in the previous section produces a likelihood
function and a super-level set with the desired topology. In the ideal
case, this super-level set is highly concentrated around the surface,
but a surface with the correct topology must still be extracted. There
are multiple ways to extract a 1D or 2D surface. One could apply
the watershed algorithm [PGK10] to the inverse of the likelihood
function, but then we lose control of the topology of the result. A
further complication is that multiple super-level sets might need to
be considered to ﬂexibly locate the desired topology.
From a topological perspective, a natural candidate for the sur-
face is a cycle representative of the desired homology class. In
practice we also consider the representative of the top dimen-
sion we consider (i..e 1-dimensional homology for curves and 2-
dimensional homology for surfaces). As described in Section 4, we
can associate a special representative, namely a Persistent Cycle
Representative (PCR), with each point in the diagram. This is a k-
dimensional cycle, which in our setting is either a 1-cycle (loop)
or 2-cycle (sphere) and is part of the super-level set of the sim-
plicial complex, Kα. The following assumes the optimization was
successful and that the PCR has the correct topology. For example
in R3, we only consider 2-cycles such that the 2-cycles have the
appropriate lower dimensional homology. If this is not the case, we
declare that the optimization has failed to converge to a good local
minimum.
The PCR, while topologically correct, is generally not the ideal
geometric representation of the surface, e.g. it can be highly non-
smooth and/or non-manifold. We therefore iteratively improve the
PCR while ensuring that the correct topology is maintained. This
is done through a local optimality function. We use the number of
simplices as a proxy for smoothness, trying to minimize the number
of simplices in the cycle while maintaining topology. This is essen-
tially improvement via simple homotopy, a sequence of “moves"
which do not change the topology.
Consider the 1D case, where we start with a 1-cycle PCR g. For
each edge in g, we consider its adjacent triangles which lie in the
superlevel set. There are two possible moves as shown Figure 5.
Say we are considering e = (a,b) and the triangle (a,b,c). If nei-
ther (b,c) nor (a,c) are part of g, the triangle is added, replacing
(a,b) by the pair (b,c) and (a,c) if c has a higher function value
than both a and b, i.e. f(a) < max(f(b), f(c)). As the function is
a proxy for a likelihood function, this has the effect of steering the
path through a higher likelihood region. Alternatively, if (a,c) is
also part of g, the triangle addition essentially removes a from the
path, which only occurs if f(a) < max(f(b), f(c)), again pushing
the path into a higher likelihood region. The procedure is illustrated
in Figure 4. At each step, we check that this does not cause any
self-intersections and the algorithm terminates when no modiﬁca-
tion can be made. The algorithm works in any dimension and by
construction the changes in the candidate surface do not change the
topology. It also extends naturally to 2D surfaces, where we con-
sider tetrahedra adjacent to triangles rather than triangles adjacent
to edges.
As mentioned, in Section 4, if there is no suitable representa-
tive (due to the requested topology), we output the simplicial com-
plex associated with the appropriate super-level set with the cor-
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 7 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
a
b
c
a
b
c
a
b
c
a
b
c
f(c) > max(f(a), f(b))
f(a) < max(f(b), f(c))
Figure 5: The two moves for modifying the candidate surface in
1D. (left) Only a and b are part of the surface. Vertex c has a higher
function value than a or b and so is added to the surface by replac-
ing (ab) by (ac). (right) a has a lower function value than b or c
and so is removed by replacing edges (ac) and (ab) by (bc).
rect topology. The choice of the super-level set can be determined
automatically from the persistence diagram. While it may occur
that there is no super-level set with the appropriate topology, which
again can be determined from the persistence diagram, we say the
algorithm has failed – see Section 7. In most cases, if the opti-
mzation converged, a suitable super-level set could be found. We
conclude that extracting a true surface in these cases is left for fu-
ture work.
7. Choice of Parameters and Failure Cases
The parameters of the likelihood functions are initialized such that
the derivative of the Gaussian is maximized at the average distance
between points. This is done to make the basis functions as ex-
pressive as possible and empirically proved to be a good balance
between a granular and a smoothed-out perspective. Other initial-
izations that favored extreme and unexpected topologies did cause
failure cases. Furthermore, a weak point can be the fact that the map
from the points in the persistent diagrams to the complex, is not sta-
ble. In practice, this was not an issue. This could be explained by
our desired topologies not being contingent on single simplices but
instead on a larger set of simplices, making the ambiguity in the
correspondences irrelevant. Lastly, large step sizes for gradient de-
scent made the optimization process unstable, as expected, but it
was straightforward to ﬁnd a step size that worked for all our prob-
lems.
8. Results
Our topology-aware surface reconstruction is novel in its ﬂexibil-
ity in computing different topological reconstructions. Our tech-
nique also accepts topological information of any dimension. In
this section, we discuss multiple results that validate our approach
and compare it to other state-of-the-art surface reconstruction tech-
niques.
We remind the reader that in describing the cost functions, we
use the convention that the points in the persistence diagram are
sorted in decreasing lifetime, i.e. |di −bi| ≥|dj −bj| for i < j.
8.1. Topological Flexibility
Our technique can produce any topology that exists at any ﬁltra-
tion value after the optimization process has ﬁnished. Thus, while
it cannot always guarantee a certain topology, in practice and when
the topology is not too far from those that are present in the origi-
nal point cloud, our technique can be expected to produce it. This
is true even if that exact topology is not present for any ﬁltration
value in the persistent homology of the original point cloud.
To illustrate this ﬂexibility, we show reconstructions from the
same point cloud with different prescribed topologies. In Figure 6,
we see that a spider can be reconstructed to have one void or two.
We used cost functions in PD(2), i.e. the persistence diagram for
2-dimensional features. To reconstruct one void we used cost func-
tion:
−((d1 −b1)2 −(d2 −b2)2)
effectively maximizing the gap between the most persistent and the
second most persistent homology classes. For two voids, we used
the cost function:
−((d2 −b2)2 −(d3 −b3)2)
maximizing the gap between the second and third most persistent
classes. Most surface reconstruction techniques provide one major
void, while the other is ﬂattened out, while in our case, we can force
this second void by means of predeﬁning the requisite topology.
This topological ﬂexibility can be further exempliﬁed in the ring
as shown in Figure 7. This is a two-dimensional case, with a point
cloud as shown in Figure 7(a). The sampling makes it ambiguous
whether the source shape contains one hole or two; but if this infor-
mation is available, then our technique can be utilized to reconstruct
the ring accordingly as in Figure 7(b) and (c) respectively. Most
surface reconstruction techniques use tangent and normal informa-
tion of points to reconstruct the underlying surface from them. This
means that the reconstruction with one hole would almost always
be produced. Specifying the underlying topology beforehand can
therefore be very useful.
The steering wheel in Figure 8 provides another example. Here,
different topologies (3,2 or 1 rings) are used to reconstruct an input
point cloud, and the reconstructions are vastly different from each
other. It is observed that for each of the different topologies, the
technique progressively closes the less persistent ring. To achieve
this, we used cost functions of the form
−((dk −bk)2 −(dk+1 −bk+1)2)
for k = 1,2,3 taken over PD(1). By equipping the technique with
a measure of topological preservation, it is able to obtain recon-
structions that are seemingly very far off from the original point
cloud or surface as in Figure 8(a) and (b), but respect topological
requirements.
8.2. Shape Completion
Shape completion is an important problem that we tackle from a
topological perspective. We perform shape completion in both 2-
dimensional and 3-dimensional examples. We show how an incom-
plete face in Figure 9 is completed by means of our technique with
the cost function: −((d1 −b1)2) over PD(1). Here, the topology,
i.e. one loop, of the face is provided as the only input apart from
the input point cloud, and the technique performs the completion
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 8 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
Figure 6: Reconstructing spider model with multiple possible
topologies. (a) Input spider point cloud. (b) Creating 1 void inside
the spider. (c) Creating 2 voids inside the spider. Cross sections in
red.
Figure 7: Reconstructing dual circle with multiple possible topolo-
gies. (a) The 2-D dual circle point cloud. (b) Reconstruction of dual
circle as two adjacent circles. (c) Reconstruction of merged dual
circle with one hole.
accordingly. It is also seen here that using the reconstruction tech-
nique described in Poranne et al. [PGK10] captures the high level
structure of the shape but introduces a lot of topological noise.
Another situation where our technique proves useful is when
sparse sampling introduces topological ambiguity. In Figure 10(a),
there are too few points in the input point cloud. By reconstructing
the surface with a single 2-dimensional hole, it forces the recon-
struction to produce a consistent, bunny-like reconstruction, Fig-
ure 10(b), using the cost function: −((d1 −b1)2 −(d2 −b2)2) taken
over PD(2). This is not necessarily the case when other reconstruc-
tion techniques are used. For example, the Poisson surface recon-
struction of the sparse Stanford bunny can be seen in Figure 10(c).
The sparsity of the point cloud creates three major concentrations
of points in the shape, therefore producing a topologically incorrect
reconstruction.
Figure 8: Reconstruction of steering wheel with multiple ring
topologies. (a) 1 ring, (b) 2 rings, and (c) 3 rings
Figure 9: (a) 2-D incomplete head point cloud (b) Completing
the point cloud with topological information (1-hole) (c) Using the
generalized distance based watershed algorithm as in Poranne et
al. [PGK10] to complete the point cloud.
Figure 10: Densifying the Stanford bunny. (a) A sparse point cloud
sampled from the Stanford bunny. (b) Reconstruction of this sam-
pling, preserving only the essential 2-dim hole (c) Poisson recon-
struction of the sparse Stanford bunny.
8.3. Medical Data
One of the central motivations for topology-aware surfaces is the
potential application to medical data. As described in Section 1, the
topology of medical data is almost always predeﬁned and invariant
across numerous instances. This is a property that can be utilized,
along with other medical priors, to reconstruct various medical or-
gans from scan data.
Here we show examples of how one could use our technique
to reconstruct scans of different medical organs. In Figure 11, we
show the reconstruction of the brain from the scan data provided in
Lerma-Usabiaga et al. [LUCPA18]. We used the cost function:
−((d2 −b2)2 −(d3 −b3)2)
over PD(2). In this case, even though the input point cloud is
dense, it is fairly complex to reconstruct. While our technique re-
constructs the brain fairly close to the input point cloud, the more
classical Poisson surface reconstruction [KBH06] fails to obtain
a topologically correct reconstruction, producing a single three-
dimensional void, while the correct topology would consist of two
three-dimensional voids, one for each hemisphere of the brain.
We also attempted to reconstruct synthetic point clouds of human
hearts. The reconstruction in Figure 12 was obtained using the cost
function:
−((d3 −b3)2 −(d4 −b4)2)
over PD(1). While our method reconstructs the four openings in the
human heart, Poisson reconstruction smooths the holes, changing
the topology of the reconstructed surface.
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 9 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
Figure 11: Reconstruction of the human brain as seen from the top-
view. (a) A point cloud of an example brain from Lerma-Usabiaga
et al. [LUCPA18]. (b) Reconstruction of this brain, preserving two
central voids. (c) Poisson Reconstruction of the brain. Cross sec-
tions in red.
Figure 12: Reconstruction of the human heart model. (a) A point
cloud of a human heart model. (b) Our reconstruction with four
openings; the red markings here indicate the four holes in the re-
construction. (c) Poisson reconstruction.
8.4. Comparisons to other Surface Reconstruction techniques
We compare our topology-aware technique to two major surface
reconstruction techniques. For 2D surface reconstruction, we com-
pare our technique to that of Poranne et al. [PGK10], and for 3D
surface reconstruction, we compare primarily to Poisson surface
reconstruction [KBH06] as implemented in MeshLab. These qual-
itative comparisons can be seen in Figures 1 and 9. While the Pois-
son reconstruction is very good on smooth water-tight surfaces, it
tends to fail for more natural cases where the inherent topology is
non-manifold. This is observed in the non-manifold examples of
Figure 1 such as chair, cup, octopus, and table examples, where the
Poisson reconstruction attempts to obtain a water-tight surface and
fails both topologically and in reconstruction quality. We perform
our 2D quantitative comparison experiments on the 2D point clouds
used in Poranne et al. [PGK10], speciﬁcally the hand, spiral, helix,
face, circle and blob datasets, and the 3D comparisons on 8 cate-
gories of the McGill segmentation benchmark dataset [SZM*08].
We develop two different metrics to compare with exist-
ing algorithms. We compare against Poisson surface reconstruc-
Method - 2D
N = 1000
N = 500
N = 200
Ours (k=0)
0
0
0
Watershed [PGK10]
0.25
0.33
0.81
Ours (k=1)
0
0
0
Watershed
2.25
3.08
3.63
Method - 3D
N = 1000
N = 500
N = 200
Ours (k=0)
0
0
0
Watershed [PGK10]
0.25
0.375
0.375
Poisson [KBH06]
0.75
1.125
0.25
Ours (k=1)
0
0
0
Watershed
1.625
2
2.125
Poisson
0.875
0.875
0.875
Ours (k=2)
0
0
0
Watershed
0.375
0.375
0.375
Poisson
0.5
0.5
0.625
Table 1: TFI0,TFI1 (and TFI2) for 2D (and 3D) surface recon-
struction comparing our technique to the generalized-distance Wa-
tershed algorithm by Poranne et al.[PGK10] (and the Poisson sur-
face reconstruction by Kazhdan et al.[KBH06] respectively) for
point clouds of size N points. The lowest values are captured in
bold.
Method - 2D
N = 1000
N = 500
N = 200
Ours
1.0040
1.0026
0.9979
Watershed [PGK10]
1.0363
1.0479
1.0723
Method - 3D
N = 1000
N = 500
N = 200
Ours
1.9170
1.9143
1.9127
Watershed [PGK10]
1.9867
1.9828
1.9800
Poisson [KBH06]
2.0348
2.0309
2.0117
Table 2: One-way Chamfer distance for 2D (and 3D) surface re-
construction comparing our technique to the generalized-distance
Watershed algorithm [PGK10] (and the Poisson surface recon-
struction [KBH06] respectively) for point clouds of size N points.
The lowest values are captured in bold.
tion [KBH06] for 3D reconstruction and Generalized distance
based reconstruction [PGK10] for both 2D and 3D reconstruc-
tion. Since our technique aims at topological accuracy, we compare
against the topological accuracy of other techniques. This is done
by computing the average number of erroneous k-dimensional com-
ponents: TFIk = 1
N ∑N
i=1 |ni,k −nrecon
i,k
|, where N refers to the num-
ber of shapes. We refer to TFIk as the k−dimensional Topological
Fidelity Index. Here, if k = 1, then ni,1 is the number of true 1-D
holes in shape i, nrecons
i,k
refers to the number of 1-D holes in the
reconstruction of shape i. We present this for multiple scan sizes of
our input point cloud. This is presented in Table 1. Our topological
ﬂexibility ensures a TFI of 0 for our technique, while a bigger TFI
value for other techniques is a measure of how far from ideal their
reconstruction capabilities are topologically. The high TFI values
for both competing methods shows the value of our method that
provides topological ﬂexibility.
While the TFI captures topological ﬁdelity, another important
factor is the quality of reconstruction. To capture this, we compute
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 10 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
the one-way Chamfer distance between a point cloud and its recon-
struction. This score is normalized for all shapes in a collection and
averaged over the collection. The comparison of these scores across
the corresponding techniques for 2D and 3D is provided in Table 2.
Here we show that our technique, in addition to high topological
ﬁdelity, also obtains high quality surface reconstruction.
9. Conclusion and Future Work
In this work we introduce a technique that performs surface re-
construction from point scans while optimizing the topology of the
surface. The results show that the optimization converges to the de-
sired topology and yields accurate reconstructions. There are many
potential future directions for improvement. For example, while
our choice of basis and cost functions are intuitive and work well
in practice, there are many other possibilities which could be ex-
plored. While we do not have guarantees on the reconstruction, if
the functional does not converge or converges to a large value, we
can detect the failure. Likewise, if the extracted generator does not
have the correct topology or is far from the points, we can again de-
tect failure. As our algorithm is based on gradient descent we can
retry with different initializations.
Our approach for extracting the surface from the super-level set
and subsequently optimizing it is currently quite simple and could
clearly be improved. As the examples show, the result is “close" and
has the correct topology, the results would be improved if local ge-
ometric features such as smoothness could be taken into account.
This could produce much higher resolution reconstructions while
remaining “close" to the topologically correct surface. This could
also take normal information into account which could further im-
prove both the optimization and the ﬁnal extracted surface. An al-
ternative approach would be to use the super-level set as a guide
or constraint on implicit methods to extract the ﬁnal surface. It re-
mains an open question how implicit methods could be modiﬁed
to extract non-manifold surfaces, e.g. manifolds with boundaries,
intersections of manifolds, etc. This is a future direction which we
intend to pursue.
The general area of topology-aware geometry processing re-
mains largely unexplored, with many possible future directions of
research. Here we addressed only single scan reconstructions while
it may be possible to jointly optimize over a collection as shape cat-
egories in collections often share many topological properties. Sim-
ilarly, this type of approach could also be applied to time-varying
scans, e.g. point clouds representing motion, where the codimen-
sion of the surface is greater than 1. Finally, in the case of collec-
tions our approach could be extended to combine topology infer-
ence with reconstruction or representation learning by inferring the
topology at the same time as optimizing with respect to it.
10. Acknowledgements
Supported by the Slovenian Research Agency (ARRS N1-0058)
and by the EU H2020-MSCA-RISE project RENOIR (grant no.
691152). Supported by Altor Equity Partners AB through Un-
box AI (unboxai.org). We would like to acknowledge the help of
Garikoitz Lerma Usabiaga for his help in understanding brain scan
data and manipulating it through the Freesurfer software.
References
[ACK13] ATTENE, MARCO, CAMPEN, MARCEL, and KOBBELT, LEIF.
“Polygon mesh repairing: An application perspective”. ACM Computing
Surveys (CSUR) 45.2 (2013), 15 2.
[AGH*09] ATTALI, DOMINIQUE, GLISSE, MARC, HORNUS, SAMUEL, et
al. “Persistence-sensitive simpliﬁcation of functions on surfaces in linear
time”. Presented at TOPOINVIS 9 (2009), 23–24 3.
[BGND*20] BRUEL-GABRIELSSON, RICKARD, NELSON, BRADLEY J,
DWARAKNATH, ANJAN, et al. “A topology layer for machine learning”.
accepted to AISTATS (2020) 3.
[BLW12] BAUER, ULRICH, LANGE, CARSTEN, and WARDETZKY, MAX.
“Optimal topological simpliﬁcation of discrete functions on surfaces”.
Discrete & Computational Geometry 47.2 (2012), 347–377 3.
[BP05] BAZIN, PIERRE-LOUIS and PHAM, DZUNG L. “Topology pre-
serving tissue classiﬁcation with fast marching and topology templates”.
Biennial International Conference on Information Processing in Medical
Imaging. Springer. 2005, 234–245 2.
[BTS*14] BERGER, MATTHEW, TAGLIASACCHI, ANDREA, SEVERSKY,
LEE, et al. “State of the art in surface reconstruction from point clouds”.
EUROGRAPHICS star reports. Vol. 1. 1. 2014, 161–185 2.
[Bub15] BUBENIK, PETER. “Statistical topological data analysis using
persistence landscapes”. The Journal of Machine Learning Research
16.1 (2015), 77–102 4.
[CBC*01] CARR, JONATHAN C, BEATSON, RICHARD K, CHERRIE, JON
B, et al. “Reconstruction and representation of 3D objects with radial ba-
sis functions”. Proceedings of the 28th annual conference on Computer
graphics and interactive techniques. ACM. 2001, 67–76 2, 3.
[CSEH07] COHEN-STEINER, DAVID, EDELSBRUNNER, HERBERT, and
HARER, JOHN. “Stability of persistence diagrams”. Discrete & Compu-
tational Geometry 37.1 (2007), 103–120 4.
[CSEHM10] COHEN-STEINER, DAVID, EDELSBRUNNER, HERBERT,
HARER, JOHN, and MILEYKO, YURIY. “Lipschitz functions have L
p-stable persistence”. Foundations of computational mathematics 10.2
(2010), 127–139 4.
[CZCG05] CARLSSON, GUNNAR, ZOMORODIAN, AFRA, COLLINS,
ANNE, and GUIBAS, LEONIDAS J. “Persistence barcodes for shapes”.
International Journal of Shape Modeling 11.02 (2005), 149–187 2.
[Dey06] DEY, TAMAL K. Curve and surface reconstruction: algorithms
with mathematical analysis. Vol. 23. Cambridge University Press,
2006 2.
[DLL*10] DEY, TAMAL K, LI, KUIYU, LUO, CHUANJIANG, et al. “Per-
sistent heat signature for pose-oblivious matching of incomplete mod-
els”. Computer Graphics Forum. Vol. 29. 5. Wiley Online Library.
2010, 1545–1554 3.
[EFR13] EDELSBRUNNER, HERBERT, FASY, BRITTANY TERESE, and
ROTE, GUNTER. “Add Isotropic Gaussian Kernels at Own Risk: More
and More Resilient Modes in Higher Dimensions”. Discrete Comput.
Geom. 49.4 (June 2013), 797–822 5.
[EH10] EDELSBRUNNER, HERBERT and HARER, JOHN. Computational
Topology - an Introduction. American Mathematical Society, 2010, I–
XII, 1–241. ISBN: 978-0-8218-4925-5 4, 5.
[ELZ00] EDELSBRUNNER,
HERBERT,
LETSCHER,
DAVID,
and
ZOMORODIAN, AFRA. “Topological persistence and simpliﬁcation”.
Foundations of Computer Science, 2000. Proceedings. 41st Annual
Symposium on. IEEE. 2000, 454–463 2.
[GG07] GUENNEBAUD, GAEL and GROSS, MARKUS. “Algebraic point
set surfaces”. ACM Transactions on Graphics (TOG). Vol. 26. 3. ACM.
2007, 23 2.
[GHO16] GAMEIRO, MARCIO, HIRAOKA, YASUAKI, and OBAYASHI,
IPPEI. “Continuation of point clouds via persistence diagrams”. Physica
D: Nonlinear Phenomena 334 (2016), 118–132 3.
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.


--- Page 11 ---

R. Brüel-Gabrielsson, V. Ganapathi-Subramanian, P. Skraba, L. Guibas / Topology-Aware Surface Reconstruction for Point Clouds
[GSDP*18] GANAPATHI-SUBRAMANIAN,
VIGNESH,
DIAMANTI,
OLGA, PIRK, SOREN, et al. “Parsing Geometry Using Structure-Aware
Shape Templates”. CoRR abs/1808.01337 (2018) 2.
[HDD*92] HOPPE, HUGUES, DEROSE, TONY, DUCHAMP, TOM, et al.
“Surface reconstruction from unorganized points”. Proceedings of the
19th annual conference on Computer graphics and interactive tech-
niques. 1992, 71–78 2.
[HZCJ17] HUANG, ZHIYANG, ZOU, MING, CARR, NATHAN, and JU,
TAO. “Topology-controlled Reconstruction of Multi-labelled Domains
from Cross-sections”. ACM Transactions on Graphics (TOG) 36.4
(2017), 76 2.
[JZH07] JU, TAO, ZHOU, QIAN-YI, and HU, SHI-MIN. “Editing the
topology of 3D models by sketching”. ACM Transactions on Graphics
(TOG) 26.3 (2007), 42 2.
[KBH06] KAZHDAN, MICHAEL, BOLITHO, MATTHEW, and HOPPE,
HUGUES. “Poisson surface reconstruction”. Proceedings of the fourth
Eurographics symposium on Geometry processing. Vol. 7. 2006 1–3, 8,
9.
[LDK*18] LAZAR, ROEE, DYM, NADAV, KUSHINSKY, YAM, et al. “Ro-
bust Optimization for Topological Surface Reconstruction”. ACM Trans.
Graph. 37.4 (July 2018), 46:1–46:10. ISSN: 0730-0301. DOI: 10 .
1145/3197517.3201348. URL: http://doi.acm.org/10.
1145/3197517.3201348 2.
[LOT19] LEYGONIE, JACOB, OUDOT, STEVE, and TILLMANN, ULRIKE.
“A Framework for Differential Calculus on Persistence Barcodes”. arXiv
preprint arXiv:1910.00960 (2019) 3, 4.
[LUCPA18] LERMA-USABIAGA, GARIKOITZ, CARREIRAS, MANUEL,
and PAZ-ALONSO, PEDRO M. “Converging evidence for functional and
structural segregation within the left ventral occipitotemporal cortex
in reading”. Proceedings of the National Academy of Sciences 115.42
(2018), E9981–E9990 8, 9.
[MWA*13] MUSIALSKI, PRZEMYSLAW, WONKA, PETER, ALIAGA,
DANIEL G, et al. “A survey of urban reconstruction”. Computer graph-
ics forum. Vol. 32. 6. Wiley Online Library. 2013, 146–177 2.
[OPT*17] OTTER, NINA, PORTER, MASON A, TILLMANN, ULRIKE, et
al. “A roadmap for the computation of persistent homology”. EPJ Data
Science 6.1 (2017), 17 5.
[PGK10] PORANNE, ROI, GOTSMAN, CRAIG, and KEREN, DANIEL. “3D
surface reconstruction using a generalized distance function”. Computer
Graphics Forum. Vol. 29. 8. Wiley Online Library. 2010, 2479–2491 2,
6, 8, 9.
[PSO18] POULENARD, ADRIEN, SKRABA, PRIMOZ, and OVSJANIKOV,
MAKS. “Topological Function Optimization for Continuous Shape
Matching”. Computer Graphics Forum. Vol. 37. 5. Wiley Online Library.
2018, 13–25 3–5.
[RM00] ROERDINK, JOS BTM and MEIJSTER, ARNOLD. “The watershed
transform: Deﬁnitions, algorithms and parallelization strategies”. Fun-
damenta informaticae 41.1, 2 (2000), 187–228 2.
[SAAY06] SAMOZINO, MARIE, ALEXA, MARC, ALLIEZ, PIERRE, and
YVINEC, MARIETTE. “Reconstruction with Voronoi centered radial ba-
sis functions”. PhD thesis. INRIA, 2006 2.
[SLS*06] SHARF, ANDREI, LEWINER, THOMAS, SHAMIR, ARIEL, et al.
“Competing fronts for coarse–to–ﬁne surface reconstruction”. Computer
Graphics Forum. Vol. 25. 3. Wiley Online Library. 2006, 389–398 2.
[SLS*07] SHARF, ANDREI, LEWINER, THOMAS, SHKLARSKI, GIL, et al.
“Interactive topology-aware surface reconstruction”. ACM Transactions
on Graphics (TOG). Vol. 26. 3. ACM. 2007, 43 2.
[SOCG10] SKRABA, PRIMOZ, OVSJANIKOV, MAKS, CHAZAL, FRED-
ERIC, and GUIBAS, LEONIDAS. “Persistence-based segmentation of
deformable shapes”. Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2010 IEEE Computer Society Conference on. IEEE.
2010, 45–52 3.
[STY17] SKRABA, PRIMOZ, THOPPE, GUGAN, and YOGESHWARAN, D.
“Randomly Weighted d−complexes: Minimal Spanning Acycles and
Persistence Diagrams”. arXiv preprint arXiv:1701.00239 (2017) 5.
[SZM*08] SIDDIQI, KALEEM, ZHANG, JUAN, MACRINI, DIEGO, et al.
“Retrieving articulated 3-D models using medial surfaces”. Machine vi-
sion and applications 19.4 (2008), 261–275 1, 9.
[WHDS04] WOOD, ZOE, HOPPE, HUGUES, DESBRUN, MATHIEU, and
SCHRODER, PETER. “Removing excess topology from isosurfaces”.
ACM Transactions on Graphics (TOG) 23.2 (2004), 190–208 2.
[YHZ*] YIN, KANGXUE, HUANG, HUI, ZHANG, HAO, et al. “Morﬁt:
interactive surface reconstruction from incomplete point clouds with
curve-driven topology and geometry control.” () 2.
[ZCL14] ZHOU, SHIZHE, CHANGYUN, JIANG, and LEFEBVRE, SYL-
VAIN. “Topology-constrained synthesis of vector patterns”. ACM Trans-
actions on Graphics 33.6 (2014) 2.
[ZSCP08] ZENG, YUN, SAMARAS, DIMITRIS, CHEN, WEI, and PENG,
QUNSHENG. “Topology cuts: A novel min-cut/max-ﬂow algorithm for
topology preserving segmentation in N–D images”. Computer vision and
image understanding 112.1 (2008), 81–90 2.
© 2020 The Author(s)
Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd.
```

---

## Section 1.41: Voronoi-based Variational Reconstruction

Source File: Voronoi-based Variational Reconstruction.txt

### Paper Content:

```
# Voronoi-based Variational Reconstruction.pdf
# Converted: 2025-07-19 12:45:30
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/Voronoi-based Variational Reconstruction.pdf
# Output: ../layer2_completion/txt/Voronoi-based Variational Reconstruction.txt


--- Page 1 ---

Eurographics Symposium on Geometry Processing (2007)
Alexander Belyaev, Michael Garland (Editors)
Voronoi-based Variational Reconstruction
of Unoriented Point Sets
P. Alliez1
D. Cohen-Steiner1
Y. Tong2
M. Desbrun2
1 INRIA Sophia-Antipolis
2 Caltech
Abstract
We introduce an algorithm for reconstructing watertight surfaces from unoriented point sets. Using the Voronoi
diagram of the input point set, we deduce a tensor ﬁeld whose principal axes and eccentricities locally represent
respectively the most likely direction of the normal to the surface, and the conﬁdence in this direction estimation.
An implicit function is then computed by solving a generalized eigenvalue problem such that its gradient is most
aligned with the principal axes of the tensor ﬁeld, providing a best-ﬁtting isosurface reconstruction. Our approach
possesses a number of distinguishing features. In particular, the implicit function optimization provides resilience
to noise, adjustable ﬁtting to the data, and controllable smoothness of the reconstructed surface. Finally, the use of
simplicial meshes (possibly restricted to a thin crust around the input data) and (an)isotropic Laplace operators
renders the numerical treatment simple and robust.
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [CG]: Computational Geometry and Object Modeling.
1
Introduction
Surface reconstruction from point clouds is motivated by
a number of CAGD, point-based graphics, and reverse en-
gineering applications where scattered point samples of a
surface need to be turned into a proper, watertight surface
mesh. Particularly challenging are point sets generated by
laser scanners and hand-held digitizers, as they are often
noisy (due to the inherent uncertainty of measurements), un-
organized (due to the merging of several scans), and possibly
containing large holes (due to occlusions during the acquisi-
tion process). In such a context, surface reconstruction can
only be approximating—instead of interpolating—as data
points are more of an indication of proximity to the surface
than actual sample points.
While a number of algorithms can now efﬁciently reconstruct
oriented points (i.e., point sets where a normal is provided
at each sample), fewer methods are able to approximate raw
(unoriented) point sets, with controllable smoothness. In this
paper, we introduce a Voronoi-based variational approach to
surface reconstruction that can equally process unoriented
point sets and oriented point sets, and can even exploit conﬁ-
dence measures on the normals if available. As our technique
is based on isocontouring of a variationally-deﬁned implicit
function deﬁned in the embedding space, we guarantee wa-
Figure 1: Sforza. Our variational method allows high-ﬁdelity recon-
struction of unprocessed point sets (3D scanned Sforza, 200K points).
The ﬁnal mesh is extracted via isocontouring of a scalar function
computed through optimization.
tertightness and smoothness while offering control over data
ﬁtting.
1.1
Related Work
Delaunay-based surface reconstruction techniques were ini-
tially designed to establish a plausible connectivity between
points [Boi84]. One of the ﬁrst reconstruction techniques that
came with theoretical guarantees was proposed by Amenta
and Bern [AB99]. The rationale behind their technique was
that when a sampling is noise-free and dense enough, all
c
⃝The Eurographics Association 2007.


--- Page 2 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
Figure 2: Our reconstruction procedure at a glance. From left to right: input point set; its Voronoi diagram; covariance matrices of the cells
shown as (rescaled) ellipses; Steiner points added through Delaunay reﬁnement (isotropic tensors are assigned to Steiner points); piecewise
linear function f (solution of a generalized eigenvalue problem) that best ﬁts the input data, with the reconstructed curve (isocontouring of f).
Voronoi cells are elongated in the direction of the normal to
the inferred surface. An analysis of the point set’s Voronoi
diagram can then be used to derive an interpolating recon-
structed surface. This technique has stimulated many im-
provements and variants: we refer the reader to [CG06] for
a survey, and to [Dey06] for a recent comprehensive mono-
graph. In practice however, most of these Voronoi-based tech-
niques are interpolatory, thus inadequate in the presence of
noise.
Noise and sparseness in typical point sets have led to an
approximating class of approaches, where an implicit func-
tion is computed such that one of its isosurfaces best ﬁts
the data (see e.g., [OBA∗03]). These implicit approaches
are particularly convenient as they all guarantee a water-
tight 2-manifold surface approximation by construction; they
mostly differ in the norm used to match the point sets and
the isosurface. Approximations of a signed distance func-
tion to the point set [CBC∗01] or to estimated tangent
planes [HDD∗92,BC02] were proposed as possible implicit
functions. More recently, a reconstruction method for ori-
ented point sets was introduced in which an implicit function
f is derived from a Poisson equation ∆f = div(n), providing
the best L2-match between the gradient of f and the input
normals n [KBH06]. In practice, the normal ﬁeld is ﬁrst inter-
polated onto an octree via an isotropic smoothing kernel, of-
fering control of the smoothness of the reconstructed surface.
This algorithm scales remarkably well since it only involves
solving a linear system, and its global L2-minimization na-
ture makes it resilient to noise. These techniques require a
consistent orientation of the normals to perform correctly.
Unfortunately, unless reliable normals are provided, ﬁnding
such an orientation has been recognized to be an ill-posed
problem when the sampling is sparse and noisy. In particular,
one of the most robust approaches to normal orientation is
through labeling a set of Voronoi poles; but it requires little
or no noise and a dense-enough (ε-)sampling to guarantee
consistent results—two conditions rarely met in practice.
Given this intrinsic difﬁculty, recent work has polarized on
handling raw, unoriented point sets without attempting to lo-
cally estimate or orient the normals (e.g., [KSO04,WCS05,
HK06,PSQ06]). Initial work includes methods such as Tensor
Voting [MM06], where local shape is inferred from neigh-
boring sample points even in the presence of noise. More
recently, a spectral reconstruction method [KSO04] has been
shown remarkably robust to outliers due to its reliance on
graph partitioning algorithms [SM00]. However, this method
is interpolatory, requiring a systematic post-smoothing for
noisy point clouds. Similarly, Hornung and Kobbelt [HK06]
propose a min-cut algorithm on a regular grid, also requir-
ing post-smoothing to remove grid aliasing. Finally, an ap-
proach based on eigen analysis but derived purely from an op-
timization standpoint, offers an approximating reconstruction
where smoothness can be controlled [WCS05]—although var-
ious coefﬁcients require adjustment to provide good results.
1.2
Overview
Our approach combines the generality and resilience of spec-
tral methods as it can take a raw, unoriented and noisy point
set as input. It also offers the quality of Poisson-based recon-
struction through global ﬁtting of normal direction evalua-
tions and control over smoothness. Our algorithm performs
equally well when normal information is included in the point
set, even exploiting conﬁdence measurements on the normals
to improve the quality of the reconstructed surface. To pro-
vide a uniﬁed, variational framework for point set surface
reconstruction, our algorithm proceeds in two main steps.
First, if no normal information is provided, we perform a
novel, Voronoi-PCA estimation of unoriented normals in-
duced by the point set (Section 2). This ﬁrst step results in a
tensor ﬁeld which encodes both the (unoriented) normal di-
rection (through its eigenvector of maximum eigenvalue) and
the conﬁdence in the approximation (through its anisotropy).
Second, an implicit function is computed via a generalized
eigenvalue problem (Section 3) so as to make its gradient
best ﬁt the normal directions. Details of the algorithm and its
implementation are given in Section 4, and a series of tests
and comparisons are presented in Section 5.
2
Estimating Unoriented Normals
Our ﬁrst goal is to estimate unoriented normals (along with
their reliability) to the inferred surface from the input point
set if no such information is available. Note that we will
not try to infer an orientation, as this global task will be
incumbent upon the second step of our approach. As a way
of motivating our estimation, we brieﬂy review a few closely
related techniques.
c
⃝The Eurographics Association 2007.


--- Page 3 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
2.1
Background
Normal estimation from point sets has received a lot of at-
tention in the past few years (see e.g., [PKKG03, MNG04,
DLS05,OF05,LP05,HXMP05]). Various strategies have been
proposed, guaranteeing high-order accuracies through, e.g.,
local ﬁtting [CP03]. However, when a point set is noisy and
unstructured, a majority of methods have recourse to one of
two popular techniques, both straightforward to implement.
Principal Component Analysis (PCA) A conventional
technique for estimating normal directions is through local
principal component analysis. The main idea is to deﬁne
a small neighborhood around each input point (e.g., the k-
nearest neighbors [HDD∗92]), compute the covariance ma-
trix of the points in this neighborhood, and deduce the nor-
mal direction from the eigenvector associated to the smallest
eigenvalue of the resulting covariance matrix. Many variants
have been proposed to improve resilience to noise (see, for
instance, [MNG04]).
Voronoi Poles Another common technique for estimating
normal directions is global by nature as it requires the con-
struction of the Voronoi diagram of the input point set. A
subset of Voronoi vertices called poles [AB99] is then ex-
tracted, and used to estimate a normal direction at each sam-
ple point. In absence of noise and for dense-enough samples
this method can be shown to provide a faithful normal es-
timate even for irregular sampling, with convergence rates
depending on the elongation of the Voronoi cells. A variant
by Dey and Sun [DS05] provides more resilience to noise.
2.2
A Voronoi-PCA Approach to Normal Estimation
Given the respective advantages of PCA and Voronoi
poles, we propose a novel normal approximation tech-
nique that combines both of their qualities. We begin
by computing the 3D Voronoi diagram of the input
point set after adding dummy sample points on a very
large bounding sphere (the resulting Voronoi cells of
the input points are therefore not extending to inﬁnity).
As the shape of these cells reﬂects the global dis-
tribution of the points in space, a key observation
is that the covariance matrix of the Voronoi cell
of a sample point provides not only an estimate of
the normal direction, but also a measure of how
reliable this estimate is. Indeed the eigenvector
associated to the largest eigenvalue indicates the
axis along which the cell is elongated (see inset)—
a good approximate of the normal direction if the
samples all lie on a common manifold. Moreover,
as described in [AB99], the conﬁdence in the es-
timate is related to how long and thin the Voronoi
cell is, i.e., to the anisotropy of the covariance tensor.
Covariance Matrix of a Voronoi Cell Let V be a ﬁnite
Voronoi cell associated to a sample point p. The covariance
matrix of V is deﬁned by its order-2 moment with respect to
its center of mass m:
cov(V) =
Z
V
(X −m)(X −m)tdX.
(1)
A simple way to compute this integral is to perform a de-
composition of the Voronoi cell V into tetrahedra, so as to
assemble the ﬁnal covariance matrix in closed form as ex-
plained in Appendix A. If the sampling is of good quality, this
procedure will be very accurate as each Voronoi cell is long
and skinny. However, as Fig. 3(middle) illustrates, Voronoi
cells can become small and isotropic if noise is present.
Covariance Matrix of a Union of Voronoi Cells To render
our estimate robust to noisy point sets, we compute the co-
variance matrix of a union of Voronoi cells (closed-form
formulas are also provided in Appendix A): as the Voronoi di-
agram of the point set partitions the whole domain, elongated
cells are present beyond the noisy area, and accumulating
enough neighbors will eventually render the union elongated
enough (see Fig 3(right)). Notice that the idea of combin-
ing the inﬂuence from neighbors to promote noise resilience
is commonplace, but our technique is adaptive in the sense
that we use as many neighbors as needed to ﬁnd a reliable
approximation as described next.
Figure 3: Voronoi cells of point sets. Left: Voronoi diagram of a
point set without noise. Middle: with noise the cells become irregular.
Right: a denser, noisy point set shows even more diverse cell shapes.
Estimation Procedure We implement our estimation proce-
dure as follows: given a sample point p, we ﬁrst compute
the covariance matrix of its Voronoi cell V(p), and mea-
sure its anisotropy σ ∈[0,1] as σ=1 −isotropy(V), where
isotropy(V) is the ratio between the smallest and the largest
eigenvalues. We then iterate over its k nearest neighbor points
{qi}, and perform the same procedure for V(p)∪V(q1), then
for V(p) ∪V(q1) ∪V(q2), etc., until either the anisotropy
has reached a threshold of 0.9, or we reach the maximum
number of nearest neighbors (typically k = 50). From these
covariance matrices, the one with maximum anisotropy is
then returned. Finally, for normalization purposes, we rescale
this resulting tensor such that its maximum eigenvalue is unit.
Notice that this evaluation procedure will stop at a single
Voronoi cell when the sampling is dense and noise-free: there
is indeed no incentive to add an extra neighborhood Voronoi
cell as it would mainly thicken the domain of integration in
Equ. (1), thus decreasing anisotropy.
This procedure beneﬁts from both the qualities of PCA (lo-
cal analysis of a number of neighboring samples) and those
c
⃝The Eurographics Association 2007.


--- Page 4 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
of the Voronoi-based approach (global analysis of the sam-
ple repartition via the Voronoi diagram). Our Voronoi-PCA
normal estimation technique can be seen as an integral PCA
approach, similar to [PWY∗06] for curvature estimation: in-
tegration (over Voronoi cells in our case) leads to more stable
estimations compared to the pole approach which bases its
estimation on the position of a single pole. As the quality of
this normal estimation is important to our reconstruction pro-
cedure, we will present in Section 5.1 numerical experiments
showing that this procedure outperforms the two normal esti-
mation techniques that it combines.
3
Generalized Eigenvalue Problem
We consider at this stage that a (covariant) tensor ﬁeld encod-
ing the normal directions has been either estimated through
our Voronoi-PCA approach, or derived from normal informa-
tion in the input data (as nnt). We now wish to compute an
implicit function such that its gradient is best aligned with
the principal axes of the tensor ﬁeld: isosurfacing this scalar
function will provide a best-ﬁtting reconstruction.
3.1
Problem formulation
While a Poisson reconstruction [KBH06] can directly give
us an implicit function such that its gradient best ﬁts a nor-
mal ﬁeld, we cannot have recourse to such a direct linear
solve as we only have a direction ﬁeld (i.e., unoriented nor-
mals) through our Voronoi-PCA tensors. We need instead to
ﬁnd an implicit function f such that its gradient ∇f is every-
where best aligned to the the maximum eigenvalue direction
of the tensor ﬁeld C, where the notion of "best alignment" is
weighted by the local conﬁdence in the normal direction. We
propose the following constrained maximization procedure
to efﬁciently ﬁnd such a function f:
Given a tensor ﬁeld C, ﬁnd the maximizer f of:
ED
C (f) =
Z
Ω
∇f t C ∇f subject to:
Z
Ω

|∆f|2+ε|f|2
= 1,
where Ωis the domain, and ∆is the Laplacian operator.
The interpretation of this optimization problem is as follows.
The energy function ED
C , called anisotropic Dirichlet energy,
directly measures the alignment of ∇f with the normal direc-
tion indicated by C. Indeed, an isotropic tensor (i.e., unknown
normal direction) has little inﬂuence on this energy, whereas
an anisotropic tensor (i.e., high conﬁdence in the normal di-
rection) will play a major role—penalizing misalignment at
reliable data point. We then add as a constraint that ED
C must
be maximized over the unit ball deﬁned by the biharmonic
energy. Just like the Dirichlet (harmonic) energy is a mea-
sure of the smoothness f, the biharmonic energy measures
the smoothness of ∇f: therefore, this added constraint im-
poses a regularization of the maximizer f. A small amount
of the L2 norm of f is added to avoid having to constraint
values of f (either on the boundary or inside the domain), as
well as to improve conditioning: the resulting constraint is a
Sobolev-like norm EB on f. In practice, however, we replace
this regularization term by a small amount of data ﬁtting as
we will discuss in Section 3.3.
Solving for this constrained maximization amounts to care-
fully balance smoothness of ∇f vs. alignment of the gradient:
it will align ∇f to C if it is particularly rewarding for the
anisotropic Dirichlet energy (i.e., when the normal direction
is particularly reliable); on areas where the tensor is isotropic,
the solver will favor smoothness of the function gradient
instead. This global balancing act implicitly induces a con-
sistent orientation to the tensor ﬁeld since ﬂipping the sign
of ∇f between two incident vertices of the simplicial mesh
signiﬁcantly increases the biharmonic energy.
3.2
Discrete Formulation
We now assume that we have a tetrahedral mesh of the 3D
domain with V vertices {vi} and E edges {ei}. Each edge
ei is arbitrarily oriented. Given this mesh with a tensor Ci at
each vertex i, we wish to solve for a node-based, piecewise
linear function f, i.e., to ﬁnd a vector F = (f1, f2,..., fV )t
that satisﬁes the aforementioned constrained maximization.
Discrete Anisotropic Dirichlet Energy The energies in-
volved in the optimization are rather simple to express. Al-
though various expressions of the anisotropic Dirichlet en-
ergy have been proposed in 2D in the context of quasi-
harmonic parameterizations [Gus02, ZRS05], we construct
our 3D energies via matrix assembly using the language of
discrete forms [DKT06]. Thus, ED
C (F) is expressed as:
ED
C (F) ≈Ft A F with A = dt
0 ⋆
1
C d0
where d0 is the transpose of the signed vertex/edge incidence
matrix (size ExV) of the mesh, and ⋆1
C is the Hodge star oper-
ator for the metric induced by C. We approximate this latter
operator by the Euclidean diagonal Hodge star ⋆1 modulated
by the tensor C, resulting in the following diagonal matrix:
∀i = 1...E, (⋆
1
C)ii = et
i Cei
et
i ei
(⋆
1)ii
with: (⋆
1)ii = |e∗
i |
|ei| ,
where ei is the ith (oriented) edge, |ei| is its length, and |e∗
i |
is the area of its dual Voronoi face. The value of the tensor
C on an edge is found by averaging the tensor values at each
vertex of the edge.
Discrete Biharmonic Energy For the biharmonic energy,
the following (simpliﬁed) discretization performs adequately
in practice (regularization will be added later through a data
ﬁtting term):
EB(f) ≈Ft B F with
B = (dt
0 ⋆
1 d0)2
where we used the same notations as above.
Optimization Procedure Using a Lagrange multiplier λ, we
can now rewrite the constrained optimization as a maximiza-
tion of the following functional:
E = FtAF +λ(1−FtBF).
A necessary condition of optimality is ∂E/∂F = 0, yielding:
AF = λBF.
This expression expresses what is known as a generalized
eigenvalue problem (GEP), where F is an eigenvector of
this GEP, and λ is its corresponding eigenvalue. In fact, the
solution to our constrained maximization is the eigenvector
of the GEP corresponding to the largest eigenvalue.
c
⃝The Eurographics Association 2007.


--- Page 5 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
Proof: Since the eigenvectors of a GEP form a basis, we can
write a function F as:
F = ∑aλFλ,
where Fλ is the eigenvector corresponding to the eigen-
value λ (i.e., AFλ = λBFλ) such that Ft
λBFλ = 1 and, for
λ1 ̸= λ2, Ft
λ1BFλ2 = 0. Therefore:
ED
C = (∑
λ
aλFλ)tB(∑
λ
λaλFλ)=∑
λ
λa2
λ ≤∑
λ
λmaxa2
λ = λmax.
Since the energy is bounded by the max eigenvalue λmax and
this value is attained by ED
C (Fλmax), we get: F = Fλmax.□
3.3
Generalizations
So far, our Voronoi-based variational approach requires no
parameter-tweaking to provide a reconstruction of a point
set. We can, however, easily extend this basic approach while
keeping the exact same framework. In particular, the matrix B
that contains the discrete biLaplacian can be modiﬁed in var-
ious ways to allow for more control over smoothness and/or
interpolation:
• Data ﬁtting: we can change the optimization results by
adding a term that controls data ﬁtting. We favor a value of
0 on the input points by adding to the constraint a ﬁtting
factor times the sum of the squares of the function values at
input points. Changing the ﬁtting factor will provide a con-
trollable data ﬁtting effect to our procedure. Variants, e.g.,
where data ﬁtting is spatially varying, are easily designed.
• Splines-under-tension energy: instead of only using the
biLaplacian, we can constrain the optimization over a unit
ball deﬁned by a linear combination of the Dirichlet and
the biharmonic energies. It will allow for a better tradeoff
between smoothness of the results vs. ﬁtting of the normal
directions, as it is tantamount to a splines-under-tension
energy [SW90].
The matrix B needs to be slightly modiﬁed in order to im-
plement these two generalizations as we will detail in Sec-
tion 4.3. While we only experimented with these two exam-
ples (and we will detail their implementation next), other
modiﬁcations of A and/or B are possible, yet indubitably
application-dependent.
4
Implementation
Our technique was implemented in C++. We now describe the
algorithm along with the implementation choices we made
for each of its steps.
4.1
Voronoi-PCA Estimation
As our algorithm has been implemented using the CGAL
library [CGA03], this step is straightforward. After reading
in the input point set and adding a few dummy points on
a large bounding sphere, we compute its Voronoi diagram.
We then use the iterative procedure described in Section 2
to ﬁnd a tensor per input point, using the formulas given in
Appendix A to optimize the covariance matrix computations.
4.2
Delaunay Reﬁnement
Before performing the constraint optimization, we must re-
ﬁne and improve the Delaunay tetrahedral mesh (dual of
the Voronoi diagram previously computed) of our domain:
since the point set can be arbitrary, any numerical approxi-
mation evaluated on the initial mesh would inevitably lead
to numerical degeneracies due to the poor aspect ratio of
tets. We thus have recourse to a Delaunay reﬁnement pro-
cedure [RY06]: this simple priority-queue-based treatment
iteratively improves tetrahedra with radius edge ratio greater
than a threshold (set to 2 in all our tests) by adding Steiner
points. At each Steiner point inserted to the triangulation, we
set the tensor Ci to the 3x3 identity matrix to remain agnostic
as to the normal direction there. In order to limit the size
of the ﬁnal mesh, we restrict the reﬁnement to an enlarged
bounding box of the input point set. Optionally, we can also
reﬁne the mesh within a thin shell around the input data by
enforcing a maximum edge length as well, providing more
degrees of freedom to the implicit function where it matters.
Figure 4 illustrates this process in 2D and 3D. Note that this
reﬁnement procedure is akin to the use of a restricted octree
in, e.g., [KBH06]. We point out however that we do, instead,
keep the original points, preventing any local “oversmooth-
ing” that appears in methods where the input point set is
discarded in favor of a substitute (regular or adaptive) grid.
Figure 4: Delaunay reﬁnement. (top) 2D Delaunay triangulation
(left) of a point set sampling a curve is iteratively reﬁned by adding
Steiner points with (right) or without (middle) sizing constraints
within a shell around input points; (bottom) Example in 3D.
4.3
Solver
We solve the generalized eigenvalue problem AF = λBF
by turning it into a classical eigenvalue problem. We ﬁrst
precompute a Cholesky factorization of B using the TAUCS
library [TCR05]; this results in a lower triangular matrix L
such that B = LLt. We now rewrite the GEP as:
AF = λLLtF ⇔L
−1AL
−tLtF = λLtF ⇔
L−1AL−tG = λG
G = LtF
We then employ the implicitly restarted Arnoldi iteration
method from the ARPACK++ library [GS], with L−1AL−t as
the Arnoldi operator and by requesting the maximum eigen-
value only. After convergence, we ﬁnd the solution F by
solving LtF = G, as it corresponds to the eigenvector cor-
responding to the maximum eigenvalue of the original GEP.
This eigenvector deﬁnes a piecewise linear function over the
tetrahedral mesh: we are now ready for contouring.
c
⃝The Eurographics Association 2007.


--- Page 6 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
Additional Terms Instead of only using the bilaplacian, we
also add to B a data ﬁtting matrix (denoted D) times a pa-
rameter µﬁt, as discussed in Section 3.3. The matrix D is a
simple diagonal matrix with 1s only on the rows correspond-
ing to input points (i.e., for all points but the Steiner points
added by Delaunay reﬁnement). The parameter µﬁt not only
allows us to tune the amount of data ﬁtting, but also to tune
the separation between connected components (see Fig. 8).
Note also that we always include at least a small amount of
data ﬁtting term (µﬁt > 10−4) to efﬁciently substitute for the
ε regularizing term in the smoothness constraint. Similarly,
we add µ∆dt
0 ⋆1 d0 to offer more control over smoothness as
described in Section 3.3. Once the user has chosen values for
the coefﬁcients µﬁt and µ∆, the solver proceeds as explained
with the matrix B set to: B = (dt
0 ⋆1 d0)2 +µﬁtD+µ∆dt
0 ⋆1 d0.
4.4
Contouring
To ﬁnd which isocontour to extract, we ﬁrst evaluate the re-
sulting function at all input points (not Steiner points), and
pick the median value for contouring (outliers may affect
the average; we found that the median provides a more ro-
bust isovalue). For the ﬁnal isocontouring, either a simple
marching-tetrahedra algorithm or a Delaunay-based surface
meshing algorithm [BO05] can be used. We prefer the latter
as it generates meshes with fewer elements, and with guar-
anteed quality of the mesh elements (all triangle angles are
between 20 and 140 degrees). Figure 9 illustrates outputs of
the Delaunay-based meshing algorithm with three uniform
sizing criteria.
5
Results
Before showing results of our reconstruction technique, we
present some numerical experiments on our Voronoi-PCA
method.
Figure 5: Poles vs. covariance matrices in 2D. A point set samples
two parallel lines; (bottom) from left to right, we translate the sam-
ples of the bottom line slowly; pole-based normal estimates are
depicted in blue, while our covariance-based normal estimates (de-
riving from the Voronoi cells displayed as well) are in red; (top) the
two curves compare the pole-based results (blue, very discontinuous)
with the covariance-based estimate (red) using the angle error (in
degrees) as the bottom line of points is shifted.
5.1
Voronoi-PCA Estimation
We ﬁrst illustrate how our integral-based normal estimation
technique provides improved numerics compared to the usual
pole-based approach by examining a very simple point set
conﬁguration, where 2D points are sampled uniformly along
two parallel lines. When the bottom line is slowly shifted, the
shape of the Voronoi cells evolves, triggering (often discontin-
uous) changes in the pole-based normal estimates. Because
our approach relies on the integrated moment of the cells,
it is signiﬁcantly less sensitive to the shift as shown by the
curve in Fig. 5. Notice also that while a k-nearest neighbor
PCA approximation can dramatically lose accuracy when
the (Euclidean-based) neighbors include points of both lines,
our approach beneﬁts from the global nature of the Voronoi
diagram, making it more robust to sparse sampling.
The next experiment compares our normal estimation tech-
nique for 3D parametric surfaces with both pole and point-
based PCA approaches. We sample a height ﬁeld z =
sin(x)cos(y) (for which normals are known analytically) with
different sampling criteria:
• Noise free: The height ﬁeld is ﬁrst sampled on a regular
grid in parameter space in the interval [−π,π]2, with rates
ranging from 20×20 samples to 100×100 samples.
• Noise in parameter space. The height ﬁeld is then sam-
pled on a jittered grid in parameter space, with the same var-
ious densities as above. The noise is uniform and isotropic,
with a maximum magnitude of half the grid spacing to
make it scale with sampling density.
• Noise in embedding space. The samples of the ﬁrst case
(regular grid) are now jittered in the embedding space using
an isotropic uniform noise of half the grid spacing.
Figure 6: Normal estimation on a parametric surface. (top) noise-
free sampling, (middle) noise added in parameter space, (bottom)
noise added in embedding space. The plots show the average angle
deviation in degrees as a function of the sampling density. Pole-based
estimation in blue, point-based PCA in green, and our covariance-
based estimation with a single Voronoi cell in red.
In these three contexts, we measure the average angle devi-
ation between normal estimates and true normals for each
sample density. For the point-based PCA technique, we al-
ways use the 8 nearest neighbors as it leads to the best esti-
mates. As Fig. 6 indicates, our approach is, across all tests,
either as good as or better than the two normal estimation
techniques that it is built upon. Comparing various normal
estimation techniques fairly is notoriously difﬁcult as many
c
⃝The Eurographics Association 2007.


--- Page 7 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
parameters come into play, such as sampling density, sam-
pling anisotropy, noise, and outliers—so our tests are not
intended to be extensive and conclusive. Nevertheless, we
conducted several other experiments (including a compari-
son with [MNG04]) that all show the numerical relevance
of using a Voronoi-based covariance approach. A theoretical
analysis of our normal estimation technique, which should
draw upon Voronoi diagrams, principal component analysis,
and possibly geometric measure theory, is left to future work.
5.2
Surface Reconstruction
As a proof-of-concept example, we begin with a 2D experi-
ment to exemplify the similarities and differences between
our approach and the Poisson-based method performed on
the same simplicial mesh (i.e., we implemented the Poisson
equation of [KBH06] on a triangle mesh using the discretiza-
tion described in [TLHD03]). As Fig. 7 illustrates, the ﬁrst
point set (dense, noise-free) is easily oriented using pole la-
beling (red/blue dots are poles), which allows us to use Pois-
son equation for reconstruction. Both methods lead to very
similar reconstructed curves. When the sampling is sparser,
the pole-based orienting process fails to provide the “right”
orientation, and the Poisson reconstruction reﬂects this error,
while the optimal implicit function remains quite unchanged.
Figure 7: Comparison with Poisson reconstruction in 2D. (top) our
approach is very similar to a Poisson reconstruction for dense point
sets as the normal orientations can reliably be deduced; (bottom)
for sparser/noisier unoriented datasets, local orientation becomes
prone to errors, while our variational approach remains valid.
We also illustrate the effects of tuning the parameters (in 2D
for clarity). µ∆allows controlling the smoothness (Fig. 8, top).
µﬁt controls the ﬁtting of the input points, and hence the sep-
aration of two components (Fig. 8, middle). It also provides
a way to increase the contrast of the implicit function for
nested components (Fig. 8, bottom left). A shape completion
example from a sparse dataset, obtained with our splines-
under-tension energy, is also shown (Fig. 8, bottom/middle).
Our last 2D example illustrates (Fig. 10) the resilience to
both sparsity of the point set as well as noise (including few
outliers) in the data.
We also processed a number of 3D point sets issued from
(laser range) scanners. A golf club head with 14K points
(Fig. 11), a kitten model of genus 1 (Fig. 9), a bunny (Fig. 12),
and a bust model with 250K points (Fig. 1) are presented. Ad-
ditionally, we show in Fig.13 that our method applied to a
Figure 8: Reconstruction Parameters. (top) adjusting µ∆(left to
right) permits easy tuning of the resulting smoothness, (middle) while
µﬁt controls the ﬁt to the input point (middle & right), allowing to
accurately separate and capture ﬁne, nearby surface layers. (bot-
tom) data ﬁtting allows the reconstruction of nested components
(left); smooth completion of sparse dataset is easily achieved using
our splines-under-tension energy (middle); separate components of
different geometric complexity can also be captured accurately.
Figure 9: Kitten. (top) 20K input point set, Steiner points and implicit
function, and shaded reconstructed surface obtained by marching-
tetrahedra. (bottom) three output meshes at increasing resolutions.
raw, unoriented point set (206K points) recovers a similar sur-
face to the Poisson reconstruction from [KBH06] for which
additional normal information was provided. We note that
even at octree depth 11, the Poisson-based mesh is compar-
atively oversmoothed, mostly due to the interpolation of the
normals onto the octree leaves. If such an oversmoothing is
desirable, we can similarly interpolate the tensor ﬁeld around
the input points, or simply increase the smoothness by tuning
our two parameters (as demonstrated in Fig. 8).
c
⃝The Eurographics Association 2007.


--- Page 8 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
Timings and Limitations On a laptop (1.8GHz, 2 GBytes)
our current implementation constructs the initial Delaunay
triangulation of the kitten (20K vertices, 132K tets, Fig. 9) in
1.2s, and Delaunay-reﬁnes it to 30K vertices in 6s. The ma-
trix A contains around 8 non-zero elements per line, while B
has around 35 non-zero elements per line. The Cholesky fac-
torization takes 23s (including TAUCS super-nodal ordering).
The 51 Arnoldi iterations to convergence take a total of 42s.
The data structures used in our algorithm scale linearly in
memory (in particular, a Delaunay tetrahedralization scales
linearly in practice, and the number of non-zero elements per
line in our matrices does not depend on the size of the model).
The only bottleneck to a good scalability of the algorithm is
the Cholesky factor: its memory requirement is high (2e+7
non-zero elements for the kitten). Out-of-core factorization is
a viable option that we use for large models like Bimba and
Sforza, but the overall timings consequently suffer (up to 25
minutes for 250K input points), and the super-nodal ordering
still requires in-core execution. For larger datasets a 64-bit
machine would be indispensable to address these memory
issues.
Discussion Our approach offers a unique mix between
Voronoi-based methods (to estimate normals via PCA),
implicit-based approaches (as an implicit function is glob-
ally optimized to allow for smooth approximation through
isocontouring), and spectral techniques (as our optimization
procedure is expressible as a generalized eigenvalue prob-
lem). It empirically matches the results of oriented point set
reconstruction techniques such as Poisson reconstruction, but
without the need for normal orientation. We note that the
closest existing approach is, from a numerical point of view,
Figure 10: Noise and Sparsity Resilience. (top) our approach can
handle from dense (left) to sparse (right) raw datasets without any
parameter tweaking; (bottom) similarly, results degrade gracefully
with noise (left, middle), even in the case of outliers (right).
Figure 11: Golf club. A scanned point set (14k points), and its recon-
struction seen from various angles.
the work of [WCS05] as they too use a GEP with multiple en-
ergy terms. However, our technique differs quite drastically
in its premises as, e.g., they do not make use of anisotropic
energies or tensors, they rely on regular grids only, and do
not try to infer normal directions prior to the GEP, resulting
in a delicate parameter tweaking to avoid spurious surface
oscillations in the results.
Finally, a hybrid approach between our contribution and Pois-
son reconstruction might be most desirable for massive data:
our approach could be used to orient a subsampled version of
a raw, unoriented point set, thus inducing an orientation for
the initial point set on which a Poisson solve can then be per-
formed. Additionally, as conﬁdence estimation of the normal
direction might become part of the measurements provided
by newer scanning devices, our technique could directly ex-
ploit this additional information. Similarly, conﬁdence in the
sample points could be incorporated through local variation
of µﬁt.
6
Conclusion
We have presented an algorithm for reconstructing watertight
surfaces, offering a uniﬁed approach for oriented and unori-
ented point sets. If no normal information is available, our
technique starts by estimating both normal direction and con-
ﬁdence in the estimate via a novel, noise-resilient Voronoi-
PCA procedure. The reconstruction then proceeds to ﬁnd an
implicit function that optimizes the alignment between its
gradient and the estimated/provided normal directions. Dis-
tinctive features of our approach include the use of tensors
in a generalized eigenvalue problem to balance ﬁtting and
smoothness based on data conﬁdence, as well as the ability
to add data ﬁtting and smoothness control.
Future work We wish to study the theoretical properties of
our normal estimation technique and its application to di-
mension detection (along the line of [DGGZ03]). Also, to
achieve what [KBH06] described as a desirable indicator
function, the L2 norm used in our paper (and in Poisson re-
construction) is obviously inappropriate, as sharp transitions
are overly penalized. An L1 norm would, instead, be best in
this case. However, it is not obvious that the computational
overhead that such a norm requires is worth the added ben-
eﬁts. Finally, our approach to turn a tensor into an oriented
vector ﬁeld (i.e., ∇f) might provide a new approach to appli-
c
⃝The Eurographics Association 2007.


--- Page 9 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
cations such as quad remeshing, as a (curvature) tensor ﬁeld
could be processed along the line of our technique to provide
curvature-aligned meshes without user interaction.
Acknowledgments The authors wish to thank David
Bommes from RWTH Aachen for his help with ARPACK.
Partial funding from the EU Network of Excellence
AIM@SHAPE (IST NoE No 506766), the EU FET-
Open project ACS (IST No 006413), NSF (CAREER
CCR-0133983, and ITR DMS-0453145), DOE (DE-FG02-
04ER25657), and Pixar.
References
[AB99]
AMENTA N., BERN M. W.: Surface reconstruction by
Voronoi ﬁltering. GEOMETRY: Discrete & Computational Ge-
ometry 22 (1999).
[BC02]
BOISSONNAT, CAZALS: Smooth surface reconstruction
via natural neighbour interpolation of distance functions. CGTA:
Computational Geometry: Theory and Applications 22 (2002).
[BO05]
BOISSONNAT J.-D., OUDOT S.: Provably good sampling
and meshing of surfaces. Graph. Models 67, 5 (2005), 405–451.
[Boi84]
BOISSONNAT J.-D.:
Geometric structures for three-
dimensional shape representation. ACM Trans. on Graphics 3, 4
(1984), 266–286.
[CBC∗01]
CARR J. C., BEATSON R. K., CHERRIE J. B.,
MITCHELL T. J., FRIGHT W. R., MCCALLUM B. C., EVANS
T. R.: Reconstruction and representation of 3D objects with radial
basis functions. In SIGGRAPH (2001), pp. 67–76.
[CG06]
CAZALS F., GIESEN J.: Delaunay triangulation based
surface reconstruction. In Effective Computational Geometry for
Curves and Surfaces, Boissonnat J., Teillaud M., (Eds.). Springer-
Verlag, Math. and Visualization, 2006, pp. 231–276.
[CGA03]
Computational Geometry Algorithms Library CGAL-
3.2. http://www.cgal.org/, 2003.
[CP03]
CAZALS F., POUGET M.: Estimating differential quanti-
ties using polynomial ﬁtting of osculating jets. In Symposium on
Geometry Processing (2003), pp. 177–187.
[Dey06]
DEY T. K.: Curve and Surface Reconstruction: Algo-
rithms with Mathematical Analysis. Cambridge Monographs on
Applied and Computational Mathematics, 2006.
[DGGZ03]
DEY T. K., GIESEN J., GOSWAMI S., ZHAO W.:
Shape dimension and approximation from samples. Discrete &
Computational Geometry 29, 3 (2003).
Figure 12: Bunny. (left) the eigenvector (in false colors) of the solu-
tion of the generalized eigenvalue problem for the Bunny dataset—
see also Fig. 4; (right) the hole at the bottom is ﬁlled.
[DKT06]
DESBRUN M., KANSO E., TONG Y.: Discrete differen-
tial forms for computational modeling. In Discrete Differential
Geometry. ACM SIGGRAPH Course Notes, 2006.
[DLS05]
DEY T. K., LI G., SUN J.: Normal estimation for point
clouds: A comparison study for a Voronoi based method. In
Symposium on Point-Based Graphics (2005), pp. 39–46.
[DS05]
DEY T. K., SUN J.: Normal and Feature Estimations from
Noisy Point Clouds. Tech. Rep. OSU-CISRC-7/50-TR50, Ohio
State University, 2005.
[GS]
GOMES F. M., SORENSEN D. C.: ARPACK++: A C++
implementation of ARPACK eigenvalue package.
[Gus02]
GUSKOV I.: An anisotropic parameterization scheme. In
Proc. of Int. Meshing Roundtable (2002), pp. 325–332.
[HDD∗92]
HOPPE H., DEROSE T., DUCHAMP T., MCDONALD
J., STUETZLE W.:
Surface reconstruction from unorganized
points. In Proc. of ACM SIGGRAPH (1992), pp. 71–78.
[HK06]
HORNUNG A., KOBBELT L.: Robust reconstruction of
watertight 3D models from non-uniformly sampled point clouds
without normal information. In Symposium on Geometry Process-
ing (2006), pp. 41–50.
[HXMP05]
HU G., XU J., MIAO L., PENG Q.: Bilateral estima-
tion of vertex normal for point-sampled models. In Int. Conf. on
Comp. Science and Appl. (2005), vol. 3480, pp. 758–768.
[KBH06]
KAZHDAN M., BOLITHO M., HOPPE H.: Poisson Sur-
face Reconstruction.
In Symposium on Geometry Processing
(2006), pp. 61–70.
[KSO04]
KOLLURI R., SHEWCHUK J. R., O’BRIEN J. F.: Spec-
tral surface reconstruction from noisy point clouds. In Symposium
on Geometry Processing (2004), pp. 11–21.
[LP05]
LANGE C., POLTHIER K.: Anisotropic smoothing of point
sets. Computer Aided Geometric Design 22, 7 (2005), 680–692.
[MM06]
MORDOHAI P., MEDIONI G.: Tensor Voting: A Percep-
tual Organization Approach to Computer Vision and Machine
Learning. Morgan & Claypool, 2006.
[MNG04]
MITRA N. J., NGUYEN A., GUIBAS L.: Estimating
surface normals in noisy point cloud data. In Int. J. of Comp.
Geometry and Applications (2004), vol. 14(4–5), pp. 261–276.
[OBA∗03]
OHTAKE Y., BELYAEV A., ALEXA M., TURK G.,
SEIDEL H.-P.: Multi-level partition of unity implicits. In Proc.
of ACM SIGGRAPH (2003), vol. 22(3), pp. 463–470.
[OF05]
OUYANG D., FENG H.-Y.: On the normal vector estima-
tion for point cloud data from smooth surfaces. Computer-Aided
Geometric Design 37, 10 (2005), 1071–1079.
[PKKG03]
PAULY M., KEISER R., KOBBELT L. P., GROSS M.:
Shape modeling with point-sampled geometry. In (SIGGRAPH)
(2003), vol. 22(3) of ACM Trans. on Graphics, pp. 641–650.
[PSQ06]
PARIS S., SILLION F. X., QUAN L.: A surface recon-
struction method using global graph cut optimization. Int. J. Com-
put. Vision 66, 2 (2006), 141–161.
[PWY∗06]
POTTMANN H., WALLNER J., YANG Y.-L., LAI Y.-
K., HU S.-M.: Principal curvatures from the integral invariant
viewpoint. Comput. Aided Geom. Design (2006). in print.
[RY06]
RINEAU L., YVINEC M.: A generic software design for
Delaunay reﬁnement meshing. Tech. Rep. 5983, INRIA, 2006.
c
⃝The Eurographics Association 2007.


--- Page 10 ---

P. Alliez et al. / Voronoi-based Variational Reconstruction of Unoriented Point Sets
[SM00]
SHI J., MALIK J.: Normalized cuts and image segmen-
tation. In Transactions on Pattern Analysis and Machine Intelli-
gence (2000), pp. 888–905.
[SW90]
SMITH W. H. F., WESSEL P.: Gridding with continuous
curvature splines in tension. Geophysics 55, 3 (1990), 293–305.
[TCR05]
TOLEDO S., CHEN D., ROTKIN V.: TAUCS. Available
at http://www.tau.ac.il/∼stoledo/taucs, 2005.
[TLHD03]
TONG Y., LOMBEYDA S., HIRANI A. N., DESBRUN
M.: Discrete multiscale vector ﬁeld decomposition. In ACM
SIGGRAPH (2003), pp. 445–452.
[WCS05]
WALDER C., CHAPELLE O., SCHÖLKOPF B.: Implicit
surface modelling as an eigenvalue problem. In Machine Learning
ICML 2005 (2005), pp. 936–939.
[ZRS05]
ZAYER R., R OSSL C., SEIDEL H.-P.: Setting the bound-
ary free: A composite approach to surface parameterization. In
Symposium on Geometry Processing (2005), pp. 91–100.
A
Computing Covariance Matrices
Consider a given polyhedron Ω(either a bounded Voronoi cell V
or the union of a set of such Voronoi cells). We can compute its
covariance matrix around an arbitrary point p (center of mass in
our application) by decomposing Ωinto tetrahedra Ti formed by
the triangles in a triangulation of the boundary surface and p, in a
fashion similar to the computation of volume. Note that some of
these tetrahedra may have negative volumes.
Without loss of generality, we can assume p to be the coor-
dinates origin (by taking the relative coordinates w.r.t. p). For
a single arbitrary tetrahedron T we start by considering the
linear transform N that maps the canonical tetrahedron
¯T =
((0,0,0)t,(1,0,0)t,(0,1,0)t,(0,0,1)t) to T = (p,a,b,c). Consider
the 3×3 matrix N deﬁned as:
N =
 a
b
c

The order-2 moment matrix of T with respect to its base point p is
deﬁned as det(N)NQNt, where Q is the order-2 moment matrix of
the canonical tetrahedron ¯T with respect to the origin:
Q =
ZZZ
¯T


x2
xy
xz
yx
y2
yz
zx
zy
z2

dxdydz =
1
120


2
1
1
1
2
1
1
1
2


The covariance matrix of Ωis then deﬁned as
cov(Ω) = ∑
i
det(Ni)Ni QNt
i .
Note that this expression is valid independently of whether Ωis
convex or not.
Computation Speedup To speedup the computation of covariance
matrices for unions of Voronoi cells U, we ﬁrst compute the covari-
ance matrix for each Voronoi cell Vi around its own center of mass.
Now consider shifting the centroid of Vi from origin to p; we have:
Z
Vi
(X + p)(X + p)tdV =
Z
Vi
(XXt +X pt + pXt + ppt)dV
= cov(Vi)+mippt,
where X means the relative coordinate w.r.t. the centroid of V, and
mi is the volume of Vi. The covariance matrix of the union becomes:
cov(U) = ∑
i

cov(Vi)+mipipt
i

,
where pi is the displacement from the centroid of U to the centroid
of Vi.
Figure 13: Comparison with Poisson reconstruction in 3D. Without
any normal information, our method (left) results in a similar shape
(albeit less oversmoothed) to a Poisson reconstruction (octree depth
11) for which oriented normals were provided.
c
⃝The Eurographics Association 2007.
```

---

## Section 1.42: VTracer _ Vision Cortex

Source File: VTracer _ Vision Cortex.txt

### Paper Content:

```
# VTracer _ Vision Cortex.pdf
# Converted: 2025-07-19 12:45:29
# Method: pymupdf
# Domain: pixel2physics
# Source: ../layer2_completion/VTracer _ Vision Cortex.pdf
# Output: ../layer2_completion/txt/VTracer _ Vision Cortex.txt


--- Page 1 ---

VTracer | Vision Cortex
about:srcdoc
1 of 14
18/07/2025, 17:36


--- Page 2 ---

VTracer
Researcher: Sanford Pun | Supervisor: Chris
Tsang | Published: 2020-11-01
VTracer is a utility to convert raster images
into vector graphics. Try the Demo Web App.
Graphic by sunshine-91 via Vecteezy
The input image is first clustered by
Hierarchical Clustering, and each of the
output clusters are traced into vector.
The algorithm of vector tracing involves 3
main stages:
1. Convert pixels into path
2. Simplify the path into polygon
3. Smoothen the polygon and approximate it
with a curve-fitter
Path Walking
Type to search...
Vision Cortex
SymCode
Impression
VTracer
Path Walking
Path Simplification
Path Smoothing
Curve Fitting
vs Potrace
vs Adobe Illustrator
VTracer | Vision Cortex
about:srcdoc
2 of 14
18/07/2025, 17:36


--- Page 3 ---

VTracer first obtains the raw paths of pixel
clusters. A walker is used to trace the outlines
of every cluster aer building an image tree.
The walker would combine consecutive steps
in the same direction.
2 results of path-walking. Le: Naïve,
Unoptimized walker. Right: Optimized
walker.
Raw pixelated polygon
Path Simplification
Path simplification consists of 2 steps:
1. Remove staircases
2. Simplify by limiting subpath penalties
Staircase Removal
VTracer | Vision Cortex
about:srcdoc
3 of 14
18/07/2025, 17:36


--- Page 4 ---

From the previous stage, we have obtained a
path whose consecutive edges must have
dierent directions, i.e. the shape is
represented by the minimum number of
edges with 100% fidelity. However, to
represent slant lines and curves in raster
graphics, “jaggies” (or pixel staircases)
inevitably occur. In this step, we aim at
removing these artifacts.
To replace a staircase with a straight line
(hence “removing” it), one may adopt an
outset (“additive”) or inset (“subtractive”)
approach. Both approaches are justifiable in
dierence contexts, so the key is to maintain
consistency in the same shape. In order to
determine which points to keep, we make use
of the signed area.
Original
Outset
Inset
The signed area of a right triangle is a metric
used to determine whether the vertices are
arranged clockwise or anti-clockwise
geometrically. With this information, we can
determine which points to keep on the
staircase.
For each point on the staircase, we calculate
the signed area of the triangle formed by the
previous point, the current point, and the next
point on the path. The decision of whether the
VTracer | Vision Cortex
about:srcdoc
4 of 14
18/07/2025, 17:36


--- Page 5 ---

current point should be kept is then made by
comparing the sign of the signed area and the
clockwise-ness of the original path.
A
B
C
Signed area at B' < 0
When the path is speciﬁed in clockwise order
A'
B'
C'
Signed area at B > 0
Two examples of signed area. If
outset(inset) is chosen, B(Bʼ) is kept and
Bʼ(B) is discarded.
Simplify by Limiting penalties
The path can be further simplified by
evaluating the penalty from replacing a
subpath with one long edge from the first to
the last point.
Le: Path with wobbles. Right: Path
approximated by red line, removing
wobbles
Given a subpath, we would like to determine if
a line drawn from the first point to the last can
approximate the whole subpath with high
VTracer | Vision Cortex
about:srcdoc
5 of 14
18/07/2025, 17:36


--- Page 6 ---

approximate the whole subpath with high
fidelity. The idea is to make sure that all
points in the subpath are close enough to the
approximating line. To avoid all the
complicated coordinate geometry, we can
simply evaluate the areas of triangles formed
by the first point, the last point, and each in-
between point.
Let ΔABC be one such triangle, with A and C
being the first and last points of the subpath
respectively, and B being any in-between
point. Let h and b be the height and the base
(length of AC) respectively. VTracer models
the penalty of ΔABC as 
, as
the area of ΔABC and b can be obtained by
simple geometry. It is crucial for the penalty
to be directly proportional to h and b.
Once the penalty is clearly defined, the
procedure of simplification is straightforward.
VTracer greedily extends a subpath until the
maximum penalty along it exceeds a specific
tolerance, then all edges in the subpath are
replaced by one line from the first to the
second last point (or equivalently, remove in-
between points from the path). Aer the
replacement, the same process is performed
starting from the next point in the path. When
the last subpath is extended to the last point
in the whole path, the simplification stage
concludes.
VTracer | Vision Cortex
about:srcdoc
6 of 14
18/07/2025, 17:36


--- Page 7 ---

Red and yellow triangles are small penalties
Extending the subpath by 1
Green is a large penalty
P
=> Blue line can approximate this subpath
=> Blue line is a poor approximation
=> Remove points up to point P
Path Smoothing
Sample Simplified 'S' Shape
What we have now is a simplified polygon
with high fidelity. However, if we feed the path
to the curve fitter as is, the curves will
approximate the shape poorly. This is because
the curves are underdetermined given the
small number of points. In order to generate
points that lie on our desired shape,
subdivision smoothing is performed.
VTracer | Vision Cortex
about:srcdoc
7 of 14
18/07/2025, 17:36


--- Page 8 ---

4-Point Scheme
VTracer adapts from the 4-Point Scheme
subdivision algorithm, which is an
interpolating smoothing method. The
problem of the 4-Point Scheme is that all
vertices would be smoothed into round
corners, which, from our point of view, is a
loss of fidelity.
P
1
8
Mid pt.
:
A1
A2
B1
B2
4-Point Scheme performed on segment
A₁A₂
P
A2
A1
B1
B2
If A2 is a corner, the 4-Point Scheme
smooths it out iteratively.
Finding corners
VTracer | Vision Cortex
about:srcdoc
8 of 14
18/07/2025, 17:36


--- Page 9 ---

To preserve corners, we first have to locate
them. VTracer finds corners by checking the
angle dierence at each vertex. If the absolute
angle dierence exceeds a certain threshold,
the vertex is considered to be a corner.
Edges as Vectors (Path speciﬁed in clockwise direction)
A
C
B
a
b
Angle dierence from A to B is small => a is not a corner
Angle dierence from B to C is large => b is a corner
Corner-Preserving 4-Point Scheme
In the original 4-Point Scheme, 2 adjacent
points are always used to generate the new
point for each segment. In our adapted
version, we do not take the adjacent point for
corners, but instead we take the corners
themselves. For segments whose points are
both corners, we simply ignore them.
P
"3-Point Scheme"
A1
B1
A2=B2
B2
Since A₂ is a corner, the smoothing procedure
does not take the adjacent point as B₂. As a
result, the corner will be (approximately)
preserved aer smoothing, even aer
VTracer | Vision Cortex
about:srcdoc
9 of 14
18/07/2025, 17:36


--- Page 10 ---

iterations.
VTracer applies a threshold on the length of
each segment during subdivision smoothing,
so that the result will not be over-dense. This
threshold should be decided carefully (mainly
based on the resolution of image), otherwise
the resulting path will be a poor
approximation.
Shown below are examples smoothed with no
iteration:
Length Thresholds 3.5/5.0/7.5
Introducing iterations, you can see more
points are generated by subdivision:
Iterations 0/1/2
VTracer's implementation defaults to 10
it
ti
d
it
l
h
f
th
VTracer | Vision Cortex
about:srcdoc
10 of 14
18/07/2025, 17:36


--- Page 11 ---

iterations, and exit early when no further
smoothing can be done on the path.
Curve Fitting
The path is now populated with nicely
positioned and suiciently dense points that
faithfully represent our desired shape. Before
feeding it to a (Bezier) curve-fitter, VTracer
determines where to cut curves (splice
points).
Finding Splice Points
To define a splice point, we make use of the
signed angle dierences. Interpreting each
edge along the path as vectors, we can define
the signed angle dierence from edge eᵢ to
eᵢ₊₁ as the 2D angle of rotation θ ∊ (-π, π]
required to rotate eᵢ to the same direction as
eᵢ₊₁ (assume positive θ in clockwise
direction).
It is suicient for a vertex to be a splice point if
it passes one of two tests: point of inflection
testand angle displacement test.
Points of inflection can be found by tracking
the signs of angle dierences along the path.
When the concavity of the path changes at a
certain point, the sign of the signed angle
dierence also changes at that point.
θ>0
A
VTracer | Vision Cortex
about:srcdoc
11 of 14
18/07/2025, 17:36


--- Page 12 ---

θ>0
θ<0
P
θ<0
B
As the sign of angle dierence changes at point P, P is a point of
inflection and hence a splice point.
Therefore, we cut (A..B) into (A..P) and (P..B).
Angle displacement at a point is defined as
the signed angle dierences accumulated
from the previous splice point (exclusive) to
the current point (inclusive). If the absolute
value of angle displacement at a certain point
exceeds a specific threshold, that point is a
splice point. The smaller the threshold, the
more curves are cut, and the resulting spline
is more accurate.
Last Splice Point
disp = 0
disp = 40°
disp = 50°
disp = 72°
disp = 85°
disp = 100°
disp = 130°
disp = 140°
disp = 180°
Angle Displacements shown along a path.
Once all splice points are found, VTracer feeds
all subpaths between every consecutive pair
of splice points into the curve-fitter.
If the smoothed path from the previous step is
fed into the curve-fitter, we get a spline like
the following:
VTracer | Vision Cortex
about:srcdoc
12 of 14
18/07/2025, 17:36


--- Page 13 ---

Final output
vs Potrace
Fidelity
Potrace is a popular bitmap tracing tool that
also transforms bitmaps into vector graphics.
Being able to produce high-quality output for
low-resolution images, Potrace traces images
by finding global optimal under certain
assumptions. VTracer favours fidelity over
simplification. Potrace and VTracer produce
dierent results especially on small objects.
Illustration of how assumptions aect tracing.
Le: Original input.
Middle: Possible shape interpretation with the assumption that
ambiguous “corners” in the original input are sharp corners.
Right: Possible shape interpretation with no assumptions:
“corners” in the original image are represented by curves or
round corners.
Eiciency
VTracer | Vision Cortex
about:srcdoc
13 of 14
18/07/2025, 17:36


--- Page 14 ---

Potrace finds the global optimal way of
tracing a shape, meaning it approximates
parts using information from the entire shape,
which leads to performance issues in high-
resolution images. On the other hand, VTracer
runs a linear algorithm on each cluster and
has lower CPU and memory usage. In fact, we
regularly use VTracer to process high
resolution scans at 10 megapixels.
vs Adobe Illustrator
Comparing to Adobe Illustrator's Live Trace,
VTracer's output is much more compact
thanks to a stacking strategy. VTracer's
preprocessing stage utilizes Vision Cortex's
image tree algorithm and never produces
shapes with holes.
VTracer | Vision Cortex
about:srcdoc
14 of 14
18/07/2025, 17:36
```

---

## Section 1.43: layer2_completion_compendium

Source File: layer2_completion_compendium.txt

### Paper Content:

```
```

---

# COMPENDIUM STATISTICS

Target Directory: ./layer2_completion/
Compendium Name: layer2_completion_compendium.txt
Total Chapters: 0
Total Papers: 43
Total Size: 2.5M
Generated: Sat 19 Jul 18:23:00 NZST 2025

Ready for layer2_completion research analysis!
